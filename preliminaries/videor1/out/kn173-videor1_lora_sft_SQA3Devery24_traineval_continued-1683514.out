/project/aip-wangcs/indrisch/LLaMA-Factory /project/aip-wangcs/indrisch/LLaMA-Factory/preliminaries/videor1
[INFO|2025-12-28 19:18:41] llamafactory.launcher:143 >> Initializing 4 distributed tasks at: 127.0.0.1:41755
W1228 19:18:43.222000 3615501 torch/distributed/run.py:803] 
W1228 19:18:43.222000 3615501 torch/distributed/run.py:803] *****************************************
W1228 19:18:43.222000 3615501 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1228 19:18:43.222000 3615501 torch/distributed/run.py:803] *****************************************
[WARNING|2025-12-28 19:18:49] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode

⚙️  Running in WANDB offline mode
[2025-12-28 19:18:52,479] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-28 19:18:52,479] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-28 19:18:52,479] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-28 19:18:52,483] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2025-12-28 19:18:57,661] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-12-28 19:18:57,661] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-12-28 19:18:57,661] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[W1228 19:18:57.396965875 ProcessGroupNCCL.cpp:924] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W1228 19:18:57.396967226 ProcessGroupNCCL.cpp:924] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[2025-12-28 19:18:57,666] [INFO] [comm.py:658:init_distributed] cdb=None
[W1228 19:18:57.399185687 ProcessGroupNCCL.cpp:924] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[2025-12-28 19:18:57,681] [INFO] [comm.py:658:init_distributed] cdb=None
[W1228 19:18:57.413803946 ProcessGroupNCCL.cpp:924] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[INFO|2025-12-28 19:18:58] llamafactory.hparams.parser:455 >> Process rank: 3, world size: 4, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-12-28 19:18:58] llamafactory.hparams.parser:455 >> Process rank: 1, world size: 4, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-12-28 19:18:58] llamafactory.hparams.parser:455 >> Process rank: 2, world size: 4, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-12-28 19:18:58] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.
[INFO|2025-12-28 19:18:58] llamafactory.hparams.parser:455 >> Process rank: 0, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|hub.py:421] 2025-12-28 19:18:58,528 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2025-12-28 19:18:58,551 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2025-12-28 19:18:58,557 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Video-R1-7B/snapshots/e6840e4cdc86b484b05aacdcb5c88ad3d7ef3d76/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-12-28 19:18:58,558 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Video-R1-7B/snapshots/e6840e4cdc86b484b05aacdcb5c88ad3d7ef3d76/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-12-28 19:18:58,558 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Video-R1-7B/snapshots/e6840e4cdc86b484b05aacdcb5c88ad3d7ef3d76/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-12-28 19:18:58,558 >> loading file added_tokens.json from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Video-R1-7B/snapshots/e6840e4cdc86b484b05aacdcb5c88ad3d7ef3d76/added_tokens.json
[INFO|tokenization_utils_base.py:2095] 2025-12-28 19:18:58,558 >> loading file special_tokens_map.json from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Video-R1-7B/snapshots/e6840e4cdc86b484b05aacdcb5c88ad3d7ef3d76/special_tokens_map.json
[INFO|tokenization_utils_base.py:2095] 2025-12-28 19:18:58,558 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Video-R1-7B/snapshots/e6840e4cdc86b484b05aacdcb5c88ad3d7ef3d76/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-12-28 19:18:58,558 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-12-28 19:18:58,927 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|hub.py:421] 2025-12-28 19:18:58,927 >> Offline mode: forcing local_files_only=True
[INFO|hub.py:421] 2025-12-28 19:18:58,928 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:316] 2025-12-28 19:18:58,930 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2025-12-28 19:18:58,932 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Video-R1-7B/snapshots/e6840e4cdc86b484b05aacdcb5c88ad3d7ef3d76/preprocessor_config.json
[INFO|image_processing_base.py:316] 2025-12-28 19:18:58,937 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2025-12-28 19:18:58,939 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Video-R1-7B/snapshots/e6840e4cdc86b484b05aacdcb5c88ad3d7ef3d76/preprocessor_config.json
[INFO|image_processing_base.py:428] 2025-12-28 19:18:58,952 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 401408,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 401408,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:1949] 2025-12-28 19:18:58,953 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2025-12-28 19:18:58,956 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Video-R1-7B/snapshots/e6840e4cdc86b484b05aacdcb5c88ad3d7ef3d76/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-12-28 19:18:58,956 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Video-R1-7B/snapshots/e6840e4cdc86b484b05aacdcb5c88ad3d7ef3d76/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-12-28 19:18:58,956 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Video-R1-7B/snapshots/e6840e4cdc86b484b05aacdcb5c88ad3d7ef3d76/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-12-28 19:18:58,956 >> loading file added_tokens.json from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Video-R1-7B/snapshots/e6840e4cdc86b484b05aacdcb5c88ad3d7ef3d76/added_tokens.json
[INFO|tokenization_utils_base.py:2095] 2025-12-28 19:18:58,956 >> loading file special_tokens_map.json from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Video-R1-7B/snapshots/e6840e4cdc86b484b05aacdcb5c88ad3d7ef3d76/special_tokens_map.json
[INFO|tokenization_utils_base.py:2095] 2025-12-28 19:18:58,956 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Video-R1-7B/snapshots/e6840e4cdc86b484b05aacdcb5c88ad3d7ef3d76/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-12-28 19:18:58,956 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-12-28 19:18:59,251 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:660] 2025-12-28 19:18:59,253 >> Offline mode: forcing local_files_only=True
[INFO|video_processing_utils.py:726] 2025-12-28 19:18:59,256 >> loading configuration file video_preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Video-R1-7B/snapshots/e6840e4cdc86b484b05aacdcb5c88ad3d7ef3d76/preprocessor_config.json
[INFO|video_processing_utils.py:770] 2025-12-28 19:18:59,263 >> Video processor Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 401408,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 401408,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:928] 2025-12-28 19:18:59,263 >> Offline mode: forcing local_files_only=True
[INFO|processing_utils.py:1116] 2025-12-28 19:18:59,268 >> loading configuration file processor_config.json from cache at None
[INFO|processing_utils.py:1199] 2025-12-28 19:18:59,634 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 401408,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 401408,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Video-R1/Video-R1-7B', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 401408,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 401408,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2_5_VLProcessor"
}

[INFO|2025-12-28 19:18:59] llamafactory.data.loader:143 >> Loading dataset /scratch/indrisch/huggingface/hub/datasets--cvis-tmu--llamafactory-sqa3d-traces-multiimage-vqa/snapshots/ce5c54adc1608d1726730c9ff334e65b6dd70e46/data/...
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once
[rank0]:[W1228 19:19:00.784667347 ProcessGroupNCCL.cpp:5072] Guessing device ID based on global rank. This can cause a hang if rank to GPU mapping is heterogeneous. You can specify device_id in init_process_group()
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
Running tokenizer on dataset (num_proc=32):   0%|          | 0/33047 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=32):   3%|▎         | 1000/33047 [14:39<7:49:38,  1.14 examples/s]Running tokenizer on dataset (num_proc=32):   6%|▌         | 2000/33047 [14:51<3:10:58,  2.71 examples/s]Running tokenizer on dataset (num_proc=32):   6%|▌         | 2032/33047 [15:03<3:11:07,  2.70 examples/s]Running tokenizer on dataset (num_proc=32):   6%|▌         | 2065/33047 [15:21<3:14:22,  2.66 examples/s]Running tokenizer on dataset (num_proc=32):   9%|▉         | 3065/33047 [16:12<1:27:42,  5.70 examples/s]Running tokenizer on dataset (num_proc=32):  12%|█▏        | 4065/33047 [16:19<46:35, 10.37 examples/s]  Running tokenizer on dataset (num_proc=32):  12%|█▏        | 4065/33047 [16:38<46:35, 10.37 examples/s]Running tokenizer on dataset (num_proc=32):  12%|█▏        | 4098/33047 [16:40<52:23,  9.21 examples/s]Running tokenizer on dataset (num_proc=32):  13%|█▎        | 4131/33047 [16:42<51:16,  9.40 examples/s]Running tokenizer on dataset (num_proc=32):  16%|█▌        | 5131/33047 [16:44<22:13, 20.94 examples/s]Running tokenizer on dataset (num_proc=32):  16%|█▌        | 5131/33047 [16:58<22:13, 20.94 examples/s]Running tokenizer on dataset (num_proc=32):  19%|█▊        | 6131/33047 [17:00<15:07, 29.65 examples/s]Running tokenizer on dataset (num_proc=32):  22%|██▏       | 7131/33047 [17:11<10:43, 40.27 examples/s]Running tokenizer on dataset (num_proc=32):  22%|██▏       | 7164/33047 [17:17<11:44, 36.74 examples/s]Running tokenizer on dataset (num_proc=32):  22%|██▏       | 7196/33047 [17:22<13:13, 32.58 examples/s]Running tokenizer on dataset (num_proc=32):  25%|██▍       | 8196/33047 [17:30<07:51, 52.76 examples/s]Running tokenizer on dataset (num_proc=32):  25%|██▍       | 8228/33047 [17:39<10:12, 40.51 examples/s]Running tokenizer on dataset (num_proc=32):  25%|██▍       | 8228/33047 [17:58<10:12, 40.51 examples/s]Running tokenizer on dataset (num_proc=32):  25%|██▍       | 8261/33047 [17:59<18:21, 22.50 examples/s]Running tokenizer on dataset (num_proc=32):  28%|██▊       | 9261/33047 [18:05<08:33, 46.37 examples/s]Running tokenizer on dataset (num_proc=32):  28%|██▊       | 9261/33047 [18:18<08:33, 46.37 examples/s]Running tokenizer on dataset (num_proc=32):  31%|███       | 10261/33047 [18:27<08:13, 46.13 examples/s]Running tokenizer on dataset (num_proc=32):  31%|███       | 10294/33047 [18:32<09:16, 40.92 examples/s]Running tokenizer on dataset (num_proc=32):  37%|███▋      | 12294/33047 [18:33<03:06, 111.50 examples/s]Running tokenizer on dataset (num_proc=32):  40%|████      | 13294/33047 [18:34<02:08, 153.49 examples/s]Running tokenizer on dataset (num_proc=32):  43%|████▎     | 14294/33047 [18:36<01:37, 192.39 examples/s]Running tokenizer on dataset (num_proc=32):  46%|████▋     | 15294/33047 [18:40<01:25, 208.47 examples/s]Running tokenizer on dataset (num_proc=32):  49%|████▉     | 16294/33047 [18:42<01:04, 259.36 examples/s]Running tokenizer on dataset (num_proc=32):  52%|█████▏    | 17294/33047 [18:46<01:03, 248.68 examples/s]Running tokenizer on dataset (num_proc=32):  55%|█████▌    | 18294/33047 [18:46<00:42, 343.57 examples/s]Running tokenizer on dataset (num_proc=32):  58%|█████▊    | 19326/33047 [18:53<00:53, 256.13 examples/s]Running tokenizer on dataset (num_proc=32):  62%|██████▏   | 20392/33047 [18:56<00:47, 264.98 examples/s]Running tokenizer on dataset (num_proc=32):  65%|██████▍   | 21425/33047 [18:58<00:35, 324.63 examples/s]Running tokenizer on dataset (num_proc=32):  71%|███████   | 23425/33047 [18:58<00:16, 587.37 examples/s]Running tokenizer on dataset (num_proc=32):  74%|███████▍  | 24425/33047 [19:02<00:18, 470.56 examples/s]Running tokenizer on dataset (num_proc=32):  77%|███████▋  | 25425/33047 [19:04<00:17, 434.23 examples/s]Running tokenizer on dataset (num_proc=32):  80%|████████  | 26458/33047 [19:06<00:13, 500.36 examples/s]Running tokenizer on dataset (num_proc=32):  83%|████████▎ | 27524/33047 [19:09<00:13, 417.00 examples/s]Running tokenizer on dataset (num_proc=32):  86%|████████▋ | 28556/33047 [19:09<00:07, 572.71 examples/s]Running tokenizer on dataset (num_proc=32):  89%|████████▉ | 29556/33047 [19:10<00:04, 736.12 examples/s]Running tokenizer on dataset (num_proc=32):  93%|█████████▎| 30589/33047 [19:11<00:03, 812.87 examples/s]Running tokenizer on dataset (num_proc=32):  96%|█████████▌| 31655/33047 [19:20<00:04, 294.66 examples/s]Running tokenizer on dataset (num_proc=32):  99%|█████████▉| 32786/33047 [19:23<00:00, 295.43 examples/s]Running tokenizer on dataset (num_proc=32): 100%|█████████▉| 32950/33047 [19:31<00:00, 164.91 examples/s]Running tokenizer on dataset (num_proc=32): 100%|██████████| 33047/33047 [19:44<00:00, 27.90 examples/s] 
training example:
input_ids:
[151644, 8948, 271, 262, 1446, 525, 264, 16585, 11129, 4142, 11528, 17847, 369, 27979, 647, 47522, 389, 264, 3175, 29519, 6109, 6839, 3941, 5248, 5335, 382, 262, 26149, 510, 262, 68252, 1283, 419, 1943, 11, 498, 686, 5258, 451, 6109, 5335, 438, 366, 1805, 29, 9492, 13, 18877, 1105, 438, 2155, 6194, 315, 279, 83490, 6109, 13, 28634, 311, 5335, 553, 220, 16, 5980, 1922, 304, 1973, 315, 11094, 25, 508, 16, 1125, 508, 17, 1125, 4593, 11, 508, 45, 29562, 262, 3596, 35304, 1718, 8575, 38439, 3298, 14017, 510, 262, 7288, 7854, 264, 12966, 220, 18, 35, 10502, 1614, 3941, 6194, 25, 3754, 6171, 3941, 5335, 11, 23583, 8674, 6249, 17040, 11, 323, 990, 5452, 14, 509, 8957, 14688, 21060, 369, 7990, 55916, 624, 262, 7288, 72077, 409, 849, 292, 3793, 26087, 983, 847, 1290, 97089, 17996, 4065, 14, 29898, 484, 32511, 56111, 7612, 1825, 35978, 28455, 448, 5091, 311, 279, 64171, 14, 8092, 13, 1416, 279, 58385, 374, 54761, 11, 1638, 311, 279, 1429, 38219, 8622, 1651, 323, 1977, 773, 624, 262, 7288, 5443, 1172, 9434, 5904, 26, 653, 537, 17023, 63133, 3565, 476, 17188, 389, 9250, 6540, 7797, 6770, 1633, 43898, 3793, 624, 262, 7288, 3197, 5248, 11178, 3000, 11, 8830, 553, 279, 1850, 2432, 311, 27979, 17133, 488, 4274, 1835, 3941, 6194, 13, 1416, 2058, 54761, 1283, 13295, 678, 5335, 11, 1584, 429, 9355, 382, 262, 30990, 320, 327, 32739, 1378, 10010, 11, 304, 419, 1973, 982, 262, 366, 26865, 397, 262, 14822, 14319, 29208, 32711, 304, 220, 18, 4142, 16, 15, 2805, 7354, 13, 356, 632, 5904, 448, 2168, 14937, 320, 68, 1302, 2572, 65750, 18, 5669, 3100, 7579, 18010, 1290, 315, 8718, 26, 508, 16, 5669, 1852, 1894, 11, 42396, 64212, 2823, 63594, 714, 4583, 320, 2640, 37294, 17, 20, 15, 11211, 7241, 5871, 568, 7036, 894, 17796, 8957, 14, 2969, 26745, 487, 323, 1246, 498, 19673, 432, 624, 262, 690, 26865, 397, 262, 366, 9217, 397, 262, 362, 3175, 63594, 4226, 1172, 11, 892, 1231, 387, 510, 262, 7288, 1416, 14016, 25, 3776, 37328, 4226, 11, 4504, 320, 68, 1302, 2572, 1036, 17, 32511, 476, 7414, 14, 2753, 320, 68, 1302, 2572, 1036, 9693, 854, 4292, 262, 7288, 1416, 537, 14016, 25, 362, 11652, 476, 1378, 624, 262, 1416, 4226, 537, 9434, 11, 421, 1052, 374, 902, 2797, 5904, 11, 476, 421, 279, 4226, 374, 35197, 54761, 25, 1036, 33260, 8253, 854, 624, 262, 690, 9217, 1339, 262, 62947, 609, 43797, 50, 510, 262, 7288, 3155, 4183, 13153, 279, 3405, 476, 1140, 5335, 26, 653, 4183, 2550, 4718, 476, 4960, 14158, 624, 262, 7288, 84468, 4285, 1894, 5036, 448, 518, 1429, 825, 22739, 26087, 4145, 3446, 838, 4322, 1574, 58689, 7256, 854, 4292, 262, 7288, 2823, 12966, 3941, 6194, 26, 421, 6194, 28295, 11, 62552, 279, 11299, 15432, 5904, 323, 5185, 432, 304, 366, 26865, 29816, 257, 151645, 198, 151644, 872, 198, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 30, 220, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151645, 198, 151644, 77091, 198, 198, 32462, 432, 11, 1077, 594, 1430, 311, 7071, 700, 1128, 594, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 62410, 13, 5512, 11, 1077, 594, 50087, 279, 14852, 6505, 13, 576, 21921, 374, 4363, 264, 4158, 11, 1378, 1737, 95112, 4982, 13, 2411, 220, 16, 16, 297, 62410, 320, 3481, 7950, 3082, 701, 582, 1184, 311, 1779, 429, 7702, 13, 21131, 518, 279, 5335, 11, 5310, 279, 825, 9027, 279, 21921, 3082, 11, 1052, 594, 264, 17846, 3403, 279, 19309, 11, 714, 3783, 11, 902, 2293, 11489, 11, 279, 3405, 374, 911, 279, 21921, 13, 13824, 11, 7196, 279, 330, 66, 75512, 1, 1588, 374, 279, 5819, 4982, 13, 13824, 11, 902, 11, 279, 19309, 702, 264, 21921, 3685, 30, 13824, 11, 902, 11, 279, 1887, 21921, 374, 279, 4158, 61607, 13, 13824, 11, 304, 279, 2168, 1380, 279, 6249, 4933, 279, 19309, 323, 21921, 11, 279, 1909, 315, 279, 21921, 2293, 11489, 11, 902, 11, 7196, 279, 17846, 30, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 2727, 330, 65, 77832, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 3263, 2055, 220, 16, 16, 297, 62410, 1035, 387, 279, 8416, 2115, 3082, 8674, 311, 279, 19309, 13, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 279, 19309, 3082, 13, 2160, 429, 949, 315, 279, 21921, 6505, 30, 13824, 11, 902, 11, 7196, 279, 21921, 374, 279, 4158, 61607, 1212, 279, 19309, 13, 13824, 11, 902, 11, 279, 1909, 315, 429, 21921, 2293, 11489, 11, 902, 11, 304, 1045, 5335, 11, 1052, 594, 264, 17846, 25911, 476, 21810, 13, 13824, 11, 902, 11, 1077, 594, 1779, 279, 1156, 2421, 5335, 13, 576, 17196, 45766, 374, 33866, 11, 1221, 279, 19309, 3082, 13, 13824, 11, 279, 4396, 4226, 25, 3330, 518, 279, 5335, 11, 279, 1633, 389, 1909, 315, 279, 14852, 21921, 320, 1782, 4158, 26482, 4982, 8, 518, 220, 16, 16, 297, 62410, 374, 264, 17846, 13, 13824, 11, 902, 11, 3783, 21677, 17846, 374, 389, 279, 7002, 3403, 279, 19309, 13, 13824, 11, 7196, 358, 5786, 878, 13, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 594, 3405, 25, 330, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 7521, 2055, 1156, 11, 10542, 279, 21921, 13, 576, 4158, 26482, 4982, 3143, 220, 16, 16, 297, 62410, 320, 3481, 7950, 568, 3555, 594, 389, 1909, 30, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 429, 3082, 13, 13824, 11, 9834, 0, 758, 279, 21844, 476, 21810, 11, 1052, 594, 264, 17846, 13, 2055, 279, 4226, 374, 264, 17846, 4192, 1249, 8253, 1128, 374, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 527, 20666, 11, 23643, 279, 27979, 6789, 323, 5424, 304, 279, 5335, 25, 18611, 16, 13, 95210, 279, 1036, 65, 77832, 21921, 854, 304, 3405, 25, 362, 4158, 11, 1378, 481, 26482, 5819, 4982, 374, 9434, 3143, 279, 1909, 481, 2115, 320, 16, 16, 297, 527, 20666, 8, 3082, 8674, 311, 1008, 37664, 1075, 279, 19309, 323, 17196, 13, 2303, 17, 13, 1374, 19991, 6171, 3403, 419, 21921, 25, 758, 279, 5335, 11, 264, 3070, 73225, 334, 374, 21810, 389, 279, 7002, 3403, 419, 21921, 3082, 13, 79226, 54216, 151645, 198]
inputs:
<|im_start|>system

    You are a careful vision–language assistant for spatial VQA on a single indoor scene shown across multiple images.

    INPUT:
    Immediately after this message, you will receive N scene images as <image> tags. Treat them as different views of the SAME scene. Refer to images by 1-based index in order of appearance: [1], [2], …, [N].

    REASONING PRINCIPLES:
    • Build a consistent 3D mental model across views: track objects across images, infer relative camera pose, and use scale/occlusion/shadows for depth cues.
    • Interpret deictic terms (“to my right/left/in front/behind”) EGOCENTRICALLY with respect to the narrator/agent. If the viewpoint is ambiguous, default to the most informative central view and say so.
    • Use only visible evidence; do not invent unseen details or rely on external knowledge beyond basic object/color terms.
    • When multiple candidates exist, resolve by the best match to spatial phrase + salience across views. If still ambiguous after checking all images, state that clearly.

    OUTPUT (exactly two blocks, in this order):
    <think>
    Step-by-step reasoning in 3–10 short steps. Cite evidence with image indices (e.g., “[3]: light wood desk right of monitor; [1]: same color, confirms”). Be concise but complete (aim ≤250 tokens unless necessary). Note any occlusion/ambiguity and how you resolved it.
    </think>
    <answer>
    A single concise answer only, which may be:
    • If sufficient: One-word answer, Count (e.g., “2”) or Yes/No (e.g., “yes”).
    • If not sufficient: A sentence or two.
    If answer not visible, if there is no clear evidence, or if the answer is genuinely ambiguous: “cannot determine”.
    </answer>

    STYLE & RULES:
    • Do NOT repeat the question or list images; do NOT output JSON or extra sections.
    • Prefer simple color names with at most one modifier (“light/dark/pale/beige”).
    • Be consistent across views; if views disagree, prioritize the clearest evidence and note it in <think>.
    <|im_end|>
<|im_start|>user
What is on top of the bathroom cabinet that is on my 11 o'clock? <|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|im_end|>
<|im_start|>assistant

Got it, let's try to figure out what's on top of the bathroom cabinet at 11 o'clock. First, let's visualize the bathroom setup. The cabinet is likely a white, two-drawer unit. At 11 o'clock (top-left area), we need to check that spot. Looking at the images, especially the one showing the cabinet area, there's a mirror above the sink, but wait, no—wait, the question is about the cabinet. Wait, maybe the "cabinet" here is the storage unit. Wait, no, the sink has a cabinet below? Wait, no, the main cabinet is the white drawers. Wait, in the image where the camera shows the sink and cabinet, the top of the cabinet—wait, no, maybe the mirror? Wait, no, let's check again. Wait, the user says "bathroom cabinet that is on my 11 o'clock". So 11 o'clock would be the upper left area relative to the sink. Looking at the images, there's a mirror mounted on the wall above the sink area. Is that part of the cabinet setup? Wait, no, maybe the cabinet is the white drawers under the sink. Wait, no, the top of that cabinet—wait, no, in some images, there's a mirror reflected or mounted. Wait, no, let's check the first few images. The shower curtain is colorful, then the sink area. Wait, the correct answer: looking at the images, the object on top of the bathroom cabinet (the white drawer unit) at 11 o'clock is a mirror. Wait, no, wait—the mirror is on the wall above the sink. Wait, maybe I misread. Wait, no, let's check again. Wait, the user's question: "What is on top of the bathroom cabinet that is on my 11 o'clock?" So first, identify the cabinet. The white drawer unit near 11 o'clock (top-left). What's on top? Looking at the images, there's a mirror mounted on the wall above that area. Wait, yes! In the reflection or mounted, there's a mirror. So the answer is a mirror.


To determine what is on top of the bathroom cabinet at 11 o’clock, analyze the spatial layout and elements in the images:  

1. Locate the “bathroom cabinet” in question: A white, two - drawer storage unit is visible near the top - left (11 o’clock) area relative to other fixtures like the sink and shower.  
2. Examine objects above this cabinet: In the images, a **mirror** is mounted on the wall above this cabinet area.  


Mirror<|im_end|>

label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 198, 32462, 432, 11, 1077, 594, 1430, 311, 7071, 700, 1128, 594, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 62410, 13, 5512, 11, 1077, 594, 50087, 279, 14852, 6505, 13, 576, 21921, 374, 4363, 264, 4158, 11, 1378, 1737, 95112, 4982, 13, 2411, 220, 16, 16, 297, 62410, 320, 3481, 7950, 3082, 701, 582, 1184, 311, 1779, 429, 7702, 13, 21131, 518, 279, 5335, 11, 5310, 279, 825, 9027, 279, 21921, 3082, 11, 1052, 594, 264, 17846, 3403, 279, 19309, 11, 714, 3783, 11, 902, 2293, 11489, 11, 279, 3405, 374, 911, 279, 21921, 13, 13824, 11, 7196, 279, 330, 66, 75512, 1, 1588, 374, 279, 5819, 4982, 13, 13824, 11, 902, 11, 279, 19309, 702, 264, 21921, 3685, 30, 13824, 11, 902, 11, 279, 1887, 21921, 374, 279, 4158, 61607, 13, 13824, 11, 304, 279, 2168, 1380, 279, 6249, 4933, 279, 19309, 323, 21921, 11, 279, 1909, 315, 279, 21921, 2293, 11489, 11, 902, 11, 7196, 279, 17846, 30, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 2727, 330, 65, 77832, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 3263, 2055, 220, 16, 16, 297, 62410, 1035, 387, 279, 8416, 2115, 3082, 8674, 311, 279, 19309, 13, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 279, 19309, 3082, 13, 2160, 429, 949, 315, 279, 21921, 6505, 30, 13824, 11, 902, 11, 7196, 279, 21921, 374, 279, 4158, 61607, 1212, 279, 19309, 13, 13824, 11, 902, 11, 279, 1909, 315, 429, 21921, 2293, 11489, 11, 902, 11, 304, 1045, 5335, 11, 1052, 594, 264, 17846, 25911, 476, 21810, 13, 13824, 11, 902, 11, 1077, 594, 1779, 279, 1156, 2421, 5335, 13, 576, 17196, 45766, 374, 33866, 11, 1221, 279, 19309, 3082, 13, 13824, 11, 279, 4396, 4226, 25, 3330, 518, 279, 5335, 11, 279, 1633, 389, 1909, 315, 279, 14852, 21921, 320, 1782, 4158, 26482, 4982, 8, 518, 220, 16, 16, 297, 62410, 374, 264, 17846, 13, 13824, 11, 902, 11, 3783, 21677, 17846, 374, 389, 279, 7002, 3403, 279, 19309, 13, 13824, 11, 7196, 358, 5786, 878, 13, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 594, 3405, 25, 330, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 7521, 2055, 1156, 11, 10542, 279, 21921, 13, 576, 4158, 26482, 4982, 3143, 220, 16, 16, 297, 62410, 320, 3481, 7950, 568, 3555, 594, 389, 1909, 30, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 429, 3082, 13, 13824, 11, 9834, 0, 758, 279, 21844, 476, 21810, 11, 1052, 594, 264, 17846, 13, 2055, 279, 4226, 374, 264, 17846, 4192, 1249, 8253, 1128, 374, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 527, 20666, 11, 23643, 279, 27979, 6789, 323, 5424, 304, 279, 5335, 25, 18611, 16, 13, 95210, 279, 1036, 65, 77832, 21921, 854, 304, 3405, 25, 362, 4158, 11, 1378, 481, 26482, 5819, 4982, 374, 9434, 3143, 279, 1909, 481, 2115, 320, 16, 16, 297, 527, 20666, 8, 3082, 8674, 311, 1008, 37664, 1075, 279, 19309, 323, 17196, 13, 2303, 17, 13, 1374, 19991, 6171, 3403, 419, 21921, 25, 758, 279, 5335, 11, 264, 3070, 73225, 334, 374, 21810, 389, 279, 7002, 3403, 419, 21921, 3082, 13, 79226, 54216, 151645, 198]
labels:

Got it, let's try to figure out what's on top of the bathroom cabinet at 11 o'clock. First, let's visualize the bathroom setup. The cabinet is likely a white, two-drawer unit. At 11 o'clock (top-left area), we need to check that spot. Looking at the images, especially the one showing the cabinet area, there's a mirror above the sink, but wait, no—wait, the question is about the cabinet. Wait, maybe the "cabinet" here is the storage unit. Wait, no, the sink has a cabinet below? Wait, no, the main cabinet is the white drawers. Wait, in the image where the camera shows the sink and cabinet, the top of the cabinet—wait, no, maybe the mirror? Wait, no, let's check again. Wait, the user says "bathroom cabinet that is on my 11 o'clock". So 11 o'clock would be the upper left area relative to the sink. Looking at the images, there's a mirror mounted on the wall above the sink area. Is that part of the cabinet setup? Wait, no, maybe the cabinet is the white drawers under the sink. Wait, no, the top of that cabinet—wait, no, in some images, there's a mirror reflected or mounted. Wait, no, let's check the first few images. The shower curtain is colorful, then the sink area. Wait, the correct answer: looking at the images, the object on top of the bathroom cabinet (the white drawer unit) at 11 o'clock is a mirror. Wait, no, wait—the mirror is on the wall above the sink. Wait, maybe I misread. Wait, no, let's check again. Wait, the user's question: "What is on top of the bathroom cabinet that is on my 11 o'clock?" So first, identify the cabinet. The white drawer unit near 11 o'clock (top-left). What's on top? Looking at the images, there's a mirror mounted on the wall above that area. Wait, yes! In the reflection or mounted, there's a mirror. So the answer is a mirror.


To determine what is on top of the bathroom cabinet at 11 o’clock, analyze the spatial layout and elements in the images:  

1. Locate the “bathroom cabinet” in question: A white, two - drawer storage unit is visible near the top - left (11 o’clock) area relative to other fixtures like the sink and shower.  
2. Examine objects above this cabinet: In the images, a **mirror** is mounted on the wall above this cabinet area.  


Mirror<|im_end|>

[INFO|hub.py:421] 2025-12-28 19:38:45,665 >> Offline mode: forcing local_files_only=True
[INFO|configuration_utils.py:765] 2025-12-28 19:38:45,668 >> loading configuration file config.json from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Video-R1-7B/snapshots/e6840e4cdc86b484b05aacdcb5c88ad3d7ef3d76/config.json
[INFO|configuration_utils.py:839] 2025-12-28 19:38:45,675 >> Model config Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 128000,
  "max_window_layers": 28,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "_name_or_path": "Video-R1/Video-R1-7B",
    "architectures": [
      "Qwen2_5_VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "dtype": "bfloat16",
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "layer_types": [
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention"
    ],
    "max_position_embeddings": 128000,
    "max_window_layers": 28,
    "model_type": "qwen2_5_vl_text",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "use_cache": false,
    "use_sliding_window": false,
    "vision_token_id": 151654,
    "vocab_size": 152064
  },
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": false,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "dtype": "float32",
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 3584,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

[WARNING|2025-12-28 19:38:45] llamafactory.model.model_utils.attention:148 >> FlashAttention-2 is not installed.
[INFO|2025-12-28 19:38:45] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
[INFO|2025-12-28 19:38:46] llamafactory.model.model_utils.liger_kernel:143 >> Liger kernel has been applied to the model.
[INFO|hub.py:421] 2025-12-28 19:38:46,321 >> Offline mode: forcing local_files_only=True
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[WARNING|logging.py:328] 2025-12-28 19:38:46,322 >> `torch_dtype` is deprecated! Use `dtype` instead!
[INFO|hub.py:421] 2025-12-28 19:38:46,322 >> Offline mode: forcing local_files_only=True
[INFO|modeling_utils.py:4837] 2025-12-28 19:38:46,324 >> Offline mode: forcing local_files_only=True
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO|modeling_utils.py:1172] 2025-12-28 19:38:46,325 >> loading weights file model.safetensors from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Video-R1-7B/snapshots/e6840e4cdc86b484b05aacdcb5c88ad3d7ef3d76/model.safetensors.index.json
[INFO|modeling_utils.py:2341] 2025-12-28 19:38:46,330 >> Instantiating Qwen2_5_VLForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:986] 2025-12-28 19:38:46,333 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "use_cache": false
}

[INFO|modeling_utils.py:2341] 2025-12-28 19:38:46,334 >> Instantiating Qwen2_5_VisionTransformerPretrainedModel model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:2341] 2025-12-28 19:38:46,380 >> Instantiating Qwen2_5_VLTextModel model under default dtype torch.bfloat16.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:22<01:08, 22.91s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:22<01:08, 22.91s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:22<01:08, 22.91s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:22<01:08, 22.91s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:47<00:48, 24.01s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:47<00:48, 24.01s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:47<00:48, 24.01s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:47<00:48, 24.01s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:08<00:22, 22.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:08<00:22, 22.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:08<00:22, 22.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:08<00:22, 22.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:16<00:00, 16.74s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:16<00:00, 16.74s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:16<00:00, 16.74s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:16<00:00, 19.09s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:16<00:00, 19.09s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:16<00:00, 19.09s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:16<00:00, 16.74s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:16<00:00, 19.09s/it]
[INFO|configuration_utils.py:941] 2025-12-28 19:40:02,963 >> loading configuration file generation_config.json from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Video-R1-7B/snapshots/e6840e4cdc86b484b05aacdcb5c88ad3d7ef3d76/generation_config.json
[INFO|configuration_utils.py:986] 2025-12-28 19:40:02,963 >> Generate config GenerationConfig {
  "attn_implementation": "flash_attention_2",
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.1,
  "top_k": 1,
  "top_p": 0.001,
  "use_cache": false
}

[INFO|dynamic_module_utils.py:423] 2025-12-28 19:40:02,965 >> Could not locate the custom_generate/generate.py inside Video-R1/Video-R1-7B.
[INFO|2025-12-28 19:40:02] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2025-12-28 19:40:02] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-12-28 19:40:02] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.
[INFO|2025-12-28 19:40:02] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA
[INFO|2025-12-28 19:40:03] llamafactory.model.adapter:143 >> Loaded adapter(s): /project/aip-wangcs/indrisch/LLaMA-Factory/saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-400/
[INFO|2025-12-28 19:40:03] llamafactory.model.loader:143 >> trainable params: 20,185,088 || all params: 8,312,351,744 || trainable%: 0.2428
name: base_model.model.model.visual.patch_embed.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.ln_q.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.0.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.0.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.embed_tokens.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.lm_head.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
[INFO|trainer.py:749] 2025-12-28 19:40:03,793 >> Using auto half precision backend
[WARNING|2025-12-28 19:40:03] llamafactory.train.callbacks:154 >> Previous trainer log in this folder will be deleted.
[WARNING|trainer.py:982] 2025-12-28 19:40:03,796 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
[DEBUG|trainer.py:2373] 2025-12-28 19:40:04,074 >> Currently training with a batch size of: 2
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 8. Using DeepSpeed's value.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Using auto half precision backend
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
[2025-12-28 19:40:04,163] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown
[2025-12-28 19:40:04,163] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 4
Currently training with a batch size of: 2
Currently training with a batch size of: 2
Currently training with a batch size of: 2
[2025-12-28 19:40:04,485] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 4
[2025-12-28 19:40:04,503] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 4
[2025-12-28 19:40:04,507] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 4
[2025-12-28 19:40:05,337] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-12-28 19:40:05,345] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-12-28 19:40:05,346] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-12-28 19:40:05,396] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2025-12-28 19:40:05,396] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2025-12-28 19:40:05,396] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-12-28 19:40:05,396] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-12-28 19:40:05,396] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-12-28 19:40:05,396] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-12-28 19:40:05,396] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: True
***** Running training *****
  Num examples = 29,742
  Num Epochs = 2
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 8
  Total optimization steps = 930
  Number of trainable parameters = 20,185,088
***** Running training *****
  Num examples = 29,742
  Num Epochs = 2
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 8
  Total optimization steps = 930
***** Running training *****
  Num examples = 29,742
  Num Epochs = 2
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 8
  Total optimization steps = 930
  Number of trainable parameters = 20,185,088
  Number of trainable parameters = 20,185,088
[2025-12-28 19:40:05,665] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-12-28 19:40:05,666] [INFO] [utils.py:782:see_memory_usage] MA 15.5 GB         Max_MA 15.51 GB         CA 15.53 GB         Max_CA 16 GB 
[2025-12-28 19:40:05,666] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 48.23 GB, percent = 2.4%
[2025-12-28 19:40:05,873] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-12-28 19:40:05,874] [INFO] [utils.py:782:see_memory_usage] MA 15.5 GB         Max_MA 15.52 GB         CA 15.55 GB         Max_CA 16 GB 
[2025-12-28 19:40:05,874] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 48.23 GB, percent = 2.4%
[2025-12-28 19:40:05,874] [INFO] [stage_1_and_2.py:550:__init__] optimizer state initialized
[2025-12-28 19:40:06,082] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-12-28 19:40:06,083] [INFO] [utils.py:782:see_memory_usage] MA 15.5 GB         Max_MA 15.5 GB         CA 15.55 GB         Max_CA 16 GB 
[2025-12-28 19:40:06,083] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 48.43 GB, percent = 2.4%
[2025-12-28 19:40:06,085] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-12-28 19:40:06,085] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-12-28 19:40:06,085] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-12-28 19:40:06,085] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2025-12-28 19:40:06,091] [INFO] [config.py:1001:print] DeepSpeedEngine configuration:
[2025-12-28 19:40:06,092] [INFO] [config.py:1005:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-12-28 19:40:06,092] [INFO] [config.py:1005:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-12-28 19:40:06,092] [INFO] [config.py:1005:print]   amp_enabled .................. False
[2025-12-28 19:40:06,092] [INFO] [config.py:1005:print]   amp_params ................... False
[2025-12-28 19:40:06,092] [INFO] [config.py:1005:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-12-28 19:40:06,092] [INFO] [config.py:1005:print]   bfloat16_enabled ............. True
[2025-12-28 19:40:06,092] [INFO] [config.py:1005:print]   bfloat16_immediate_grad_update  False
[2025-12-28 19:40:06,092] [INFO] [config.py:1005:print]   checkpoint_parallel_write_pipeline  False
[2025-12-28 19:40:06,092] [INFO] [config.py:1005:print]   checkpoint_tag_validation_enabled  True
[2025-12-28 19:40:06,092] [INFO] [config.py:1005:print]   checkpoint_tag_validation_fail  False
[2025-12-28 19:40:06,092] [INFO] [config.py:1005:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x155249d79220>
[2025-12-28 19:40:06,092] [INFO] [config.py:1005:print]   communication_data_type ...... None
[2025-12-28 19:40:06,092] [INFO] [config.py:1005:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-12-28 19:40:06,092] [INFO] [config.py:1005:print]   curriculum_enabled_legacy .... False
[2025-12-28 19:40:06,092] [INFO] [config.py:1005:print]   curriculum_params_legacy ..... False
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   data_efficiency_enabled ...... False
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   dataloader_drop_last ......... False
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   disable_allgather ............ False
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   dump_state ................... False
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   dynamic_loss_scale_args ...... None
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   eigenvalue_enabled ........... False
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   eigenvalue_gas_boundary_resolution  1
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   eigenvalue_layer_num ......... 0
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   eigenvalue_max_iter .......... 100
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   eigenvalue_stability ......... 1e-06
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   eigenvalue_tol ............... 0.01
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   eigenvalue_verbose ........... False
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   elasticity_enabled ........... False
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   fp16_auto_cast ............... None
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   fp16_enabled ................. False
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   fp16_master_weights_and_gradients  False
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   global_rank .................. 0
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   grad_accum_dtype ............. None
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   gradient_accumulation_steps .. 8
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   gradient_clipping ............ 1.0
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   gradient_predivide_factor .... 1.0
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   graph_harvesting ............. False
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   initial_dynamic_scale ........ 1
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   load_universal_checkpoint .... False
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   loss_scale ................... 1.0
[2025-12-28 19:40:06,093] [INFO] [config.py:1005:print]   memory_breakdown ............. False
[2025-12-28 19:40:06,094] [INFO] [config.py:1005:print]   mics_hierarchial_params_gather  False
[2025-12-28 19:40:06,094] [INFO] [config.py:1005:print]   mics_shard_size .............. -1
[2025-12-28 19:40:06,094] [INFO] [config.py:1005:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-12-28 19:40:06,094] [INFO] [config.py:1005:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-12-28 19:40:06,094] [INFO] [config.py:1005:print]   optimizer_legacy_fusion ...... False
[2025-12-28 19:40:06,094] [INFO] [config.py:1005:print]   optimizer_name ............... None
[2025-12-28 19:40:06,094] [INFO] [config.py:1005:print]   optimizer_params ............. None
[2025-12-28 19:40:06,094] [INFO] [config.py:1005:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-12-28 19:40:06,094] [INFO] [config.py:1005:print]   pld_enabled .................. False
[2025-12-28 19:40:06,094] [INFO] [config.py:1005:print]   pld_params ................... False
[2025-12-28 19:40:06,094] [INFO] [config.py:1005:print]   prescale_gradients ........... False
[2025-12-28 19:40:06,094] [INFO] [config.py:1005:print]   scheduler_name ............... None
[2025-12-28 19:40:06,094] [INFO] [config.py:1005:print]   scheduler_params ............. None
[2025-12-28 19:40:06,094] [INFO] [config.py:1005:print]   seq_parallel_communication_data_type  torch.float32
[2025-12-28 19:40:06,094] [INFO] [config.py:1005:print]   sparse_attention ............. None
[2025-12-28 19:40:06,094] [INFO] [config.py:1005:print]   sparse_gradients_enabled ..... False
[2025-12-28 19:40:06,094] [INFO] [config.py:1005:print]   steps_per_print .............. inf
[2025-12-28 19:40:06,094] [INFO] [config.py:1005:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-12-28 19:40:06,094] [INFO] [config.py:1005:print]   timers_config ................ enabled=True synchronized=True
[2025-12-28 19:40:06,094] [INFO] [config.py:1005:print]   train_batch_size ............. 64
[2025-12-28 19:40:06,094] [INFO] [config.py:1005:print]   train_micro_batch_size_per_gpu  2
[2025-12-28 19:40:06,094] [INFO] [config.py:1005:print]   use_data_before_expert_parallel_  False
[2025-12-28 19:40:06,094] [INFO] [config.py:1005:print]   use_node_local_storage ....... False
[2025-12-28 19:40:06,094] [INFO] [config.py:1005:print]   wall_clock_breakdown ......... False
[2025-12-28 19:40:06,094] [INFO] [config.py:1005:print]   weight_quantization_config ... None
[2025-12-28 19:40:06,094] [INFO] [config.py:1005:print]   world_size ................... 4
[2025-12-28 19:40:06,094] [INFO] [config.py:1005:print]   zero_allow_untested_optimizer  True
[2025-12-28 19:40:06,095] [INFO] [config.py:1005:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=True zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-12-28 19:40:06,095] [INFO] [config.py:1005:print]   zero_enabled ................. True
[2025-12-28 19:40:06,095] [INFO] [config.py:1005:print]   zero_force_ds_cpu_optimizer .. True
[2025-12-28 19:40:06,095] [INFO] [config.py:1005:print]   zero_optimization_stage ...... 2
[2025-12-28 19:40:06,095] [INFO] [config.py:991:print_user_config]   json = {
    "train_batch_size": 64, 
    "train_micro_batch_size_per_gpu": 2, 
    "gradient_accumulation_steps": 8, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": false, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true, 
        "round_robin_gradients": true
    }, 
    "steps_per_print": inf
}
[INFO|trainer.py:2519] 2025-12-28 19:40:06,096 >> ***** Running training *****
[INFO|trainer.py:2520] 2025-12-28 19:40:06,096 >>   Num examples = 29,742
[INFO|trainer.py:2521] 2025-12-28 19:40:06,096 >>   Num Epochs = 2
[INFO|trainer.py:2522] 2025-12-28 19:40:06,096 >>   Instantaneous batch size per device = 2
[INFO|trainer.py:2525] 2025-12-28 19:40:06,096 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2526] 2025-12-28 19:40:06,096 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2527] 2025-12-28 19:40:06,096 >>   Total optimization steps = 930
[INFO|trainer.py:2528] 2025-12-28 19:40:06,100 >>   Number of trainable parameters = 20,185,088
[INFO|integration_utils.py:867] 2025-12-28 19:40:06,105 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: WARNING Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to https://wandb.me/wandb-init.
wandb: Tracking run with wandb version 0.21.2
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /project/aip-wangcs/indrisch/LLaMA-Factory/wandb/wandb/offline-run-20251228_194006-edpt5fgw
  0%|          | 0/930 [00:00<?, ?it/s]  0%|          | 1/930 [02:27<38:07:18, 147.73s/it]  0%|          | 2/930 [04:31<34:23:49, 133.44s/it]  0%|          | 3/930 [07:54<42:35:00, 165.37s/it]  0%|          | 4/930 [10:29<41:28:13, 161.22s/it]  1%|          | 5/930 [12:57<40:14:09, 156.59s/it]  1%|          | 6/930 [14:54<36:41:10, 142.93s/it]  1%|          | 7/930 [16:41<33:40:36, 131.35s/it]  1%|          | 8/930 [19:08<34:53:08, 136.21s/it]  1%|          | 9/930 [21:47<36:42:03, 143.46s/it]  1%|          | 10/930 [23:56<35:29:54, 138.91s/it]                                                    {'loss': 0.7109, 'grad_norm': 0.43276387453079224, 'learning_rate': 9.67741935483871e-06, 'epoch': 0.02}
  1%|          | 10/930 [23:56<35:29:54, 138.91s/it]  1%|          | 11/930 [25:56<33:57:24, 133.02s/it]  1%|▏         | 12/930 [28:06<33:43:07, 132.23s/it]  1%|▏         | 13/930 [30:47<35:55:07, 141.01s/it]  2%|▏         | 14/930 [32:50<34:28:09, 135.47s/it]  2%|▏         | 15/930 [35:42<37:13:03, 146.43s/it]  2%|▏         | 16/930 [37:23<33:43:54, 132.86s/it]  2%|▏         | 17/930 [39:28<33:03:36, 130.36s/it]  2%|▏         | 18/930 [41:43<33:22:26, 131.74s/it]  2%|▏         | 19/930 [44:43<37:00:27, 146.24s/it]  2%|▏         | 20/930 [47:14<37:21:54, 147.82s/it]                                                    {'loss': 0.7196, 'grad_norm': 0.4703013300895691, 'learning_rate': 2.0430107526881722e-05, 'epoch': 0.04}
  2%|▏         | 20/930 [47:14<37:21:54, 147.82s/it]  2%|▏         | 21/930 [49:43<37:24:14, 148.13s/it]  2%|▏         | 22/930 [51:51<35:48:51, 141.99s/it]  2%|▏         | 23/930 [54:15<35:55:23, 142.58s/it]  3%|▎         | 24/930 [56:20<34:34:34, 137.39s/it]  3%|▎         | 25/930 [58:56<35:58:46, 143.12s/it]  3%|▎         | 26/930 [1:01:30<36:43:22, 146.24s/it]  3%|▎         | 27/930 [1:03:11<33:18:20, 132.78s/it]  3%|▎         | 28/930 [1:05:18<32:49:20, 131.00s/it]  3%|▎         | 29/930 [1:07:49<34:18:15, 137.06s/it]  3%|▎         | 30/930 [1:10:21<35:20:17, 141.35s/it]                                                      {'loss': 0.7015, 'grad_norm': 0.42771342396736145, 'learning_rate': 3.118279569892473e-05, 'epoch': 0.06}
  3%|▎         | 30/930 [1:10:21<35:20:17, 141.35s/it]  3%|▎         | 31/930 [1:13:15<37:45:15, 151.19s/it]  3%|▎         | 32/930 [1:15:43<37:28:51, 150.26s/it]  4%|▎         | 33/930 [1:17:49<35:37:53, 143.00s/it]  4%|▎         | 34/930 [1:19:50<33:57:31, 136.44s/it]  4%|▍         | 35/930 [1:21:56<33:07:00, 133.21s/it]  4%|▍         | 36/930 [1:24:12<33:19:37, 134.20s/it]  4%|▍         | 37/930 [1:26:37<34:06:24, 137.50s/it]  4%|▍         | 38/930 [1:28:56<34:08:26, 137.79s/it]  4%|▍         | 39/930 [1:31:38<35:56:25, 145.21s/it]  4%|▍         | 40/930 [1:34:02<35:47:15, 144.76s/it]                                                      {'loss': 0.6925, 'grad_norm': 0.48475202918052673, 'learning_rate': 4.1935483870967746e-05, 'epoch': 0.09}
  4%|▍         | 40/930 [1:34:02<35:47:15, 144.76s/it]  4%|▍         | 41/930 [1:36:07<34:15:12, 138.71s/it]  5%|▍         | 42/930 [1:38:10<33:01:58, 133.92s/it]  5%|▍         | 43/930 [1:40:36<33:55:33, 137.69s/it]  5%|▍         | 44/930 [1:43:23<36:01:53, 146.40s/it]  5%|▍         | 45/930 [1:46:01<36:53:16, 150.05s/it]  5%|▍         | 46/930 [1:48:22<36:07:30, 147.12s/it]  5%|▌         | 47/930 [1:50:59<36:48:39, 150.08s/it]  5%|▌         | 48/930 [1:53:33<37:06:43, 151.48s/it]  5%|▌         | 49/930 [1:55:30<34:32:37, 141.16s/it]  5%|▌         | 50/930 [1:58:11<35:56:06, 147.01s/it]                                                      {'loss': 0.6975, 'grad_norm': 0.517647385597229, 'learning_rate': 5.268817204301075e-05, 'epoch': 0.11}
  5%|▌         | 50/930 [1:58:11<35:56:06, 147.01s/it]  5%|▌         | 51/930 [2:01:24<39:16:43, 160.87s/it]  6%|▌         | 52/930 [2:03:50<38:08:13, 156.37s/it]  6%|▌         | 53/930 [2:06:04<36:28:30, 149.73s/it]  6%|▌         | 54/930 [2:08:14<34:59:23, 143.79s/it]  6%|▌         | 55/930 [2:10:02<32:17:33, 132.86s/it]  6%|▌         | 56/930 [2:12:27<33:08:59, 136.54s/it]  6%|▌         | 57/930 [2:15:21<35:51:17, 147.85s/it]  6%|▌         | 58/930 [2:17:51<35:57:08, 148.43s/it]  6%|▋         | 59/930 [2:20:39<37:20:36, 154.35s/it]  6%|▋         | 60/930 [2:22:49<35:32:28, 147.07s/it]                                                      {'loss': 0.6953, 'grad_norm': 0.5044444799423218, 'learning_rate': 6.344086021505376e-05, 'epoch': 0.13}
  6%|▋         | 60/930 [2:22:49<35:32:28, 147.07s/it]  7%|▋         | 61/930 [2:24:57<34:06:44, 141.32s/it]  7%|▋         | 62/930 [2:27:10<33:28:28, 138.83s/it]  7%|▋         | 63/930 [2:29:27<33:20:09, 138.42s/it]  7%|▋         | 64/930 [2:31:31<32:12:57, 133.92s/it]  7%|▋         | 65/930 [2:34:02<33:26:44, 139.20s/it]  7%|▋         | 66/930 [2:36:16<33:00:21, 137.52s/it]  7%|▋         | 67/930 [2:38:35<33:02:44, 137.85s/it]  7%|▋         | 68/930 [2:41:08<34:09:20, 142.65s/it]  7%|▋         | 69/930 [2:43:25<33:40:47, 140.82s/it]  8%|▊         | 70/930 [2:45:12<31:11:03, 130.54s/it]                                                      {'loss': 0.6848, 'grad_norm': 0.509800136089325, 'learning_rate': 7.419354838709677e-05, 'epoch': 0.15}
  8%|▊         | 70/930 [2:45:12<31:11:03, 130.54s/it]  8%|▊         | 71/930 [2:48:06<34:18:48, 143.81s/it]  8%|▊         | 72/930 [2:49:58<31:58:50, 134.18s/it]  8%|▊         | 73/930 [2:52:18<32:22:09, 135.97s/it]  8%|▊         | 74/930 [2:54:25<31:39:07, 133.12s/it]  8%|▊         | 75/930 [2:56:15<29:58:01, 126.18s/it]  8%|▊         | 76/930 [2:58:40<31:16:36, 131.85s/it]  8%|▊         | 77/930 [3:01:08<32:23:00, 136.67s/it]  8%|▊         | 78/930 [3:03:26<32:28:53, 137.25s/it]  8%|▊         | 79/930 [3:05:48<32:46:47, 138.67s/it]  9%|▊         | 80/930 [3:08:35<34:43:00, 147.04s/it]                                                      {'loss': 0.6986, 'grad_norm': 0.5397575497627258, 'learning_rate': 8.494623655913979e-05, 'epoch': 0.17}
  9%|▊         | 80/930 [3:08:35<34:43:00, 147.04s/it]  9%|▊         | 81/930 [3:10:55<34:11:07, 144.96s/it]  9%|▉         | 82/930 [3:13:38<35:24:28, 150.32s/it]  9%|▉         | 83/930 [3:16:04<35:06:57, 149.25s/it]  9%|▉         | 84/930 [3:18:14<33:42:01, 143.41s/it]  9%|▉         | 85/930 [3:19:43<29:48:00, 126.96s/it]  9%|▉         | 86/930 [3:22:40<33:17:04, 141.97s/it]  9%|▉         | 87/930 [3:25:17<34:16:56, 146.40s/it]  9%|▉         | 88/930 [3:27:40<34:00:32, 145.41s/it] 10%|▉         | 89/930 [3:29:40<32:14:48, 138.04s/it] 10%|▉         | 90/930 [3:31:35<30:33:31, 130.97s/it]                                                      {'loss': 0.6803, 'grad_norm': 0.48527389764785767, 'learning_rate': 9.56989247311828e-05, 'epoch': 0.19}
 10%|▉         | 90/930 [3:31:35<30:33:31, 130.97s/it] 10%|▉         | 91/930 [3:33:45<30:29:25, 130.83s/it] 10%|▉         | 92/930 [3:36:11<31:29:57, 135.32s/it] 10%|█         | 93/930 [3:39:17<34:57:26, 150.35s/it] 10%|█         | 94/930 [3:41:14<32:37:30, 140.49s/it] 10%|█         | 95/930 [3:43:52<33:46:35, 145.62s/it] 10%|█         | 96/930 [3:46:38<35:11:39, 151.92s/it] 10%|█         | 97/930 [3:48:47<33:31:54, 144.92s/it] 11%|█         | 98/930 [3:51:23<34:14:00, 148.13s/it] 11%|█         | 99/930 [3:54:36<37:17:56, 161.58s/it] 11%|█         | 100/930 [3:56:39<34:36:50, 150.13s/it]                                                       {'loss': 0.683, 'grad_norm': 0.479020357131958, 'learning_rate': 9.998732135085665e-05, 'epoch': 0.22}
 11%|█         | 100/930 [3:56:39<34:36:50, 150.13s/it] 11%|█         | 101/930 [3:59:59<38:02:06, 165.17s/it] 11%|█         | 102/930 [4:02:38<37:33:16, 163.28s/it] 11%|█         | 103/930 [4:05:34<38:21:32, 166.98s/it] 11%|█         | 104/930 [4:07:42<35:39:23, 155.40s/it] 11%|█▏        | 105/930 [4:09:44<33:20:41, 145.50s/it] 11%|█▏        | 106/930 [4:12:20<34:00:04, 148.55s/it] 12%|█▏        | 107/930 [4:14:35<33:02:05, 144.50s/it] 12%|█▏        | 108/930 [4:16:39<31:32:37, 138.15s/it] 12%|█▏        | 109/930 [4:18:45<30:43:09, 134.70s/it] 12%|█▏        | 110/930 [4:20:52<30:08:10, 132.30s/it]                                                       {'loss': 0.6977, 'grad_norm': 0.5130512118339539, 'learning_rate': 9.990986400130607e-05, 'epoch': 0.24}
 12%|█▏        | 110/930 [4:20:52<30:08:10, 132.30s/it] 12%|█▏        | 111/930 [4:23:47<32:59:35, 145.02s/it] 12%|█▏        | 112/930 [4:26:00<32:11:05, 141.65s/it] 12%|█▏        | 113/930 [4:29:04<34:59:32, 154.19s/it] 12%|█▏        | 114/930 [4:31:37<34:51:54, 153.82s/it] 12%|█▏        | 115/930 [4:34:08<34:38:17, 153.00s/it] 12%|█▏        | 116/930 [4:36:34<34:08:41, 151.01s/it] 13%|█▎        | 117/930 [4:39:21<35:08:58, 155.64s/it] 13%|█▎        | 118/930 [4:41:49<34:36:23, 153.43s/it] 13%|█▎        | 119/930 [4:44:20<34:24:04, 152.71s/it] 13%|█▎        | 120/930 [4:46:16<31:53:33, 141.75s/it]                                                       {'loss': 0.6953, 'grad_norm': 0.5031633973121643, 'learning_rate': 9.976210197283718e-05, 'epoch': 0.26}
 13%|█▎        | 120/930 [4:46:16<31:53:33, 141.75s/it] 13%|█▎        | 121/930 [4:49:11<34:05:33, 151.71s/it] 13%|█▎        | 122/930 [4:52:54<38:50:48, 173.08s/it] 13%|█▎        | 123/930 [4:54:49<34:54:56, 155.76s/it] 13%|█▎        | 124/930 [4:57:23<34:42:14, 155.01s/it] 13%|█▎        | 125/930 [4:59:26<32:31:06, 145.42s/it] 14%|█▎        | 126/930 [5:01:24<30:37:38, 137.14s/it] 14%|█▎        | 127/930 [5:03:46<30:55:21, 138.63s/it] 14%|█▍        | 128/930 [5:06:07<31:02:09, 139.31s/it] 14%|█▍        | 129/930 [5:09:12<34:03:40, 153.08s/it] 14%|█▍        | 130/930 [5:12:10<35:42:16, 160.67s/it]                                                       {'loss': 0.6709, 'grad_norm': 0.5805301666259766, 'learning_rate': 9.954424340791196e-05, 'epoch': 0.28}
 14%|█▍        | 130/930 [5:12:10<35:42:16, 160.67s/it] 14%|█▍        | 131/930 [5:14:18<33:27:10, 150.73s/it] 14%|█▍        | 132/930 [5:16:54<33:48:06, 152.49s/it] 14%|█▍        | 133/930 [5:18:54<31:36:35, 142.78s/it] 14%|█▍        | 134/930 [5:21:01<30:28:04, 137.79s/it] 15%|█▍        | 135/930 [5:22:59<29:08:58, 132.00s/it] 15%|█▍        | 136/930 [5:25:52<31:50:41, 144.39s/it] 15%|█▍        | 137/930 [5:27:59<30:38:48, 139.13s/it] 15%|█▍        | 138/930 [5:30:45<32:24:04, 147.28s/it] 15%|█▍        | 139/930 [5:33:02<31:40:40, 144.17s/it] 15%|█▌        | 140/930 [5:35:36<32:16:23, 147.07s/it]                                                       {'loss': 0.671, 'grad_norm': 0.5377289056777954, 'learning_rate': 9.925659518928315e-05, 'epoch': 0.3}
 15%|█▌        | 140/930 [5:35:36<32:16:23, 147.07s/it] 15%|█▌        | 141/930 [5:37:37<30:31:49, 139.30s/it] 15%|█▌        | 142/930 [5:40:08<31:15:28, 142.80s/it] 15%|█▌        | 143/930 [5:42:14<30:06:06, 137.70s/it] 15%|█▌        | 144/930 [5:44:19<29:12:58, 133.81s/it] 16%|█▌        | 145/930 [5:46:46<30:01:15, 137.68s/it] 16%|█▌        | 146/930 [5:49:34<32:00:35, 146.98s/it] 16%|█▌        | 147/930 [5:52:09<32:27:16, 149.22s/it] 16%|█▌        | 148/930 [5:54:02<30:04:24, 138.45s/it] 16%|█▌        | 149/930 [5:57:07<33:02:05, 152.27s/it] 16%|█▌        | 150/930 [5:59:23<31:58:08, 147.55s/it]                                                       {'loss': 0.6867, 'grad_norm': 0.517046332359314, 'learning_rate': 9.889956250770932e-05, 'epoch': 0.32}
 16%|█▌        | 150/930 [5:59:23<31:58:08, 147.55s/it] 16%|█▌        | 151/930 [6:01:46<31:35:57, 146.03s/it] 16%|█▋        | 152/930 [6:03:44<29:45:49, 137.72s/it] 16%|█▋        | 153/930 [6:06:02<29:46:31, 137.96s/it] 17%|█▋        | 154/930 [6:08:23<29:55:57, 138.86s/it] 17%|█▋        | 155/930 [6:10:22<28:34:00, 132.70s/it] 17%|█▋        | 156/930 [6:13:02<30:20:22, 141.11s/it] 17%|█▋        | 157/930 [6:14:42<27:35:44, 128.52s/it] 17%|█▋        | 158/930 [6:17:10<28:50:30, 134.50s/it] 17%|█▋        | 159/930 [6:19:29<29:04:49, 135.78s/it] 17%|█▋        | 160/930 [6:21:43<28:55:21, 135.22s/it]                                                       {'loss': 0.6901, 'grad_norm': 0.504304051399231, 'learning_rate': 9.847364829118963e-05, 'epoch': 0.34}
 17%|█▋        | 160/930 [6:21:43<28:55:21, 135.22s/it] 17%|█▋        | 161/930 [6:23:20<26:28:14, 123.92s/it] 17%|█▋        | 162/930 [6:25:12<25:39:34, 120.28s/it] 18%|█▊        | 163/930 [6:27:15<25:47:58, 121.09s/it] 18%|█▊        | 164/930 [6:29:37<27:07:34, 127.49s/it] 18%|█▊        | 165/930 [6:32:04<28:17:54, 133.17s/it] 18%|█▊        | 166/930 [6:35:18<32:07:00, 151.34s/it] 18%|█▊        | 167/930 [6:37:54<32:22:24, 152.74s/it] 18%|█▊        | 168/930 [6:39:44<29:37:48, 139.99s/it] 18%|█▊        | 169/930 [6:41:36<27:50:42, 131.72s/it] 18%|█▊        | 170/930 [6:43:32<26:48:03, 126.95s/it]                                                       {'loss': 0.6695, 'grad_norm': 0.5113282203674316, 'learning_rate': 9.797945249652295e-05, 'epoch': 0.37}
 18%|█▊        | 170/930 [6:43:32<26:48:03, 126.95s/it] 18%|█▊        | 171/930 [6:46:00<28:04:05, 133.13s/it] 18%|█▊        | 172/930 [6:48:37<29:32:54, 140.34s/it] 19%|█▊        | 173/930 [6:50:42<28:33:59, 135.85s/it] 19%|█▊        | 174/930 [6:53:04<28:55:16, 137.72s/it] 19%|█▉        | 175/930 [6:55:11<28:10:03, 134.31s/it] 19%|█▉        | 176/930 [6:57:34<28:41:50, 137.02s/it] 19%|█▉        | 177/930 [6:59:33<27:31:00, 131.55s/it] 19%|█▉        | 178/930 [7:01:31<26:37:08, 127.43s/it] 19%|█▉        | 179/930 [7:03:25<25:46:39, 123.57s/it] 19%|█▉        | 180/930 [7:05:55<27:23:26, 131.48s/it]                                                       {'loss': 0.6861, 'grad_norm': 0.5083709359169006, 'learning_rate': 9.741767126418897e-05, 'epoch': 0.39}
 19%|█▉        | 180/930 [7:05:55<27:23:26, 131.48s/it] 19%|█▉        | 181/930 [7:07:52<26:26:10, 127.06s/it] 20%|█▉        | 182/930 [7:10:05<26:48:14, 129.00s/it] 20%|█▉        | 183/930 [7:11:52<25:21:44, 122.23s/it] 20%|█▉        | 184/930 [7:13:55<25:24:58, 122.65s/it] 20%|█▉        | 185/930 [7:15:56<25:14:58, 122.01s/it] 20%|██        | 186/930 [7:18:14<26:12:23, 126.81s/it] 20%|██        | 187/930 [7:20:45<27:38:53, 133.96s/it] 20%|██        | 188/930 [7:22:45<26:45:52, 129.85s/it] 20%|██        | 189/930 [7:25:47<29:57:22, 145.54s/it] 20%|██        | 190/930 [7:28:01<29:10:31, 141.93s/it]                                                       {'loss': 0.6846, 'grad_norm': 0.5283152461051941, 'learning_rate': 9.67890959377418e-05, 'epoch': 0.41}
 20%|██        | 190/930 [7:28:01<29:10:31, 141.93s/it] 21%|██        | 191/930 [7:30:03<27:55:46, 136.06s/it] 21%|██        | 192/930 [7:31:46<25:51:03, 126.10s/it] 21%|██        | 193/930 [7:34:18<27:24:37, 133.89s/it] 21%|██        | 194/930 [7:36:21<26:41:31, 130.56s/it] 21%|██        | 195/930 [7:38:36<26:57:49, 132.07s/it] 21%|██        | 196/930 [7:40:51<27:07:12, 133.01s/it] 21%|██        | 197/930 [7:43:40<29:15:29, 143.70s/it] 21%|██▏       | 198/930 [7:46:22<30:21:01, 149.26s/it] 21%|██▏       | 199/930 [7:49:34<32:52:47, 161.93s/it] 22%|██▏       | 200/930 [7:51:51<31:18:18, 154.38s/it]                                                       {'loss': 0.669, 'grad_norm': 0.5344386100769043, 'learning_rate': 9.60946119490972e-05, 'epoch': 0.43}
 22%|██▏       | 200/930 [7:51:51<31:18:18, 154.38s/it]
***** Running Evaluation *****

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1
  Num examples = 3305
  Batch size = 1
[INFO|trainer.py:4643] 2025-12-29 03:31:58,641 >> 
***** Running Evaluation *****

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1
[INFO|trainer.py:4645] 2025-12-29 03:31:58,642 >>   Num examples = 3305
[INFO|trainer.py:4648] 2025-12-29 03:31:58,642 >>   Batch size = 1

  0%|          | 0/827 [00:00<?, ?it/s][A
  0%|          | 2/827 [00:04<31:54,  2.32s/it][A
  0%|          | 3/827 [00:07<34:30,  2.51s/it][A
  0%|          | 4/827 [00:10<35:15,  2.57s/it][A
  1%|          | 5/827 [00:14<42:27,  3.10s/it][A
  1%|          | 6/827 [00:17<44:41,  3.27s/it][A
  1%|          | 7/827 [00:21<47:31,  3.48s/it][A
  1%|          | 8/827 [00:26<54:48,  4.01s/it][A
  1%|          | 9/827 [00:34<1:07:55,  4.98s/it][A
  1%|          | 10/827 [00:43<1:24:36,  6.21s/it][A
  1%|▏         | 11/827 [00:46<1:12:56,  5.36s/it][A
  1%|▏         | 12/827 [00:49<1:03:41,  4.69s/it][A
  2%|▏         | 13/827 [00:52<56:25,  4.16s/it]  [A
  2%|▏         | 14/827 [00:55<53:01,  3.91s/it][A
  2%|▏         | 15/827 [01:00<55:56,  4.13s/it][A
  2%|▏         | 16/827 [01:04<53:47,  3.98s/it][A
  2%|▏         | 17/827 [01:07<49:40,  3.68s/it][A
  2%|▏         | 18/827 [01:10<48:06,  3.57s/it][A
  2%|▏         | 19/827 [01:14<50:44,  3.77s/it][A
  2%|▏         | 20/827 [01:20<1:00:01,  4.46s/it][A
  3%|▎         | 21/827 [01:24<56:50,  4.23s/it]  [A
  3%|▎         | 22/827 [01:29<1:01:50,  4.61s/it][A
  3%|▎         | 23/827 [01:32<54:16,  4.05s/it]  [A
  3%|▎         | 24/827 [01:36<52:23,  3.91s/it][A
  3%|▎         | 25/827 [01:40<55:15,  4.13s/it][A
  3%|▎         | 26/827 [01:47<1:05:25,  4.90s/it][A
  3%|▎         | 27/827 [01:52<1:06:27,  4.98s/it][A
  3%|▎         | 28/827 [01:55<57:11,  4.30s/it]  [A
  4%|▎         | 29/827 [01:59<56:56,  4.28s/it][A
  4%|▎         | 30/827 [02:05<1:02:50,  4.73s/it][A
  4%|▎         | 31/827 [02:09<1:01:43,  4.65s/it][A
  4%|▍         | 32/827 [02:13<56:42,  4.28s/it]  [A
  4%|▍         | 33/827 [02:19<1:05:20,  4.94s/it][A
  4%|▍         | 34/827 [02:23<58:54,  4.46s/it]  [A
  4%|▍         | 35/827 [02:26<56:09,  4.25s/it][A
  4%|▍         | 36/827 [02:29<50:10,  3.81s/it][A
  4%|▍         | 37/827 [02:33<51:11,  3.89s/it][A
  5%|▍         | 38/827 [02:40<1:01:25,  4.67s/it][A
  5%|▍         | 39/827 [02:45<1:03:44,  4.85s/it][A
  5%|▍         | 40/827 [02:52<1:10:01,  5.34s/it][A
  5%|▍         | 41/827 [02:54<1:00:32,  4.62s/it][A
  5%|▌         | 42/827 [02:58<58:02,  4.44s/it]  [A
  5%|▌         | 43/827 [03:02<55:34,  4.25s/it][A
  5%|▌         | 44/827 [03:06<51:21,  3.94s/it][A
  5%|▌         | 45/827 [03:09<51:28,  3.95s/it][A
  6%|▌         | 46/827 [03:14<55:27,  4.26s/it][A
  6%|▌         | 47/827 [03:20<1:00:55,  4.69s/it][A
  6%|▌         | 48/827 [03:23<55:29,  4.27s/it]  [A
  6%|▌         | 49/827 [03:27<52:21,  4.04s/it][A
  6%|▌         | 50/827 [03:30<47:49,  3.69s/it][A
  6%|▌         | 51/827 [03:34<49:20,  3.81s/it][A
  6%|▋         | 52/827 [03:41<1:02:20,  4.83s/it][A
  6%|▋         | 53/827 [03:44<54:31,  4.23s/it]  [A
  7%|▋         | 54/827 [03:47<48:38,  3.77s/it][A
  7%|▋         | 55/827 [03:50<46:05,  3.58s/it][A
  7%|▋         | 56/827 [03:53<42:38,  3.32s/it][A
  7%|▋         | 57/827 [03:57<45:40,  3.56s/it][A
  7%|▋         | 58/827 [04:00<43:51,  3.42s/it][A
  7%|▋         | 59/827 [04:03<44:24,  3.47s/it][A
  7%|▋         | 60/827 [04:07<45:10,  3.53s/it][A
  7%|▋         | 61/827 [04:13<53:51,  4.22s/it][A
  7%|▋         | 62/827 [04:16<48:09,  3.78s/it][A
  8%|▊         | 63/827 [04:18<44:16,  3.48s/it][A
  8%|▊         | 64/827 [04:21<42:29,  3.34s/it][A
  8%|▊         | 65/827 [04:25<42:05,  3.31s/it][A
  8%|▊         | 66/827 [04:30<48:26,  3.82s/it][A
  8%|▊         | 67/827 [04:35<55:47,  4.40s/it][A
  8%|▊         | 68/827 [04:38<47:36,  3.76s/it][A
  8%|▊         | 69/827 [04:41<45:26,  3.60s/it][A
  8%|▊         | 70/827 [04:44<42:42,  3.38s/it][A
  9%|▊         | 71/827 [04:47<43:09,  3.42s/it][A
  9%|▊         | 72/827 [04:53<52:16,  4.15s/it][A
  9%|▉         | 73/827 [04:56<47:52,  3.81s/it][A
  9%|▉         | 74/827 [05:01<53:13,  4.24s/it][A
  9%|▉         | 75/827 [05:07<59:39,  4.76s/it][A
  9%|▉         | 76/827 [05:10<52:26,  4.19s/it][A
  9%|▉         | 77/827 [05:13<48:25,  3.87s/it][A
  9%|▉         | 78/827 [05:17<46:14,  3.70s/it][A
 10%|▉         | 79/827 [05:19<42:49,  3.44s/it][A
 10%|▉         | 80/827 [05:23<43:53,  3.53s/it][A
 10%|▉         | 81/827 [05:29<52:13,  4.20s/it][A
 10%|▉         | 82/827 [05:33<52:35,  4.23s/it][A
 10%|█         | 83/827 [05:37<51:15,  4.13s/it][A
 10%|█         | 84/827 [05:41<50:56,  4.11s/it][A
 10%|█         | 85/827 [05:46<51:58,  4.20s/it][A
 10%|█         | 86/827 [05:49<47:23,  3.84s/it][A
 11%|█         | 87/827 [05:54<51:28,  4.17s/it][A
 11%|█         | 88/827 [06:06<1:20:54,  6.57s/it][A
 11%|█         | 89/827 [06:14<1:26:56,  7.07s/it][A
 11%|█         | 90/827 [06:21<1:24:47,  6.90s/it][A
 11%|█         | 91/827 [06:24<1:11:35,  5.84s/it][A
 11%|█         | 92/827 [06:28<1:04:45,  5.29s/it][A
 11%|█         | 93/827 [06:31<55:29,  4.54s/it]  [A
 11%|█▏        | 94/827 [06:34<50:13,  4.11s/it][A
 11%|█▏        | 95/827 [06:37<45:35,  3.74s/it][A
 12%|█▏        | 96/827 [06:40<43:20,  3.56s/it][A
 12%|█▏        | 97/827 [06:42<38:51,  3.19s/it][A
 12%|█▏        | 98/827 [06:44<35:42,  2.94s/it][A
 12%|█▏        | 99/827 [06:47<34:44,  2.86s/it][A
 12%|█▏        | 100/827 [06:49<32:19,  2.67s/it][A
 12%|█▏        | 101/827 [06:53<35:24,  2.93s/it][A
 12%|█▏        | 102/827 [06:56<35:51,  2.97s/it][A
 12%|█▏        | 103/827 [06:59<35:35,  2.95s/it][A
 13%|█▎        | 104/827 [07:02<35:27,  2.94s/it][A
 13%|█▎        | 105/827 [07:05<36:29,  3.03s/it][A
 13%|█▎        | 106/827 [07:11<45:16,  3.77s/it][A
 13%|█▎        | 107/827 [07:19<1:01:58,  5.17s/it][A
 13%|█▎        | 108/827 [07:23<59:29,  4.97s/it]  [A
 13%|█▎        | 109/827 [07:28<56:55,  4.76s/it][A
 13%|█▎        | 110/827 [07:31<52:30,  4.39s/it][A
 13%|█▎        | 111/827 [07:36<51:51,  4.35s/it][A
 14%|█▎        | 112/827 [07:42<1:00:20,  5.06s/it][A
 14%|█▎        | 113/827 [07:46<55:35,  4.67s/it]  [A
 14%|█▍        | 114/827 [07:49<50:56,  4.29s/it][A
 14%|█▍        | 115/827 [07:52<46:01,  3.88s/it][A
 14%|█▍        | 116/827 [07:56<46:52,  3.96s/it][A
 14%|█▍        | 117/827 [08:00<47:06,  3.98s/it][A
 14%|█▍        | 118/827 [08:06<51:25,  4.35s/it][A
 14%|█▍        | 119/827 [08:12<57:36,  4.88s/it][A
 15%|█▍        | 120/827 [08:17<59:03,  5.01s/it][A
 15%|█▍        | 121/827 [08:20<52:03,  4.42s/it][A
 15%|█▍        | 122/827 [08:24<48:08,  4.10s/it][A
 15%|█▍        | 123/827 [08:26<42:15,  3.60s/it][A
 15%|█▍        | 124/827 [08:29<41:09,  3.51s/it][A
 15%|█▌        | 125/827 [08:33<42:29,  3.63s/it][A
 15%|█▌        | 126/827 [08:37<41:54,  3.59s/it][A
 15%|█▌        | 127/827 [08:40<39:47,  3.41s/it][A
 15%|█▌        | 128/827 [08:44<41:50,  3.59s/it][A
 16%|█▌        | 129/827 [08:46<38:53,  3.34s/it][A
 16%|█▌        | 130/827 [08:50<40:07,  3.45s/it][A
 16%|█▌        | 131/827 [08:54<40:49,  3.52s/it][A
 16%|█▌        | 132/827 [08:59<47:10,  4.07s/it][A
 16%|█▌        | 133/827 [09:04<49:01,  4.24s/it][A
 16%|█▌        | 134/827 [09:07<46:13,  4.00s/it][A
 16%|█▋        | 135/827 [09:11<46:03,  3.99s/it][A
 16%|█▋        | 136/827 [09:17<53:25,  4.64s/it][A
 17%|█▋        | 137/827 [09:20<47:37,  4.14s/it][A
 17%|█▋        | 138/827 [09:26<52:26,  4.57s/it][A
 17%|█▋        | 139/827 [09:34<1:03:00,  5.50s/it][A
 17%|█▋        | 140/827 [09:38<57:51,  5.05s/it]  [A
 17%|█▋        | 141/827 [09:42<54:40,  4.78s/it][A
 17%|█▋        | 142/827 [09:45<48:46,  4.27s/it][A
 17%|█▋        | 143/827 [09:48<46:22,  4.07s/it][A
 17%|█▋        | 144/827 [09:51<41:55,  3.68s/it][A
 18%|█▊        | 145/827 [09:55<41:05,  3.62s/it][A
 18%|█▊        | 146/827 [09:59<42:54,  3.78s/it][A
 18%|█▊        | 147/827 [10:02<40:41,  3.59s/it][A
 18%|█▊        | 148/827 [10:05<39:29,  3.49s/it][A
 18%|█▊        | 149/827 [10:08<38:18,  3.39s/it][A
 18%|█▊        | 150/827 [10:11<36:25,  3.23s/it][A
 18%|█▊        | 151/827 [10:13<32:49,  2.91s/it][A
 18%|█▊        | 152/827 [10:16<32:41,  2.91s/it][A
 19%|█▊        | 153/827 [10:20<34:55,  3.11s/it][A
 19%|█▊        | 154/827 [10:24<38:27,  3.43s/it][A
 19%|█▊        | 155/827 [10:29<42:42,  3.81s/it][A
 19%|█▉        | 156/827 [10:34<48:09,  4.31s/it][A
 19%|█▉        | 157/827 [10:37<43:40,  3.91s/it][A
 19%|█▉        | 158/827 [10:41<41:33,  3.73s/it][A
 19%|█▉        | 159/827 [10:44<42:15,  3.80s/it][A
 19%|█▉        | 160/827 [10:50<47:28,  4.27s/it][A
 19%|█▉        | 161/827 [10:52<41:42,  3.76s/it][A
 20%|█▉        | 162/827 [10:56<40:14,  3.63s/it][A
 20%|█▉        | 163/827 [11:00<41:19,  3.73s/it][A
 20%|█▉        | 164/827 [11:03<40:42,  3.68s/it][A
 20%|█▉        | 165/827 [11:06<38:05,  3.45s/it][A
 20%|██        | 166/827 [11:11<42:11,  3.83s/it][A
 20%|██        | 167/827 [11:21<1:04:03,  5.82s/it][A
 20%|██        | 168/827 [11:28<1:07:11,  6.12s/it][A
 20%|██        | 169/827 [11:31<57:17,  5.22s/it]  [A
 21%|██        | 170/827 [11:35<52:26,  4.79s/it][A
 21%|██        | 171/827 [11:39<50:37,  4.63s/it][A
 21%|██        | 172/827 [11:42<45:11,  4.14s/it][A
 21%|██        | 173/827 [11:45<39:44,  3.65s/it][A
 21%|██        | 174/827 [11:48<38:47,  3.56s/it][A
 21%|██        | 175/827 [11:52<40:35,  3.73s/it][A
 21%|██▏       | 176/827 [11:55<36:37,  3.37s/it][A
 21%|██▏       | 177/827 [11:58<34:46,  3.21s/it][A
 22%|██▏       | 178/827 [12:01<35:11,  3.25s/it][A
 22%|██▏       | 179/827 [12:08<45:56,  4.25s/it][A
 22%|██▏       | 180/827 [12:12<45:14,  4.20s/it][A
 22%|██▏       | 181/827 [12:15<41:45,  3.88s/it][A
 22%|██▏       | 182/827 [12:20<45:47,  4.26s/it][A
 22%|██▏       | 183/827 [12:23<41:30,  3.87s/it][A
 22%|██▏       | 184/827 [12:27<41:47,  3.90s/it][A
 22%|██▏       | 185/827 [12:32<46:26,  4.34s/it][A
 22%|██▏       | 186/827 [12:35<41:58,  3.93s/it][A
 23%|██▎       | 187/827 [12:38<37:24,  3.51s/it][A
 23%|██▎       | 188/827 [12:41<37:19,  3.51s/it][A
 23%|██▎       | 189/827 [12:47<42:47,  4.02s/it][A
 23%|██▎       | 190/827 [12:50<40:13,  3.79s/it][A
 23%|██▎       | 191/827 [12:54<40:59,  3.87s/it][A
 23%|██▎       | 192/827 [12:58<42:07,  3.98s/it][A
 23%|██▎       | 193/827 [13:03<44:26,  4.21s/it][A
 23%|██▎       | 194/827 [13:06<41:35,  3.94s/it][A
 24%|██▎       | 195/827 [13:09<39:18,  3.73s/it][A
 24%|██▎       | 196/827 [13:12<34:57,  3.32s/it][A
 24%|██▍       | 197/827 [13:15<33:34,  3.20s/it][A
 24%|██▍       | 198/827 [13:18<32:40,  3.12s/it][A
 24%|██▍       | 199/827 [13:22<36:02,  3.44s/it][A
 24%|██▍       | 200/827 [13:26<38:23,  3.67s/it][A
 24%|██▍       | 201/827 [13:32<45:46,  4.39s/it][A
 24%|██▍       | 202/827 [13:37<47:19,  4.54s/it][A
 25%|██▍       | 203/827 [13:42<48:50,  4.70s/it][A
 25%|██▍       | 204/827 [13:49<55:37,  5.36s/it][A
 25%|██▍       | 205/827 [13:52<48:32,  4.68s/it][A
 25%|██▍       | 206/827 [13:56<45:02,  4.35s/it][A
 25%|██▌       | 207/827 [13:59<42:11,  4.08s/it][A
 25%|██▌       | 208/827 [14:02<38:54,  3.77s/it][A
 25%|██▌       | 209/827 [14:06<37:47,  3.67s/it][A
 25%|██▌       | 210/827 [14:09<35:48,  3.48s/it][A
 26%|██▌       | 211/827 [14:13<39:03,  3.80s/it][A
 26%|██▌       | 212/827 [14:18<43:45,  4.27s/it][A
 26%|██▌       | 213/827 [14:25<50:20,  4.92s/it][A
 26%|██▌       | 214/827 [14:29<46:35,  4.56s/it][A
 26%|██▌       | 215/827 [14:32<41:52,  4.10s/it][A
 26%|██▌       | 216/827 [14:35<38:26,  3.77s/it][A
 26%|██▌       | 217/827 [14:39<40:23,  3.97s/it][A
 26%|██▋       | 218/827 [14:44<42:04,  4.15s/it][A
 26%|██▋       | 219/827 [14:47<39:01,  3.85s/it][A
 27%|██▋       | 220/827 [14:51<39:03,  3.86s/it][A
 27%|██▋       | 221/827 [14:57<45:48,  4.54s/it][A
 27%|██▋       | 222/827 [15:00<42:12,  4.19s/it][A
 27%|██▋       | 223/827 [15:04<40:07,  3.99s/it][A
 27%|██▋       | 224/827 [15:07<37:09,  3.70s/it][A
 27%|██▋       | 225/827 [15:10<36:23,  3.63s/it][A
 27%|██▋       | 226/827 [15:13<32:53,  3.28s/it][A
 27%|██▋       | 227/827 [15:16<34:21,  3.44s/it][A
 28%|██▊       | 228/827 [15:23<42:02,  4.21s/it][A
 28%|██▊       | 229/827 [15:26<38:40,  3.88s/it][A
 28%|██▊       | 230/827 [15:30<40:37,  4.08s/it][A
 28%|██▊       | 231/827 [15:34<39:19,  3.96s/it][A
 28%|██▊       | 232/827 [15:37<36:42,  3.70s/it][A
 28%|██▊       | 233/827 [15:41<38:05,  3.85s/it][A
 28%|██▊       | 234/827 [15:47<44:16,  4.48s/it][A
 28%|██▊       | 235/827 [15:54<50:28,  5.12s/it][A
 29%|██▊       | 236/827 [15:58<47:35,  4.83s/it][A
 29%|██▊       | 237/827 [16:03<48:21,  4.92s/it][A
 29%|██▉       | 238/827 [16:09<51:05,  5.20s/it][A
 29%|██▉       | 239/827 [16:13<47:27,  4.84s/it][A
 29%|██▉       | 240/827 [16:20<54:25,  5.56s/it][A
 29%|██▉       | 241/827 [16:26<55:22,  5.67s/it][A
 29%|██▉       | 242/827 [16:30<51:12,  5.25s/it][A
 29%|██▉       | 243/827 [16:34<46:04,  4.73s/it][A
 30%|██▉       | 244/827 [16:41<52:15,  5.38s/it][A
 30%|██▉       | 245/827 [16:44<45:50,  4.73s/it][A
 30%|██▉       | 246/827 [16:48<43:58,  4.54s/it][A
 30%|██▉       | 247/827 [16:52<41:57,  4.34s/it][A
 30%|██▉       | 248/827 [16:59<49:32,  5.13s/it][A
 30%|███       | 249/827 [17:04<50:38,  5.26s/it][A
 30%|███       | 250/827 [17:07<42:12,  4.39s/it][A
 30%|███       | 251/827 [17:10<38:19,  3.99s/it][A
 30%|███       | 252/827 [17:15<42:50,  4.47s/it][A
 31%|███       | 253/827 [17:18<38:07,  3.99s/it][A
 31%|███       | 254/827 [17:23<38:50,  4.07s/it][A
 31%|███       | 255/827 [17:32<52:50,  5.54s/it][A
 31%|███       | 256/827 [17:38<54:32,  5.73s/it][A
 31%|███       | 257/827 [17:43<52:18,  5.51s/it][A
 31%|███       | 258/827 [17:46<45:51,  4.83s/it][A
 31%|███▏      | 259/827 [17:52<49:56,  5.28s/it][A
 31%|███▏      | 260/827 [17:58<52:09,  5.52s/it][A
 32%|███▏      | 261/827 [18:03<49:49,  5.28s/it][A
 32%|███▏      | 262/827 [18:08<48:18,  5.13s/it][A
 32%|███▏      | 263/827 [18:16<56:57,  6.06s/it][A
 32%|███▏      | 264/827 [18:19<47:31,  5.07s/it][A
 32%|███▏      | 265/827 [18:21<39:32,  4.22s/it][A
 32%|███▏      | 266/827 [18:25<37:18,  3.99s/it][A
 32%|███▏      | 267/827 [18:27<34:10,  3.66s/it][A
 32%|███▏      | 268/827 [18:31<33:28,  3.59s/it][A
 33%|███▎      | 269/827 [18:34<33:19,  3.58s/it][A
 33%|███▎      | 270/827 [18:40<39:30,  4.26s/it][A
 33%|███▎      | 271/827 [18:43<35:59,  3.88s/it][A
 33%|███▎      | 272/827 [18:47<35:14,  3.81s/it][A
 33%|███▎      | 273/827 [18:52<37:41,  4.08s/it][A
 33%|███▎      | 274/827 [19:01<53:40,  5.82s/it][A
 33%|███▎      | 275/827 [19:08<55:55,  6.08s/it][A
 33%|███▎      | 276/827 [19:12<48:50,  5.32s/it][A
 33%|███▎      | 277/827 [19:16<45:18,  4.94s/it][A
 34%|███▎      | 278/827 [19:20<43:21,  4.74s/it][A
 34%|███▎      | 279/827 [19:23<38:58,  4.27s/it][A
 34%|███▍      | 280/827 [19:26<35:28,  3.89s/it][A
 34%|███▍      | 281/827 [19:30<34:34,  3.80s/it][A
 34%|███▍      | 282/827 [19:34<34:54,  3.84s/it][A
 34%|███▍      | 283/827 [19:37<32:48,  3.62s/it][A
 34%|███▍      | 284/827 [19:41<34:08,  3.77s/it][A
 34%|███▍      | 285/827 [19:45<35:02,  3.88s/it][A
 35%|███▍      | 286/827 [19:51<40:49,  4.53s/it][A
 35%|███▍      | 287/827 [19:54<35:13,  3.91s/it][A
 35%|███▍      | 288/827 [19:57<32:35,  3.63s/it][A
 35%|███▍      | 289/827 [20:00<30:40,  3.42s/it][A
 35%|███▌      | 290/827 [20:03<30:37,  3.42s/it][A
 35%|███▌      | 291/827 [20:06<28:50,  3.23s/it][A
 35%|███▌      | 292/827 [20:09<28:58,  3.25s/it][A
 35%|███▌      | 293/827 [20:12<28:18,  3.18s/it][A
 36%|███▌      | 294/827 [20:16<29:11,  3.29s/it][A
 36%|███▌      | 295/827 [20:19<30:50,  3.48s/it][A
 36%|███▌      | 296/827 [20:24<33:24,  3.77s/it][A
 36%|███▌      | 297/827 [20:28<34:16,  3.88s/it][A
 36%|███▌      | 298/827 [20:31<31:30,  3.57s/it][A
 36%|███▌      | 299/827 [20:34<29:56,  3.40s/it][A
 36%|███▋      | 300/827 [20:37<29:19,  3.34s/it][A
 36%|███▋      | 301/827 [20:40<27:19,  3.12s/it][A
 37%|███▋      | 302/827 [20:43<28:45,  3.29s/it][A
 37%|███▋      | 303/827 [20:46<25:45,  2.95s/it][A
 37%|███▋      | 304/827 [20:49<26:02,  2.99s/it][A
 37%|███▋      | 305/827 [20:53<28:23,  3.26s/it][A
 37%|███▋      | 306/827 [20:57<32:31,  3.75s/it][A
 37%|███▋      | 307/827 [21:01<31:00,  3.58s/it][A
 37%|███▋      | 308/827 [21:04<29:16,  3.38s/it][A
 37%|███▋      | 309/827 [21:07<29:27,  3.41s/it][A
 37%|███▋      | 310/827 [21:10<27:39,  3.21s/it][A
 38%|███▊      | 311/827 [21:12<25:30,  2.97s/it][A
 38%|███▊      | 312/827 [21:16<27:08,  3.16s/it][A
 38%|███▊      | 313/827 [21:21<32:04,  3.74s/it][A
 38%|███▊      | 314/827 [21:24<30:20,  3.55s/it][A
 38%|███▊      | 315/827 [21:28<32:39,  3.83s/it][A
 38%|███▊      | 316/827 [21:33<33:35,  3.94s/it][A
 38%|███▊      | 317/827 [21:38<38:07,  4.48s/it][A
 38%|███▊      | 318/827 [21:44<40:13,  4.74s/it][A
 39%|███▊      | 319/827 [21:48<37:46,  4.46s/it][A
 39%|███▊      | 320/827 [21:51<33:57,  4.02s/it][A
 39%|███▉      | 321/827 [21:55<35:25,  4.20s/it][A
 39%|███▉      | 322/827 [22:00<35:41,  4.24s/it][A
 39%|███▉      | 323/827 [22:03<33:02,  3.93s/it][A
 39%|███▉      | 324/827 [22:08<37:20,  4.45s/it][A
 39%|███▉      | 325/827 [22:12<34:31,  4.13s/it][A
 39%|███▉      | 326/827 [22:15<32:10,  3.85s/it][A
 40%|███▉      | 327/827 [22:20<36:13,  4.35s/it][A
 40%|███▉      | 328/827 [22:24<34:59,  4.21s/it][A
 40%|███▉      | 329/827 [22:31<40:53,  4.93s/it][A
 40%|███▉      | 330/827 [22:36<42:01,  5.07s/it][A
 40%|████      | 331/827 [22:42<43:20,  5.24s/it][A
 40%|████      | 332/827 [22:45<38:51,  4.71s/it][A
 40%|████      | 333/827 [22:52<42:38,  5.18s/it][A
 40%|████      | 334/827 [22:56<39:02,  4.75s/it][A
 41%|████      | 335/827 [22:59<37:01,  4.52s/it][A
 41%|████      | 336/827 [23:02<31:32,  3.86s/it][A
 41%|████      | 337/827 [23:06<31:23,  3.84s/it][A
 41%|████      | 338/827 [23:09<29:38,  3.64s/it][A
 41%|████      | 339/827 [23:12<27:57,  3.44s/it][A
 41%|████      | 340/827 [23:15<27:12,  3.35s/it][A
 41%|████      | 341/827 [23:19<28:25,  3.51s/it][A
 41%|████▏     | 342/827 [23:22<28:10,  3.49s/it][A
 41%|████▏     | 343/827 [23:25<27:37,  3.42s/it][A
 42%|████▏     | 344/827 [23:30<30:13,  3.75s/it][A
 42%|████▏     | 345/827 [23:36<34:59,  4.36s/it][A
 42%|████▏     | 346/827 [23:40<33:41,  4.20s/it][A
 42%|████▏     | 347/827 [23:43<31:50,  3.98s/it][A
 42%|████▏     | 348/827 [23:46<30:15,  3.79s/it][A
 42%|████▏     | 349/827 [23:50<30:47,  3.86s/it][A
 42%|████▏     | 350/827 [23:56<34:57,  4.40s/it][A
 42%|████▏     | 351/827 [24:00<32:45,  4.13s/it][A
 43%|████▎     | 352/827 [24:04<33:10,  4.19s/it][A
 43%|████▎     | 353/827 [24:07<30:57,  3.92s/it][A
 43%|████▎     | 354/827 [24:10<28:58,  3.67s/it][A
 43%|████▎     | 355/827 [24:14<29:23,  3.74s/it][A
 43%|████▎     | 356/827 [24:20<34:14,  4.36s/it][A
 43%|████▎     | 357/827 [24:24<33:26,  4.27s/it][A
 43%|████▎     | 358/827 [24:27<30:06,  3.85s/it][A
 43%|████▎     | 359/827 [24:30<28:44,  3.69s/it][A
 44%|████▎     | 360/827 [24:35<30:21,  3.90s/it][A
 44%|████▎     | 361/827 [24:39<32:15,  4.15s/it][A
 44%|████▍     | 362/827 [24:42<27:34,  3.56s/it][A
 44%|████▍     | 363/827 [24:46<28:33,  3.69s/it][A
 44%|████▍     | 364/827 [24:50<29:47,  3.86s/it][A
 44%|████▍     | 365/827 [24:53<27:36,  3.58s/it][A
 44%|████▍     | 366/827 [24:58<32:04,  4.17s/it][A
 44%|████▍     | 367/827 [25:04<34:44,  4.53s/it][A
 44%|████▍     | 368/827 [25:06<30:28,  3.98s/it][A
 45%|████▍     | 369/827 [25:10<29:23,  3.85s/it][A
 45%|████▍     | 370/827 [25:13<27:54,  3.66s/it][A
 45%|████▍     | 371/827 [25:18<30:07,  3.96s/it][A
 45%|████▍     | 372/827 [25:22<29:41,  3.91s/it][A
 45%|████▌     | 373/827 [25:26<29:49,  3.94s/it][A
 45%|████▌     | 374/827 [25:33<37:16,  4.94s/it][A
 45%|████▌     | 375/827 [25:41<45:16,  6.01s/it][A
 45%|████▌     | 376/827 [25:51<54:02,  7.19s/it][A
 46%|████▌     | 377/827 [25:56<48:10,  6.42s/it][A
 46%|████▌     | 378/827 [26:01<45:12,  6.04s/it][A
 46%|████▌     | 379/827 [26:12<55:27,  7.43s/it][A
 46%|████▌     | 380/827 [26:18<52:38,  7.07s/it][A
 46%|████▌     | 381/827 [26:22<45:24,  6.11s/it][A
 46%|████▌     | 382/827 [26:26<40:44,  5.49s/it][A
 46%|████▋     | 383/827 [26:31<39:44,  5.37s/it][A
 46%|████▋     | 384/827 [26:35<36:29,  4.94s/it][A
 47%|████▋     | 385/827 [26:38<32:46,  4.45s/it][A
 47%|████▋     | 386/827 [26:41<29:31,  4.02s/it][A
 47%|████▋     | 387/827 [26:45<29:07,  3.97s/it][A
 47%|████▋     | 388/827 [26:49<29:21,  4.01s/it][A
 47%|████▋     | 389/827 [26:54<29:51,  4.09s/it][A
 47%|████▋     | 390/827 [26:57<28:45,  3.95s/it][A
 47%|████▋     | 391/827 [27:00<27:12,  3.74s/it][A
 47%|████▋     | 392/827 [27:03<25:16,  3.49s/it][A
 48%|████▊     | 393/827 [27:08<28:08,  3.89s/it][A
 48%|████▊     | 394/827 [27:13<30:39,  4.25s/it][A
 48%|████▊     | 395/827 [27:18<32:33,  4.52s/it][A
 48%|████▊     | 396/827 [27:24<34:30,  4.80s/it][A
 48%|████▊     | 397/827 [27:26<29:41,  4.14s/it][A
 48%|████▊     | 398/827 [27:30<28:41,  4.01s/it][A
 48%|████▊     | 399/827 [27:34<27:49,  3.90s/it][A
 48%|████▊     | 400/827 [27:38<27:37,  3.88s/it][A
 48%|████▊     | 401/827 [27:42<29:21,  4.13s/it][A
 49%|████▊     | 402/827 [27:46<27:54,  3.94s/it][A
 49%|████▊     | 403/827 [27:51<31:24,  4.44s/it][A
 49%|████▉     | 404/827 [27:54<27:01,  3.83s/it][A
 49%|████▉     | 405/827 [27:57<25:19,  3.60s/it][A
 49%|████▉     | 406/827 [28:01<25:34,  3.65s/it][A
 49%|████▉     | 407/827 [28:03<23:44,  3.39s/it][A
 49%|████▉     | 408/827 [28:07<23:00,  3.29s/it][A
 49%|████▉     | 409/827 [28:10<22:38,  3.25s/it][A
 50%|████▉     | 410/827 [28:12<21:13,  3.05s/it][A
 50%|████▉     | 411/827 [28:15<20:28,  2.95s/it][A
 50%|████▉     | 412/827 [28:18<20:45,  3.00s/it][A
 50%|████▉     | 413/827 [28:22<22:40,  3.29s/it][A
 50%|█████     | 414/827 [28:27<26:18,  3.82s/it][A
 50%|█████     | 415/827 [28:30<24:32,  3.57s/it][A
 50%|█████     | 416/827 [28:34<24:33,  3.59s/it][A
 50%|█████     | 417/827 [28:40<29:59,  4.39s/it][A
 51%|█████     | 418/827 [28:46<33:48,  4.96s/it][A
 51%|█████     | 419/827 [28:50<30:25,  4.47s/it][A
 51%|█████     | 420/827 [28:55<31:37,  4.66s/it][A
 51%|█████     | 421/827 [28:59<30:54,  4.57s/it][A
 51%|█████     | 422/827 [29:02<27:41,  4.10s/it][A
 51%|█████     | 423/827 [29:05<25:22,  3.77s/it][A
 51%|█████▏    | 424/827 [29:09<24:44,  3.68s/it][A
 51%|█████▏    | 425/827 [29:12<25:09,  3.76s/it][A
 52%|█████▏    | 426/827 [29:19<30:04,  4.50s/it][A
 52%|█████▏    | 427/827 [29:23<29:51,  4.48s/it][A
 52%|█████▏    | 428/827 [29:29<33:11,  4.99s/it][A
 52%|█████▏    | 429/827 [29:34<31:29,  4.75s/it][A
 52%|█████▏    | 430/827 [29:37<29:07,  4.40s/it][A
 52%|█████▏    | 431/827 [29:42<29:49,  4.52s/it][A
 52%|█████▏    | 432/827 [29:47<31:42,  4.82s/it][A
 52%|█████▏    | 433/827 [29:56<38:46,  5.90s/it][A
 52%|█████▏    | 434/827 [30:02<39:18,  6.00s/it][A
 53%|█████▎    | 435/827 [30:05<33:10,  5.08s/it][A
 53%|█████▎    | 436/827 [30:09<30:41,  4.71s/it][A
 53%|█████▎    | 437/827 [30:14<31:18,  4.82s/it][A
 53%|█████▎    | 438/827 [30:17<28:19,  4.37s/it][A
 53%|█████▎    | 439/827 [30:20<25:58,  4.02s/it][A
 53%|█████▎    | 440/827 [30:25<26:55,  4.18s/it][A
 53%|█████▎    | 441/827 [30:28<24:51,  3.86s/it][A
 53%|█████▎    | 442/827 [30:31<22:20,  3.48s/it][A
 54%|█████▎    | 443/827 [30:34<22:15,  3.48s/it][A
 54%|█████▎    | 444/827 [30:37<21:10,  3.32s/it][A
 54%|█████▍    | 445/827 [30:43<26:44,  4.20s/it][A
 54%|█████▍    | 446/827 [30:47<24:40,  3.88s/it][A
 54%|█████▍    | 447/827 [30:51<26:24,  4.17s/it][A
 54%|█████▍    | 448/827 [30:55<26:06,  4.13s/it][A
 54%|█████▍    | 449/827 [31:03<31:44,  5.04s/it][A
 54%|█████▍    | 450/827 [31:09<33:48,  5.38s/it][A
 55%|█████▍    | 451/827 [31:12<30:03,  4.80s/it][A
 55%|█████▍    | 452/827 [31:17<29:22,  4.70s/it][A
 55%|█████▍    | 453/827 [31:23<33:06,  5.31s/it][A
 55%|█████▍    | 454/827 [31:27<29:52,  4.80s/it][A
 55%|█████▌    | 455/827 [31:30<27:01,  4.36s/it][A
 55%|█████▌    | 456/827 [31:37<31:12,  5.05s/it][A
 55%|█████▌    | 457/827 [31:42<30:12,  4.90s/it][A
 55%|█████▌    | 458/827 [31:46<28:22,  4.61s/it][A
 56%|█████▌    | 459/827 [31:50<28:43,  4.68s/it][A
 56%|█████▌    | 460/827 [31:55<29:26,  4.81s/it][A
 56%|█████▌    | 461/827 [32:04<35:23,  5.80s/it][A
 56%|█████▌    | 462/827 [32:09<34:56,  5.74s/it][A
 56%|█████▌    | 463/827 [32:12<30:02,  4.95s/it][A
 56%|█████▌    | 464/827 [32:16<28:33,  4.72s/it][A
 56%|█████▌    | 465/827 [32:22<29:26,  4.88s/it][A
 56%|█████▋    | 466/827 [32:25<26:49,  4.46s/it][A
 56%|█████▋    | 467/827 [32:29<26:23,  4.40s/it][A
 57%|█████▋    | 468/827 [32:33<24:52,  4.16s/it][A
 57%|█████▋    | 469/827 [32:38<26:19,  4.41s/it][A
 57%|█████▋    | 470/827 [32:41<24:04,  4.05s/it][A
 57%|█████▋    | 471/827 [32:45<23:12,  3.91s/it][A
 57%|█████▋    | 472/827 [32:51<27:30,  4.65s/it][A
 57%|█████▋    | 473/827 [32:56<28:03,  4.76s/it][A
 57%|█████▋    | 474/827 [33:01<27:23,  4.66s/it][A
 57%|█████▋    | 475/827 [33:04<24:49,  4.23s/it][A
 58%|█████▊    | 476/827 [33:06<21:21,  3.65s/it][A
 58%|█████▊    | 477/827 [33:09<18:57,  3.25s/it][A
 58%|█████▊    | 478/827 [33:12<18:38,  3.20s/it][A
 58%|█████▊    | 479/827 [33:17<23:02,  3.97s/it][A
 58%|█████▊    | 480/827 [33:21<22:24,  3.87s/it][A
 58%|█████▊    | 481/827 [33:27<25:21,  4.40s/it][A
 58%|█████▊    | 482/827 [33:30<23:09,  4.03s/it][A
 58%|█████▊    | 483/827 [33:32<20:07,  3.51s/it][A
 59%|█████▊    | 484/827 [33:35<19:12,  3.36s/it][A
 59%|█████▊    | 485/827 [33:38<18:13,  3.20s/it][A
 59%|█████▉    | 486/827 [33:41<18:26,  3.25s/it][A
 59%|█████▉    | 487/827 [33:45<18:55,  3.34s/it][A
 59%|█████▉    | 488/827 [33:50<21:17,  3.77s/it][A
 59%|█████▉    | 489/827 [33:54<22:24,  3.98s/it][A
 59%|█████▉    | 490/827 [33:59<24:31,  4.37s/it][A
 59%|█████▉    | 491/827 [34:02<21:55,  3.92s/it][A
 59%|█████▉    | 492/827 [34:06<20:53,  3.74s/it][A
 60%|█████▉    | 493/827 [34:09<20:07,  3.62s/it][A
 60%|█████▉    | 494/827 [34:12<20:04,  3.62s/it][A
 60%|█████▉    | 495/827 [34:17<21:12,  3.83s/it][A
 60%|█████▉    | 496/827 [34:21<21:22,  3.87s/it][A
 60%|██████    | 497/827 [34:24<19:26,  3.54s/it][A
 60%|██████    | 498/827 [34:27<18:43,  3.41s/it][A
 60%|██████    | 499/827 [34:31<20:29,  3.75s/it][A
 60%|██████    | 500/827 [34:35<20:07,  3.69s/it][A
 61%|██████    | 501/827 [34:37<18:19,  3.37s/it][A
 61%|██████    | 502/827 [34:41<18:55,  3.49s/it][A
 61%|██████    | 503/827 [34:47<22:22,  4.14s/it][A
 61%|██████    | 504/827 [34:50<20:48,  3.86s/it][A
 61%|██████    | 505/827 [34:54<20:23,  3.80s/it][A
 61%|██████    | 506/827 [34:57<20:02,  3.74s/it][A
 61%|██████▏   | 507/827 [35:02<21:04,  3.95s/it][A
 61%|██████▏   | 508/827 [35:10<28:04,  5.28s/it][A
 62%|██████▏   | 509/827 [35:13<23:26,  4.42s/it][A
 62%|██████▏   | 510/827 [35:17<22:42,  4.30s/it][A
 62%|██████▏   | 511/827 [35:20<20:49,  3.95s/it][A
 62%|██████▏   | 512/827 [35:22<18:39,  3.55s/it][A
 62%|██████▏   | 513/827 [35:26<18:29,  3.53s/it][A
 62%|██████▏   | 514/827 [35:30<19:05,  3.66s/it][A
 62%|██████▏   | 515/827 [35:33<17:53,  3.44s/it][A
 62%|██████▏   | 516/827 [35:36<18:12,  3.51s/it][A
 63%|██████▎   | 517/827 [35:40<18:07,  3.51s/it][A
 63%|██████▎   | 518/827 [35:43<16:49,  3.27s/it][A
 63%|██████▎   | 519/827 [35:46<16:16,  3.17s/it][A
 63%|██████▎   | 520/827 [35:48<15:38,  3.06s/it][A
 63%|██████▎   | 521/827 [35:53<17:47,  3.49s/it][A
 63%|██████▎   | 522/827 [35:59<21:24,  4.21s/it][A
 63%|██████▎   | 523/827 [36:02<19:27,  3.84s/it][A
 63%|██████▎   | 524/827 [36:05<19:22,  3.84s/it][A
 63%|██████▎   | 525/827 [36:11<21:38,  4.30s/it][A
 64%|██████▎   | 526/827 [36:18<25:32,  5.09s/it][A
 64%|██████▎   | 527/827 [36:22<23:45,  4.75s/it][A
 64%|██████▍   | 528/827 [36:27<24:01,  4.82s/it][A
 64%|██████▍   | 529/827 [36:31<23:39,  4.76s/it][A
 64%|██████▍   | 530/827 [36:37<24:52,  5.03s/it][A
 64%|██████▍   | 531/827 [36:40<21:22,  4.33s/it][A
 64%|██████▍   | 532/827 [36:44<20:39,  4.20s/it][A
 64%|██████▍   | 533/827 [36:48<21:19,  4.35s/it][A
 65%|██████▍   | 534/827 [36:55<24:10,  4.95s/it][A
 65%|██████▍   | 535/827 [37:01<26:10,  5.38s/it][A
 65%|██████▍   | 536/827 [37:05<23:23,  4.82s/it][A
 65%|██████▍   | 537/827 [37:07<20:19,  4.21s/it][A
 65%|██████▌   | 538/827 [37:10<18:30,  3.84s/it][A
 65%|██████▌   | 539/827 [37:13<17:23,  3.62s/it][A
 65%|██████▌   | 540/827 [37:17<17:11,  3.59s/it][A
 65%|██████▌   | 541/827 [37:21<17:43,  3.72s/it][A
 66%|██████▌   | 542/827 [37:25<18:20,  3.86s/it][A
 66%|██████▌   | 543/827 [37:30<19:28,  4.11s/it][A
 66%|██████▌   | 544/827 [37:35<20:22,  4.32s/it][A
 66%|██████▌   | 545/827 [37:39<20:27,  4.35s/it][A
 66%|██████▌   | 546/827 [37:43<20:18,  4.34s/it][A
 66%|██████▌   | 547/827 [37:48<21:06,  4.52s/it][A
 66%|██████▋   | 548/827 [37:51<18:26,  3.97s/it][A
 66%|██████▋   | 549/827 [37:53<16:13,  3.50s/it][A
 67%|██████▋   | 550/827 [37:57<16:40,  3.61s/it][A
 67%|██████▋   | 551/827 [38:02<17:31,  3.81s/it][A
 67%|██████▋   | 552/827 [38:05<17:19,  3.78s/it][A
 67%|██████▋   | 553/827 [38:09<17:28,  3.83s/it][A
 67%|██████▋   | 554/827 [38:12<15:26,  3.39s/it][A
 67%|██████▋   | 555/827 [38:15<14:45,  3.26s/it][A
 67%|██████▋   | 556/827 [38:18<14:55,  3.31s/it][A
 67%|██████▋   | 557/827 [38:21<13:49,  3.07s/it][A
 67%|██████▋   | 558/827 [38:24<14:16,  3.18s/it][A
 68%|██████▊   | 559/827 [38:29<16:16,  3.65s/it][A
 68%|██████▊   | 560/827 [38:32<15:55,  3.58s/it][A
 68%|██████▊   | 561/827 [38:36<15:48,  3.57s/it][A
 68%|██████▊   | 562/827 [38:40<16:15,  3.68s/it][A
 68%|██████▊   | 563/827 [38:43<15:48,  3.59s/it][A
 68%|██████▊   | 564/827 [38:47<15:39,  3.57s/it][A
 68%|██████▊   | 565/827 [38:51<16:39,  3.81s/it][A
 68%|██████▊   | 566/827 [38:56<18:46,  4.32s/it][A
 69%|██████▊   | 567/827 [38:59<16:53,  3.90s/it][A
 69%|██████▊   | 568/827 [39:03<16:46,  3.89s/it][A
 69%|██████▉   | 569/827 [39:06<15:56,  3.71s/it][A
 69%|██████▉   | 570/827 [39:10<15:32,  3.63s/it][A
 69%|██████▉   | 571/827 [39:15<18:00,  4.22s/it][A
 69%|██████▉   | 572/827 [39:21<20:00,  4.71s/it][A
 69%|██████▉   | 573/827 [39:27<21:42,  5.13s/it][A
 69%|██████▉   | 574/827 [39:32<20:16,  4.81s/it][A
 70%|██████▉   | 575/827 [39:37<21:23,  5.09s/it][A
 70%|██████▉   | 576/827 [39:41<20:04,  4.80s/it][A
 70%|██████▉   | 577/827 [39:44<17:36,  4.23s/it][A
 70%|██████▉   | 578/827 [39:48<16:54,  4.08s/it][A
 70%|███████   | 579/827 [39:51<15:46,  3.82s/it][A
 70%|███████   | 580/827 [39:55<15:49,  3.85s/it][A
 70%|███████   | 581/827 [40:03<20:39,  5.04s/it][A
 70%|███████   | 582/827 [40:08<20:19,  4.98s/it][A
 70%|███████   | 583/827 [40:11<17:47,  4.38s/it][A
 71%|███████   | 584/827 [40:14<16:28,  4.07s/it][A
 71%|███████   | 585/827 [40:17<15:05,  3.74s/it][A
 71%|███████   | 586/827 [40:21<15:18,  3.81s/it][A
 71%|███████   | 587/827 [40:27<18:12,  4.55s/it][A
 71%|███████   | 588/827 [40:38<24:50,  6.24s/it][A
 71%|███████   | 589/827 [40:42<23:11,  5.85s/it][A
 71%|███████▏  | 590/827 [40:48<22:23,  5.67s/it][A
 71%|███████▏  | 591/827 [40:56<25:22,  6.45s/it][A
 72%|███████▏  | 592/827 [41:00<21:55,  5.60s/it][A
 72%|███████▏  | 593/827 [41:03<19:04,  4.89s/it][A
 72%|███████▏  | 594/827 [41:06<17:07,  4.41s/it][A
 72%|███████▏  | 595/827 [41:09<15:39,  4.05s/it][A
 72%|███████▏  | 596/827 [41:15<17:57,  4.67s/it][A
 72%|███████▏  | 597/827 [41:23<20:58,  5.47s/it][A
 72%|███████▏  | 598/827 [41:26<18:14,  4.78s/it][A
 72%|███████▏  | 599/827 [41:29<16:30,  4.34s/it][A
 73%|███████▎  | 600/827 [41:33<15:58,  4.22s/it][A
 73%|███████▎  | 601/827 [41:38<16:12,  4.30s/it][A
 73%|███████▎  | 602/827 [41:42<16:27,  4.39s/it][A
 73%|███████▎  | 603/827 [41:45<14:46,  3.96s/it][A
 73%|███████▎  | 604/827 [41:49<14:25,  3.88s/it][A
 73%|███████▎  | 605/827 [41:52<13:26,  3.63s/it][A
 73%|███████▎  | 606/827 [41:55<13:09,  3.57s/it][A
 73%|███████▎  | 607/827 [42:00<13:59,  3.82s/it][A
 74%|███████▎  | 608/827 [42:03<13:02,  3.57s/it][A
 74%|███████▎  | 609/827 [42:07<13:37,  3.75s/it][A
 74%|███████▍  | 610/827 [42:15<18:40,  5.16s/it][A
 74%|███████▍  | 611/827 [42:19<17:02,  4.73s/it][A
 74%|███████▍  | 612/827 [42:23<16:23,  4.57s/it][A
 74%|███████▍  | 613/827 [42:28<16:04,  4.51s/it][A
 74%|███████▍  | 614/827 [42:33<16:26,  4.63s/it][A
 74%|███████▍  | 615/827 [42:36<15:13,  4.31s/it][A
 74%|███████▍  | 616/827 [42:39<14:01,  3.99s/it][A
 75%|███████▍  | 617/827 [42:43<13:54,  3.97s/it][A
 75%|███████▍  | 618/827 [42:51<18:05,  5.19s/it][A
 75%|███████▍  | 619/827 [42:55<16:07,  4.65s/it][A
 75%|███████▍  | 620/827 [42:58<15:03,  4.36s/it][A
 75%|███████▌  | 621/827 [43:02<13:40,  3.99s/it][A
 75%|███████▌  | 622/827 [43:05<12:45,  3.73s/it][A
 75%|███████▌  | 623/827 [43:08<11:46,  3.46s/it][A
 75%|███████▌  | 624/827 [43:12<13:04,  3.87s/it][A
 76%|███████▌  | 625/827 [43:16<13:13,  3.93s/it][A
 76%|███████▌  | 626/827 [43:20<13:03,  3.90s/it][A
 76%|███████▌  | 627/827 [43:24<12:49,  3.85s/it][A
 76%|███████▌  | 628/827 [43:27<11:51,  3.57s/it][A
 76%|███████▌  | 629/827 [43:30<11:06,  3.37s/it][A
 76%|███████▌  | 630/827 [43:33<10:46,  3.28s/it][A
 76%|███████▋  | 631/827 [43:37<11:57,  3.66s/it][A
 76%|███████▋  | 632/827 [43:43<14:00,  4.31s/it][A
 77%|███████▋  | 633/827 [43:47<13:26,  4.16s/it][A
 77%|███████▋  | 634/827 [43:56<17:31,  5.45s/it][A
 77%|███████▋  | 635/827 [43:59<15:15,  4.77s/it][A
 77%|███████▋  | 636/827 [44:02<14:09,  4.45s/it][A
 77%|███████▋  | 637/827 [44:07<14:28,  4.57s/it][A
 77%|███████▋  | 638/827 [44:13<15:12,  4.83s/it][A
 77%|███████▋  | 639/827 [44:16<13:44,  4.38s/it][A
 77%|███████▋  | 640/827 [44:19<12:03,  3.87s/it][A
 78%|███████▊  | 641/827 [44:23<12:13,  3.94s/it][A
 78%|███████▊  | 642/827 [44:27<12:10,  3.95s/it][A
 78%|███████▊  | 643/827 [44:31<12:01,  3.92s/it][A
 78%|███████▊  | 644/827 [44:36<13:25,  4.40s/it][A
 78%|███████▊  | 645/827 [44:40<12:46,  4.21s/it][A
 78%|███████▊  | 646/827 [44:43<11:47,  3.91s/it][A
 78%|███████▊  | 647/827 [44:46<10:59,  3.66s/it][A
 78%|███████▊  | 648/827 [44:50<10:55,  3.66s/it][A
 78%|███████▊  | 649/827 [44:54<11:08,  3.75s/it][A
 79%|███████▊  | 650/827 [44:57<10:16,  3.48s/it][A
 79%|███████▊  | 651/827 [45:00<09:57,  3.40s/it][A
 79%|███████▉  | 652/827 [45:03<09:30,  3.26s/it][A
 79%|███████▉  | 653/827 [45:06<09:30,  3.28s/it][A
 79%|███████▉  | 654/827 [45:11<10:59,  3.81s/it][A
 79%|███████▉  | 655/827 [45:18<13:49,  4.82s/it][A
 79%|███████▉  | 656/827 [45:21<11:38,  4.08s/it][A
 79%|███████▉  | 657/827 [45:24<10:48,  3.82s/it][A
 80%|███████▉  | 658/827 [45:27<10:30,  3.73s/it][A
 80%|███████▉  | 659/827 [45:36<14:33,  5.20s/it][A
 80%|███████▉  | 660/827 [45:43<15:30,  5.57s/it][A
 80%|███████▉  | 661/827 [45:49<15:46,  5.70s/it][A
 80%|████████  | 662/827 [45:52<14:08,  5.14s/it][A
 80%|████████  | 663/827 [45:56<12:37,  4.62s/it][A
 80%|████████  | 664/827 [46:01<13:26,  4.95s/it][A
 80%|████████  | 665/827 [46:05<12:13,  4.53s/it][A
 81%|████████  | 666/827 [46:10<12:28,  4.65s/it][A
 81%|████████  | 667/827 [46:13<11:12,  4.20s/it][A
 81%|████████  | 668/827 [46:17<11:03,  4.17s/it][A
 81%|████████  | 669/827 [46:20<10:03,  3.82s/it][A
 81%|████████  | 670/827 [46:23<09:14,  3.53s/it][A
 81%|████████  | 671/827 [46:26<08:55,  3.44s/it][A
 81%|████████▏ | 672/827 [46:31<09:30,  3.68s/it][A
 81%|████████▏ | 673/827 [46:34<08:54,  3.47s/it][A
 81%|████████▏ | 674/827 [46:37<08:49,  3.46s/it][A
 82%|████████▏ | 675/827 [46:41<08:50,  3.49s/it][A
 82%|████████▏ | 676/827 [46:45<09:08,  3.63s/it][A
 82%|████████▏ | 677/827 [46:50<10:14,  4.10s/it][A
 82%|████████▏ | 678/827 [46:54<10:18,  4.15s/it][A
 82%|████████▏ | 679/827 [46:59<11:09,  4.52s/it][A
 82%|████████▏ | 680/827 [47:02<09:46,  3.99s/it][A
 82%|████████▏ | 681/827 [47:06<09:43,  3.99s/it][A
 82%|████████▏ | 682/827 [47:13<11:45,  4.86s/it][A
 83%|████████▎ | 683/827 [47:16<10:16,  4.28s/it][A
 83%|████████▎ | 684/827 [47:19<09:22,  3.94s/it][A
 83%|████████▎ | 685/827 [47:22<08:15,  3.49s/it][A
 83%|████████▎ | 686/827 [47:26<08:57,  3.81s/it][A
 83%|████████▎ | 687/827 [47:35<12:13,  5.24s/it][A
 83%|████████▎ | 688/827 [47:39<11:34,  4.99s/it][A
 83%|████████▎ | 689/827 [47:45<12:07,  5.27s/it][A
 83%|████████▎ | 690/827 [47:48<10:49,  4.74s/it][A
 84%|████████▎ | 691/827 [47:52<09:35,  4.23s/it][A
 84%|████████▎ | 692/827 [47:56<09:30,  4.23s/it][A
 84%|████████▍ | 693/827 [47:59<08:43,  3.91s/it][A
 84%|████████▍ | 694/827 [48:02<08:12,  3.71s/it][A
 84%|████████▍ | 695/827 [48:06<08:12,  3.73s/it][A
 84%|████████▍ | 696/827 [48:11<09:16,  4.25s/it][A
 84%|████████▍ | 697/827 [48:15<08:38,  3.99s/it][A
 84%|████████▍ | 698/827 [48:18<07:54,  3.68s/it][A
 85%|████████▍ | 699/827 [48:20<07:11,  3.37s/it][A
 85%|████████▍ | 700/827 [48:26<08:50,  4.17s/it][A
 85%|████████▍ | 701/827 [48:34<10:47,  5.14s/it][A
 85%|████████▍ | 702/827 [48:38<10:01,  4.81s/it][A
 85%|████████▌ | 703/827 [48:42<09:42,  4.70s/it][A
 85%|████████▌ | 704/827 [48:46<08:48,  4.30s/it][A
 85%|████████▌ | 705/827 [48:48<07:34,  3.73s/it][A
 85%|████████▌ | 706/827 [48:51<06:52,  3.41s/it][A
 85%|████████▌ | 707/827 [48:54<06:50,  3.42s/it][A
 86%|████████▌ | 708/827 [48:58<06:58,  3.52s/it][A
 86%|████████▌ | 709/827 [49:02<07:19,  3.73s/it][A
 86%|████████▌ | 710/827 [49:05<06:54,  3.54s/it][A
 86%|████████▌ | 711/827 [49:09<06:56,  3.59s/it][A
 86%|████████▌ | 712/827 [49:12<06:37,  3.45s/it][A
 86%|████████▌ | 713/827 [49:15<06:28,  3.41s/it][A
 86%|████████▋ | 714/827 [49:19<06:35,  3.50s/it][A
 86%|████████▋ | 715/827 [49:25<07:44,  4.14s/it][A
 87%|████████▋ | 716/827 [49:28<07:06,  3.84s/it][A
 87%|████████▋ | 717/827 [49:34<08:05,  4.41s/it][A
 87%|████████▋ | 718/827 [49:40<09:00,  4.95s/it][A
 87%|████████▋ | 719/827 [49:43<07:53,  4.39s/it][A
 87%|████████▋ | 720/827 [49:46<07:22,  4.14s/it][A
 87%|████████▋ | 721/827 [49:50<06:49,  3.86s/it][A
 87%|████████▋ | 722/827 [49:53<06:39,  3.81s/it][A
 87%|████████▋ | 723/827 [49:57<06:24,  3.70s/it][A
 88%|████████▊ | 724/827 [49:59<05:45,  3.36s/it][A
 88%|████████▊ | 725/827 [50:03<05:58,  3.52s/it][A
 88%|████████▊ | 726/827 [50:10<07:47,  4.63s/it][A
 88%|████████▊ | 727/827 [50:15<07:39,  4.60s/it][A
 88%|████████▊ | 728/827 [50:18<06:59,  4.24s/it][A
 88%|████████▊ | 729/827 [50:21<05:53,  3.61s/it][A
 88%|████████▊ | 730/827 [50:25<06:04,  3.75s/it][A
 88%|████████▊ | 731/827 [50:30<06:41,  4.18s/it][A
 89%|████████▊ | 732/827 [50:35<07:13,  4.56s/it][A
 89%|████████▊ | 733/827 [50:40<07:26,  4.75s/it][A
 89%|████████▉ | 734/827 [50:43<06:13,  4.02s/it][A
 89%|████████▉ | 735/827 [50:46<05:57,  3.89s/it][A
 89%|████████▉ | 736/827 [50:52<06:29,  4.28s/it][A
 89%|████████▉ | 737/827 [50:57<07:09,  4.77s/it][A
 89%|████████▉ | 738/827 [51:01<06:19,  4.27s/it][A
 89%|████████▉ | 739/827 [51:05<06:12,  4.23s/it][A
 89%|████████▉ | 740/827 [51:11<07:06,  4.91s/it][A
 90%|████████▉ | 741/827 [51:15<06:29,  4.53s/it][A
 90%|████████▉ | 742/827 [51:18<06:00,  4.24s/it][A
 90%|████████▉ | 743/827 [51:24<06:42,  4.79s/it][A
 90%|████████▉ | 744/827 [51:29<06:25,  4.65s/it][A
 90%|█████████ | 745/827 [51:33<06:19,  4.62s/it][A
 90%|█████████ | 746/827 [51:38<06:23,  4.74s/it][A
 90%|█████████ | 747/827 [51:45<07:05,  5.32s/it][A
 90%|█████████ | 748/827 [51:48<06:08,  4.66s/it][A
 91%|█████████ | 749/827 [51:52<05:41,  4.38s/it][A
 91%|█████████ | 750/827 [51:56<05:42,  4.45s/it][A
 91%|█████████ | 751/827 [52:00<05:07,  4.05s/it][A
 91%|█████████ | 752/827 [52:03<04:39,  3.73s/it][A
 91%|█████████ | 753/827 [52:05<04:15,  3.45s/it][A
 91%|█████████ | 754/827 [52:08<03:56,  3.23s/it][A
 91%|█████████▏| 755/827 [52:12<03:57,  3.29s/it][A
 91%|█████████▏| 756/827 [52:18<04:52,  4.12s/it][A
 92%|█████████▏| 757/827 [52:21<04:27,  3.83s/it][A
 92%|█████████▏| 758/827 [52:26<04:51,  4.22s/it][A
 92%|█████████▏| 759/827 [52:29<04:21,  3.85s/it][A
 92%|█████████▏| 760/827 [52:33<04:24,  3.95s/it][A
 92%|█████████▏| 761/827 [52:42<05:54,  5.37s/it][A
 92%|█████████▏| 762/827 [52:47<05:42,  5.28s/it][A
 92%|█████████▏| 763/827 [52:55<06:31,  6.12s/it][A
 92%|█████████▏| 764/827 [52:58<05:26,  5.19s/it][A
 93%|█████████▎| 765/827 [53:01<04:45,  4.60s/it][A
 93%|█████████▎| 766/827 [53:04<04:13,  4.16s/it][A
 93%|█████████▎| 767/827 [53:08<04:02,  4.04s/it][A
 93%|█████████▎| 768/827 [53:11<03:40,  3.73s/it][A
 93%|█████████▎| 769/827 [53:17<04:09,  4.31s/it][A
 93%|█████████▎| 770/827 [53:21<03:57,  4.16s/it][A
 93%|█████████▎| 771/827 [53:24<03:48,  4.08s/it][A
 93%|█████████▎| 772/827 [53:28<03:33,  3.88s/it][A
 93%|█████████▎| 773/827 [53:32<03:33,  3.95s/it][A
 94%|█████████▎| 774/827 [53:35<03:22,  3.83s/it][A
 94%|█████████▎| 775/827 [53:39<03:15,  3.77s/it][A
 94%|█████████▍| 776/827 [53:43<03:10,  3.74s/it][A
 94%|█████████▍| 777/827 [53:46<03:05,  3.71s/it][A
 94%|█████████▍| 778/827 [53:50<03:02,  3.73s/it][A
 94%|█████████▍| 779/827 [53:53<02:46,  3.46s/it][A
 94%|█████████▍| 780/827 [53:56<02:36,  3.34s/it][A
 94%|█████████▍| 781/827 [53:59<02:28,  3.23s/it][A
 95%|█████████▍| 782/827 [54:02<02:22,  3.17s/it][A
 95%|█████████▍| 783/827 [54:06<02:25,  3.30s/it][A
 95%|█████████▍| 784/827 [54:09<02:17,  3.19s/it][A
 95%|█████████▍| 785/827 [54:12<02:21,  3.38s/it][A
 95%|█████████▌| 786/827 [54:16<02:16,  3.33s/it][A
 95%|█████████▌| 787/827 [54:21<02:40,  4.02s/it][A
 95%|█████████▌| 788/827 [54:25<02:35,  3.98s/it][A
 95%|█████████▌| 789/827 [54:29<02:35,  4.08s/it][A
 96%|█████████▌| 790/827 [54:33<02:26,  3.97s/it][A
 96%|█████████▌| 791/827 [54:36<02:14,  3.75s/it][A
 96%|█████████▌| 792/827 [54:39<01:59,  3.42s/it][A
 96%|█████████▌| 793/827 [54:43<01:58,  3.47s/it][A
 96%|█████████▌| 794/827 [54:50<02:31,  4.60s/it][A
 96%|█████████▌| 795/827 [54:56<02:39,  4.98s/it][A
 96%|█████████▋| 796/827 [55:02<02:41,  5.22s/it][A
 96%|█████████▋| 797/827 [55:04<02:14,  4.49s/it][A
 96%|█████████▋| 798/827 [55:07<01:57,  4.06s/it][A
 97%|█████████▋| 799/827 [55:10<01:39,  3.55s/it][A
 97%|█████████▋| 800/827 [55:15<01:50,  4.08s/it][A
 97%|█████████▋| 801/827 [55:25<02:31,  5.83s/it][A
 97%|█████████▋| 802/827 [55:31<02:23,  5.75s/it][A
 97%|█████████▋| 803/827 [55:36<02:13,  5.55s/it][A
 97%|█████████▋| 804/827 [55:42<02:11,  5.71s/it][A
 97%|█████████▋| 805/827 [55:45<01:48,  4.93s/it][A
 97%|█████████▋| 806/827 [55:47<01:28,  4.24s/it][A
 98%|█████████▊| 807/827 [55:51<01:19,  3.97s/it][A
 98%|█████████▊| 808/827 [55:55<01:14,  3.90s/it][A
 98%|█████████▊| 809/827 [55:58<01:06,  3.70s/it][A
 98%|█████████▊| 810/827 [56:01<01:01,  3.63s/it][A
 98%|█████████▊| 811/827 [56:05<00:58,  3.65s/it][A
 98%|█████████▊| 812/827 [56:08<00:53,  3.54s/it][A
 98%|█████████▊| 813/827 [56:14<00:59,  4.28s/it][A
 98%|█████████▊| 814/827 [56:21<01:05,  5.01s/it][A
 99%|█████████▊| 815/827 [56:24<00:53,  4.47s/it][A
 99%|█████████▊| 816/827 [56:29<00:50,  4.58s/it][A
 99%|█████████▉| 817/827 [56:32<00:41,  4.10s/it][A
 99%|█████████▉| 818/827 [56:37<00:38,  4.33s/it][A
 99%|█████████▉| 819/827 [56:43<00:38,  4.81s/it][A
 99%|█████████▉| 820/827 [56:48<00:33,  4.80s/it][A
 99%|█████████▉| 821/827 [56:51<00:26,  4.44s/it][A
 99%|█████████▉| 822/827 [56:55<00:21,  4.23s/it][A
100%|█████████▉| 823/827 [56:58<00:15,  3.79s/it][A
100%|█████████▉| 824/827 [57:00<00:10,  3.41s/it][A
100%|█████████▉| 825/827 [57:04<00:07,  3.50s/it][A
100%|█████████▉| 826/827 [57:11<00:04,  4.57s/it][A
100%|██████████| 827/827 [57:14<00:00,  4.05s/it][A                                                       
                                                 [A{'eval_loss': 0.6858135461807251, 'eval_runtime': 3439.2061, 'eval_samples_per_second': 0.961, 'eval_steps_per_second': 0.24, 'epoch': 0.43}
 22%|██▏       | 200/930 [8:49:10<31:18:18, 154.38s/it]
100%|██████████| 827/827 [57:14<00:00,  4.05s/it][A
                                                 [A[INFO|trainer.py:4309] 2025-12-29 04:29:26,441 >> Saving model checkpoint to saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-200
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in Video-R1/Video-R1-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2421] 2025-12-29 04:29:26,550 >> chat template saved in saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-200/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-29 04:29:26,553 >> tokenizer config file saved in saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-29 04:29:26,555 >> Special tokens file saved in saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-200/special_tokens_map.json
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once
[2025-12-29 04:29:27,474] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!
[2025-12-29 04:29:27,504] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-200/global_step200/mp_rank_00_model_states.pt
[2025-12-29 04:29:27,504] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-200/global_step200/mp_rank_00_model_states.pt...
[2025-12-29 04:29:27,573] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-200/global_step200/mp_rank_00_model_states.pt.
[2025-12-29 04:29:27,577] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-12-29 04:29:27,577] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-200/global_step200/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2025-12-29 04:29:27,577] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-200/global_step200/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2025-12-29 04:29:27,577] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-200/global_step200/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2025-12-29 04:29:27,744] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-200/global_step200/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2025-12-29 04:29:27,745] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-200/global_step200/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2025-12-29 04:29:27,745] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2025-12-29 04:29:27,749] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-12-29 04:29:27,751] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-200/global_step200/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2025-12-29 04:29:27,751] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-200/global_step200/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2025-12-29 04:29:27,751] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2025-12-29 04:29:27,753] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-200/global_step200/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2025-12-29 04:29:27,754] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-200/global_step200/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2025-12-29 04:29:27,754] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2025-12-29 04:29:27,757] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-12-29 04:29:27,757] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[INFO|image_processing_base.py:253] 2025-12-29 04:29:27,775 >> Image processor saved in saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-200/preprocessor_config.json
[INFO|tokenization_utils_base.py:2421] 2025-12-29 04:29:27,779 >> chat template saved in saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-200/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-29 04:29:27,782 >> tokenizer config file saved in saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-29 04:29:27,784 >> Special tokens file saved in saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-200/special_tokens_map.json
[INFO|video_processing_utils.py:600] 2025-12-29 04:29:27,987 >> Video processor saved in saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-200/video_preprocessor_config.json
[INFO|processing_utils.py:814] 2025-12-29 04:29:27,989 >> chat template saved in saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-200/chat_template.jinja
 22%|██▏       | 201/930 [8:51:14<238:20:57, 1177.03s/it] 22%|██▏       | 202/930 [8:53:25<174:33:59, 863.24s/it]  22%|██▏       | 203/930 [8:55:26<129:23:34, 640.73s/it] 22%|██▏       | 204/930 [8:58:08<100:13:29, 496.98s/it] 22%|██▏       | 205/930 [9:00:46<79:34:55, 395.17s/it]  22%|██▏       | 206/930 [9:03:11<64:26:16, 320.41s/it] 22%|██▏       | 207/930 [9:05:54<54:51:02, 273.12s/it] 22%|██▏       | 208/930 [9:08:18<46:57:59, 234.18s/it] 22%|██▏       | 209/930 [9:10:36<41:07:42, 205.36s/it] 23%|██▎       | 210/930 [9:13:04<37:38:57, 188.25s/it]                                                       {'loss': 0.6733, 'grad_norm': 0.524276614189148, 'learning_rate': 9.533519757128426e-05, 'epoch': 0.45}
 23%|██▎       | 210/930 [9:13:04<37:38:57, 188.25s/it] 23%|██▎       | 211/930 [9:15:36<35:23:47, 177.23s/it] 23%|██▎       | 212/930 [9:18:00<33:23:30, 167.42s/it] 23%|██▎       | 213/930 [9:19:56<30:17:11, 152.07s/it] 23%|██▎       | 214/930 [9:21:50<27:57:39, 140.59s/it] 23%|██▎       | 215/930 [9:24:03<27:26:04, 138.13s/it] 23%|██▎       | 216/930 [9:26:48<29:00:58, 146.30s/it] 23%|██▎       | 217/930 [9:29:50<31:04:57, 156.94s/it] 23%|██▎       | 218/930 [9:32:03<29:38:34, 149.88s/it] 24%|██▎       | 219/930 [9:34:01<27:41:51, 140.24s/it] 24%|██▎       | 220/930 [9:36:24<27:48:35, 141.01s/it]                                                       {'loss': 0.686, 'grad_norm': 0.5333829522132874, 'learning_rate': 9.451192254041758e-05, 'epoch': 0.47}
 24%|██▎       | 220/930 [9:36:24<27:48:35, 141.01s/it] 24%|██▍       | 221/930 [9:39:13<29:28:00, 149.62s/it] 24%|██▍       | 222/930 [9:42:39<32:44:24, 166.48s/it] 24%|██▍       | 223/930 [9:44:44<30:13:17, 153.89s/it] 24%|██▍       | 224/930 [9:47:14<29:59:17, 152.91s/it] 24%|██▍       | 225/930 [9:49:29<28:53:37, 147.54s/it] 24%|██▍       | 226/930 [9:51:45<28:09:38, 144.00s/it] 24%|██▍       | 227/930 [9:54:32<29:26:39, 150.78s/it] 25%|██▍       | 228/930 [9:56:58<29:10:03, 149.58s/it] 25%|██▍       | 229/930 [9:59:00<27:28:51, 141.13s/it] 25%|██▍       | 230/930 [10:00:39<24:59:43, 128.55s/it]                                                        {'loss': 0.6687, 'grad_norm': 0.5247042179107666, 'learning_rate': 9.362594654883185e-05, 'epoch': 0.49}
 25%|██▍       | 230/930 [10:00:39<24:59:43, 128.55s/it] 25%|██▍       | 231/930 [10:02:13<22:57:06, 118.21s/it] 25%|██▍       | 232/930 [10:04:30<23:59:36, 123.75s/it] 25%|██▌       | 233/930 [10:07:19<26:37:25, 137.51s/it] 25%|██▌       | 234/930 [10:09:53<27:29:33, 142.20s/it] 25%|██▌       | 235/930 [10:12:28<28:14:05, 146.25s/it] 25%|██▌       | 236/930 [10:15:08<28:58:58, 150.34s/it] 25%|██▌       | 237/930 [10:17:33<28:38:12, 148.76s/it] 26%|██▌       | 238/930 [10:19:58<28:23:16, 147.68s/it] 26%|██▌       | 239/930 [10:22:08<27:20:06, 142.41s/it] 26%|██▌       | 240/930 [10:25:07<29:22:58, 153.30s/it]                                                        {'loss': 0.678, 'grad_norm': 0.5659500360488892, 'learning_rate': 9.267851761150093e-05, 'epoch': 0.52}
 26%|██▌       | 240/930 [10:25:07<29:22:58, 153.30s/it] 26%|██▌       | 241/930 [10:27:34<28:57:20, 151.29s/it] 26%|██▌       | 242/930 [10:29:39<27:25:06, 143.47s/it] 26%|██▌       | 243/930 [10:32:40<29:32:04, 154.77s/it] 26%|██▌       | 244/930 [10:35:10<29:14:12, 153.43s/it] 26%|██▋       | 245/930 [10:37:47<29:23:09, 154.44s/it] 26%|██▋       | 246/930 [10:40:24<29:29:20, 155.21s/it] 27%|██▋       | 247/930 [10:42:27<27:36:09, 145.49s/it] 27%|██▋       | 248/930 [10:44:43<27:01:48, 142.68s/it] 27%|██▋       | 249/930 [10:47:30<28:21:49, 149.94s/it] 27%|██▋       | 250/930 [10:49:56<28:07:20, 148.88s/it]                                                        {'loss': 0.6728, 'grad_norm': 0.5065827369689941, 'learning_rate': 9.167097030804288e-05, 'epoch': 0.54}
 27%|██▋       | 250/930 [10:49:56<28:07:20, 148.88s/it] 27%|██▋       | 251/930 [10:52:19<27:41:37, 146.83s/it] 27%|██▋       | 252/930 [10:54:14<25:52:39, 137.40s/it] 27%|██▋       | 253/930 [10:56:04<24:17:12, 129.15s/it] 27%|██▋       | 254/930 [10:58:22<24:45:10, 131.82s/it] 27%|██▋       | 255/930 [11:00:22<24:04:26, 128.40s/it] 28%|██▊       | 256/930 [11:02:09<22:50:47, 122.03s/it] 28%|██▊       | 257/930 [11:04:33<24:01:23, 128.50s/it] 28%|██▊       | 258/930 [11:07:28<26:36:06, 142.51s/it] 28%|██▊       | 259/930 [11:09:47<26:19:40, 141.25s/it] 28%|██▊       | 260/930 [11:12:28<27:25:00, 147.31s/it]                                                        {'loss': 0.6814, 'grad_norm': 0.5032489895820618, 'learning_rate': 9.060472390278718e-05, 'epoch': 0.56}
 28%|██▊       | 260/930 [11:12:28<27:25:00, 147.31s/it] 28%|██▊       | 261/930 [11:15:15<28:29:41, 153.34s/it] 28%|██▊       | 262/930 [11:17:26<27:11:07, 146.51s/it] 28%|██▊       | 263/930 [11:19:53<27:08:57, 146.53s/it] 28%|██▊       | 264/930 [11:21:29<24:19:13, 131.46s/it] 28%|██▊       | 265/930 [11:23:31<23:45:37, 128.63s/it] 29%|██▊       | 266/930 [11:26:05<25:08:51, 136.34s/it] 29%|██▊       | 267/930 [11:28:31<25:38:22, 139.22s/it] 29%|██▉       | 268/930 [11:30:54<25:47:44, 140.28s/it] 29%|██▉       | 269/930 [11:32:59<24:53:37, 135.58s/it] 29%|██▉       | 270/930 [11:34:51<23:34:13, 128.57s/it]                                                        {'loss': 0.6678, 'grad_norm': 0.5576427578926086, 'learning_rate': 8.94812803455521e-05, 'epoch': 0.58}
 29%|██▉       | 270/930 [11:34:51<23:34:13, 128.57s/it] 29%|██▉       | 271/930 [11:37:34<25:25:36, 138.90s/it] 29%|██▉       | 272/930 [11:40:42<28:06:09, 153.75s/it] 29%|██▉       | 273/930 [11:42:39<26:02:44, 142.72s/it] 29%|██▉       | 274/930 [11:45:28<27:26:29, 150.59s/it] 30%|██▉       | 275/930 [11:47:09<24:40:38, 135.63s/it] 30%|██▉       | 276/930 [11:48:51<22:47:45, 125.48s/it] 30%|██▉       | 277/930 [11:50:49<22:23:23, 123.44s/it] 30%|██▉       | 278/930 [11:53:38<24:47:53, 136.92s/it] 30%|███       | 279/930 [11:55:41<23:59:50, 132.70s/it] 30%|███       | 280/930 [11:57:34<22:56:10, 127.03s/it]                                                        {'loss': 0.6761, 'grad_norm': 0.5996046662330627, 'learning_rate': 8.83022221559489e-05, 'epoch': 0.6}
 30%|███       | 280/930 [11:57:34<22:56:10, 127.03s/it] 30%|███       | 281/930 [12:00:15<24:41:46, 136.99s/it] 30%|███       | 282/930 [12:02:59<26:07:04, 145.10s/it] 30%|███       | 283/930 [12:05:21<25:57:40, 144.45s/it] 31%|███       | 284/930 [12:08:11<27:15:19, 151.89s/it] 31%|███       | 285/930 [12:10:50<27:36:01, 154.05s/it] 31%|███       | 286/930 [12:12:43<25:21:46, 141.78s/it] 31%|███       | 287/930 [12:14:58<24:58:09, 139.80s/it] 31%|███       | 288/930 [12:17:44<26:18:45, 147.55s/it] 31%|███       | 289/930 [12:19:46<24:54:46, 139.92s/it] 31%|███       | 290/930 [12:22:20<25:38:07, 144.20s/it]                                                        {'loss': 0.66, 'grad_norm': 0.5342867374420166, 'learning_rate': 8.706921019419236e-05, 'epoch': 0.62}
 31%|███       | 290/930 [12:22:20<25:38:07, 144.20s/it] 31%|███▏      | 291/930 [12:23:59<23:11:14, 130.63s/it] 31%|███▏      | 292/930 [12:26:38<24:39:27, 139.13s/it] 32%|███▏      | 293/930 [12:29:09<25:15:30, 142.75s/it] 32%|███▏      | 294/930 [12:31:43<25:48:05, 146.05s/it] 32%|███▏      | 295/930 [12:34:01<25:19:35, 143.58s/it] 32%|███▏      | 296/930 [12:36:41<26:10:24, 148.62s/it] 32%|███▏      | 297/930 [12:39:46<28:03:41, 159.59s/it] 32%|███▏      | 298/930 [12:41:46<25:53:20, 147.47s/it] 32%|███▏      | 299/930 [12:44:10<25:42:37, 146.68s/it] 32%|███▏      | 300/930 [12:46:39<25:45:44, 147.21s/it]                                                        {'loss': 0.6664, 'grad_norm': 0.5543545484542847, 'learning_rate': 8.578398132155845e-05, 'epoch': 0.65}
 32%|███▏      | 300/930 [12:46:39<25:45:44, 147.21s/it] 32%|███▏      | 301/930 [12:48:35<24:05:42, 137.91s/it] 32%|███▏      | 302/930 [12:50:52<24:00:57, 137.67s/it] 33%|███▎      | 303/930 [12:53:37<25:23:03, 145.75s/it] 33%|███▎      | 304/930 [12:55:47<24:30:36, 140.95s/it] 33%|███▎      | 305/930 [12:58:56<26:59:07, 155.44s/it] 33%|███▎      | 306/930 [13:01:26<26:41:03, 153.95s/it] 33%|███▎      | 307/930 [13:03:22<24:38:39, 142.41s/it] 33%|███▎      | 308/930 [13:05:39<24:20:57, 140.93s/it] 33%|███▎      | 309/930 [13:07:59<24:16:36, 140.74s/it] 33%|███▎      | 310/930 [13:10:58<26:11:54, 152.12s/it]                                                        {'loss': 0.6805, 'grad_norm': 0.5113258361816406, 'learning_rate': 8.444834595378434e-05, 'epoch': 0.67}
 33%|███▎      | 310/930 [13:10:58<26:11:54, 152.12s/it] 33%|███▎      | 311/930 [13:14:05<27:58:17, 162.68s/it] 34%|███▎      | 312/930 [13:16:44<27:42:37, 161.42s/it] 34%|███▎      | 313/930 [13:19:13<27:01:24, 157.67s/it] 34%|███▍      | 314/930 [13:21:56<27:15:50, 159.33s/it] 34%|███▍      | 315/930 [13:24:10<25:55:39, 151.77s/it] 34%|███▍      | 316/930 [13:26:43<25:57:45, 152.22s/it] 34%|███▍      | 317/930 [13:28:41<24:08:29, 141.78s/it] 34%|███▍      | 318/930 [13:30:58<23:51:24, 140.33s/it] 34%|███▍      | 319/930 [13:33:36<24:44:01, 145.73s/it] 34%|███▍      | 320/930 [13:35:51<24:09:50, 142.61s/it]                                                        {'loss': 0.6628, 'grad_norm': 0.5882104635238647, 'learning_rate': 8.306418551085706e-05, 'epoch': 0.69}
 34%|███▍      | 320/930 [13:35:52<24:09:50, 142.61s/it] 35%|███▍      | 321/930 [13:37:57<23:14:30, 137.39s/it] 35%|███▍      | 322/930 [13:40:25<23:45:31, 140.68s/it] 35%|███▍      | 323/930 [13:42:39<23:22:25, 138.62s/it] 35%|███▍      | 324/930 [13:44:37<22:18:32, 132.53s/it] 35%|███▍      | 325/930 [13:46:57<22:39:29, 134.82s/it] 35%|███▌      | 326/930 [13:49:27<23:20:40, 139.14s/it] 35%|███▌      | 327/930 [13:51:48<23:25:47, 139.88s/it] 35%|███▌      | 328/930 [13:54:09<23:27:32, 140.29s/it] 35%|███▌      | 329/930 [13:56:31<23:28:31, 140.62s/it] 35%|███▌      | 330/930 [13:58:43<23:00:40, 138.07s/it]                                                        {'loss': 0.6549, 'grad_norm': 0.5405155420303345, 'learning_rate': 8.163344976678342e-05, 'epoch': 0.71}
 35%|███▌      | 330/930 [13:58:43<23:00:40, 138.07s/it] 36%|███▌      | 331/930 [14:01:03<23:05:05, 138.74s/it] 36%|███▌      | 332/930 [14:03:38<23:49:34, 143.44s/it] 36%|███▌      | 333/930 [14:05:51<23:16:54, 140.39s/it] 36%|███▌      | 334/930 [14:08:02<22:46:40, 137.59s/it] 36%|███▌      | 335/930 [14:09:54<21:29:08, 130.00s/it] 36%|███▌      | 336/930 [14:11:46<20:31:50, 124.43s/it] 36%|███▌      | 337/930 [14:14:38<22:51:47, 138.80s/it] 36%|███▋      | 338/930 [14:16:47<22:20:40, 135.88s/it] 36%|███▋      | 339/930 [14:18:55<21:56:11, 133.62s/it] 37%|███▋      | 340/930 [14:21:10<21:57:18, 133.96s/it]                                                        {'loss': 0.6625, 'grad_norm': 0.5454481840133667, 'learning_rate': 8.015815410307398e-05, 'epoch': 0.73}
 37%|███▋      | 340/930 [14:21:10<21:57:18, 133.96s/it] 37%|███▋      | 341/930 [14:23:39<22:39:41, 138.51s/it] 37%|███▋      | 342/930 [14:26:03<22:53:47, 140.18s/it] 37%|███▋      | 343/930 [14:28:20<22:39:58, 139.01s/it] 37%|███▋      | 344/930 [14:31:02<23:44:51, 145.89s/it] 37%|███▋      | 345/930 [14:33:18<23:13:48, 142.95s/it] 37%|███▋      | 346/930 [14:35:41<23:12:44, 143.09s/it] 37%|███▋      | 347/930 [14:37:58<22:51:30, 141.15s/it] 37%|███▋      | 348/930 [14:40:26<23:09:20, 143.23s/it] 38%|███▊      | 349/930 [14:42:22<21:47:35, 135.04s/it] 38%|███▊      | 350/930 [14:44:46<22:12:57, 137.89s/it]                                                        {'loss': 0.6575, 'grad_norm': 0.49048295617103577, 'learning_rate': 7.864037666981037e-05, 'epoch': 0.75}
 38%|███▊      | 350/930 [14:44:46<22:12:57, 137.89s/it] 38%|███▊      | 351/930 [14:47:29<23:21:00, 145.18s/it] 38%|███▊      | 352/930 [14:49:26<21:58:20, 136.85s/it] 38%|███▊      | 353/930 [14:51:59<22:44:00, 141.84s/it] 38%|███▊      | 354/930 [14:54:10<22:08:15, 138.36s/it] 38%|███▊      | 355/930 [14:56:28<22:06:48, 138.45s/it] 38%|███▊      | 356/930 [14:59:12<23:18:10, 146.15s/it] 38%|███▊      | 357/930 [15:01:50<23:47:34, 149.48s/it] 38%|███▊      | 358/930 [15:03:38<21:46:48, 137.08s/it] 39%|███▊      | 359/930 [15:05:28<20:27:44, 129.01s/it] 39%|███▊      | 360/930 [15:07:52<21:08:07, 133.49s/it]                                                        {'loss': 0.6708, 'grad_norm': 0.5421736240386963, 'learning_rate': 7.708225545829446e-05, 'epoch': 0.77}
 39%|███▊      | 360/930 [15:07:52<21:08:07, 133.49s/it] 39%|███▉      | 361/930 [15:10:17<21:38:01, 136.87s/it] 39%|███▉      | 362/930 [15:12:26<21:15:10, 134.70s/it] 39%|███▉      | 363/930 [15:14:08<19:39:25, 124.81s/it] 39%|███▉      | 364/930 [15:15:59<18:59:06, 120.75s/it] 39%|███▉      | 365/930 [15:18:17<19:44:48, 125.82s/it] 39%|███▉      | 366/930 [15:20:42<20:37:08, 131.61s/it] 39%|███▉      | 367/930 [15:23:11<21:23:33, 136.79s/it] 40%|███▉      | 368/930 [15:25:15<20:46:00, 133.03s/it] 40%|███▉      | 369/930 [15:27:09<19:50:19, 127.31s/it] 40%|███▉      | 370/930 [15:29:32<20:32:54, 132.10s/it]                                                        {'loss': 0.6749, 'grad_norm': 0.5484383702278137, 'learning_rate': 7.548598528940353e-05, 'epoch': 0.8}
 40%|███▉      | 370/930 [15:29:32<20:32:54, 132.10s/it] 40%|███▉      | 371/930 [15:31:53<20:53:23, 134.53s/it] 40%|████      | 372/930 [15:34:01<20:32:34, 132.54s/it] 40%|████      | 373/930 [15:36:07<20:12:43, 130.63s/it] 40%|████      | 374/930 [15:38:50<21:41:49, 140.48s/it] 40%|████      | 375/930 [15:41:25<22:19:05, 144.77s/it] 40%|████      | 376/930 [15:43:51<22:19:38, 145.09s/it] 41%|████      | 377/930 [15:46:16<22:17:50, 145.15s/it] 41%|████      | 378/930 [15:48:29<21:41:49, 141.50s/it] 41%|████      | 379/930 [15:50:44<21:22:05, 139.61s/it] 41%|████      | 380/930 [15:52:40<20:13:17, 132.36s/it]                                                        {'loss': 0.6836, 'grad_norm': 0.5051620006561279, 'learning_rate': 7.385381472189321e-05, 'epoch': 0.82}
 41%|████      | 380/930 [15:52:40<20:13:17, 132.36s/it] 41%|████      | 381/930 [15:54:52<20:09:44, 132.21s/it] 41%|████      | 382/930 [15:57:46<22:03:35, 144.92s/it] 41%|████      | 383/930 [16:00:12<22:03:40, 145.19s/it] 41%|████▏     | 384/930 [16:02:21<21:17:53, 140.43s/it] 41%|████▏     | 385/930 [16:04:57<21:56:47, 144.97s/it] 42%|████▏     | 386/930 [16:07:13<21:30:44, 142.36s/it] 42%|████▏     | 387/930 [16:09:06<20:09:32, 133.65s/it] 42%|████▏     | 388/930 [16:11:59<21:52:33, 145.30s/it] 42%|████▏     | 389/930 [16:14:24<21:48:57, 145.17s/it] 42%|████▏     | 390/930 [16:16:42<21:28:23, 143.15s/it]                                                        {'loss': 0.673, 'grad_norm': 0.5291925072669983, 'learning_rate': 7.218804288500343e-05, 'epoch': 0.84}
 42%|████▏     | 390/930 [16:16:42<21:28:23, 143.15s/it] 42%|████▏     | 391/930 [16:19:40<23:00:09, 153.64s/it] 42%|████▏     | 392/930 [16:21:50<21:53:25, 146.48s/it] 42%|████▏     | 393/930 [16:24:28<22:22:30, 150.00s/it] 42%|████▏     | 394/930 [16:26:49<21:53:44, 147.06s/it] 42%|████▏     | 395/930 [16:29:24<22:12:18, 149.42s/it] 43%|████▎     | 396/930 [16:31:40<21:34:13, 145.42s/it] 43%|████▎     | 397/930 [16:34:17<22:05:01, 149.16s/it] 43%|████▎     | 398/930 [16:36:31<21:21:25, 144.52s/it] 43%|████▎     | 399/930 [16:39:22<22:30:02, 152.55s/it] 43%|████▎     | 400/930 [16:41:37<21:40:59, 147.28s/it]                                                        {'loss': 0.668, 'grad_norm': 0.5396954417228699, 'learning_rate': 7.049101623982937e-05, 'epoch': 0.86}
 43%|████▎     | 400/930 [16:41:37<21:40:59, 147.28s/it]
***** Running Evaluation *****

***** Running Evaluation *****

***** Running Evaluation *****
[INFO|trainer.py:4643] 2025-12-29 12:21:45,545 >> 
***** Running Evaluation *****
  Num examples = 3305
  Num examples = 3305
  Num examples = 3305
  Batch size = 1
  Batch size = 1
  Batch size = 1
[INFO|trainer.py:4645] 2025-12-29 12:21:45,546 >>   Num examples = 3305
[INFO|trainer.py:4648] 2025-12-29 12:21:45,546 >>   Batch size = 1

  0%|          | 0/827 [00:00<?, ?it/s][A
  0%|          | 2/827 [00:04<31:56,  2.32s/it][A
  0%|          | 3/827 [00:07<34:36,  2.52s/it][A
  0%|          | 4/827 [00:10<35:16,  2.57s/it][A
  1%|          | 5/827 [00:14<42:37,  3.11s/it][A
  1%|          | 6/827 [00:17<44:59,  3.29s/it][A
  1%|          | 7/827 [00:21<46:31,  3.40s/it][A
  1%|          | 8/827 [00:26<55:13,  4.05s/it][A
  1%|          | 9/827 [00:34<1:08:24,  5.02s/it][A
  1%|          | 10/827 [00:43<1:24:52,  6.23s/it][A
  1%|▏         | 11/827 [00:46<1:13:15,  5.39s/it][A
  1%|▏         | 12/827 [00:49<1:04:07,  4.72s/it][A
  2%|▏         | 13/827 [00:52<56:49,  4.19s/it]  [A
  2%|▏         | 14/827 [00:56<53:38,  3.96s/it][A
  2%|▏         | 15/827 [01:00<56:38,  4.19s/it][A
  2%|▏         | 16/827 [01:04<54:14,  4.01s/it][A
  2%|▏         | 17/827 [01:07<50:11,  3.72s/it][A
  2%|▏         | 18/827 [01:10<48:36,  3.61s/it][A
  2%|▏         | 19/827 [01:15<51:06,  3.80s/it][A
  2%|▏         | 20/827 [01:21<1:00:22,  4.49s/it][A
  3%|▎         | 21/827 [01:24<57:11,  4.26s/it]  [A
  3%|▎         | 22/827 [01:30<1:02:05,  4.63s/it][A
  3%|▎         | 23/827 [01:33<54:22,  4.06s/it]  [A
  3%|▎         | 24/827 [01:36<52:43,  3.94s/it][A
  3%|▎         | 25/827 [01:41<55:31,  4.15s/it][A
  3%|▎         | 26/827 [01:48<1:06:28,  4.98s/it][A
  3%|▎         | 27/827 [01:53<1:07:21,  5.05s/it][A
  3%|▎         | 28/827 [01:56<57:52,  4.35s/it]  [A
  4%|▎         | 29/827 [02:00<57:29,  4.32s/it][A
  4%|▎         | 30/827 [02:06<1:04:16,  4.84s/it][A
  4%|▎         | 31/827 [02:11<1:03:55,  4.82s/it][A
  4%|▍         | 32/827 [02:14<58:20,  4.40s/it]  [A
  4%|▍         | 33/827 [02:21<1:06:25,  5.02s/it][A
  4%|▍         | 34/827 [02:24<59:40,  4.51s/it]  [A
  4%|▍         | 35/827 [02:28<56:43,  4.30s/it][A
  4%|▍         | 36/827 [02:31<50:42,  3.85s/it][A
  4%|▍         | 37/827 [02:35<51:49,  3.94s/it][A
  5%|▍         | 38/827 [02:42<1:02:49,  4.78s/it][A
  5%|▍         | 39/827 [02:47<1:03:59,  4.87s/it][A
  5%|▍         | 40/827 [02:53<1:09:10,  5.27s/it][A
  5%|▍         | 41/827 [02:56<1:00:05,  4.59s/it][A
  5%|▌         | 42/827 [03:00<57:54,  4.43s/it]  [A
  5%|▌         | 43/827 [03:04<55:22,  4.24s/it][A
  5%|▌         | 44/827 [03:07<51:26,  3.94s/it][A
  5%|▌         | 45/827 [03:11<52:42,  4.04s/it][A
  6%|▌         | 46/827 [03:16<56:42,  4.36s/it][A
  6%|▌         | 47/827 [03:22<1:01:48,  4.75s/it][A
  6%|▌         | 48/827 [03:25<56:11,  4.33s/it]  [A
  6%|▌         | 49/827 [03:29<52:48,  4.07s/it][A
  6%|▌         | 50/827 [03:32<48:06,  3.72s/it][A
  6%|▌         | 51/827 [03:36<49:44,  3.85s/it][A
  6%|▋         | 52/827 [03:43<1:03:37,  4.93s/it][A
  6%|▋         | 53/827 [03:46<55:02,  4.27s/it]  [A
  7%|▋         | 54/827 [03:49<49:01,  3.80s/it][A
  7%|▋         | 55/827 [03:52<47:23,  3.68s/it][A
  7%|▋         | 56/827 [03:55<43:39,  3.40s/it][A
  7%|▋         | 57/827 [03:59<46:28,  3.62s/it][A
  7%|▋         | 58/827 [04:02<43:26,  3.39s/it][A
  7%|▋         | 59/827 [04:06<45:02,  3.52s/it][A
  7%|▋         | 60/827 [04:09<45:37,  3.57s/it][A
  7%|▋         | 61/827 [04:15<54:18,  4.25s/it][A
  7%|▋         | 62/827 [04:18<48:43,  3.82s/it][A
  8%|▊         | 63/827 [04:21<44:30,  3.50s/it][A
  8%|▊         | 64/827 [04:24<42:31,  3.34s/it][A
  8%|▊         | 65/827 [04:27<41:57,  3.30s/it][A
  8%|▊         | 66/827 [04:32<49:30,  3.90s/it][A
  8%|▊         | 67/827 [04:38<56:41,  4.48s/it][A
  8%|▊         | 68/827 [04:40<48:18,  3.82s/it][A
  8%|▊         | 69/827 [04:44<45:59,  3.64s/it][A
  8%|▊         | 70/827 [04:46<43:09,  3.42s/it][A
  9%|▊         | 71/827 [04:50<43:35,  3.46s/it][A
  9%|▊         | 72/827 [04:56<52:49,  4.20s/it][A
  9%|▉         | 73/827 [04:59<48:19,  3.85s/it][A
  9%|▉         | 74/827 [05:04<53:43,  4.28s/it][A
  9%|▉         | 75/827 [05:11<1:01:00,  4.87s/it][A
  9%|▉         | 76/827 [05:13<53:32,  4.28s/it]  [A
  9%|▉         | 77/827 [05:17<49:17,  3.94s/it][A
  9%|▉         | 78/827 [05:20<46:51,  3.75s/it][A
 10%|▉         | 79/827 [05:23<43:17,  3.47s/it][A
 10%|▉         | 80/827 [05:26<44:17,  3.56s/it][A
 10%|▉         | 81/827 [05:32<51:29,  4.14s/it][A
 10%|▉         | 82/827 [05:36<52:06,  4.20s/it][A
 10%|█         | 83/827 [05:40<51:00,  4.11s/it][A
 10%|█         | 84/827 [05:44<50:47,  4.10s/it][A
 10%|█         | 85/827 [05:49<51:56,  4.20s/it][A
 10%|█         | 86/827 [05:52<47:23,  3.84s/it][A
 11%|█         | 87/827 [05:57<52:46,  4.28s/it][A
 11%|█         | 88/827 [06:09<1:20:10,  6.51s/it][A
 11%|█         | 89/827 [06:17<1:27:31,  7.12s/it][A
 11%|█         | 90/827 [06:24<1:26:18,  7.03s/it][A
 11%|█         | 91/827 [06:27<1:12:42,  5.93s/it][A
 11%|█         | 92/827 [06:31<1:05:32,  5.35s/it][A
 11%|█         | 93/827 [06:34<56:01,  4.58s/it]  [A
 11%|█▏        | 94/827 [06:37<50:39,  4.15s/it][A
 11%|█▏        | 95/827 [06:40<45:59,  3.77s/it][A
 12%|█▏        | 96/827 [06:43<43:33,  3.58s/it][A
 12%|█▏        | 97/827 [06:46<39:06,  3.21s/it][A
 12%|█▏        | 98/827 [06:48<35:57,  2.96s/it][A
 12%|█▏        | 99/827 [06:51<34:49,  2.87s/it][A
 12%|█▏        | 100/827 [06:53<32:24,  2.67s/it][A
 12%|█▏        | 101/827 [06:57<35:31,  2.94s/it][A
 12%|█▏        | 102/827 [07:00<35:55,  2.97s/it][A
 12%|█▏        | 103/827 [07:03<35:38,  2.95s/it][A
 13%|█▎        | 104/827 [07:05<35:23,  2.94s/it][A
 13%|█▎        | 105/827 [07:08<35:39,  2.96s/it][A
 13%|█▎        | 106/827 [07:14<44:58,  3.74s/it][A
 13%|█▎        | 107/827 [07:23<1:02:31,  5.21s/it][A
 13%|█▎        | 108/827 [07:27<1:00:04,  5.01s/it][A
 13%|█▎        | 109/827 [07:32<58:10,  4.86s/it]  [A
 13%|█▎        | 110/827 [07:35<53:29,  4.48s/it][A
 13%|█▎        | 111/827 [07:40<52:50,  4.43s/it][A
 14%|█▎        | 112/827 [07:46<1:00:23,  5.07s/it][A
 14%|█▎        | 113/827 [07:50<55:50,  4.69s/it]  [A
 14%|█▍        | 114/827 [07:53<51:17,  4.32s/it][A
 14%|█▍        | 115/827 [07:56<46:28,  3.92s/it][A
 14%|█▍        | 116/827 [08:00<46:18,  3.91s/it][A
 14%|█▍        | 117/827 [08:04<46:46,  3.95s/it][A
 14%|█▍        | 118/827 [08:10<51:22,  4.35s/it][A
 14%|█▍        | 119/827 [08:16<56:49,  4.81s/it][A
 15%|█▍        | 120/827 [08:21<58:32,  4.97s/it][A
 15%|█▍        | 121/827 [08:24<51:46,  4.40s/it][A
 15%|█▍        | 122/827 [08:27<48:02,  4.09s/it][A
 15%|█▍        | 123/827 [08:30<42:20,  3.61s/it][A
 15%|█▍        | 124/827 [08:33<41:19,  3.53s/it][A
 15%|█▌        | 125/827 [08:37<42:36,  3.64s/it][A
 15%|█▌        | 126/827 [08:41<43:08,  3.69s/it][A
 15%|█▌        | 127/827 [08:44<40:39,  3.49s/it][A
 15%|█▌        | 128/827 [08:48<42:27,  3.65s/it][A
 16%|█▌        | 129/827 [08:51<39:21,  3.38s/it][A
 16%|█▌        | 130/827 [08:54<40:37,  3.50s/it][A
 16%|█▌        | 131/827 [08:58<41:12,  3.55s/it][A
 16%|█▌        | 132/827 [09:06<57:21,  4.95s/it][A
 16%|█▌        | 133/827 [09:11<56:18,  4.87s/it][A
 16%|█▌        | 134/827 [09:14<51:27,  4.46s/it][A
 16%|█▋        | 135/827 [09:18<49:57,  4.33s/it][A
 16%|█▋        | 136/827 [09:25<56:14,  4.88s/it][A
 17%|█▋        | 137/827 [09:28<50:33,  4.40s/it][A
 17%|█▋        | 138/827 [09:34<54:49,  4.77s/it][A
 17%|█▋        | 139/827 [09:41<1:04:55,  5.66s/it][A
 17%|█▋        | 140/827 [09:46<1:00:04,  5.25s/it][A
 17%|█▋        | 141/827 [09:50<56:14,  4.92s/it]  [A
 17%|█▋        | 142/827 [09:53<49:47,  4.36s/it][A
 17%|█▋        | 143/827 [09:56<47:10,  4.14s/it][A
 17%|█▋        | 144/827 [09:59<42:05,  3.70s/it][A
 18%|█▊        | 145/827 [10:03<41:21,  3.64s/it][A
 18%|█▊        | 146/827 [10:07<44:14,  3.90s/it][A
 18%|█▊        | 147/827 [10:10<41:48,  3.69s/it][A
 18%|█▊        | 148/827 [10:14<40:23,  3.57s/it][A
 18%|█▊        | 149/827 [10:17<39:03,  3.46s/it][A
 18%|█▊        | 150/827 [10:20<37:52,  3.36s/it][A
 18%|█▊        | 151/827 [10:22<33:51,  3.00s/it][A
 18%|█▊        | 152/827 [10:25<33:25,  2.97s/it][A
 19%|█▊        | 153/827 [10:29<35:24,  3.15s/it][A
 19%|█▊        | 154/827 [10:33<38:54,  3.47s/it][A
 19%|█▊        | 155/827 [10:38<43:10,  3.85s/it][A
 19%|█▉        | 156/827 [10:43<48:29,  4.34s/it][A
 19%|█▉        | 157/827 [10:46<44:09,  3.95s/it][A
 19%|█▉        | 158/827 [10:49<41:51,  3.75s/it][A
 19%|█▉        | 159/827 [10:53<42:30,  3.82s/it][A
 19%|█▉        | 160/827 [10:58<46:47,  4.21s/it][A
 19%|█▉        | 161/827 [11:01<41:19,  3.72s/it][A
 20%|█▉        | 162/827 [11:04<40:04,  3.62s/it][A
 20%|█▉        | 163/827 [11:08<41:16,  3.73s/it][A
 20%|█▉        | 164/827 [11:12<41:42,  3.77s/it][A
 20%|█▉        | 165/827 [11:15<38:47,  3.52s/it][A
 20%|██        | 166/827 [11:20<42:44,  3.88s/it][A
 20%|██        | 167/827 [11:30<1:03:31,  5.77s/it][A
 20%|██        | 168/827 [11:37<1:06:43,  6.08s/it][A
 20%|██        | 169/827 [11:40<57:02,  5.20s/it]  [A
 21%|██        | 170/827 [11:44<52:28,  4.79s/it][A
 21%|██        | 171/827 [11:48<51:39,  4.73s/it][A
 21%|██        | 172/827 [11:52<46:49,  4.29s/it][A
 21%|██        | 173/827 [11:54<40:48,  3.74s/it][A
 21%|██        | 174/827 [11:57<38:41,  3.56s/it][A
 21%|██        | 175/827 [12:01<40:34,  3.73s/it][A
 21%|██▏       | 176/827 [12:04<36:38,  3.38s/it][A
 21%|██▏       | 177/827 [12:07<34:46,  3.21s/it][A
 22%|██▏       | 178/827 [12:10<35:12,  3.25s/it][A
 22%|██▏       | 179/827 [12:16<45:08,  4.18s/it][A
 22%|██▏       | 180/827 [12:21<44:41,  4.14s/it][A
 22%|██▏       | 181/827 [12:24<41:19,  3.84s/it][A
 22%|██▏       | 182/827 [12:29<45:29,  4.23s/it][A
 22%|██▏       | 183/827 [12:32<41:14,  3.84s/it][A
 22%|██▏       | 184/827 [12:36<41:40,  3.89s/it][A
 22%|██▏       | 185/827 [12:41<46:28,  4.34s/it][A
 22%|██▏       | 186/827 [12:44<42:04,  3.94s/it][A
 23%|██▎       | 187/827 [12:47<37:37,  3.53s/it][A
 23%|██▎       | 188/827 [12:50<37:37,  3.53s/it][A
 23%|██▎       | 189/827 [12:55<42:55,  4.04s/it][A
 23%|██▎       | 190/827 [12:59<40:34,  3.82s/it][A
 23%|██▎       | 191/827 [13:03<41:23,  3.90s/it][A
 23%|██▎       | 192/827 [13:07<43:30,  4.11s/it][A
 23%|██▎       | 193/827 [13:12<45:25,  4.30s/it][A
 23%|██▎       | 194/827 [13:16<42:16,  4.01s/it][A
 24%|██▎       | 195/827 [13:19<39:52,  3.79s/it][A
 24%|██▎       | 196/827 [13:21<35:44,  3.40s/it][A
 24%|██▍       | 197/827 [13:24<34:03,  3.24s/it][A
 24%|██▍       | 198/827 [13:27<33:08,  3.16s/it][A
 24%|██▍       | 199/827 [13:31<35:37,  3.40s/it][A
 24%|██▍       | 200/827 [13:35<38:14,  3.66s/it][A
 24%|██▍       | 201/827 [13:41<44:55,  4.31s/it][A
 24%|██▍       | 202/827 [13:46<46:38,  4.48s/it][A
 25%|██▍       | 203/827 [13:51<48:15,  4.64s/it][A
 25%|██▍       | 204/827 [13:58<56:16,  5.42s/it][A
 25%|██▍       | 205/827 [14:01<48:58,  4.72s/it][A
 25%|██▍       | 206/827 [14:05<45:24,  4.39s/it][A
 25%|██▌       | 207/827 [14:09<42:32,  4.12s/it][A
 25%|██▌       | 208/827 [14:12<39:08,  3.79s/it][A
 25%|██▌       | 209/827 [14:15<38:00,  3.69s/it][A
 25%|██▌       | 210/827 [14:18<35:53,  3.49s/it][A
 26%|██▌       | 211/827 [14:23<40:03,  3.90s/it][A
 26%|██▌       | 212/827 [14:28<44:40,  4.36s/it][A
 26%|██▌       | 213/827 [14:35<51:01,  4.99s/it][A
 26%|██▌       | 214/827 [14:38<46:03,  4.51s/it][A
 26%|██▌       | 215/827 [14:41<42:13,  4.14s/it][A
 26%|██▌       | 216/827 [14:44<38:46,  3.81s/it][A
 26%|██▌       | 217/827 [14:49<40:40,  4.00s/it][A
 26%|██▋       | 218/827 [14:53<41:30,  4.09s/it][A
 26%|██▋       | 219/827 [14:56<38:40,  3.82s/it][A
 27%|██▋       | 220/827 [15:00<38:51,  3.84s/it][A
 27%|██▋       | 221/827 [15:07<46:38,  4.62s/it][A
 27%|██▋       | 222/827 [15:10<42:49,  4.25s/it][A
 27%|██▋       | 223/827 [15:14<40:32,  4.03s/it][A
 27%|██▋       | 224/827 [15:17<37:27,  3.73s/it][A
 27%|██▋       | 225/827 [15:20<36:37,  3.65s/it][A
 27%|██▋       | 226/827 [15:23<33:04,  3.30s/it][A
 27%|██▋       | 227/827 [15:26<34:38,  3.46s/it][A
 28%|██▊       | 228/827 [15:33<42:18,  4.24s/it][A
 28%|██▊       | 229/827 [15:36<38:51,  3.90s/it][A
 28%|██▊       | 230/827 [15:40<40:44,  4.09s/it][A
 28%|██▊       | 231/827 [15:44<39:25,  3.97s/it][A
 28%|██▊       | 232/827 [15:47<36:43,  3.70s/it][A
 28%|██▊       | 233/827 [15:51<38:06,  3.85s/it][A
 28%|██▊       | 234/827 [15:57<44:16,  4.48s/it][A
 28%|██▊       | 235/827 [16:03<49:48,  5.05s/it][A
 29%|██▊       | 236/827 [16:07<46:23,  4.71s/it][A
 29%|██▊       | 237/827 [16:13<47:39,  4.85s/it][A
 29%|██▉       | 238/827 [16:19<51:33,  5.25s/it][A
 29%|██▉       | 239/827 [16:23<48:02,  4.90s/it][A
 29%|██▉       | 240/827 [16:30<55:42,  5.69s/it][A
 29%|██▉       | 241/827 [16:36<56:25,  5.78s/it][A
 29%|██▉       | 242/827 [16:40<51:19,  5.26s/it][A
 29%|██▉       | 243/827 [16:44<46:22,  4.76s/it][A
 30%|██▉       | 244/827 [16:51<52:29,  5.40s/it][A
 30%|██▉       | 245/827 [16:54<46:02,  4.75s/it][A
 30%|██▉       | 246/827 [16:58<44:08,  4.56s/it][A
 30%|██▉       | 247/827 [17:02<41:20,  4.28s/it][A
 30%|██▉       | 248/827 [17:09<49:52,  5.17s/it][A
 30%|███       | 249/827 [17:15<50:56,  5.29s/it][A
 30%|███       | 250/827 [17:17<42:31,  4.42s/it][A
 30%|███       | 251/827 [17:20<38:38,  4.03s/it][A
 30%|███       | 252/827 [17:26<43:53,  4.58s/it][A
 31%|███       | 253/827 [17:29<38:52,  4.06s/it][A
 31%|███       | 254/827 [17:33<39:13,  4.11s/it][A
 31%|███       | 255/827 [17:42<52:00,  5.46s/it][A
 31%|███       | 256/827 [17:48<53:57,  5.67s/it][A
 31%|███       | 257/827 [17:53<51:56,  5.47s/it][A
 31%|███       | 258/827 [17:56<45:46,  4.83s/it][A
 31%|███▏      | 259/827 [18:03<49:53,  5.27s/it][A
 31%|███▏      | 260/827 [18:09<52:07,  5.52s/it][A
 32%|███▏      | 261/827 [18:13<49:49,  5.28s/it][A
 32%|███▏      | 262/827 [18:18<48:39,  5.17s/it][A
 32%|███▏      | 263/827 [18:27<58:04,  6.18s/it][A
 32%|███▏      | 264/827 [18:29<48:12,  5.14s/it][A
 32%|███▏      | 265/827 [18:32<40:12,  4.29s/it][A
 32%|███▏      | 266/827 [18:35<37:17,  3.99s/it][A
 32%|███▏      | 267/827 [18:38<34:09,  3.66s/it][A
 32%|███▏      | 268/827 [18:41<32:41,  3.51s/it][A
 33%|███▎      | 269/827 [18:45<32:40,  3.51s/it][A
 33%|███▎      | 270/827 [18:51<39:52,  4.29s/it][A
 33%|███▎      | 271/827 [18:54<36:16,  3.91s/it][A
 33%|███▎      | 272/827 [18:57<34:44,  3.76s/it][A
 33%|███▎      | 273/827 [19:02<37:13,  4.03s/it][A
 33%|███▎      | 274/827 [19:12<54:03,  5.87s/it][A
 33%|███▎      | 275/827 [19:18<55:32,  6.04s/it][A
 33%|███▎      | 276/827 [19:22<48:41,  5.30s/it][A
 33%|███▎      | 277/827 [19:26<45:14,  4.94s/it][A
 34%|███▎      | 278/827 [19:30<43:20,  4.74s/it][A
 34%|███▎      | 279/827 [19:34<39:06,  4.28s/it][A
 34%|███▍      | 280/827 [19:37<35:36,  3.91s/it][A
 34%|███▍      | 281/827 [19:40<34:45,  3.82s/it][A
 34%|███▍      | 282/827 [19:44<35:05,  3.86s/it][A
 34%|███▍      | 283/827 [19:48<33:40,  3.71s/it][A
 34%|███▍      | 284/827 [19:51<34:00,  3.76s/it][A
 34%|███▍      | 285/827 [19:56<35:02,  3.88s/it][A
 35%|███▍      | 286/827 [20:02<40:50,  4.53s/it][A
 35%|███▍      | 287/827 [20:04<35:15,  3.92s/it][A
 35%|███▍      | 288/827 [20:07<33:22,  3.71s/it][A
 35%|███▍      | 289/827 [20:10<31:11,  3.48s/it][A
 35%|███▌      | 290/827 [20:14<30:57,  3.46s/it][A
 35%|███▌      | 291/827 [20:17<29:05,  3.26s/it][A
 35%|███▌      | 292/827 [20:20<29:14,  3.28s/it][A
 35%|███▌      | 293/827 [20:23<28:25,  3.19s/it][A
 36%|███▌      | 294/827 [20:26<29:30,  3.32s/it][A
 36%|███▌      | 295/827 [20:30<31:07,  3.51s/it][A
 36%|███▌      | 296/827 [20:35<33:40,  3.81s/it][A
 36%|███▌      | 297/827 [20:39<34:33,  3.91s/it][A
 36%|███▌      | 298/827 [20:42<32:17,  3.66s/it][A
 36%|███▌      | 299/827 [20:45<30:38,  3.48s/it][A
 36%|███▋      | 300/827 [20:48<29:53,  3.40s/it][A
 36%|███▋      | 301/827 [20:51<27:53,  3.18s/it][A
 37%|███▋      | 302/827 [20:55<29:09,  3.33s/it][A
 37%|███▋      | 303/827 [20:57<26:47,  3.07s/it][A
 37%|███▋      | 304/827 [21:00<26:43,  3.07s/it][A
 37%|███▋      | 305/827 [21:04<29:02,  3.34s/it][A
 37%|███▋      | 306/827 [21:09<33:02,  3.81s/it][A
 37%|███▋      | 307/827 [21:12<31:21,  3.62s/it][A
 37%|███▋      | 308/827 [21:15<29:30,  3.41s/it][A
 37%|███▋      | 309/827 [21:19<29:45,  3.45s/it][A
 37%|███▋      | 310/827 [21:22<27:59,  3.25s/it][A
 38%|███▊      | 311/827 [21:24<25:46,  3.00s/it][A
 38%|███▊      | 312/827 [21:27<26:47,  3.12s/it][A
 38%|███▊      | 313/827 [21:32<31:49,  3.72s/it][A
 38%|███▊      | 314/827 [21:36<30:04,  3.52s/it][A
 38%|███▊      | 315/827 [21:40<31:43,  3.72s/it][A
 38%|███▊      | 316/827 [21:44<32:58,  3.87s/it][A
 38%|███▊      | 317/827 [21:50<37:44,  4.44s/it][A
 38%|███▊      | 318/827 [21:55<40:00,  4.72s/it][A
 39%|███▊      | 319/827 [21:59<37:46,  4.46s/it][A
 39%|███▊      | 320/827 [22:02<33:58,  4.02s/it][A
 39%|███▉      | 321/827 [22:07<35:29,  4.21s/it][A
 39%|███▉      | 322/827 [22:11<36:27,  4.33s/it][A
 39%|███▉      | 323/827 [22:14<33:40,  4.01s/it][A
 39%|███▉      | 324/827 [22:20<37:48,  4.51s/it][A
 39%|███▉      | 325/827 [22:24<34:51,  4.17s/it][A
 39%|███▉      | 326/827 [22:27<32:28,  3.89s/it][A
 40%|███▉      | 327/827 [22:32<35:38,  4.28s/it][A
 40%|███▉      | 328/827 [22:36<34:32,  4.15s/it][A
 40%|███▉      | 329/827 [22:42<40:44,  4.91s/it][A
 40%|███▉      | 330/827 [22:48<41:17,  4.98s/it][A
 40%|████      | 331/827 [22:53<42:51,  5.18s/it][A
 40%|████      | 332/827 [22:57<38:35,  4.68s/it][A
 40%|████      | 333/827 [23:03<41:52,  5.09s/it][A
 40%|████      | 334/827 [23:07<38:30,  4.69s/it][A
 41%|████      | 335/827 [23:11<37:23,  4.56s/it][A
 41%|████      | 336/827 [23:13<31:44,  3.88s/it][A
 41%|████      | 337/827 [23:17<32:13,  3.95s/it][A
 41%|████      | 338/827 [23:20<30:13,  3.71s/it][A
 41%|████      | 339/827 [23:23<28:17,  3.48s/it][A
 41%|████      | 340/827 [23:27<27:34,  3.40s/it][A
 41%|████      | 341/827 [23:30<28:37,  3.53s/it][A
 41%|████▏     | 342/827 [23:34<28:23,  3.51s/it][A
 41%|████▏     | 343/827 [23:37<27:49,  3.45s/it][A
 42%|████▏     | 344/827 [23:42<30:24,  3.78s/it][A
 42%|████▏     | 345/827 [23:47<35:12,  4.38s/it][A
 42%|████▏     | 346/827 [23:51<33:58,  4.24s/it][A
 42%|████▏     | 347/827 [23:55<32:00,  4.00s/it][A
 42%|████▏     | 348/827 [23:58<30:20,  3.80s/it][A
 42%|████▏     | 349/827 [24:02<30:15,  3.80s/it][A
 42%|████▏     | 350/827 [24:08<34:33,  4.35s/it][A
 42%|████▏     | 351/827 [24:11<32:37,  4.11s/it][A
 43%|████▎     | 352/827 [24:16<33:07,  4.18s/it][A
 43%|████▎     | 353/827 [24:19<30:57,  3.92s/it][A
 43%|████▎     | 354/827 [24:22<29:05,  3.69s/it][A
 43%|████▎     | 355/827 [24:26<29:43,  3.78s/it][A
 43%|████▎     | 356/827 [24:32<33:53,  4.32s/it][A
 43%|████▎     | 357/827 [24:36<33:47,  4.31s/it][A
 43%|████▎     | 358/827 [24:39<30:27,  3.90s/it][A
 43%|████▎     | 359/827 [24:42<29:00,  3.72s/it][A
 44%|████▎     | 360/827 [24:46<30:32,  3.92s/it][A
 44%|████▎     | 361/827 [24:52<33:05,  4.26s/it][A
 44%|████▍     | 362/827 [24:54<28:16,  3.65s/it][A
 44%|████▍     | 363/827 [24:58<29:03,  3.76s/it][A
 44%|████▍     | 364/827 [25:02<30:07,  3.90s/it][A
 44%|████▍     | 365/827 [25:05<27:47,  3.61s/it][A
 44%|████▍     | 366/827 [25:11<32:50,  4.27s/it][A
 44%|████▍     | 367/827 [25:16<35:15,  4.60s/it][A
 44%|████▍     | 368/827 [25:19<30:53,  4.04s/it][A
 45%|████▍     | 369/827 [25:22<29:36,  3.88s/it][A
 45%|████▍     | 370/827 [25:26<28:02,  3.68s/it][A
 45%|████▍     | 371/827 [25:30<30:15,  3.98s/it][A
 45%|████▍     | 372/827 [25:34<29:47,  3.93s/it][A
 45%|████▌     | 373/827 [25:38<29:56,  3.96s/it][A
 45%|████▌     | 374/827 [25:45<37:25,  4.96s/it][A
 45%|████▌     | 375/827 [25:54<45:27,  6.03s/it][A
 45%|████▌     | 376/827 [26:04<54:09,  7.21s/it][A
 46%|████▌     | 377/827 [26:08<48:11,  6.42s/it][A
 46%|████▌     | 378/827 [26:14<45:22,  6.06s/it][A
 46%|████▌     | 379/827 [26:25<56:13,  7.53s/it][A
 46%|████▌     | 380/827 [26:31<52:43,  7.08s/it][A
 46%|████▌     | 381/827 [26:35<45:35,  6.13s/it][A
 46%|████▌     | 382/827 [26:39<40:57,  5.52s/it][A
 46%|████▋     | 383/827 [26:44<39:53,  5.39s/it][A
 46%|████▋     | 384/827 [26:47<36:04,  4.89s/it][A
 47%|████▋     | 385/827 [26:51<32:29,  4.41s/it][A
 47%|████▋     | 386/827 [26:54<29:21,  3.99s/it][A
 47%|████▋     | 387/827 [26:58<28:59,  3.95s/it][A
 47%|████▋     | 388/827 [27:02<29:15,  4.00s/it][A
 47%|████▋     | 389/827 [27:06<29:46,  4.08s/it][A
 47%|████▋     | 390/827 [27:10<28:43,  3.94s/it][A
 47%|████▋     | 391/827 [27:13<27:14,  3.75s/it][A
 47%|████▋     | 392/827 [27:16<25:24,  3.50s/it][A
 48%|████▊     | 393/827 [27:21<28:54,  4.00s/it][A
 48%|████▊     | 394/827 [27:26<31:51,  4.41s/it][A
 48%|████▊     | 395/827 [27:32<33:18,  4.63s/it][A
 48%|████▊     | 396/827 [27:37<35:43,  4.97s/it][A
 48%|████▊     | 397/827 [27:40<30:33,  4.26s/it][A
 48%|████▊     | 398/827 [27:44<29:20,  4.10s/it][A
 48%|████▊     | 399/827 [27:47<28:13,  3.96s/it][A
 48%|████▊     | 400/827 [27:51<27:55,  3.92s/it][A
 48%|████▊     | 401/827 [27:56<29:32,  4.16s/it][A
 49%|████▊     | 402/827 [27:59<28:09,  3.98s/it][A
 49%|████▊     | 403/827 [28:05<31:38,  4.48s/it][A
 49%|████▉     | 404/827 [28:07<27:07,  3.85s/it][A
 49%|████▉     | 405/827 [28:10<25:25,  3.61s/it][A
 49%|████▉     | 406/827 [28:14<25:39,  3.66s/it][A
 49%|████▉     | 407/827 [28:17<23:46,  3.40s/it][A
 49%|████▉     | 408/827 [28:20<22:29,  3.22s/it][A
 49%|████▉     | 409/827 [28:23<22:15,  3.19s/it][A
 50%|████▉     | 410/827 [28:25<20:38,  2.97s/it][A
 50%|████▉     | 411/827 [28:28<20:12,  2.91s/it][A
 50%|████▉     | 412/827 [28:32<21:13,  3.07s/it][A
 50%|████▉     | 413/827 [28:36<22:59,  3.33s/it][A
 50%|█████     | 414/827 [28:41<26:26,  3.84s/it][A
 50%|█████     | 415/827 [28:44<24:37,  3.59s/it][A
 50%|█████     | 416/827 [28:47<24:46,  3.62s/it][A
 50%|█████     | 417/827 [28:53<29:39,  4.34s/it][A
 51%|█████     | 418/827 [29:00<34:10,  5.01s/it][A
 51%|█████     | 419/827 [29:03<30:46,  4.53s/it][A
 51%|█████     | 420/827 [29:08<31:52,  4.70s/it][A
 51%|█████     | 421/827 [29:13<31:02,  4.59s/it][A
 51%|█████     | 422/827 [29:16<27:43,  4.11s/it][A
 51%|█████     | 423/827 [29:19<25:16,  3.75s/it][A
 51%|█████▏    | 424/827 [29:22<24:41,  3.68s/it][A
 51%|█████▏    | 425/827 [29:26<25:02,  3.74s/it][A
 52%|█████▏    | 426/827 [29:32<29:25,  4.40s/it][A
 52%|█████▏    | 427/827 [29:36<29:25,  4.41s/it][A
 52%|█████▏    | 428/827 [29:42<32:43,  4.92s/it][A
 52%|█████▏    | 429/827 [29:47<31:44,  4.79s/it][A
 52%|█████▏    | 430/827 [29:51<29:23,  4.44s/it][A
 52%|█████▏    | 431/827 [29:55<29:58,  4.54s/it][A
 52%|█████▏    | 432/827 [30:01<31:48,  4.83s/it][A
 52%|█████▏    | 433/827 [30:10<39:22,  6.00s/it][A
 52%|█████▏    | 434/827 [30:16<39:37,  6.05s/it][A
 53%|█████▎    | 435/827 [30:19<33:17,  5.10s/it][A
 53%|█████▎    | 436/827 [30:22<30:46,  4.72s/it][A
 53%|█████▎    | 437/827 [30:27<30:49,  4.74s/it][A
 53%|█████▎    | 438/827 [30:31<28:04,  4.33s/it][A
 53%|█████▎    | 439/827 [30:34<25:47,  3.99s/it][A
 53%|█████▎    | 440/827 [30:39<27:18,  4.23s/it][A
 53%|█████▎    | 441/827 [30:42<25:10,  3.91s/it][A
 53%|█████▎    | 442/827 [30:44<22:20,  3.48s/it][A
 54%|█████▎    | 443/827 [30:48<22:16,  3.48s/it][A
 54%|█████▎    | 444/827 [30:51<21:16,  3.33s/it][A
 54%|█████▍    | 445/827 [30:57<27:29,  4.32s/it][A
 54%|█████▍    | 446/827 [31:01<25:12,  3.97s/it][A
 54%|█████▍    | 447/827 [31:05<26:47,  4.23s/it][A
 54%|█████▍    | 448/827 [31:09<26:26,  4.19s/it][A
 54%|█████▍    | 449/827 [31:17<32:30,  5.16s/it][A
 54%|█████▍    | 450/827 [31:23<34:21,  5.47s/it][A
 55%|█████▍    | 451/827 [31:27<30:27,  4.86s/it][A
 55%|█████▍    | 452/827 [31:31<29:51,  4.78s/it][A
 55%|█████▍    | 453/827 [31:38<33:30,  5.37s/it][A
 55%|█████▍    | 454/827 [31:41<30:10,  4.85s/it][A
 55%|█████▌    | 455/827 [31:45<27:20,  4.41s/it][A
 55%|█████▌    | 456/827 [31:52<31:28,  5.09s/it][A
 55%|█████▌    | 457/827 [31:56<30:57,  5.02s/it][A
 55%|█████▌    | 458/827 [32:00<28:59,  4.71s/it][A
 56%|█████▌    | 459/827 [32:05<28:42,  4.68s/it][A
 56%|█████▌    | 460/827 [32:10<29:33,  4.83s/it][A
 56%|█████▌    | 461/827 [32:18<35:32,  5.83s/it][A
 56%|█████▌    | 462/827 [32:24<35:32,  5.84s/it][A
 56%|█████▌    | 463/827 [32:27<30:13,  4.98s/it][A
 56%|█████▌    | 464/827 [32:32<29:12,  4.83s/it][A
 56%|█████▌    | 465/827 [32:37<29:52,  4.95s/it][A
 56%|█████▋    | 466/827 [32:40<27:07,  4.51s/it][A
 56%|█████▋    | 467/827 [32:44<26:11,  4.37s/it][A
 57%|█████▋    | 468/827 [32:48<24:48,  4.15s/it][A
 57%|█████▋    | 469/827 [32:53<26:16,  4.40s/it][A
 57%|█████▋    | 470/827 [32:56<23:30,  3.95s/it][A
 57%|█████▋    | 471/827 [33:00<23:07,  3.90s/it][A
 57%|█████▋    | 472/827 [33:06<27:02,  4.57s/it][A
 57%|█████▋    | 473/827 [33:11<27:47,  4.71s/it][A
 57%|█████▋    | 474/827 [33:15<27:14,  4.63s/it][A
 57%|█████▋    | 475/827 [33:19<24:46,  4.22s/it][A
 58%|█████▊    | 476/827 [33:21<21:18,  3.64s/it][A
 58%|█████▊    | 477/827 [33:23<19:02,  3.26s/it][A
 58%|█████▊    | 478/827 [33:26<18:46,  3.23s/it][A
 58%|█████▊    | 479/827 [33:32<23:41,  4.08s/it][A
 58%|█████▊    | 480/827 [33:36<22:52,  3.96s/it][A
 58%|█████▊    | 481/827 [33:42<25:39,  4.45s/it][A
 58%|█████▊    | 482/827 [33:45<23:21,  4.06s/it][A
 58%|█████▊    | 483/827 [33:47<20:13,  3.53s/it][A
 59%|█████▊    | 484/827 [33:50<19:21,  3.39s/it][A
 59%|█████▊    | 485/827 [33:53<18:20,  3.22s/it][A
 59%|█████▉    | 486/827 [33:56<18:30,  3.26s/it][A
 59%|█████▉    | 487/827 [34:00<18:58,  3.35s/it][A
 59%|█████▉    | 488/827 [34:05<21:54,  3.88s/it][A
 59%|█████▉    | 489/827 [34:10<23:24,  4.15s/it][A
 59%|█████▉    | 490/827 [34:15<25:15,  4.50s/it][A
 59%|█████▉    | 491/827 [34:18<22:34,  4.03s/it][A
 59%|█████▉    | 492/827 [34:21<21:21,  3.82s/it][A
 60%|█████▉    | 493/827 [34:25<20:04,  3.61s/it][A
 60%|█████▉    | 494/827 [34:28<20:07,  3.62s/it][A
 60%|█████▉    | 495/827 [34:32<20:46,  3.75s/it][A
 60%|█████▉    | 496/827 [34:36<21:04,  3.82s/it][A
 60%|██████    | 497/827 [34:39<19:14,  3.50s/it][A
 60%|██████    | 498/827 [34:42<18:09,  3.31s/it][A
 60%|██████    | 499/827 [34:46<20:07,  3.68s/it][A
 60%|██████    | 500/827 [34:50<19:48,  3.63s/it][A
 61%|██████    | 501/827 [34:53<18:05,  3.33s/it][A
 61%|██████    | 502/827 [34:56<18:48,  3.47s/it][A
 61%|██████    | 503/827 [35:02<22:15,  4.12s/it][A
 61%|██████    | 504/827 [35:05<20:47,  3.86s/it][A
 61%|██████    | 505/827 [35:09<20:24,  3.80s/it][A
 61%|██████    | 506/827 [35:13<20:04,  3.75s/it][A
 61%|██████▏   | 507/827 [35:17<21:35,  4.05s/it][A
 61%|██████▏   | 508/827 [35:26<28:57,  5.45s/it][A
 62%|██████▏   | 509/827 [35:28<24:03,  4.54s/it][A
 62%|██████▏   | 510/827 [35:32<22:45,  4.31s/it][A
 62%|██████▏   | 511/827 [35:35<20:54,  3.97s/it][A
 62%|██████▏   | 512/827 [35:38<19:09,  3.65s/it][A
 62%|██████▏   | 513/827 [35:42<18:35,  3.55s/it][A
 62%|██████▏   | 514/827 [35:46<19:11,  3.68s/it][A
 62%|██████▏   | 515/827 [35:49<17:57,  3.45s/it][A
 62%|██████▏   | 516/827 [35:52<18:20,  3.54s/it][A
 63%|██████▎   | 517/827 [35:56<18:09,  3.51s/it][A
 63%|██████▎   | 518/827 [35:58<16:51,  3.27s/it][A
 63%|██████▎   | 519/827 [36:01<16:18,  3.18s/it][A
 63%|██████▎   | 520/827 [36:04<15:44,  3.08s/it][A
 63%|██████▎   | 521/827 [36:09<18:23,  3.61s/it][A
 63%|██████▎   | 522/827 [36:15<21:53,  4.31s/it][A
 63%|██████▎   | 523/827 [36:18<19:52,  3.92s/it][A
 63%|██████▎   | 524/827 [36:22<19:42,  3.90s/it][A
 63%|██████▎   | 525/827 [36:27<21:36,  4.29s/it][A
 64%|██████▎   | 526/827 [36:34<25:57,  5.18s/it][A
 64%|██████▎   | 527/827 [36:38<24:06,  4.82s/it][A
 64%|██████▍   | 528/827 [36:43<24:18,  4.88s/it][A
 64%|██████▍   | 529/827 [36:48<23:55,  4.82s/it][A
 64%|██████▍   | 530/827 [36:53<24:41,  4.99s/it][A
 64%|██████▍   | 531/827 [36:56<21:17,  4.32s/it][A
 64%|██████▍   | 532/827 [37:00<20:36,  4.19s/it][A
 64%|██████▍   | 533/827 [37:05<21:19,  4.35s/it][A
 65%|██████▍   | 534/827 [37:11<23:46,  4.87s/it][A
 65%|██████▍   | 535/827 [37:17<25:32,  5.25s/it][A
 65%|██████▍   | 536/827 [37:20<22:53,  4.72s/it][A
 65%|██████▍   | 537/827 [37:23<20:01,  4.14s/it][A
 65%|██████▌   | 538/827 [37:26<18:20,  3.81s/it][A
 65%|██████▌   | 539/827 [37:29<17:16,  3.60s/it][A
 65%|██████▌   | 540/827 [37:33<17:08,  3.58s/it][A
 65%|██████▌   | 541/827 [37:37<17:39,  3.71s/it][A
 66%|██████▌   | 542/827 [37:41<17:48,  3.75s/it][A
 66%|██████▌   | 543/827 [37:46<19:08,  4.04s/it][A
 66%|██████▌   | 544/827 [37:50<20:06,  4.26s/it][A
 66%|██████▌   | 545/827 [37:55<20:02,  4.26s/it][A
 66%|██████▌   | 546/827 [37:59<20:28,  4.37s/it][A
 66%|██████▌   | 547/827 [38:04<21:38,  4.64s/it][A
 66%|██████▋   | 548/827 [38:07<18:50,  4.05s/it][A
 66%|██████▋   | 549/827 [38:10<16:34,  3.58s/it][A
 67%|██████▋   | 550/827 [38:13<16:56,  3.67s/it][A
 67%|██████▋   | 551/827 [38:18<17:41,  3.85s/it][A
 67%|██████▋   | 552/827 [38:21<17:05,  3.73s/it][A
 67%|██████▋   | 553/827 [38:25<17:44,  3.89s/it][A
 67%|██████▋   | 554/827 [38:28<15:33,  3.42s/it][A
 67%|██████▋   | 555/827 [38:31<14:48,  3.27s/it][A
 67%|██████▋   | 556/827 [38:34<14:59,  3.32s/it][A
 67%|██████▋   | 557/827 [38:37<13:50,  3.08s/it][A
 67%|██████▋   | 558/827 [38:40<14:22,  3.21s/it][A
 68%|██████▊   | 559/827 [38:45<15:59,  3.58s/it][A
 68%|██████▊   | 560/827 [38:48<15:43,  3.53s/it][A
 68%|██████▊   | 561/827 [38:52<15:41,  3.54s/it][A
 68%|██████▊   | 562/827 [38:56<16:11,  3.67s/it][A
 68%|██████▊   | 563/827 [38:59<15:46,  3.58s/it][A
 68%|██████▊   | 564/827 [39:02<15:36,  3.56s/it][A
 68%|██████▊   | 565/827 [39:07<16:17,  3.73s/it][A
 68%|██████▊   | 566/827 [39:12<18:32,  4.26s/it][A
 69%|██████▊   | 567/827 [39:15<16:47,  3.87s/it][A
 69%|██████▊   | 568/827 [39:19<16:43,  3.87s/it][A
 69%|██████▉   | 569/827 [39:22<15:53,  3.69s/it][A
 69%|██████▉   | 570/827 [39:26<15:30,  3.62s/it][A
 69%|██████▉   | 571/827 [39:31<18:02,  4.23s/it][A
 69%|██████▉   | 572/827 [39:37<19:40,  4.63s/it][A
 69%|██████▉   | 573/827 [39:43<21:29,  5.08s/it][A
 69%|██████▉   | 574/827 [39:47<19:50,  4.71s/it][A
 70%|██████▉   | 575/827 [39:53<21:26,  5.11s/it][A
 70%|██████▉   | 576/827 [39:57<20:05,  4.80s/it][A
 70%|██████▉   | 577/827 [40:00<17:37,  4.23s/it][A
 70%|██████▉   | 578/827 [40:04<16:56,  4.08s/it][A
 70%|███████   | 579/827 [40:07<15:42,  3.80s/it][A
 70%|███████   | 580/827 [40:11<15:50,  3.85s/it][A
 70%|███████   | 581/827 [40:19<21:01,  5.13s/it][A
 70%|███████   | 582/827 [40:24<20:33,  5.03s/it][A
 70%|███████   | 583/827 [40:27<17:57,  4.41s/it][A
 71%|███████   | 584/827 [40:30<16:30,  4.08s/it][A
 71%|███████   | 585/827 [40:33<15:07,  3.75s/it][A
 71%|███████   | 586/827 [40:37<14:58,  3.73s/it][A
 71%|███████   | 587/827 [40:43<18:03,  4.51s/it][A
 71%|███████   | 588/827 [40:53<24:43,  6.21s/it][A
 71%|███████   | 589/827 [40:58<23:07,  5.83s/it][A
 71%|███████▏  | 590/827 [41:03<22:24,  5.67s/it][A
 71%|███████▏  | 591/827 [41:12<25:41,  6.53s/it][A
 72%|███████▏  | 592/827 [41:15<22:08,  5.65s/it][A
 72%|███████▏  | 593/827 [41:19<19:17,  4.95s/it][A
 72%|███████▏  | 594/827 [41:22<17:16,  4.45s/it][A
 72%|███████▏  | 595/827 [41:25<15:27,  4.00s/it][A
 72%|███████▏  | 596/827 [41:31<17:52,  4.64s/it][A
 72%|███████▏  | 597/827 [41:39<20:56,  5.46s/it][A
 72%|███████▏  | 598/827 [41:42<18:14,  4.78s/it][A
 72%|███████▏  | 599/827 [41:45<16:32,  4.35s/it][A
 73%|███████▎  | 600/827 [41:49<16:04,  4.25s/it][A
 73%|███████▎  | 601/827 [41:54<16:36,  4.41s/it][A
 73%|███████▎  | 602/827 [41:58<16:46,  4.47s/it][A
 73%|███████▎  | 603/827 [42:01<14:59,  4.02s/it][A
 73%|███████▎  | 604/827 [42:05<14:37,  3.93s/it][A
 73%|███████▎  | 605/827 [42:08<13:34,  3.67s/it][A
 73%|███████▎  | 606/827 [42:12<13:13,  3.59s/it][A
 73%|███████▎  | 607/827 [42:16<14:04,  3.84s/it][A
 74%|███████▎  | 608/827 [42:19<13:09,  3.60s/it][A
 74%|███████▎  | 609/827 [42:23<13:45,  3.79s/it][A
 74%|███████▍  | 610/827 [42:32<19:05,  5.28s/it][A
 74%|███████▍  | 611/827 [42:36<17:21,  4.82s/it][A
 74%|███████▍  | 612/827 [42:40<16:39,  4.65s/it][A
 74%|███████▍  | 613/827 [42:44<16:18,  4.57s/it][A
 74%|███████▍  | 614/827 [42:49<16:17,  4.59s/it][A
 74%|███████▍  | 615/827 [42:53<15:09,  4.29s/it][A
 74%|███████▍  | 616/827 [42:56<14:00,  3.98s/it][A
 75%|███████▍  | 617/827 [43:00<13:56,  3.99s/it][A
 75%|███████▍  | 618/827 [43:08<18:12,  5.23s/it][A
 75%|███████▍  | 619/827 [43:11<16:12,  4.67s/it][A
 75%|███████▍  | 620/827 [43:15<15:06,  4.38s/it][A
 75%|███████▌  | 621/827 [43:18<13:26,  3.92s/it][A
 75%|███████▌  | 622/827 [43:21<12:54,  3.78s/it][A
 75%|███████▌  | 623/827 [43:24<11:52,  3.50s/it][A
 75%|███████▌  | 624/827 [43:29<13:07,  3.88s/it][A
 76%|███████▌  | 625/827 [43:33<12:55,  3.84s/it][A
 76%|███████▌  | 626/827 [43:37<12:53,  3.85s/it][A
 76%|███████▌  | 627/827 [43:40<12:42,  3.81s/it][A
 76%|███████▌  | 628/827 [43:43<11:46,  3.55s/it][A
 76%|███████▌  | 629/827 [43:46<11:07,  3.37s/it][A
 76%|███████▌  | 630/827 [43:49<10:46,  3.28s/it][A
 76%|███████▋  | 631/827 [43:54<11:58,  3.67s/it][A
 76%|███████▋  | 632/827 [44:00<14:00,  4.31s/it][A
 77%|███████▋  | 633/827 [44:04<13:30,  4.18s/it][A
 77%|███████▋  | 634/827 [44:12<17:51,  5.55s/it][A
 77%|███████▋  | 635/827 [44:15<15:27,  4.83s/it][A
 77%|███████▋  | 636/827 [44:19<14:21,  4.51s/it][A
 77%|███████▋  | 637/827 [44:24<14:38,  4.62s/it][A
 77%|███████▋  | 638/827 [44:30<15:20,  4.87s/it][A
 77%|███████▋  | 639/827 [44:33<13:50,  4.42s/it][A
 77%|███████▋  | 640/827 [44:36<12:08,  3.89s/it][A
 78%|███████▊  | 641/827 [44:40<12:19,  3.97s/it][A
 78%|███████▊  | 642/827 [44:44<12:16,  3.98s/it][A
 78%|███████▊  | 643/827 [44:48<12:05,  3.95s/it][A
 78%|███████▊  | 644/827 [44:53<13:29,  4.42s/it][A
 78%|███████▊  | 645/827 [44:57<12:50,  4.23s/it][A
 78%|███████▊  | 646/827 [45:00<11:50,  3.93s/it][A
 78%|███████▊  | 647/827 [45:03<11:02,  3.68s/it][A
 78%|███████▊  | 648/827 [45:07<11:00,  3.69s/it][A
 78%|███████▊  | 649/827 [45:11<11:12,  3.78s/it][A
 79%|███████▊  | 650/827 [45:14<10:18,  3.50s/it][A
 79%|███████▊  | 651/827 [45:17<09:59,  3.40s/it][A
 79%|███████▉  | 652/827 [45:20<09:32,  3.27s/it][A
 79%|███████▉  | 653/827 [45:23<09:33,  3.30s/it][A
 79%|███████▉  | 654/827 [45:28<11:03,  3.83s/it][A
 79%|███████▉  | 655/827 [45:35<13:38,  4.76s/it][A
 79%|███████▉  | 656/827 [45:38<11:34,  4.06s/it][A
 79%|███████▉  | 657/827 [45:41<10:45,  3.80s/it][A
 80%|███████▉  | 658/827 [45:45<10:30,  3.73s/it][A
 80%|███████▉  | 659/827 [45:53<14:21,  5.13s/it][A
 80%|███████▉  | 660/827 [45:59<15:22,  5.52s/it][A
 80%|███████▉  | 661/827 [46:06<15:56,  5.76s/it][A
 80%|████████  | 662/827 [46:10<14:17,  5.19s/it][A
 80%|████████  | 663/827 [46:13<12:45,  4.67s/it][A
 80%|████████  | 664/827 [46:19<13:34,  4.99s/it][A
 80%|████████  | 665/827 [46:22<12:21,  4.58s/it][A
 81%|████████  | 666/827 [46:27<12:33,  4.68s/it][A
 81%|████████  | 667/827 [46:30<11:14,  4.22s/it][A
 81%|████████  | 668/827 [46:35<11:06,  4.19s/it][A
 81%|████████  | 669/827 [46:37<09:52,  3.75s/it][A
 81%|████████  | 670/827 [46:40<09:05,  3.47s/it][A
 81%|████████  | 671/827 [46:43<08:50,  3.40s/it][A
 81%|████████▏ | 672/827 [46:47<09:14,  3.58s/it][A
 81%|████████▏ | 673/827 [46:50<08:45,  3.41s/it][A
 81%|████████▏ | 674/827 [46:54<08:44,  3.43s/it][A
 82%|████████▏ | 675/827 [46:57<08:49,  3.48s/it][A
 82%|████████▏ | 676/827 [47:01<09:09,  3.64s/it][A
 82%|████████▏ | 677/827 [47:07<10:16,  4.11s/it][A
 82%|████████▏ | 678/827 [47:11<10:34,  4.26s/it][A
 82%|████████▏ | 679/827 [47:17<11:20,  4.60s/it][A
 82%|████████▏ | 680/827 [47:19<09:55,  4.05s/it][A
 82%|████████▏ | 681/827 [47:24<09:56,  4.08s/it][A
 82%|████████▏ | 682/827 [47:30<11:55,  4.93s/it][A
 83%|████████▎ | 683/827 [47:34<10:34,  4.41s/it][A
 83%|████████▎ | 684/827 [47:37<09:35,  4.02s/it][A
 83%|████████▎ | 685/827 [47:39<08:24,  3.56s/it][A
 83%|████████▎ | 686/827 [47:44<09:08,  3.89s/it][A
 83%|████████▎ | 687/827 [47:52<12:22,  5.30s/it][A
 83%|████████▎ | 688/827 [47:57<11:41,  5.05s/it][A
 83%|████████▎ | 689/827 [48:03<12:11,  5.30s/it][A
 83%|████████▎ | 690/827 [48:06<10:48,  4.73s/it][A
 84%|████████▎ | 691/827 [48:09<09:26,  4.17s/it][A
 84%|████████▎ | 692/827 [48:13<09:26,  4.20s/it][A
 84%|████████▍ | 693/827 [48:17<08:41,  3.89s/it][A
 84%|████████▍ | 694/827 [48:20<08:20,  3.77s/it][A
 84%|████████▍ | 695/827 [48:24<08:20,  3.79s/it][A
 84%|████████▍ | 696/827 [48:29<09:22,  4.29s/it][A
 84%|████████▍ | 697/827 [48:33<08:44,  4.03s/it][A
 84%|████████▍ | 698/827 [48:36<07:58,  3.71s/it][A
 85%|████████▍ | 699/827 [48:38<07:13,  3.39s/it][A
 85%|████████▍ | 700/827 [48:45<09:04,  4.29s/it][A
 85%|████████▍ | 701/827 [48:52<10:59,  5.23s/it][A
 85%|████████▍ | 702/827 [48:56<10:08,  4.87s/it][A
 85%|████████▌ | 703/827 [49:00<09:39,  4.67s/it][A
 85%|████████▌ | 704/827 [49:04<08:46,  4.28s/it][A
 85%|████████▌ | 705/827 [49:06<07:33,  3.72s/it][A
 85%|████████▌ | 706/827 [49:09<06:51,  3.40s/it][A
 85%|████████▌ | 707/827 [49:12<06:53,  3.44s/it][A
 86%|████████▌ | 708/827 [49:16<07:02,  3.55s/it][A
 86%|████████▌ | 709/827 [49:20<07:23,  3.76s/it][A
 86%|████████▌ | 710/827 [49:24<06:58,  3.57s/it][A
 86%|████████▌ | 711/827 [49:27<07:00,  3.62s/it][A
 86%|████████▌ | 712/827 [49:31<06:50,  3.57s/it][A
 86%|████████▌ | 713/827 [49:34<06:39,  3.50s/it][A
 86%|████████▋ | 714/827 [49:38<06:43,  3.57s/it][A
 86%|████████▋ | 715/827 [49:43<07:39,  4.10s/it][A
 87%|████████▋ | 716/827 [49:46<07:04,  3.82s/it][A
 87%|████████▋ | 717/827 [49:52<08:05,  4.41s/it][A
 87%|████████▋ | 718/827 [49:58<08:52,  4.88s/it][A
 87%|████████▋ | 719/827 [50:01<07:48,  4.34s/it][A
 87%|████████▋ | 720/827 [50:05<07:21,  4.13s/it][A
 87%|████████▋ | 721/827 [50:08<06:48,  3.86s/it][A
 87%|████████▋ | 722/827 [50:12<06:41,  3.82s/it][A
 87%|████████▋ | 723/827 [50:15<06:26,  3.71s/it][A
 88%|████████▊ | 724/827 [50:18<05:46,  3.37s/it][A
 88%|████████▊ | 725/827 [50:22<06:00,  3.54s/it][A
 88%|████████▊ | 726/827 [50:29<07:40,  4.56s/it][A
 88%|████████▊ | 727/827 [50:33<07:35,  4.56s/it][A
 88%|████████▊ | 728/827 [50:37<06:56,  4.21s/it][A
 88%|████████▊ | 729/827 [50:39<05:52,  3.59s/it][A
 88%|████████▊ | 730/827 [50:43<05:56,  3.67s/it][A
 88%|████████▊ | 731/827 [50:48<06:37,  4.14s/it][A
 89%|████████▊ | 732/827 [50:54<07:19,  4.62s/it][A
 89%|████████▊ | 733/827 [50:59<07:30,  4.80s/it][A
 89%|████████▉ | 734/827 [51:01<06:16,  4.05s/it][A
 89%|████████▉ | 735/827 [51:05<05:59,  3.90s/it][A
 89%|████████▉ | 736/827 [51:10<06:30,  4.29s/it][A
 89%|████████▉ | 737/827 [51:15<07:00,  4.68s/it][A
 89%|████████▉ | 738/827 [51:19<06:13,  4.19s/it][A
 89%|████████▉ | 739/827 [51:23<06:08,  4.19s/it][A
 89%|████████▉ | 740/827 [51:29<07:04,  4.88s/it][A
 90%|████████▉ | 741/827 [51:33<06:29,  4.53s/it][A
 90%|████████▉ | 742/827 [51:36<06:01,  4.25s/it][A
 90%|████████▉ | 743/827 [51:42<06:38,  4.75s/it][A
 90%|████████▉ | 744/827 [51:47<06:22,  4.61s/it][A
 90%|█████████ | 745/827 [51:51<06:17,  4.60s/it][A
 90%|█████████ | 746/827 [51:56<06:16,  4.65s/it][A
 90%|█████████ | 747/827 [52:03<07:00,  5.26s/it][A
 90%|█████████ | 748/827 [52:06<06:05,  4.63s/it][A
 91%|█████████ | 749/827 [52:10<05:39,  4.35s/it][A
 91%|█████████ | 750/827 [52:14<05:41,  4.43s/it][A
 91%|█████████ | 751/827 [52:17<05:06,  4.04s/it][A
 91%|█████████ | 752/827 [52:20<04:39,  3.72s/it][A
 91%|█████████ | 753/827 [52:23<04:15,  3.45s/it][A
 91%|█████████ | 754/827 [52:26<03:55,  3.23s/it][A
 91%|█████████▏| 755/827 [52:29<03:57,  3.30s/it][A
 91%|█████████▏| 756/827 [52:36<04:58,  4.21s/it][A
 92%|█████████▏| 757/827 [52:39<04:33,  3.91s/it][A
 92%|█████████▏| 758/827 [52:44<04:54,  4.27s/it][A
 92%|█████████▏| 759/827 [52:47<04:23,  3.88s/it][A
 92%|█████████▏| 760/827 [52:51<04:26,  3.98s/it][A
 92%|█████████▏| 761/827 [53:00<06:01,  5.47s/it][A
 92%|█████████▏| 762/827 [53:05<05:48,  5.37s/it][A
 92%|█████████▏| 763/827 [53:14<06:41,  6.27s/it][A
 92%|█████████▏| 764/827 [53:17<05:33,  5.30s/it][A
 93%|█████████▎| 765/827 [53:20<04:50,  4.68s/it][A
 93%|█████████▎| 766/827 [53:23<04:16,  4.21s/it][A
 93%|█████████▎| 767/827 [53:27<04:05,  4.09s/it][A
 93%|█████████▎| 768/827 [53:30<03:42,  3.77s/it][A
 93%|█████████▎| 769/827 [53:35<04:06,  4.25s/it][A
 93%|█████████▎| 770/827 [53:39<03:54,  4.12s/it][A
 93%|█████████▎| 771/827 [53:43<03:46,  4.04s/it][A
 93%|█████████▎| 772/827 [53:47<03:36,  3.93s/it][A
 93%|█████████▎| 773/827 [53:51<03:35,  3.98s/it][A
 94%|█████████▎| 774/827 [53:54<03:24,  3.85s/it][A
 94%|█████████▎| 775/827 [53:58<03:16,  3.78s/it][A
 94%|█████████▍| 776/827 [54:01<03:11,  3.75s/it][A
 94%|█████████▍| 777/827 [54:05<03:05,  3.71s/it][A
 94%|█████████▍| 778/827 [54:09<03:02,  3.73s/it][A
 94%|█████████▍| 779/827 [54:12<02:46,  3.46s/it][A
 94%|█████████▍| 780/827 [54:15<02:37,  3.34s/it][A
 94%|█████████▍| 781/827 [54:18<02:33,  3.33s/it][A
 95%|█████████▍| 782/827 [54:21<02:25,  3.24s/it][A
 95%|█████████▍| 783/827 [54:25<02:27,  3.36s/it][A
 95%|█████████▍| 784/827 [54:28<02:18,  3.22s/it][A
 95%|█████████▍| 785/827 [54:31<02:22,  3.40s/it][A
 95%|█████████▌| 786/827 [54:35<02:17,  3.35s/it][A
 95%|█████████▌| 787/827 [54:41<02:44,  4.12s/it][A
 95%|█████████▌| 788/827 [54:44<02:34,  3.96s/it][A
 95%|█████████▌| 789/827 [54:48<02:34,  4.07s/it][A
 96%|█████████▌| 790/827 [54:52<02:26,  3.96s/it][A
 96%|█████████▌| 791/827 [54:55<02:14,  3.73s/it][A
 96%|█████████▌| 792/827 [54:58<01:59,  3.41s/it][A
 96%|█████████▌| 793/827 [55:02<01:58,  3.49s/it][A
 96%|█████████▌| 794/827 [55:09<02:29,  4.53s/it][A
 96%|█████████▌| 795/827 [55:15<02:38,  4.95s/it][A
 96%|█████████▋| 796/827 [55:20<02:41,  5.21s/it][A
 96%|█████████▋| 797/827 [55:23<02:14,  4.48s/it][A
 96%|█████████▋| 798/827 [55:27<02:00,  4.14s/it][A
 97%|█████████▋| 799/827 [55:29<01:40,  3.59s/it][A
 97%|█████████▋| 800/827 [55:34<01:50,  4.11s/it][A
 97%|█████████▋| 801/827 [55:44<02:34,  5.94s/it][A
 97%|█████████▋| 802/827 [55:50<02:25,  5.82s/it][A
 97%|█████████▋| 803/827 [55:55<02:13,  5.55s/it][A
 97%|█████████▋| 804/827 [56:01<02:11,  5.71s/it][A
 97%|█████████▋| 805/827 [56:04<01:48,  4.93s/it][A
 97%|█████████▋| 806/827 [56:07<01:31,  4.37s/it][A
 98%|█████████▊| 807/827 [56:10<01:20,  4.02s/it][A
 98%|█████████▊| 808/827 [56:14<01:14,  3.95s/it][A
 98%|█████████▊| 809/827 [56:17<01:07,  3.73s/it][A
 98%|█████████▊| 810/827 [56:21<01:02,  3.66s/it][A
 98%|█████████▊| 811/827 [56:24<00:58,  3.66s/it][A
 98%|█████████▊| 812/827 [56:28<00:53,  3.56s/it][A
 98%|█████████▊| 813/827 [56:34<01:01,  4.39s/it][A
 98%|█████████▊| 814/827 [56:41<01:06,  5.10s/it][A
 99%|█████████▊| 815/827 [56:44<00:54,  4.55s/it][A
 99%|█████████▊| 816/827 [56:49<00:51,  4.65s/it][A
 99%|█████████▉| 817/827 [56:52<00:41,  4.15s/it][A
 99%|█████████▉| 818/827 [56:57<00:39,  4.39s/it][A
 99%|█████████▉| 819/827 [57:03<00:38,  4.86s/it][A
 99%|█████████▉| 820/827 [57:08<00:33,  4.84s/it][A
 99%|█████████▉| 821/827 [57:12<00:27,  4.57s/it][A
 99%|█████████▉| 822/827 [57:15<00:21,  4.34s/it][A
100%|█████████▉| 823/827 [57:18<00:15,  3.87s/it][A
100%|█████████▉| 824/827 [57:21<00:10,  3.48s/it][A
100%|█████████▉| 825/827 [57:25<00:07,  3.58s/it][A
100%|█████████▉| 826/827 [57:32<00:04,  4.71s/it][A
100%|██████████| 827/827 [57:35<00:00,  4.15s/it][A                                                        
                                                 [A{'eval_loss': 0.6727138161659241, 'eval_runtime': 3460.2491, 'eval_samples_per_second': 0.955, 'eval_steps_per_second': 0.239, 'epoch': 0.86}
 43%|████▎     | 400/930 [17:39:18<21:40:59, 147.28s/it]
100%|██████████| 827/827 [57:35<00:00,  4.15s/it][A
                                                 [A[INFO|trainer.py:4309] 2025-12-29 13:19:34,162 >> Saving model checkpoint to saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-400
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in Video-R1/Video-R1-7B - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2421] 2025-12-29 13:19:34,264 >> chat template saved in saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-400/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-29 13:19:34,267 >> tokenizer config file saved in saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-29 13:19:34,270 >> Special tokens file saved in saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-400/special_tokens_map.json
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once
[2025-12-29 13:19:34,927] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step400 is about to be saved!
[2025-12-29 13:19:34,951] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-400/global_step400/mp_rank_00_model_states.pt
[2025-12-29 13:19:34,951] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-400/global_step400/mp_rank_00_model_states.pt...
[2025-12-29 13:19:35,012] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-400/global_step400/mp_rank_00_model_states.pt.
[2025-12-29 13:19:35,015] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-12-29 13:19:35,015] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-400/global_step400/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2025-12-29 13:19:35,015] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-400/global_step400/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2025-12-29 13:19:35,015] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-400/global_step400/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2025-12-29 13:19:35,175] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-400/global_step400/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2025-12-29 13:19:35,175] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-400/global_step400/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2025-12-29 13:19:35,175] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2025-12-29 13:19:35,185] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-400/global_step400/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2025-12-29 13:19:35,185] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-400/global_step400/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2025-12-29 13:19:35,185] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2025-12-29 13:19:35,185] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-400/global_step400/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2025-12-29 13:19:35,186] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-400/global_step400/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2025-12-29 13:19:35,186] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2025-12-29 13:19:35,190] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-12-29 13:19:35,197] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-12-29 13:19:35,197] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[INFO|image_processing_base.py:253] 2025-12-29 13:19:35,218 >> Image processor saved in saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-400/preprocessor_config.json
[INFO|tokenization_utils_base.py:2421] 2025-12-29 13:19:35,221 >> chat template saved in saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-400/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-29 13:19:35,224 >> tokenizer config file saved in saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-29 13:19:35,226 >> Special tokens file saved in saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-400/special_tokens_map.json
[INFO|video_processing_utils.py:600] 2025-12-29 13:19:35,426 >> Video processor saved in saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-400/video_preprocessor_config.json
[INFO|processing_utils.py:814] 2025-12-29 13:19:35,429 >> chat template saved in saves/videor1/lora/sft/SQA3Devery24_traineval_continued/checkpoint-400/chat_template.jinja
 43%|████▎     | 401/930 [17:41:43<174:04:43, 1184.66s/it] 43%|████▎     | 402/930 [17:44:23<128:41:22, 877.43s/it]  43%|████▎     | 403/930 [17:46:38<95:50:11, 654.67s/it]  43%|████▎     | 404/930 [17:48:38<72:11:44, 494.11s/it] 44%|████▎     | 405/930 [17:50:40<55:47:31, 382.57s/it] 44%|████▎     | 406/930 [17:52:23<43:29:27, 298.79s/it] 44%|████▍     | 407/930 [17:55:00<37:13:30, 256.23s/it] 44%|████▍     | 408/930 [17:56:58<31:09:12, 214.85s/it] 44%|████▍     | 409/930 [17:59:15<27:42:17, 191.43s/it] 44%|████▍     | 410/930 [18:01:25<24:58:01, 172.85s/it]                                                        {'loss': 0.6821, 'grad_norm': 0.5300106406211853, 'learning_rate': 6.876512527401897e-05, 'epoch': 0.88}
 44%|████▍     | 410/930 [18:01:25<24:58:01, 172.85s/it] 44%|████▍     | 411/930 [18:04:42<25:59:33, 180.30s/it] 44%|████▍     | 412/930 [18:06:48<23:34:08, 163.80s/it] 44%|████▍     | 413/930 [18:09:49<24:15:59, 168.97s/it] 45%|████▍     | 414/930 [18:12:26<23:43:54, 165.57s/it] 45%|████▍     | 415/930 [18:15:17<23:53:44, 167.04s/it] 45%|████▍     | 416/930 [18:17:37<22:41:52, 158.97s/it] 45%|████▍     | 417/930 [18:20:21<22:52:32, 160.53s/it] 45%|████▍     | 418/930 [18:22:15<20:50:06, 146.50s/it] 45%|████▌     | 419/930 [18:25:05<21:48:26, 153.63s/it] 45%|████▌     | 420/930 [18:27:23<21:06:18, 148.98s/it]                                                        {'loss': 0.6948, 'grad_norm': 0.45200908184051514, 'learning_rate': 6.701280113445324e-05, 'epoch': 0.9}
 45%|████▌     | 420/930 [18:27:23<21:06:18, 148.98s/it] 45%|████▌     | 421/930 [18:29:23<19:49:07, 140.17s/it] 45%|████▌     | 422/930 [18:31:57<20:21:04, 144.22s/it] 45%|████▌     | 423/930 [18:34:11<19:53:34, 141.25s/it] 46%|████▌     | 424/930 [18:36:59<20:59:00, 149.29s/it] 46%|████▌     | 425/930 [18:39:33<21:08:21, 150.70s/it] 46%|████▌     | 426/930 [18:41:56<20:46:29, 148.39s/it] 46%|████▌     | 427/930 [18:44:01<19:44:26, 141.29s/it] 46%|████▌     | 428/930 [18:46:37<20:20:24, 145.86s/it] 46%|████▌     | 429/930 [18:49:32<21:29:15, 154.40s/it] 46%|████▌     | 430/930 [18:51:51<20:49:50, 149.98s/it]                                                        {'loss': 0.6911, 'grad_norm': 0.48408710956573486, 'learning_rate': 6.523651220265268e-05, 'epoch': 0.93}
 46%|████▌     | 430/930 [18:51:51<20:49:50, 149.98s/it] 46%|████▋     | 431/930 [18:54:25<20:57:17, 151.18s/it] 46%|████▋     | 432/930 [18:56:53<20:46:11, 150.14s/it] 47%|████▋     | 433/930 [18:58:52<19:26:21, 140.81s/it] 47%|████▋     | 434/930 [19:01:52<21:00:55, 152.53s/it] 47%|████▋     | 435/930 [19:04:49<21:59:28, 159.94s/it] 47%|████▋     | 436/930 [19:06:38<19:51:16, 144.69s/it] 47%|████▋     | 437/930 [19:09:07<19:59:03, 145.93s/it] 47%|████▋     | 438/930 [19:11:21<19:27:07, 142.33s/it] 47%|████▋     | 439/930 [19:13:34<19:01:10, 139.45s/it] 47%|████▋     | 440/930 [19:16:25<20:16:10, 148.92s/it]                                                        {'loss': 0.6982, 'grad_norm': 0.5150611400604248, 'learning_rate': 6.343876061773385e-05, 'epoch': 0.95}
 47%|████▋     | 440/930 [19:16:25<20:16:10, 148.92s/it] 47%|████▋     | 441/930 [19:18:47<19:57:46, 146.97s/it] 48%|████▊     | 442/930 [19:21:06<19:35:16, 144.50s/it] 48%|████▊     | 443/930 [19:23:20<19:07:18, 141.35s/it] 48%|████▊     | 444/930 [19:25:36<18:53:12, 139.90s/it] 48%|████▊     | 445/930 [19:27:56<18:51:08, 139.94s/it] 48%|████▊     | 446/930 [19:30:26<19:11:54, 142.80s/it] 48%|████▊     | 447/930 [19:32:38<18:42:44, 139.47s/it] 48%|████▊     | 448/930 [19:35:08<19:07:44, 142.87s/it] 48%|████▊     | 449/930 [19:37:02<17:56:09, 134.24s/it] 48%|████▊     | 450/930 [19:38:48<16:45:52, 125.73s/it]                                                        {'loss': 0.695, 'grad_norm': 0.5035961270332336, 'learning_rate': 6.162207875181354e-05, 'epoch': 0.97}
 48%|████▊     | 450/930 [19:38:48<16:45:52, 125.73s/it] 48%|████▊     | 451/930 [19:41:07<17:15:44, 129.74s/it] 49%|████▊     | 452/930 [19:43:20<17:19:28, 130.48s/it] 49%|████▊     | 453/930 [19:45:50<18:06:00, 136.60s/it] 49%|████▉     | 454/930 [19:49:12<20:38:49, 156.15s/it] 49%|████▉     | 455/930 [19:51:16<19:18:49, 146.38s/it] 49%|████▉     | 456/930 [19:53:36<19:02:38, 144.64s/it] 49%|████▉     | 457/930 [19:55:36<18:01:00, 137.12s/it] 49%|████▉     | 458/930 [19:57:55<18:03:57, 137.79s/it] 49%|████▉     | 459/930 [19:59:47<16:59:35, 129.88s/it] 49%|████▉     | 460/930 [20:02:40<18:39:38, 142.93s/it]                                                        {'loss': 0.6862, 'grad_norm': 0.5575382113456726, 'learning_rate': 5.978902564282616e-05, 'epoch': 0.99}
 49%|████▉     | 460/930 [20:02:40<18:39:38, 142.93s/it] 50%|████▉     | 461/930 [20:04:53<18:13:26, 139.89s/it] 50%|████▉     | 462/930 [20:07:45<19:26:09, 149.51s/it] 50%|████▉     | 463/930 [20:10:18<19:32:57, 150.70s/it] 50%|████▉     | 464/930 [20:12:52<19:38:10, 151.70s/it] 50%|█████     | 465/930 [20:14:00<16:19:43, 126.42s/it] 50%|█████     | 466/930 [20:16:08<16:21:06, 126.87s/it] 50%|█████     | 467/930 [20:18:25<16:43:41, 130.07s/it] 50%|█████     | 468/930 [20:21:04<17:47:00, 138.57s/it] 50%|█████     | 469/930 [20:23:48<18:44:24, 146.34s/it] 51%|█████     | 470/930 [20:25:44<17:32:21, 137.26s/it]                                                        {'loss': 0.6604, 'grad_norm': 0.5119956135749817, 'learning_rate': 5.794218338977854e-05, 'epoch': 1.01}
 51%|█████     | 470/930 [20:25:44<17:32:21, 137.26s/it] 51%|█████     | 471/930 [20:28:11<17:50:43, 139.97s/it] 51%|█████     | 472/930 [20:30:43<18:17:22, 143.76s/it] 51%|█████     | 473/930 [20:33:11<18:24:44, 145.04s/it] 51%|█████     | 474/930 [20:34:45<16:26:34, 129.81s/it] 51%|█████     | 475/930 [20:37:27<17:36:04, 139.26s/it] 51%|█████     | 476/930 [20:40:30<19:12:32, 152.32s/it] 51%|█████▏    | 477/930 [20:43:13<19:36:05, 155.77s/it] 51%|█████▏    | 478/930 [20:45:42<19:17:26, 153.64s/it] 52%|█████▏    | 479/930 [20:47:42<17:58:18, 143.45s/it] 52%|█████▏    | 480/930 [20:50:17<18:23:08, 147.09s/it]                                                        {'loss': 0.6409, 'grad_norm': 0.5551992058753967, 'learning_rate': 5.6084153515520134e-05, 'epoch': 1.03}
 52%|█████▏    | 480/930 [20:50:17<18:23:08, 147.09s/it] 52%|█████▏    | 481/930 [20:52:07<16:55:42, 135.73s/it] 52%|█████▏    | 482/930 [20:54:45<17:45:28, 142.70s/it] 52%|█████▏    | 483/930 [20:56:56<17:15:39, 139.01s/it] 52%|█████▏    | 484/930 [20:58:54<16:26:11, 132.67s/it] 52%|█████▏    | 485/930 [21:01:09<16:30:23, 133.54s/it] 52%|█████▏    | 486/930 [21:03:45<17:16:28, 140.06s/it] 52%|█████▏    | 487/930 [21:06:35<18:21:15, 149.15s/it] 52%|█████▏    | 488/930 [21:08:50<17:47:42, 144.94s/it] 53%|█████▎    | 489/930 [21:11:17<17:48:46, 145.41s/it] 53%|█████▎    | 490/930 [21:13:36<17:32:55, 143.58s/it]                                                        {'loss': 0.6729, 'grad_norm': 0.49012938141822815, 'learning_rate': 5.4217553302152237e-05, 'epoch': 1.05}
 53%|█████▎    | 490/930 [21:13:36<17:32:55, 143.58s/it] 53%|█████▎    | 491/930 [21:15:45<16:57:53, 139.12s/it] 53%|█████▎    | 492/930 [21:17:58<16:43:45, 137.50s/it] 53%|█████▎    | 493/930 [21:20:33<17:18:48, 142.63s/it] 53%|█████▎    | 494/930 [21:23:12<17:51:24, 147.44s/it] 53%|█████▎    | 495/930 [21:25:28<17:25:16, 144.18s/it] 53%|█████▎    | 496/930 [21:27:57<17:33:14, 145.61s/it] 53%|█████▎    | 497/930 [21:30:28<17:42:31, 147.23s/it] 54%|█████▎    | 498/930 [21:32:48<17:24:20, 145.05s/it] 54%|█████▎    | 499/930 [21:35:49<18:38:20, 155.69s/it] 54%|█████▍    | 500/930 [21:38:06<17:56:00, 150.14s/it]                                                        {'loss': 0.6563, 'grad_norm': 0.5062316656112671, 'learning_rate': 5.23450121042383e-05, 'epoch': 1.08}
 54%|█████▍    | 500/930 [21:38:06<17:56:00, 150.14s/it] 54%|█████▍    | 501/930 [21:40:07<16:52:20, 141.59s/it] 54%|█████▍    | 502/930 [21:42:29<16:49:32, 141.52s/it] 54%|█████▍    | 503/930 [21:44:46<16:37:33, 140.17s/it] 54%|█████▍    | 504/930 [21:46:44<15:49:16, 133.70s/it] 54%|█████▍    | 505/930 [21:49:37<17:10:26, 145.48s/it] 54%|█████▍    | 506/930 [21:52:29<18:04:10, 153.42s/it] 55%|█████▍    | 507/930 [21:54:46<17:25:17, 148.27s/it] 55%|█████▍    | 508/930 [21:57:27<17:49:45, 152.10s/it] 55%|█████▍    | 509/930 [22:00:37<19:06:48, 163.44s/it] 55%|█████▍    | 510/930 [22:02:38<17:34:54, 150.70s/it]                                                        {'loss': 0.6586, 'grad_norm': 0.47996529936790466, 'learning_rate': 5.046916764500824e-05, 'epoch': 1.1}
 55%|█████▍    | 510/930 [22:02:38<17:34:54, 150.70s/it] 55%|█████▍    | 511/930 [22:04:45<16:44:01, 143.77s/it] 55%|█████▌    | 512/930 [22:07:34<17:34:30, 151.37s/it] 55%|█████▌    | 513/930 [22:10:32<18:26:12, 159.17s/it] 55%|█████▌    | 514/930 [22:14:00<20:06:01, 173.95s/it] 55%|█████▌    | 515/930 [22:16:15<18:42:44, 162.32s/it] 55%|█████▌    | 516/930 [22:19:11<19:08:54, 166.51s/it] 56%|█████▌    | 517/930 [22:21:26<18:00:55, 157.03s/it] 56%|█████▌    | 518/930 [22:23:16<16:20:30, 142.79s/it] 56%|█████▌    | 519/930 [22:26:21<17:45:07, 155.49s/it] 56%|█████▌    | 520/930 [22:28:45<17:18:19, 151.95s/it]                                                        {'loss': 0.6686, 'grad_norm': 0.4996934235095978, 'learning_rate': 4.859266230077474e-05, 'epoch': 1.12}
 56%|█████▌    | 520/930 [22:28:45<17:18:19, 151.95s/it] 56%|█████▌    | 521/930 [22:30:51<16:22:58, 144.20s/it] 56%|█████▌    | 522/930 [22:32:42<15:14:04, 134.42s/it] 56%|█████▌    | 523/930 [22:35:10<15:38:16, 138.32s/it] 56%|█████▋    | 524/930 [22:37:39<15:58:09, 141.60s/it] 56%|█████▋    | 525/930 [22:40:43<17:21:16, 154.26s/it] 57%|█████▋    | 526/930 [22:43:17<17:17:16, 154.05s/it] 57%|█████▋    | 527/930 [22:45:40<16:52:30, 150.75s/it] 57%|█████▋    | 528/930 [22:47:46<16:01:36, 143.52s/it] 57%|█████▋    | 529/930 [22:49:46<15:10:54, 136.30s/it] 57%|█████▋    | 530/930 [22:51:53<14:50:15, 133.54s/it]                                                        {'loss': 0.6556, 'grad_norm': 0.5304762125015259, 'learning_rate': 4.671813937879494e-05, 'epoch': 1.14}
 57%|█████▋    | 530/930 [22:51:53<14:50:15, 133.54s/it] 57%|█████▋    | 531/930 [22:54:12<14:59:05, 135.20s/it] 57%|█████▋    | 532/930 [22:57:06<16:13:26, 146.75s/it] 57%|█████▋    | 533/930 [22:59:37<16:20:19, 148.16s/it] 57%|█████▋    | 534/930 [23:01:47<15:41:14, 142.61s/it] 58%|█████▊    | 535/930 [23:03:47<14:55:10, 135.98s/it] 58%|█████▊    | 536/930 [23:07:02<16:49:46, 153.77s/it] 58%|█████▊    | 537/930 [23:09:18<16:11:32, 148.33s/it] 58%|█████▊    | 538/930 [23:11:45<16:06:58, 148.01s/it] 58%|█████▊    | 539/930 [23:13:31<14:42:04, 135.36s/it] 58%|█████▊    | 540/930 [23:16:17<15:38:41, 144.42s/it]                                                        {'loss': 0.6592, 'grad_norm': 0.518592119216919, 'learning_rate': 4.4848239393820564e-05, 'epoch': 1.16}
 58%|█████▊    | 540/930 [23:16:17<15:38:41, 144.42s/it] 58%|█████▊    | 541/930 [23:18:33<15:19:34, 141.84s/it] 58%|█████▊    | 542/930 [23:20:58<15:24:56, 143.03s/it] 58%|█████▊    | 543/930 [23:23:01<14:43:43, 137.01s/it] 58%|█████▊    | 544/930 [23:25:07<14:18:43, 133.48s/it] 59%|█████▊    | 545/930 [23:27:21<14:17:37, 133.66s/it] 59%|█████▊    | 546/930 [23:29:18<13:43:14, 128.63s/it] 59%|█████▉    | 547/930 [23:31:47<14:20:28, 134.80s/it] 59%|█████▉    | 548/930 [23:34:08<14:29:41, 136.60s/it] 59%|█████▉    | 549/930 [23:36:42<15:02:00, 142.05s/it]slurmstepd: error: *** JOB 1683514 ON kn173 CANCELLED AT 2025-12-29T19:18:30 DUE TO TIME LIMIT ***
