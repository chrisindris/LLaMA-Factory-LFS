/project/aip-wangcs/indrisch/LLaMA-Factory /project/aip-wangcs/indrisch/LLaMA-Factory/preliminaries/videor1
[INFO|2025-12-16 03:00:09] llamafactory.launcher:143 >> Initializing 1 distributed tasks at: 127.0.0.1:35173
[WARNING|2025-12-16 03:00:18] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
⚙️  Running in WANDB offline mode
[2025-12-16 03:00:21,208] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
df: /tmp/.triton_cache: No such file or directory
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2025-12-16 03:00:25,118] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-12-16 03:00:25,118] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[W1216 03:00:25.578351462 ProcessGroupNCCL.cpp:924] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[INFO|2025-12-16 03:00:25] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.
[INFO|2025-12-16 03:00:25] llamafactory.hparams.parser:455 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|hub.py:421] 2025-12-16 03:00:25,370 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2025-12-16 03:00:25,395 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2025-12-16 03:00:25,403 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Qwen2.5-VL-7B-COT-SFT/snapshots/f71f0f1e22c015007fccd080eef87824fe292a10/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-12-16 03:00:25,403 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Qwen2.5-VL-7B-COT-SFT/snapshots/f71f0f1e22c015007fccd080eef87824fe292a10/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-12-16 03:00:25,403 >> loading file added_tokens.json from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Qwen2.5-VL-7B-COT-SFT/snapshots/f71f0f1e22c015007fccd080eef87824fe292a10/added_tokens.json
[INFO|tokenization_utils_base.py:2095] 2025-12-16 03:00:25,403 >> loading file special_tokens_map.json from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Qwen2.5-VL-7B-COT-SFT/snapshots/f71f0f1e22c015007fccd080eef87824fe292a10/special_tokens_map.json
[INFO|tokenization_utils_base.py:2095] 2025-12-16 03:00:25,403 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Qwen2.5-VL-7B-COT-SFT/snapshots/f71f0f1e22c015007fccd080eef87824fe292a10/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-12-16 03:00:25,403 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Qwen2.5-VL-7B-COT-SFT/snapshots/f71f0f1e22c015007fccd080eef87824fe292a10/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-12-16 03:00:25,403 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-12-16 03:00:25,586 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|hub.py:421] 2025-12-16 03:00:25,586 >> Offline mode: forcing local_files_only=True
[INFO|hub.py:421] 2025-12-16 03:00:25,586 >> Offline mode: forcing local_files_only=True
[INFO|hub.py:421] 2025-12-16 03:00:25,586 >> Offline mode: forcing local_files_only=True
[INFO|hub.py:421] 2025-12-16 03:00:25,587 >> Offline mode: forcing local_files_only=True
[INFO|hub.py:421] 2025-12-16 03:00:25,587 >> Offline mode: forcing local_files_only=True
[INFO|hub.py:421] 2025-12-16 03:00:25,587 >> Offline mode: forcing local_files_only=True
[INFO|configuration_utils.py:765] 2025-12-16 03:00:25,590 >> loading configuration file config.json from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Qwen2.5-VL-7B-COT-SFT/snapshots/f71f0f1e22c015007fccd080eef87824fe292a10/config.json
[INFO|configuration_utils.py:839] 2025-12-16 03:00:25,602 >> Model config Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 128000,
  "max_window_layers": 28,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "_name_or_path": "Video-R1/Qwen2.5-VL-7B-COT-SFT",
    "architectures": [
      "Qwen2_5_VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "dtype": "bfloat16",
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "layer_types": [
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention"
    ],
    "max_position_embeddings": 128000,
    "max_window_layers": 28,
    "model_type": "qwen2_5_vl_text",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "use_cache": true,
    "use_sliding_window": false,
    "vision_token_id": 151654,
    "vocab_size": 152064
  },
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "dtype": "float32",
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 3584,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

[INFO|image_processing_base.py:316] 2025-12-16 03:00:25,607 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2025-12-16 03:00:25,611 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Qwen2.5-VL-7B-COT-SFT/snapshots/f71f0f1e22c015007fccd080eef87824fe292a10/preprocessor_config.json
[INFO|image_processing_base.py:428] 2025-12-16 03:00:25,618 >> Image processor Qwen2VLImageProcessor {
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "max_pixels": 401408,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 401408,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:1949] 2025-12-16 03:00:25,620 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2025-12-16 03:00:25,623 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Qwen2.5-VL-7B-COT-SFT/snapshots/f71f0f1e22c015007fccd080eef87824fe292a10/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-12-16 03:00:25,623 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Qwen2.5-VL-7B-COT-SFT/snapshots/f71f0f1e22c015007fccd080eef87824fe292a10/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-12-16 03:00:25,623 >> loading file added_tokens.json from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Qwen2.5-VL-7B-COT-SFT/snapshots/f71f0f1e22c015007fccd080eef87824fe292a10/added_tokens.json
[INFO|tokenization_utils_base.py:2095] 2025-12-16 03:00:25,623 >> loading file special_tokens_map.json from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Qwen2.5-VL-7B-COT-SFT/snapshots/f71f0f1e22c015007fccd080eef87824fe292a10/special_tokens_map.json
[INFO|tokenization_utils_base.py:2095] 2025-12-16 03:00:25,623 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Qwen2.5-VL-7B-COT-SFT/snapshots/f71f0f1e22c015007fccd080eef87824fe292a10/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-12-16 03:00:25,623 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Qwen2.5-VL-7B-COT-SFT/snapshots/f71f0f1e22c015007fccd080eef87824fe292a10/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-12-16 03:00:25,623 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-12-16 03:00:25,771 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:660] 2025-12-16 03:00:25,774 >> Offline mode: forcing local_files_only=True
[INFO|video_processing_utils.py:726] 2025-12-16 03:00:25,777 >> loading configuration file video_preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Video-R1--Qwen2.5-VL-7B-COT-SFT/snapshots/f71f0f1e22c015007fccd080eef87824fe292a10/preprocessor_config.json
[INFO|video_processing_utils.py:770] 2025-12-16 03:00:25,785 >> Video processor Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 401408,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 401408,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:928] 2025-12-16 03:00:25,785 >> Offline mode: forcing local_files_only=True
[INFO|processing_utils.py:1116] 2025-12-16 03:00:25,790 >> loading configuration file processor_config.json from cache at None
[INFO|processing_utils.py:1199] 2025-12-16 03:00:26,120 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessor {
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "max_pixels": 401408,
  "merge_size": 2,
  "min_pixels": 3136,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "longest_edge": 401408,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2Tokenizer(name_or_path='Video-R1/Qwen2.5-VL-7B-COT-SFT', vocab_size=151643, model_max_length=131072, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 401408,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 401408,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2_5_VLProcessor"
}

[INFO|2025-12-16 03:00:26] llamafactory.data.loader:143 >> Loading dataset /scratch/indrisch/huggingface/hub/datasets--cvis-tmu--llamafactory-sqa3d-traces-multiimage-vqa/snapshots/ce5c54adc1608d1726730c9ff334e65b6dd70e46/data/...
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
Converting format of dataset (num_proc=32):   0%|          | 0/33047 [00:00<?, ? examples/s]Converting format of dataset (num_proc=32):   0%|          | 1/33047 [00:00<4:06:23,  2.24 examples/s]Converting format of dataset (num_proc=32):   0%|          | 90/33047 [00:00<02:32, 216.57 examples/s]Converting format of dataset (num_proc=32):   1%|          | 231/33047 [00:00<01:03, 514.27 examples/s]Converting format of dataset (num_proc=32):   1%|▏         | 448/33047 [00:00<00:34, 938.41 examples/s]Converting format of dataset (num_proc=32):   2%|▏         | 698/33047 [00:00<00:23, 1358.03 examples/s]Converting format of dataset (num_proc=32):   3%|▎         | 1150/33047 [00:00<00:14, 2209.72 examples/s]Converting format of dataset (num_proc=32):   5%|▍         | 1561/33047 [00:01<00:11, 2737.51 examples/s]Converting format of dataset (num_proc=32):   9%|▊         | 2858/33047 [00:01<00:05, 5649.35 examples/s]Converting format of dataset (num_proc=32):  13%|█▎        | 4174/33047 [00:01<00:03, 7812.48 examples/s]Converting format of dataset (num_proc=32):  17%|█▋        | 5464/33047 [00:01<00:02, 9261.88 examples/s]Converting format of dataset (num_proc=32):  21%|██        | 6882/33047 [00:01<00:02, 10533.36 examples/s]Converting format of dataset (num_proc=32):  25%|██▌       | 8319/33047 [00:01<00:02, 11621.32 examples/s]Converting format of dataset (num_proc=32):  29%|██▉       | 9666/33047 [00:01<00:01, 12093.49 examples/s]Converting format of dataset (num_proc=32):  33%|███▎      | 11059/33047 [00:01<00:01, 12597.24 examples/s]Converting format of dataset (num_proc=32):  37%|███▋      | 12348/33047 [00:01<00:01, 12535.40 examples/s]Converting format of dataset (num_proc=32):  42%|████▏     | 13806/33047 [00:01<00:01, 13134.84 examples/s]Converting format of dataset (num_proc=32):  46%|████▌     | 15128/33047 [00:02<00:01, 13043.28 examples/s]Converting format of dataset (num_proc=32):  50%|████▉     | 16447/33047 [00:02<00:01, 12937.41 examples/s]Converting format of dataset (num_proc=32):  54%|█████▍    | 17979/33047 [00:02<00:01, 13611.67 examples/s]Converting format of dataset (num_proc=32):  59%|█████▉    | 19423/33047 [00:02<00:01, 13563.01 examples/s]Converting format of dataset (num_proc=32):  63%|██████▎   | 20862/33047 [00:02<00:00, 13566.69 examples/s]Converting format of dataset (num_proc=32):  67%|██████▋   | 22222/33047 [00:02<00:00, 12724.50 examples/s]Converting format of dataset (num_proc=32):  72%|███████▏  | 23651/33047 [00:02<00:00, 12839.34 examples/s]Converting format of dataset (num_proc=32):  76%|███████▌  | 25112/33047 [00:02<00:00, 13319.47 examples/s]Converting format of dataset (num_proc=32):  80%|████████  | 26462/33047 [00:02<00:00, 12902.55 examples/s]Converting format of dataset (num_proc=32):  84%|████████▍ | 27796/33047 [00:03<00:00, 12996.03 examples/s]Converting format of dataset (num_proc=32):  88%|████████▊ | 29119/33047 [00:03<00:00, 12415.13 examples/s]Converting format of dataset (num_proc=32):  92%|█████████▏| 30385/33047 [00:03<00:00, 10783.94 examples/s]Converting format of dataset (num_proc=32):  95%|█████████▌| 31504/33047 [00:03<00:00, 9606.94 examples/s] Converting format of dataset (num_proc=32):  98%|█████████▊| 32513/33047 [00:03<00:00, 8354.46 examples/s]Converting format of dataset (num_proc=32): 100%|██████████| 33047/33047 [00:04<00:00, 7756.56 examples/s]
Running tokenizer on dataset (num_proc=32):   0%|          | 0/33047 [00:00<?, ? examples/s]slurmstepd: error: *** JOB 1631045 ON kn174 CANCELLED AT 2025-12-16T03:10:06 DUE TO TIME LIMIT ***
