
==========
== CUDA ==
==========

CUDA Version 12.4.1

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

[INFO|2025-12-04 10:08:55] llamafactory.launcher:143 >> Initializing 1 distributed tasks at: 127.0.0.1:56537
⚙️  Running in WANDB offline mode
[2025-12-04 10:09:04,847] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2025-12-04 10:09:14,352] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-04 10:09:14,352] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[INFO|2025-12-04 10:09:14] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.
[INFO|2025-12-04 10:09:14] llamafactory.hparams.parser:423 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|hub.py:421] 2025-12-04 10:09:14,495 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2025-12-04 10:09:14,577 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2025-12-04 10:09:14,588 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-12-04 10:09:14,588 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-12-04 10:09:14,588 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-12-04 10:09:14,588 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-04 10:09:14,588 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-04 10:09:14,589 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-12-04 10:09:14,589 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-12-04 10:09:15,031 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|hub.py:421] 2025-12-04 10:09:15,031 >> Offline mode: forcing local_files_only=True
[INFO|hub.py:421] 2025-12-04 10:09:15,033 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:316] 2025-12-04 10:09:15,034 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2025-12-04 10:09:15,035 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:316] 2025-12-04 10:09:15,039 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2025-12-04 10:09:15,041 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:428] 2025-12-04 10:09:15,051 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:1949] 2025-12-04 10:09:15,051 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2025-12-04 10:09:15,056 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-12-04 10:09:15,056 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-12-04 10:09:15,056 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-12-04 10:09:15,056 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-04 10:09:15,056 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-04 10:09:15,056 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-12-04 10:09:15,056 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-12-04 10:09:15,274 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:660] 2025-12-04 10:09:15,278 >> Offline mode: forcing local_files_only=True
[INFO|video_processing_utils.py:726] 2025-12-04 10:09:15,281 >> loading configuration file video_preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|video_processing_utils.py:770] 2025-12-04 10:09:15,286 >> Video processor Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:928] 2025-12-04 10:09:15,286 >> Offline mode: forcing local_files_only=True
[INFO|processing_utils.py:1116] 2025-12-04 10:09:15,290 >> loading configuration file processor_config.json from cache at None
[INFO|processing_utils.py:1199] 2025-12-04 10:09:15,647 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2_5_VLProcessor"
}

[INFO|2025-12-04 10:09:15] llamafactory.data.loader:143 >> Loading dataset /scratch/indrisch/huggingface/hub/datasets--cvis-tmu--llamafactory-sqa3d-traces-multiimage-vqa/snapshots/ce5c54adc1608d1726730c9ff334e65b6dd70e46/data/...
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
training example:
input_ids:
[151644, 8948, 271, 262, 1446, 525, 264, 16585, 11129, 4142, 11528, 17847, 369, 27979, 647, 47522, 389, 264, 3175, 29519, 6109, 6839, 3941, 5248, 5335, 382, 262, 26149, 510, 262, 68252, 1283, 419, 1943, 11, 498, 686, 5258, 451, 6109, 5335, 438, 366, 1805, 29, 9492, 13, 18877, 1105, 438, 2155, 6194, 315, 279, 83490, 6109, 13, 28634, 311, 5335, 553, 220, 16, 5980, 1922, 304, 1973, 315, 11094, 25, 508, 16, 1125, 508, 17, 1125, 4593, 11, 508, 45, 29562, 262, 3596, 35304, 1718, 8575, 38439, 3298, 14017, 510, 262, 7288, 7854, 264, 12966, 220, 18, 35, 10502, 1614, 3941, 6194, 25, 3754, 6171, 3941, 5335, 11, 23583, 8674, 6249, 17040, 11, 323, 990, 5452, 14, 509, 8957, 14688, 21060, 369, 7990, 55916, 624, 262, 7288, 72077, 409, 849, 292, 3793, 26087, 983, 847, 1290, 97089, 17996, 4065, 14, 29898, 484, 32511, 56111, 7612, 1825, 35978, 28455, 448, 5091, 311, 279, 64171, 14, 8092, 13, 1416, 279, 58385, 374, 54761, 11, 1638, 311, 279, 1429, 38219, 8622, 1651, 323, 1977, 773, 624, 262, 7288, 5443, 1172, 9434, 5904, 26, 653, 537, 17023, 63133, 3565, 476, 17188, 389, 9250, 6540, 7797, 6770, 1633, 43898, 3793, 624, 262, 7288, 3197, 5248, 11178, 3000, 11, 8830, 553, 279, 1850, 2432, 311, 27979, 17133, 488, 4274, 1835, 3941, 6194, 13, 1416, 2058, 54761, 1283, 13295, 678, 5335, 11, 1584, 429, 9355, 382, 262, 30990, 320, 327, 32739, 1378, 10010, 11, 304, 419, 1973, 982, 262, 366, 26865, 397, 262, 14822, 14319, 29208, 32711, 304, 220, 18, 4142, 16, 15, 2805, 7354, 13, 356, 632, 5904, 448, 2168, 14937, 320, 68, 1302, 2572, 65750, 18, 5669, 3100, 7579, 18010, 1290, 315, 8718, 26, 508, 16, 5669, 1852, 1894, 11, 42396, 64212, 2823, 63594, 714, 4583, 320, 2640, 37294, 17, 20, 15, 11211, 7241, 5871, 568, 7036, 894, 17796, 8957, 14, 2969, 26745, 487, 323, 1246, 498, 19673, 432, 624, 262, 690, 26865, 397, 262, 366, 9217, 397, 262, 362, 3175, 63594, 4226, 1172, 11, 892, 1231, 387, 510, 262, 7288, 1416, 14016, 25, 3776, 37328, 4226, 11, 4504, 320, 68, 1302, 2572, 1036, 17, 32511, 476, 7414, 14, 2753, 320, 68, 1302, 2572, 1036, 9693, 854, 4292, 262, 7288, 1416, 537, 14016, 25, 362, 11652, 476, 1378, 624, 262, 1416, 4226, 537, 9434, 11, 421, 1052, 374, 902, 2797, 5904, 11, 476, 421, 279, 4226, 374, 35197, 54761, 25, 1036, 33260, 8253, 854, 624, 262, 690, 9217, 1339, 262, 62947, 609, 43797, 50, 510, 262, 7288, 3155, 4183, 13153, 279, 3405, 476, 1140, 5335, 26, 653, 4183, 2550, 4718, 476, 4960, 14158, 624, 262, 7288, 84468, 4285, 1894, 5036, 448, 518, 1429, 825, 22739, 26087, 4145, 3446, 838, 4322, 1574, 58689, 7256, 854, 4292, 262, 7288, 2823, 12966, 3941, 6194, 26, 421, 6194, 28295, 11, 62552, 279, 11299, 15432, 5904, 323, 5185, 432, 304, 366, 26865, 29816, 257, 151645, 198, 151644, 872, 198, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 30, 220, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151645, 198, 151644, 77091, 198, 198, 32462, 432, 11, 1077, 594, 1430, 311, 7071, 700, 1128, 594, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 62410, 13, 5512, 11, 1077, 594, 50087, 279, 14852, 6505, 13, 576, 21921, 374, 4363, 264, 4158, 11, 1378, 1737, 95112, 4982, 13, 2411, 220, 16, 16, 297, 62410, 320, 3481, 7950, 3082, 701, 582, 1184, 311, 1779, 429, 7702, 13, 21131, 518, 279, 5335, 11, 5310, 279, 825, 9027, 279, 21921, 3082, 11, 1052, 594, 264, 17846, 3403, 279, 19309, 11, 714, 3783, 11, 902, 2293, 11489, 11, 279, 3405, 374, 911, 279, 21921, 13, 13824, 11, 7196, 279, 330, 66, 75512, 1, 1588, 374, 279, 5819, 4982, 13, 13824, 11, 902, 11, 279, 19309, 702, 264, 21921, 3685, 30, 13824, 11, 902, 11, 279, 1887, 21921, 374, 279, 4158, 61607, 13, 13824, 11, 304, 279, 2168, 1380, 279, 6249, 4933, 279, 19309, 323, 21921, 11, 279, 1909, 315, 279, 21921, 2293, 11489, 11, 902, 11, 7196, 279, 17846, 30, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 2727, 330, 65, 77832, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 3263, 2055, 220, 16, 16, 297, 62410, 1035, 387, 279, 8416, 2115, 3082, 8674, 311, 279, 19309, 13, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 279, 19309, 3082, 13, 2160, 429, 949, 315, 279, 21921, 6505, 30, 13824, 11, 902, 11, 7196, 279, 21921, 374, 279, 4158, 61607, 1212, 279, 19309, 13, 13824, 11, 902, 11, 279, 1909, 315, 429, 21921, 2293, 11489, 11, 902, 11, 304, 1045, 5335, 11, 1052, 594, 264, 17846, 25911, 476, 21810, 13, 13824, 11, 902, 11, 1077, 594, 1779, 279, 1156, 2421, 5335, 13, 576, 17196, 45766, 374, 33866, 11, 1221, 279, 19309, 3082, 13, 13824, 11, 279, 4396, 4226, 25, 3330, 518, 279, 5335, 11, 279, 1633, 389, 1909, 315, 279, 14852, 21921, 320, 1782, 4158, 26482, 4982, 8, 518, 220, 16, 16, 297, 62410, 374, 264, 17846, 13, 13824, 11, 902, 11, 3783, 21677, 17846, 374, 389, 279, 7002, 3403, 279, 19309, 13, 13824, 11, 7196, 358, 5786, 878, 13, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 594, 3405, 25, 330, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 7521, 2055, 1156, 11, 10542, 279, 21921, 13, 576, 4158, 26482, 4982, 3143, 220, 16, 16, 297, 62410, 320, 3481, 7950, 568, 3555, 594, 389, 1909, 30, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 429, 3082, 13, 13824, 11, 9834, 0, 758, 279, 21844, 476, 21810, 11, 1052, 594, 264, 17846, 13, 2055, 279, 4226, 374, 264, 17846, 4192, 1249, 8253, 1128, 374, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 527, 20666, 11, 23643, 279, 27979, 6789, 323, 5424, 304, 279, 5335, 25, 18611, 16, 13, 95210, 279, 1036, 65, 77832, 21921, 854, 304, 3405, 25, 362, 4158, 11, 1378, 481, 26482, 5819, 4982, 374, 9434, 3143, 279, 1909, 481, 2115, 320, 16, 16, 297, 527, 20666, 8, 3082, 8674, 311, 1008, 37664, 1075, 279, 19309, 323, 17196, 13, 2303, 17, 13, 1374, 19991, 6171, 3403, 419, 21921, 25, 758, 279, 5335, 11, 264, 3070, 73225, 334, 374, 21810, 389, 279, 7002, 3403, 419, 21921, 3082, 13, 79226, 54216, 151645, 198]
inputs:
<|im_start|>system

    You are a careful vision–language assistant for spatial VQA on a single indoor scene shown across multiple images.

    INPUT:
    Immediately after this message, you will receive N scene images as <image> tags. Treat them as different views of the SAME scene. Refer to images by 1-based index in order of appearance: [1], [2], …, [N].

    REASONING PRINCIPLES:
    • Build a consistent 3D mental model across views: track objects across images, infer relative camera pose, and use scale/occlusion/shadows for depth cues.
    • Interpret deictic terms (“to my right/left/in front/behind”) EGOCENTRICALLY with respect to the narrator/agent. If the viewpoint is ambiguous, default to the most informative central view and say so.
    • Use only visible evidence; do not invent unseen details or rely on external knowledge beyond basic object/color terms.
    • When multiple candidates exist, resolve by the best match to spatial phrase + salience across views. If still ambiguous after checking all images, state that clearly.

    OUTPUT (exactly two blocks, in this order):
    <think>
    Step-by-step reasoning in 3–10 short steps. Cite evidence with image indices (e.g., “[3]: light wood desk right of monitor; [1]: same color, confirms”). Be concise but complete (aim ≤250 tokens unless necessary). Note any occlusion/ambiguity and how you resolved it.
    </think>
    <answer>
    A single concise answer only, which may be:
    • If sufficient: One-word answer, Count (e.g., “2”) or Yes/No (e.g., “yes”).
    • If not sufficient: A sentence or two.
    If answer not visible, if there is no clear evidence, or if the answer is genuinely ambiguous: “cannot determine”.
    </answer>

    STYLE & RULES:
    • Do NOT repeat the question or list images; do NOT output JSON or extra sections.
    • Prefer simple color names with at most one modifier (“light/dark/pale/beige”).
    • Be consistent across views; if views disagree, prioritize the clearest evidence and note it in <think>.
    <|im_end|>
<|im_start|>user
What is on top of the bathroom cabinet that is on my 11 o'clock? <|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|im_end|>
<|im_start|>assistant

Got it, let's try to figure out what's on top of the bathroom cabinet at 11 o'clock. First, let's visualize the bathroom setup. The cabinet is likely a white, two-drawer unit. At 11 o'clock (top-left area), we need to check that spot. Looking at the images, especially the one showing the cabinet area, there's a mirror above the sink, but wait, no—wait, the question is about the cabinet. Wait, maybe the "cabinet" here is the storage unit. Wait, no, the sink has a cabinet below? Wait, no, the main cabinet is the white drawers. Wait, in the image where the camera shows the sink and cabinet, the top of the cabinet—wait, no, maybe the mirror? Wait, no, let's check again. Wait, the user says "bathroom cabinet that is on my 11 o'clock". So 11 o'clock would be the upper left area relative to the sink. Looking at the images, there's a mirror mounted on the wall above the sink area. Is that part of the cabinet setup? Wait, no, maybe the cabinet is the white drawers under the sink. Wait, no, the top of that cabinet—wait, no, in some images, there's a mirror reflected or mounted. Wait, no, let's check the first few images. The shower curtain is colorful, then the sink area. Wait, the correct answer: looking at the images, the object on top of the bathroom cabinet (the white drawer unit) at 11 o'clock is a mirror. Wait, no, wait—the mirror is on the wall above the sink. Wait, maybe I misread. Wait, no, let's check again. Wait, the user's question: "What is on top of the bathroom cabinet that is on my 11 o'clock?" So first, identify the cabinet. The white drawer unit near 11 o'clock (top-left). What's on top? Looking at the images, there's a mirror mounted on the wall above that area. Wait, yes! In the reflection or mounted, there's a mirror. So the answer is a mirror.


To determine what is on top of the bathroom cabinet at 11 o’clock, analyze the spatial layout and elements in the images:  

1. Locate the “bathroom cabinet” in question: A white, two - drawer storage unit is visible near the top - left (11 o’clock) area relative to other fixtures like the sink and shower.  
2. Examine objects above this cabinet: In the images, a **mirror** is mounted on the wall above this cabinet area.  


Mirror<|im_end|>

label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 198, 32462, 432, 11, 1077, 594, 1430, 311, 7071, 700, 1128, 594, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 62410, 13, 5512, 11, 1077, 594, 50087, 279, 14852, 6505, 13, 576, 21921, 374, 4363, 264, 4158, 11, 1378, 1737, 95112, 4982, 13, 2411, 220, 16, 16, 297, 62410, 320, 3481, 7950, 3082, 701, 582, 1184, 311, 1779, 429, 7702, 13, 21131, 518, 279, 5335, 11, 5310, 279, 825, 9027, 279, 21921, 3082, 11, 1052, 594, 264, 17846, 3403, 279, 19309, 11, 714, 3783, 11, 902, 2293, 11489, 11, 279, 3405, 374, 911, 279, 21921, 13, 13824, 11, 7196, 279, 330, 66, 75512, 1, 1588, 374, 279, 5819, 4982, 13, 13824, 11, 902, 11, 279, 19309, 702, 264, 21921, 3685, 30, 13824, 11, 902, 11, 279, 1887, 21921, 374, 279, 4158, 61607, 13, 13824, 11, 304, 279, 2168, 1380, 279, 6249, 4933, 279, 19309, 323, 21921, 11, 279, 1909, 315, 279, 21921, 2293, 11489, 11, 902, 11, 7196, 279, 17846, 30, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 2727, 330, 65, 77832, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 3263, 2055, 220, 16, 16, 297, 62410, 1035, 387, 279, 8416, 2115, 3082, 8674, 311, 279, 19309, 13, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 279, 19309, 3082, 13, 2160, 429, 949, 315, 279, 21921, 6505, 30, 13824, 11, 902, 11, 7196, 279, 21921, 374, 279, 4158, 61607, 1212, 279, 19309, 13, 13824, 11, 902, 11, 279, 1909, 315, 429, 21921, 2293, 11489, 11, 902, 11, 304, 1045, 5335, 11, 1052, 594, 264, 17846, 25911, 476, 21810, 13, 13824, 11, 902, 11, 1077, 594, 1779, 279, 1156, 2421, 5335, 13, 576, 17196, 45766, 374, 33866, 11, 1221, 279, 19309, 3082, 13, 13824, 11, 279, 4396, 4226, 25, 3330, 518, 279, 5335, 11, 279, 1633, 389, 1909, 315, 279, 14852, 21921, 320, 1782, 4158, 26482, 4982, 8, 518, 220, 16, 16, 297, 62410, 374, 264, 17846, 13, 13824, 11, 902, 11, 3783, 21677, 17846, 374, 389, 279, 7002, 3403, 279, 19309, 13, 13824, 11, 7196, 358, 5786, 878, 13, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 594, 3405, 25, 330, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 7521, 2055, 1156, 11, 10542, 279, 21921, 13, 576, 4158, 26482, 4982, 3143, 220, 16, 16, 297, 62410, 320, 3481, 7950, 568, 3555, 594, 389, 1909, 30, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 429, 3082, 13, 13824, 11, 9834, 0, 758, 279, 21844, 476, 21810, 11, 1052, 594, 264, 17846, 13, 2055, 279, 4226, 374, 264, 17846, 4192, 1249, 8253, 1128, 374, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 527, 20666, 11, 23643, 279, 27979, 6789, 323, 5424, 304, 279, 5335, 25, 18611, 16, 13, 95210, 279, 1036, 65, 77832, 21921, 854, 304, 3405, 25, 362, 4158, 11, 1378, 481, 26482, 5819, 4982, 374, 9434, 3143, 279, 1909, 481, 2115, 320, 16, 16, 297, 527, 20666, 8, 3082, 8674, 311, 1008, 37664, 1075, 279, 19309, 323, 17196, 13, 2303, 17, 13, 1374, 19991, 6171, 3403, 419, 21921, 25, 758, 279, 5335, 11, 264, 3070, 73225, 334, 374, 21810, 389, 279, 7002, 3403, 419, 21921, 3082, 13, 79226, 54216, 151645, 198]
labels:

Got it, let's try to figure out what's on top of the bathroom cabinet at 11 o'clock. First, let's visualize the bathroom setup. The cabinet is likely a white, two-drawer unit. At 11 o'clock (top-left area), we need to check that spot. Looking at the images, especially the one showing the cabinet area, there's a mirror above the sink, but wait, no—wait, the question is about the cabinet. Wait, maybe the "cabinet" here is the storage unit. Wait, no, the sink has a cabinet below? Wait, no, the main cabinet is the white drawers. Wait, in the image where the camera shows the sink and cabinet, the top of the cabinet—wait, no, maybe the mirror? Wait, no, let's check again. Wait, the user says "bathroom cabinet that is on my 11 o'clock". So 11 o'clock would be the upper left area relative to the sink. Looking at the images, there's a mirror mounted on the wall above the sink area. Is that part of the cabinet setup? Wait, no, maybe the cabinet is the white drawers under the sink. Wait, no, the top of that cabinet—wait, no, in some images, there's a mirror reflected or mounted. Wait, no, let's check the first few images. The shower curtain is colorful, then the sink area. Wait, the correct answer: looking at the images, the object on top of the bathroom cabinet (the white drawer unit) at 11 o'clock is a mirror. Wait, no, wait—the mirror is on the wall above the sink. Wait, maybe I misread. Wait, no, let's check again. Wait, the user's question: "What is on top of the bathroom cabinet that is on my 11 o'clock?" So first, identify the cabinet. The white drawer unit near 11 o'clock (top-left). What's on top? Looking at the images, there's a mirror mounted on the wall above that area. Wait, yes! In the reflection or mounted, there's a mirror. So the answer is a mirror.


To determine what is on top of the bathroom cabinet at 11 o’clock, analyze the spatial layout and elements in the images:  

1. Locate the “bathroom cabinet” in question: A white, two - drawer storage unit is visible near the top - left (11 o’clock) area relative to other fixtures like the sink and shower.  
2. Examine objects above this cabinet: In the images, a **mirror** is mounted on the wall above this cabinet area.  


Mirror<|im_end|>

[INFO|hub.py:421] 2025-12-04 10:09:16,390 >> Offline mode: forcing local_files_only=True
[INFO|configuration_utils.py:765] 2025-12-04 10:09:16,391 >> loading configuration file config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
[INFO|configuration_utils.py:839] 2025-12-04 10:09:16,395 >> Model config Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 128000,
  "max_window_layers": 28,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "_name_or_path": "Qwen/Qwen2.5-VL-7B-Instruct",
    "architectures": [
      "Qwen2_5_VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "dtype": "bfloat16",
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "layer_types": [
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention"
    ],
    "max_position_embeddings": 128000,
    "max_window_layers": 28,
    "model_type": "qwen2_5_vl_text",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "use_cache": true,
    "use_sliding_window": false,
    "vision_token_id": 151654,
    "vocab_size": 152064
  },
  "tie_word_embeddings": false,
  "transformers_version": "4.57.0",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 3584,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

[INFO|2025-12-04 10:09:16] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
[INFO|2025-12-04 10:09:16] llamafactory.model.model_utils.liger_kernel:143 >> Liger kernel has been applied to the model.
[INFO|hub.py:421] 2025-12-04 10:09:16,854 >> Offline mode: forcing local_files_only=True
[WARNING|logging.py:328] 2025-12-04 10:09:16,856 >> `torch_dtype` is deprecated! Use `dtype` instead!
[INFO|hub.py:421] 2025-12-04 10:09:16,856 >> Offline mode: forcing local_files_only=True
[INFO|modeling_utils.py:4840] 2025-12-04 10:09:16,858 >> Offline mode: forcing local_files_only=True
[INFO|modeling_utils.py:1176] 2025-12-04 10:09:16,861 >> loading weights file model.safetensors from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/model.safetensors.index.json
[INFO|modeling_utils.py:2345] 2025-12-04 10:09:16,869 >> Instantiating Qwen2_5_VLForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:986] 2025-12-04 10:09:16,890 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

[INFO|modeling_utils.py:2345] 2025-12-04 10:09:16,902 >> Instantiating Qwen2_5_VisionTransformerPretrainedModel model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:2345] 2025-12-04 10:09:17,014 >> Instantiating Qwen2_5_VLTextModel model under default dtype torch.bfloat16.
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:02,  1.36it/s]Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:01,  1.56it/s]Loading checkpoint shards:  60%|██████    | 3/5 [00:01<00:01,  1.63it/s]Loading checkpoint shards:  80%|████████  | 4/5 [00:02<00:00,  1.67it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:02<00:00,  2.25it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:02<00:00,  1.90it/s]
[INFO|configuration_utils.py:941] 2025-12-04 10:09:20,253 >> loading configuration file generation_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/generation_config.json
[INFO|configuration_utils.py:986] 2025-12-04 10:09:20,254 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 1e-06
}

[INFO|dynamic_module_utils.py:423] 2025-12-04 10:09:20,255 >> Could not locate the custom_generate/generate.py inside Qwen/Qwen2.5-VL-7B-Instruct.
[INFO|2025-12-04 10:09:20] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2025-12-04 10:09:20] llamafactory.model.model_utils.attention:143 >> Using FlashAttention-2 for faster training and inference.
[INFO|2025-12-04 10:09:20] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.
[INFO|2025-12-04 10:09:20] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA
[INFO|2025-12-04 10:09:21] llamafactory.model.adapter:143 >> Loaded adapter(s): /project/aip-wangcs/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465_converted/world_size_1/
[INFO|2025-12-04 10:09:21] llamafactory.model.loader:143 >> trainable params: 20,185,088 || all params: 8,312,351,744 || trainable%: 0.2428
name: base_model.model.model.visual.patch_embed.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.ln_q.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.0.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.0.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.embed_tokens.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.lm_head.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
[INFO|trainer.py:749] 2025-12-04 10:09:21,681 >> Using auto half precision backend
[WARNING|2025-12-04 10:09:21] llamafactory.train.callbacks:154 >> Previous trainer log in this folder will be deleted.
[WARNING|trainer.py:982] 2025-12-04 10:09:21,685 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
[DEBUG|trainer.py:2373] 2025-12-04 10:09:21,899 >> Currently training with a batch size of: 2
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 8. Using DeepSpeed's value.
[INFO|deepspeed.py:383] 2025-12-04 10:09:21,913 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Using /tmp/.cache/torch_extensions as PyTorch extensions root...
Creating extension directory /tmp/.cache/torch_extensions/cpu_adam...
Detected CUDA files, patching ldflags
Emitting ninja build file /tmp/.cache/torch_extensions/cpu_adam/build.ninja...
/opt/conda/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module cpu_adam...
Using envvar MAX_JOBS (16) as the number of workers...
[1/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.11/site-packages/torch/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o 
[2/3] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.11/site-packages/torch/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o 
[3/3] c++ cpu_adam.o cpu_adam_impl.o -shared -lcurand -L/usr/local/cuda/lib64 -L/opt/conda/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so
Loading extension module cpu_adam...
Time to load cpu_adam op: 28.42589831352234 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-12-04 10:09:51,558] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.9, git-hash=unknown, git-branch=unknown
[2025-12-04 10:09:51,558] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 1
[2025-12-04 10:09:52,689] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-12-04 10:09:52,693] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-12-04 10:09:52,693] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-12-04 10:09:52,716] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2025-12-04 10:09:52,716] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2025-12-04 10:09:52,716] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-12-04 10:09:52,716] [INFO] [stage_1_and_2.py:150:__init__] Reduce bucket size 500000000
[2025-12-04 10:09:52,716] [INFO] [stage_1_and_2.py:151:__init__] Allgather bucket size 500000000
[2025-12-04 10:09:52,716] [INFO] [stage_1_and_2.py:152:__init__] CPU Offload: True
[2025-12-04 10:09:52,716] [INFO] [stage_1_and_2.py:153:__init__] Round robin gradient partitioning: True
[2025-12-04 10:09:53,183] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-12-04 10:09:53,183] [INFO] [utils.py:782:see_memory_usage] MA 15.48 GB         Max_MA 15.48 GB         CA 15.49 GB         Max_CA 15 GB 
[2025-12-04 10:09:53,183] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 123.86 GB, percent = 6.1%
[2025-12-04 10:09:53,496] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-12-04 10:09:53,496] [INFO] [utils.py:782:see_memory_usage] MA 15.48 GB         Max_MA 15.48 GB         CA 15.49 GB         Max_CA 15 GB 
[2025-12-04 10:09:53,496] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 123.99 GB, percent = 6.2%
[2025-12-04 10:09:53,497] [INFO] [stage_1_and_2.py:557:__init__] optimizer state initialized
[2025-12-04 10:09:53,699] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-12-04 10:09:53,700] [INFO] [utils.py:782:see_memory_usage] MA 15.48 GB         Max_MA 15.48 GB         CA 15.49 GB         Max_CA 15 GB 
[2025-12-04 10:09:53,700] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 123.97 GB, percent = 6.2%
[2025-12-04 10:09:53,706] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-12-04 10:09:53,706] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-12-04 10:09:53,706] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-12-04 10:09:53,706] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2025-12-04 10:09:53,711] [INFO] [config.py:1003:print] DeepSpeedEngine configuration:
[2025-12-04 10:09:53,711] [INFO] [config.py:1007:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-12-04 10:09:53,711] [INFO] [config.py:1007:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-12-04 10:09:53,711] [INFO] [config.py:1007:print]   amp_enabled .................. False
[2025-12-04 10:09:53,711] [INFO] [config.py:1007:print]   amp_params ................... False
[2025-12-04 10:09:53,711] [INFO] [config.py:1007:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-12-04 10:09:53,711] [INFO] [config.py:1007:print]   bfloat16_enabled ............. True
[2025-12-04 10:09:53,711] [INFO] [config.py:1007:print]   bfloat16_immediate_grad_update  True
[2025-12-04 10:09:53,711] [INFO] [config.py:1007:print]   checkpoint_parallel_write_pipeline  False
[2025-12-04 10:09:53,711] [INFO] [config.py:1007:print]   checkpoint_tag_validation_enabled  True
[2025-12-04 10:09:53,711] [INFO] [config.py:1007:print]   checkpoint_tag_validation_fail  False
[2025-12-04 10:09:53,711] [INFO] [config.py:1007:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x15520d1f73d0>
[2025-12-04 10:09:53,711] [INFO] [config.py:1007:print]   communication_data_type ...... None
[2025-12-04 10:09:53,711] [INFO] [config.py:1007:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   curriculum_enabled_legacy .... False
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   curriculum_params_legacy ..... False
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   data_efficiency_enabled ...... False
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   dataloader_drop_last ......... False
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   disable_allgather ............ False
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   dump_state ................... False
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   dynamic_loss_scale_args ...... None
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   eigenvalue_enabled ........... False
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   eigenvalue_gas_boundary_resolution  1
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   eigenvalue_layer_num ......... 0
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   eigenvalue_max_iter .......... 100
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   eigenvalue_stability ......... 1e-06
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   eigenvalue_tol ............... 0.01
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   eigenvalue_verbose ........... False
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   elasticity_enabled ........... False
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   fp16_auto_cast ............... None
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   fp16_enabled ................. False
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   fp16_master_weights_and_gradients  False
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   global_rank .................. 0
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   grad_accum_dtype ............. None
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   gradient_accumulation_steps .. 8
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   gradient_clipping ............ 1.0
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   gradient_predivide_factor .... 1.0
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   graph_harvesting ............. False
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   initial_dynamic_scale ........ 1
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   load_universal_checkpoint .... False
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   loss_scale ................... 1.0
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   memory_breakdown ............. False
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   mics_hierarchial_params_gather  False
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   mics_shard_size .............. -1
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   optimizer_legacy_fusion ...... False
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   optimizer_name ............... None
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   optimizer_params ............. None
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   pld_enabled .................. False
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   pld_params ................... False
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   prescale_gradients ........... False
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   scheduler_name ............... None
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   scheduler_params ............. None
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   seq_parallel_communication_data_type  torch.float32
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   sparse_attention ............. None
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   sparse_gradients_enabled ..... False
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   steps_per_print .............. inf
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   timers_config ................ enabled=True synchronized=True
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   train_batch_size ............. 16
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   train_micro_batch_size_per_gpu  2
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   use_data_before_expert_parallel_  False
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   use_node_local_storage ....... False
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   wall_clock_breakdown ......... False
[2025-12-04 10:09:53,712] [INFO] [config.py:1007:print]   weight_quantization_config ... None
[2025-12-04 10:09:53,713] [INFO] [config.py:1007:print]   world_size ................... 1
[2025-12-04 10:09:53,713] [INFO] [config.py:1007:print]   zero_allow_untested_optimizer  True
[2025-12-04 10:09:53,713] [INFO] [config.py:1007:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=True zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-12-04 10:09:53,713] [INFO] [config.py:1007:print]   zero_enabled ................. True
[2025-12-04 10:09:53,713] [INFO] [config.py:1007:print]   zero_force_ds_cpu_optimizer .. True
[2025-12-04 10:09:53,713] [INFO] [config.py:1007:print]   zero_optimization_stage ...... 2
[2025-12-04 10:09:53,713] [INFO] [config.py:993:print_user_config]   json = {
    "train_batch_size": 16, 
    "train_micro_batch_size_per_gpu": 2, 
    "gradient_accumulation_steps": 8, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": false, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true, 
        "round_robin_gradients": true
    }, 
    "steps_per_print": inf
}
[INFO|trainer.py:2519] 2025-12-04 10:09:53,714 >> ***** Running training *****
[INFO|trainer.py:2520] 2025-12-04 10:09:53,714 >>   Num examples = 29,742
[INFO|trainer.py:2521] 2025-12-04 10:09:53,714 >>   Num Epochs = 1
[INFO|trainer.py:2522] 2025-12-04 10:09:53,714 >>   Instantaneous batch size per device = 2
[INFO|trainer.py:2525] 2025-12-04 10:09:53,714 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2526] 2025-12-04 10:09:53,714 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2527] 2025-12-04 10:09:53,714 >>   Total optimization steps = 1,859
[INFO|trainer.py:2528] 2025-12-04 10:09:53,718 >>   Number of trainable parameters = 20,185,088
[INFO|integration_utils.py:867] 2025-12-04 10:09:53,721 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Tracking run with wandb version 0.23.0
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /project/aip-wangcs/indrisch/LLaMA-Factory/wandb/wandb/offline-run-20251204_100953-yxmrx5aw
  0%|          | 0/1859 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /tmp/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
  0%|          | 1/1859 [01:28<45:33:43, 88.28s/it]  0%|          | 2/1859 [02:48<43:05:14, 83.53s/it]  0%|          | 3/1859 [03:53<38:47:17, 75.24s/it]  0%|          | 4/1859 [05:15<40:05:07, 77.79s/it]  0%|          | 5/1859 [06:27<38:58:17, 75.67s/it]  0%|          | 6/1859 [07:31<36:59:07, 71.85s/it]  0%|          | 7/1859 [08:32<35:08:49, 68.32s/it]  0%|          | 8/1859 [09:34<34:02:12, 66.20s/it]  0%|          | 9/1859 [11:13<39:13:55, 76.34s/it]  1%|          | 10/1859 [12:31<39:27:54, 76.84s/it]                                                    {'loss': 0.7443, 'grad_norm': 0.3702159821987152, 'learning_rate': 4.838709677419355e-06, 'epoch': 0.01}
  1%|          | 10/1859 [12:31<39:27:54, 76.84s/it]  1%|          | 11/1859 [13:43<38:41:10, 75.36s/it]  1%|          | 12/1859 [15:07<40:05:03, 78.13s/it]  1%|          | 13/1859 [16:17<38:44:38, 75.56s/it]  1%|          | 14/1859 [17:36<39:17:13, 76.66s/it]  1%|          | 15/1859 [18:56<39:42:22, 77.52s/it]  1%|          | 16/1859 [20:10<39:15:22, 76.68s/it]  1%|          | 17/1859 [21:11<36:46:34, 71.88s/it]  1%|          | 18/1859 [22:16<35:43:45, 69.87s/it]  1%|          | 19/1859 [23:31<36:24:42, 71.24s/it]  1%|          | 20/1859 [24:54<38:11:21, 74.76s/it]                                                    {'loss': 0.7516, 'grad_norm': 0.3344869613647461, 'learning_rate': 1.0215053763440861e-05, 'epoch': 0.01}
  1%|          | 20/1859 [24:54<38:11:21, 74.76s/it]  1%|          | 21/1859 [26:10<38:28:48, 75.37s/it]  1%|          | 22/1859 [27:12<36:18:45, 71.16s/it]  1%|          | 23/1859 [28:12<34:38:46, 67.93s/it]  1%|▏         | 24/1859 [29:24<35:16:52, 69.22s/it]  1%|▏         | 25/1859 [30:32<35:04:26, 68.85s/it]  1%|▏         | 26/1859 [31:45<35:35:22, 69.90s/it]  1%|▏         | 27/1859 [32:42<33:39:29, 66.14s/it]  2%|▏         | 28/1859 [33:43<32:50:06, 64.56s/it]  2%|▏         | 29/1859 [34:55<33:58:09, 66.82s/it]  2%|▏         | 30/1859 [36:03<34:06:59, 67.15s/it]                                                    {'loss': 0.7518, 'grad_norm': 0.36625853180885315, 'learning_rate': 1.5591397849462366e-05, 'epoch': 0.02}
  2%|▏         | 30/1859 [36:03<34:06:59, 67.15s/it]  2%|▏         | 31/1859 [37:07<33:42:01, 66.37s/it]  2%|▏         | 32/1859 [38:25<35:24:35, 69.77s/it]  2%|▏         | 33/1859 [39:44<36:43:36, 72.41s/it]  2%|▏         | 34/1859 [41:02<37:38:51, 74.26s/it]  2%|▏         | 35/1859 [42:23<38:36:03, 76.19s/it]  2%|▏         | 36/1859 [43:48<39:54:58, 78.83s/it]  2%|▏         | 37/1859 [44:53<37:43:36, 74.54s/it]  2%|▏         | 38/1859 [45:54<35:46:39, 70.73s/it]  2%|▏         | 39/1859 [47:15<37:12:39, 73.60s/it]  2%|▏         | 40/1859 [48:20<35:53:06, 71.02s/it]                                                    {'loss': 0.7625, 'grad_norm': 0.35667866468429565, 'learning_rate': 2.0967741935483873e-05, 'epoch': 0.02}
  2%|▏         | 40/1859 [48:20<35:53:06, 71.02s/it]  2%|▏         | 41/1859 [49:34<36:26:25, 72.16s/it]  2%|▏         | 42/1859 [50:44<35:59:57, 71.33s/it]  2%|▏         | 43/1859 [52:01<36:50:56, 73.05s/it]  2%|▏         | 44/1859 [53:02<34:59:13, 69.40s/it]  2%|▏         | 45/1859 [54:15<35:30:56, 70.48s/it]  2%|▏         | 46/1859 [55:22<35:02:46, 69.59s/it]  3%|▎         | 47/1859 [56:25<33:58:59, 67.52s/it]  3%|▎         | 48/1859 [57:33<34:05:15, 67.76s/it]  3%|▎         | 49/1859 [58:50<35:22:21, 70.35s/it]  3%|▎         | 50/1859 [1:00:01<35:29:22, 70.63s/it]                                                      {'loss': 0.7529, 'grad_norm': 0.4372948408126831, 'learning_rate': 2.6344086021505376e-05, 'epoch': 0.03}
  3%|▎         | 50/1859 [1:00:01<35:29:22, 70.63s/it]  3%|▎         | 51/1859 [1:01:31<38:26:27, 76.54s/it]  3%|▎         | 52/1859 [1:02:54<39:20:23, 78.37s/it]  3%|▎         | 53/1859 [1:04:12<39:13:35, 78.19s/it]  3%|▎         | 54/1859 [1:05:19<37:34:52, 74.95s/it]  3%|▎         | 55/1859 [1:06:32<37:15:44, 74.36s/it]  3%|▎         | 56/1859 [1:07:30<34:42:37, 69.31s/it]  3%|▎         | 57/1859 [1:08:55<37:10:02, 74.25s/it]  3%|▎         | 58/1859 [1:10:18<38:21:22, 76.67s/it]  3%|▎         | 59/1859 [1:11:23<36:34:21, 73.15s/it]  3%|▎         | 60/1859 [1:12:46<38:02:40, 76.13s/it]                                                      {'loss': 0.7739, 'grad_norm': 0.3581831753253937, 'learning_rate': 3.172043010752688e-05, 'epoch': 0.03}
  3%|▎         | 60/1859 [1:12:46<38:02:40, 76.13s/it]  3%|▎         | 61/1859 [1:13:43<35:10:31, 70.43s/it]  3%|▎         | 62/1859 [1:14:52<34:58:45, 70.08s/it]  3%|▎         | 63/1859 [1:15:45<32:21:28, 64.86s/it]  3%|▎         | 64/1859 [1:16:50<32:26:12, 65.05s/it]  3%|▎         | 65/1859 [1:18:04<33:46:12, 67.77s/it]  4%|▎         | 66/1859 [1:19:00<31:59:17, 64.23s/it]  4%|▎         | 67/1859 [1:20:00<31:15:26, 62.79s/it]  4%|▎         | 68/1859 [1:21:18<33:27:19, 67.25s/it]  4%|▎         | 69/1859 [1:22:44<36:22:24, 73.15s/it]  4%|▍         | 70/1859 [1:23:56<36:09:25, 72.76s/it]                                                      {'loss': 0.741, 'grad_norm': 0.5564361214637756, 'learning_rate': 3.7096774193548386e-05, 'epoch': 0.04}
  4%|▍         | 70/1859 [1:23:56<36:09:25, 72.76s/it]  4%|▍         | 71/1859 [1:24:55<34:03:51, 68.59s/it]  4%|▍         | 72/1859 [1:25:56<32:53:39, 66.27s/it]  4%|▍         | 73/1859 [1:27:26<36:22:11, 73.31s/it]  4%|▍         | 74/1859 [1:28:44<37:02:37, 74.71s/it]  4%|▍         | 75/1859 [1:29:57<36:44:42, 74.15s/it]  4%|▍         | 76/1859 [1:31:28<39:19:23, 79.40s/it]  4%|▍         | 77/1859 [1:32:35<37:26:33, 75.64s/it]  4%|▍         | 78/1859 [1:33:55<38:03:41, 76.94s/it]  4%|▍         | 79/1859 [1:35:03<36:42:58, 74.26s/it]  4%|▍         | 80/1859 [1:36:23<37:29:06, 75.86s/it]                                                      {'loss': 0.8226, 'grad_norm': 0.5801034569740295, 'learning_rate': 4.247311827956989e-05, 'epoch': 0.04}
  4%|▍         | 80/1859 [1:36:23<37:29:06, 75.86s/it]  4%|▍         | 81/1859 [1:37:25<35:31:03, 71.91s/it]  4%|▍         | 82/1859 [1:38:45<36:37:08, 74.19s/it]  4%|▍         | 83/1859 [1:40:07<37:51:13, 76.73s/it]  5%|▍         | 84/1859 [1:41:12<35:57:32, 72.93s/it]  5%|▍         | 85/1859 [1:42:16<34:45:01, 70.52s/it]  5%|▍         | 86/1859 [1:43:27<34:46:26, 70.61s/it]  5%|▍         | 87/1859 [1:44:19<32:01:33, 65.06s/it]  5%|▍         | 88/1859 [1:45:39<34:06:13, 69.32s/it]  5%|▍         | 89/1859 [1:46:46<33:43:20, 68.59s/it]  5%|▍         | 90/1859 [1:47:58<34:17:19, 69.78s/it]                                                      {'loss': 0.7524, 'grad_norm': 0.4015292227268219, 'learning_rate': 4.78494623655914e-05, 'epoch': 0.05}
  5%|▍         | 90/1859 [1:47:58<34:17:19, 69.78s/it]  5%|▍         | 91/1859 [1:49:05<33:55:01, 69.06s/it]  5%|▍         | 92/1859 [1:50:13<33:43:16, 68.70s/it]  5%|▌         | 93/1859 [1:51:52<38:07:06, 77.70s/it]  5%|▌         | 94/1859 [1:52:56<36:07:42, 73.69s/it]  5%|▌         | 95/1859 [1:54:11<36:14:52, 73.98s/it]  5%|▌         | 96/1859 [1:55:11<34:11:09, 69.81s/it]  5%|▌         | 97/1859 [1:56:23<34:30:04, 70.49s/it]  5%|▌         | 98/1859 [1:57:44<35:56:54, 73.49s/it]  5%|▌         | 99/1859 [1:58:53<35:22:12, 72.35s/it]  5%|▌         | 100/1859 [2:00:24<38:03:34, 77.89s/it]                                                       {'loss': 0.7773, 'grad_norm': 0.6813278794288635, 'learning_rate': 5.32258064516129e-05, 'epoch': 0.05}
  5%|▌         | 100/1859 [2:00:24<38:03:34, 77.89s/it]  5%|▌         | 101/1859 [2:01:44<38:21:37, 78.55s/it]  5%|▌         | 102/1859 [2:02:55<37:09:32, 76.14s/it]  6%|▌         | 103/1859 [2:04:16<37:49:50, 77.56s/it]  6%|▌         | 104/1859 [2:05:33<37:45:23, 77.45s/it]  6%|▌         | 105/1859 [2:06:48<37:21:47, 76.69s/it]  6%|▌         | 106/1859 [2:07:44<34:18:05, 70.44s/it]  6%|▌         | 107/1859 [2:08:53<34:05:41, 70.06s/it]  6%|▌         | 108/1859 [2:09:54<32:43:53, 67.29s/it]  6%|▌         | 109/1859 [2:11:14<34:40:21, 71.33s/it]  6%|▌         | 110/1859 [2:12:23<34:15:16, 70.51s/it]                                                       {'loss': 0.7678, 'grad_norm': 0.25465524196624756, 'learning_rate': 5.860215053763441e-05, 'epoch': 0.06}
  6%|▌         | 110/1859 [2:12:23<34:15:16, 70.51s/it]  6%|▌         | 111/1859 [2:13:27<33:20:17, 68.66s/it]  6%|▌         | 112/1859 [2:14:37<33:27:34, 68.95s/it]  6%|▌         | 113/1859 [2:15:43<33:03:53, 68.18s/it]  6%|▌         | 114/1859 [2:17:10<35:40:16, 73.59s/it]  6%|▌         | 115/1859 [2:18:22<35:31:19, 73.33s/it]  6%|▌         | 116/1859 [2:19:30<34:45:00, 71.77s/it]  6%|▋         | 117/1859 [2:20:47<35:26:40, 73.25s/it]  6%|▋         | 118/1859 [2:21:51<34:00:35, 70.32s/it]  6%|▋         | 119/1859 [2:23:07<34:49:16, 72.04s/it]  6%|▋         | 120/1859 [2:24:50<39:18:36, 81.38s/it]                                                       {'loss': 0.7809, 'grad_norm': 0.320041298866272, 'learning_rate': 6.397849462365592e-05, 'epoch': 0.06}
  6%|▋         | 120/1859 [2:24:50<39:18:36, 81.38s/it]  7%|▋         | 121/1859 [2:26:15<39:46:39, 82.39s/it]  7%|▋         | 122/1859 [2:27:32<39:04:44, 80.99s/it]  7%|▋         | 123/1859 [2:28:44<37:41:53, 78.18s/it]  7%|▋         | 124/1859 [2:30:05<38:02:08, 78.92s/it]  7%|▋         | 125/1859 [2:31:11<36:15:39, 75.28s/it]  7%|▋         | 126/1859 [2:32:15<34:37:39, 71.93s/it]  7%|▋         | 127/1859 [2:33:32<35:15:36, 73.29s/it]  7%|▋         | 128/1859 [2:34:59<37:17:57, 77.57s/it]  7%|▋         | 129/1859 [2:36:16<37:04:32, 77.15s/it]  7%|▋         | 130/1859 [2:37:17<34:50:21, 72.54s/it]                                                       {'loss': 0.7281, 'grad_norm': 0.4150218367576599, 'learning_rate': 6.935483870967743e-05, 'epoch': 0.07}
  7%|▋         | 130/1859 [2:37:17<34:50:21, 72.54s/it]  7%|▋         | 131/1859 [2:38:28<34:30:21, 71.89s/it]  7%|▋         | 132/1859 [2:39:34<33:44:02, 70.32s/it]  7%|▋         | 133/1859 [2:40:29<31:25:44, 65.55s/it]  7%|▋         | 134/1859 [2:41:31<30:55:51, 64.55s/it]  7%|▋         | 135/1859 [2:42:57<33:59:41, 70.99s/it]  7%|▋         | 136/1859 [2:43:58<32:35:25, 68.09s/it]  7%|▋         | 137/1859 [2:45:08<32:49:03, 68.61s/it]  7%|▋         | 138/1859 [2:46:16<32:36:28, 68.21s/it]  7%|▋         | 139/1859 [2:47:33<33:56:44, 71.05s/it]  8%|▊         | 140/1859 [2:48:52<35:01:01, 73.33s/it]                                                       {'loss': 0.7941, 'grad_norm': 0.3887403905391693, 'learning_rate': 7.473118279569893e-05, 'epoch': 0.08}
  8%|▊         | 140/1859 [2:48:52<35:01:01, 73.33s/it]  8%|▊         | 141/1859 [2:50:14<36:18:02, 76.07s/it]  8%|▊         | 142/1859 [2:51:21<34:53:31, 73.16s/it]  8%|▊         | 143/1859 [2:52:22<33:10:40, 69.60s/it]  8%|▊         | 144/1859 [2:53:43<34:48:51, 73.08s/it]  8%|▊         | 145/1859 [2:55:00<35:18:50, 74.17s/it]  8%|▊         | 146/1859 [2:56:02<33:34:33, 70.56s/it]  8%|▊         | 147/1859 [2:57:09<33:03:44, 69.52s/it]  8%|▊         | 148/1859 [2:58:46<36:55:32, 77.69s/it]  8%|▊         | 149/1859 [2:59:54<35:32:59, 74.84s/it]  8%|▊         | 150/1859 [3:00:59<34:08:54, 71.93s/it]                                                       {'loss': 0.7582, 'grad_norm': 0.34625017642974854, 'learning_rate': 8.010752688172043e-05, 'epoch': 0.08}
  8%|▊         | 150/1859 [3:00:59<34:08:54, 71.93s/it]  8%|▊         | 151/1859 [3:02:15<34:44:27, 73.22s/it]  8%|▊         | 152/1859 [3:03:32<35:11:58, 74.23s/it]  8%|▊         | 153/1859 [3:05:07<38:03:36, 80.31s/it]  8%|▊         | 154/1859 [3:06:19<36:53:30, 77.89s/it]  8%|▊         | 155/1859 [3:07:44<37:57:32, 80.20s/it]  8%|▊         | 156/1859 [3:08:42<34:46:16, 73.50s/it]  8%|▊         | 157/1859 [3:10:08<36:25:46, 77.05s/it]  8%|▊         | 158/1859 [3:11:13<34:45:18, 73.56s/it]  9%|▊         | 159/1859 [3:12:19<33:37:07, 71.19s/it]  9%|▊         | 160/1859 [3:13:40<34:57:50, 74.09s/it]                                                       {'loss': 0.7811, 'grad_norm': 0.47228410840034485, 'learning_rate': 8.548387096774195e-05, 'epoch': 0.09}
  9%|▊         | 160/1859 [3:13:40<34:57:50, 74.09s/it]  9%|▊         | 161/1859 [3:14:42<33:18:12, 70.61s/it]  9%|▊         | 162/1859 [3:15:46<32:24:22, 68.75s/it]  9%|▉         | 163/1859 [3:17:06<33:51:15, 71.86s/it]  9%|▉         | 164/1859 [3:18:16<33:34:19, 71.30s/it]  9%|▉         | 165/1859 [3:19:31<34:11:18, 72.66s/it]  9%|▉         | 166/1859 [3:20:52<35:17:29, 75.04s/it]  9%|▉         | 167/1859 [3:22:03<34:45:11, 73.94s/it]  9%|▉         | 168/1859 [3:23:02<32:37:30, 69.46s/it]  9%|▉         | 169/1859 [3:24:20<33:49:54, 72.07s/it]  9%|▉         | 170/1859 [3:25:51<36:21:02, 77.48s/it]                                                       {'loss': 0.7699, 'grad_norm': 0.3831373453140259, 'learning_rate': 9.086021505376345e-05, 'epoch': 0.09}
  9%|▉         | 170/1859 [3:25:51<36:21:02, 77.48s/it]  9%|▉         | 171/1859 [3:26:47<33:18:19, 71.03s/it]  9%|▉         | 172/1859 [3:27:48<31:56:02, 68.15s/it]  9%|▉         | 173/1859 [3:29:03<32:52:41, 70.20s/it]  9%|▉         | 174/1859 [3:30:18<33:32:22, 71.66s/it]  9%|▉         | 175/1859 [3:31:44<35:29:53, 75.89s/it]  9%|▉         | 176/1859 [3:33:06<36:18:56, 77.68s/it] 10%|▉         | 177/1859 [3:34:22<36:02:10, 77.13s/it] 10%|▉         | 178/1859 [3:35:29<34:41:30, 74.30s/it] 10%|▉         | 179/1859 [3:36:46<35:01:41, 75.06s/it] 10%|▉         | 180/1859 [3:37:59<34:39:44, 74.32s/it]                                                       {'loss': 0.7626, 'grad_norm': 0.28280332684516907, 'learning_rate': 9.623655913978496e-05, 'epoch': 0.1}
 10%|▉         | 180/1859 [3:37:59<34:39:44, 74.32s/it] 10%|▉         | 181/1859 [3:39:09<34:05:39, 73.15s/it] 10%|▉         | 182/1859 [3:40:27<34:44:42, 74.59s/it] 10%|▉         | 183/1859 [3:41:32<33:24:12, 71.75s/it] 10%|▉         | 184/1859 [3:42:44<33:23:34, 71.77s/it] 10%|▉         | 185/1859 [3:43:52<32:49:23, 70.59s/it] 10%|█         | 186/1859 [3:45:00<32:24:57, 69.75s/it] 10%|█         | 187/1859 [3:46:37<36:16:13, 78.09s/it] 10%|█         | 188/1859 [3:47:46<34:58:43, 75.36s/it] 10%|█         | 189/1859 [3:49:01<34:50:46, 75.12s/it] 10%|█         | 190/1859 [3:50:08<33:42:33, 72.71s/it]                                                       {'loss': 0.7626, 'grad_norm': 0.3153783977031708, 'learning_rate': 9.999920660541323e-05, 'epoch': 0.1}
 10%|█         | 190/1859 [3:50:08<33:42:33, 72.71s/it] 10%|█         | 191/1859 [3:51:22<33:52:47, 73.12s/it] 10%|█         | 192/1859 [3:52:56<36:50:36, 79.57s/it] 10%|█         | 193/1859 [3:53:55<33:52:50, 73.21s/it] 10%|█         | 194/1859 [3:55:11<34:15:34, 74.07s/it] 10%|█         | 195/1859 [3:56:05<31:28:00, 68.08s/it] 11%|█         | 196/1859 [3:57:10<30:59:28, 67.09s/it] 11%|█         | 197/1859 [3:58:26<32:16:25, 69.91s/it] 11%|█         | 198/1859 [3:59:30<31:21:09, 67.95s/it] 11%|█         | 199/1859 [4:01:11<35:56:22, 77.94s/it] 11%|█         | 200/1859 [4:02:38<37:07:09, 80.55s/it]                                                       {'loss': 0.8213, 'grad_norm': 0.32911136746406555, 'learning_rate': 9.99851025132013e-05, 'epoch': 0.11}
 11%|█         | 200/1859 [4:02:38<37:07:09, 80.55s/it] 11%|█         | 201/1859 [4:04:20<40:06:09, 87.07s/it] 11%|█         | 202/1859 [4:05:31<37:53:51, 82.34s/it] 11%|█         | 203/1859 [4:07:09<40:00:43, 86.98s/it] 11%|█         | 204/1859 [4:08:21<37:56:05, 82.52s/it] 11%|█         | 205/1859 [4:09:28<35:42:05, 77.71s/it] 11%|█         | 206/1859 [4:10:45<35:36:28, 77.55s/it] 11%|█         | 207/1859 [4:12:11<36:44:09, 80.05s/it] 11%|█         | 208/1859 [4:13:39<37:48:29, 82.44s/it] 11%|█         | 209/1859 [4:15:10<38:58:57, 85.05s/it] 11%|█▏        | 210/1859 [4:16:10<35:35:23, 77.70s/it]                                                       {'loss': 0.7507, 'grad_norm': 0.3797777593135834, 'learning_rate': 9.995337315463582e-05, 'epoch': 0.11}
 11%|█▏        | 210/1859 [4:16:10<35:35:23, 77.70s/it] 11%|█▏        | 211/1859 [4:17:14<33:38:39, 73.49s/it] 11%|█▏        | 212/1859 [4:18:31<34:05:21, 74.51s/it] 11%|█▏        | 213/1859 [4:19:39<33:09:41, 72.53s/it] 12%|█▏        | 214/1859 [4:21:12<35:56:11, 78.65s/it] 12%|█▏        | 215/1859 [4:22:26<35:21:08, 77.41s/it] 12%|█▏        | 216/1859 [4:23:29<33:15:52, 72.89s/it] 12%|█▏        | 217/1859 [4:24:37<32:40:36, 71.64s/it] 12%|█▏        | 218/1859 [4:25:42<31:44:02, 69.62s/it] 12%|█▏        | 219/1859 [4:26:44<30:37:56, 67.24s/it] 12%|█▏        | 220/1859 [4:27:50<30:25:55, 66.84s/it]                                                       {'loss': 0.7422, 'grad_norm': 0.3350107967853546, 'learning_rate': 9.990402971781818e-05, 'epoch': 0.12}
 12%|█▏        | 220/1859 [4:27:50<30:25:55, 66.84s/it] 12%|█▏        | 221/1859 [4:28:52<29:50:08, 65.57s/it] 12%|█▏        | 222/1859 [4:30:09<31:21:55, 68.98s/it] 12%|█▏        | 223/1859 [4:31:32<33:08:51, 72.94s/it] 12%|█▏        | 224/1859 [4:32:56<34:39:12, 76.30s/it] 12%|█▏        | 225/1859 [4:34:19<35:36:41, 78.46s/it] 12%|█▏        | 226/1859 [4:35:35<35:14:23, 77.69s/it] 12%|█▏        | 227/1859 [4:37:05<36:49:31, 81.23s/it] 12%|█▏        | 228/1859 [4:38:14<35:12:02, 77.70s/it] 12%|█▏        | 229/1859 [4:39:29<34:51:46, 77.00s/it] 12%|█▏        | 230/1859 [4:40:56<36:06:56, 79.81s/it]                                                       {'loss': 0.8301, 'grad_norm': 0.3259562849998474, 'learning_rate': 9.983708960175677e-05, 'epoch': 0.12}
 12%|█▏        | 230/1859 [4:40:56<36:06:56, 79.81s/it] 12%|█▏        | 231/1859 [4:42:10<35:17:14, 78.03s/it] 12%|█▏        | 232/1859 [4:43:30<35:38:07, 78.85s/it] 13%|█▎        | 233/1859 [4:44:47<35:18:15, 78.16s/it] 13%|█▎        | 234/1859 [4:45:53<33:35:14, 74.41s/it] 13%|█▎        | 235/1859 [4:47:18<35:01:32, 77.64s/it] 13%|█▎        | 236/1859 [4:48:43<36:00:25, 79.87s/it] 13%|█▎        | 237/1859 [4:49:52<34:34:41, 76.75s/it] 13%|█▎        | 238/1859 [4:50:58<33:06:32, 73.53s/it] 13%|█▎        | 239/1859 [4:52:24<34:44:08, 77.19s/it] 13%|█▎        | 240/1859 [4:53:21<32:00:51, 71.19s/it]                                                       {'loss': 0.7878, 'grad_norm': 0.29552775621414185, 'learning_rate': 9.975257641023182e-05, 'epoch': 0.13}
 13%|█▎        | 240/1859 [4:53:21<32:00:51, 71.19s/it] 13%|█▎        | 241/1859 [4:54:28<31:24:37, 69.89s/it] 13%|█▎        | 242/1859 [4:55:27<29:57:32, 66.70s/it] 13%|█▎        | 243/1859 [4:56:38<30:26:48, 67.83s/it] 13%|█▎        | 244/1859 [4:58:06<33:12:10, 74.01s/it] 13%|█▎        | 245/1859 [4:59:15<32:29:37, 72.48s/it] 13%|█▎        | 246/1859 [5:00:27<32:25:00, 72.35s/it] 13%|█▎        | 247/1859 [5:01:39<32:21:52, 72.28s/it] 13%|█▎        | 248/1859 [5:02:53<32:35:29, 72.83s/it] 13%|█▎        | 249/1859 [5:04:01<31:54:44, 71.36s/it] 13%|█▎        | 250/1859 [5:04:54<29:20:26, 65.65s/it]                                                       {'loss': 0.7752, 'grad_norm': 0.45997190475463867, 'learning_rate': 9.96505199434725e-05, 'epoch': 0.13}
 13%|█▎        | 250/1859 [5:04:54<29:20:26, 65.65s/it] 14%|█▎        | 251/1859 [5:06:25<32:44:48, 73.31s/it] 14%|█▎        | 252/1859 [5:07:45<33:40:11, 75.43s/it] 14%|█▎        | 253/1859 [5:08:54<32:45:46, 73.44s/it] 14%|█▎        | 254/1859 [5:10:01<31:53:18, 71.53s/it] 14%|█▎        | 255/1859 [5:10:52<29:07:52, 65.38s/it] 14%|█▍        | 256/1859 [5:12:07<30:22:16, 68.21s/it] 14%|█▍        | 257/1859 [5:13:30<32:17:10, 72.55s/it] 14%|█▍        | 258/1859 [5:14:35<31:14:42, 70.26s/it] 14%|█▍        | 259/1859 [5:15:45<31:18:03, 70.43s/it] 14%|█▍        | 260/1859 [5:16:56<31:21:09, 70.59s/it]                                                       {'loss': 0.7634, 'grad_norm': 0.27377399802207947, 'learning_rate': 9.95309561876491e-05, 'epoch': 0.14}
 14%|█▍        | 260/1859 [5:16:56<31:21:09, 70.59s/it] 14%|█▍        | 261/1859 [5:18:03<30:47:13, 69.36s/it] 14%|█▍        | 262/1859 [5:19:19<31:43:10, 71.50s/it] 14%|█▍        | 263/1859 [5:20:29<31:29:27, 71.03s/it] 14%|█▍        | 264/1859 [5:21:33<30:30:22, 68.85s/it] 14%|█▍        | 265/1859 [5:22:47<31:12:37, 70.49s/it] 14%|█▍        | 266/1859 [5:23:51<30:13:30, 68.31s/it] 14%|█▍        | 267/1859 [5:25:22<33:17:38, 75.29s/it] 14%|█▍        | 268/1859 [5:26:28<32:01:49, 72.48s/it] 14%|█▍        | 269/1859 [5:27:46<32:43:24, 74.09s/it] 15%|█▍        | 270/1859 [5:29:17<34:57:52, 79.22s/it]                                                       {'loss': 0.7831, 'grad_norm': 0.4159303903579712, 'learning_rate': 9.939392730218387e-05, 'epoch': 0.15}
 15%|█▍        | 270/1859 [5:29:17<34:57:52, 79.22s/it] 15%|█▍        | 271/1859 [5:30:15<32:05:14, 72.74s/it] 15%|█▍        | 272/1859 [5:31:19<30:55:52, 70.17s/it] 15%|█▍        | 273/1859 [5:32:18<29:24:38, 66.76s/it] 15%|█▍        | 274/1859 [5:33:31<30:18:49, 68.85s/it] 15%|█▍        | 275/1859 [5:34:42<30:35:01, 69.51s/it] 15%|█▍        | 276/1859 [5:36:06<32:25:42, 73.75s/it] 15%|█▍        | 277/1859 [5:37:20<32:24:37, 73.75s/it] 15%|█▍        | 278/1859 [5:38:17<30:15:24, 68.90s/it] 15%|█▌        | 279/1859 [5:39:27<30:19:33, 69.10s/it] 15%|█▌        | 280/1859 [5:40:33<29:57:32, 68.30s/it]                                                       {'loss': 0.7611, 'grad_norm': 0.3286915123462677, 'learning_rate': 9.923948160488514e-05, 'epoch': 0.15}
 15%|█▌        | 280/1859 [5:40:33<29:57:32, 68.30s/it] 15%|█▌        | 281/1859 [5:42:04<32:49:58, 74.90s/it] 15%|█▌        | 282/1859 [5:43:05<31:04:02, 70.92s/it] 15%|█▌        | 283/1859 [5:44:32<33:05:48, 75.60s/it] 15%|█▌        | 284/1859 [5:45:39<31:56:38, 73.01s/it] 15%|█▌        | 285/1859 [5:46:45<30:59:06, 70.87s/it] 15%|█▌        | 286/1859 [5:47:51<30:20:38, 69.45s/it] 15%|█▌        | 287/1859 [5:48:50<29:01:32, 66.47s/it] 15%|█▌        | 288/1859 [5:50:05<30:01:49, 68.82s/it] 16%|█▌        | 289/1859 [5:51:09<29:23:03, 67.38s/it] 16%|█▌        | 290/1859 [5:52:19<29:46:14, 68.31s/it]                                                       {'loss': 0.7842, 'grad_norm': 0.36082375049591064, 'learning_rate': 9.90676735549101e-05, 'epoch': 0.16}
 16%|█▌        | 290/1859 [5:52:19<29:46:14, 68.31s/it] 16%|█▌        | 291/1859 [5:53:46<32:13:01, 73.97s/it] 16%|█▌        | 292/1859 [5:54:49<30:42:11, 70.54s/it] 16%|█▌        | 293/1859 [5:55:59<30:36:26, 70.36s/it] 16%|█▌        | 294/1859 [5:57:00<29:20:26, 67.49s/it] 16%|█▌        | 295/1859 [5:58:05<28:59:11, 66.72s/it] 16%|█▌        | 296/1859 [5:59:25<30:43:35, 70.77s/it] 16%|█▌        | 297/1859 [6:00:34<30:33:00, 70.41s/it] 16%|█▌        | 298/1859 [6:01:49<31:06:37, 71.75s/it] 16%|█▌        | 299/1859 [6:02:57<30:31:53, 70.46s/it] 16%|█▌        | 300/1859 [6:03:48<28:05:17, 64.86s/it]                                                       {'loss': 0.7777, 'grad_norm': 0.3547758460044861, 'learning_rate': 9.887856373356171e-05, 'epoch': 0.16}
 16%|█▌        | 300/1859 [6:03:48<28:05:17, 64.86s/it] 16%|█▌        | 301/1859 [6:05:00<28:59:12, 66.98s/it] 16%|█▌        | 302/1859 [6:06:17<30:17:07, 70.02s/it] 16%|█▋        | 303/1859 [6:07:21<29:25:57, 68.10s/it] 16%|█▋        | 304/1859 [6:08:49<31:55:08, 73.90s/it] 16%|█▋        | 305/1859 [6:10:05<32:17:10, 74.79s/it] 16%|█▋        | 306/1859 [6:11:27<33:10:18, 76.90s/it] 17%|█▋        | 307/1859 [6:12:37<32:16:30, 74.87s/it] 17%|█▋        | 308/1859 [6:13:41<30:46:14, 71.42s/it] 17%|█▋        | 309/1859 [6:14:42<29:29:33, 68.50s/it] 17%|█▋        | 310/1859 [6:15:47<28:58:12, 67.33s/it]                                                       {'loss': 0.7903, 'grad_norm': 0.29150086641311646, 'learning_rate': 9.867221882292737e-05, 'epoch': 0.17}
 17%|█▋        | 310/1859 [6:15:47<28:58:12, 67.33s/it] 17%|█▋        | 311/1859 [6:17:01<29:49:12, 69.35s/it] 17%|█▋        | 312/1859 [6:18:24<31:34:46, 73.49s/it] 17%|█▋        | 313/1859 [6:19:45<32:28:12, 75.61s/it] 17%|█▋        | 314/1859 [6:21:03<32:45:29, 76.33s/it] 17%|█▋        | 315/1859 [6:22:21<33:00:59, 76.98s/it] 17%|█▋        | 316/1859 [6:23:27<31:35:07, 73.69s/it] 17%|█▋        | 317/1859 [6:24:42<31:41:43, 74.00s/it] 17%|█▋        | 318/1859 [6:26:12<33:44:14, 78.82s/it] 17%|█▋        | 319/1859 [6:27:22<32:38:23, 76.30s/it] 17%|█▋        | 320/1859 [6:28:43<33:06:18, 77.44s/it]                                                       {'loss': 0.7755, 'grad_norm': 0.4641018509864807, 'learning_rate': 9.844871158236591e-05, 'epoch': 0.17}
 17%|█▋        | 320/1859 [6:28:43<33:06:18, 77.44s/it] 17%|█▋        | 321/1859 [6:29:52<31:59:36, 74.89s/it] 17%|█▋        | 322/1859 [6:31:05<31:45:05, 74.37s/it] 17%|█▋        | 323/1859 [6:32:18<31:35:32, 74.04s/it] 17%|█▋        | 324/1859 [6:33:18<29:47:30, 69.87s/it] 17%|█▋        | 325/1859 [6:34:38<31:04:13, 72.92s/it] 18%|█▊        | 326/1859 [6:35:56<31:44:48, 74.55s/it] 18%|█▊        | 327/1859 [6:37:04<30:50:16, 72.47s/it] 18%|█▊        | 328/1859 [6:38:16<30:44:28, 72.29s/it] 18%|█▊        | 329/1859 [6:39:34<31:25:52, 73.96s/it] 18%|█▊        | 330/1859 [6:41:02<33:13:32, 78.23s/it]                                                       {'loss': 0.7924, 'grad_norm': 0.3136468827724457, 'learning_rate': 9.820812082285195e-05, 'epoch': 0.18}
 18%|█▊        | 330/1859 [6:41:02<33:13:32, 78.23s/it] 18%|█▊        | 331/1859 [6:42:12<32:06:49, 75.66s/it] 18%|█▊        | 332/1859 [6:43:39<33:37:30, 79.27s/it] 18%|█▊        | 333/1859 [6:44:56<33:15:04, 78.44s/it] 18%|█▊        | 334/1859 [6:46:03<31:46:23, 75.01s/it] 18%|█▊        | 335/1859 [6:47:09<30:37:16, 72.33s/it] 18%|█▊        | 336/1859 [6:48:20<30:24:39, 71.88s/it] 18%|█▊        | 337/1859 [6:49:15<28:13:10, 66.75s/it] 18%|█▊        | 338/1859 [6:50:24<28:34:30, 67.63s/it] 18%|█▊        | 339/1859 [6:51:20<27:02:11, 64.03s/it] 18%|█▊        | 340/1859 [6:52:21<26:42:34, 63.30s/it]                                                       {'loss': 0.7612, 'grad_norm': 0.3586122393608093, 'learning_rate': 9.795053137918639e-05, 'epoch': 0.18}
 18%|█▊        | 340/1859 [6:52:21<26:42:34, 63.30s/it] 18%|█▊        | 341/1859 [6:53:33<27:41:20, 65.67s/it] 18%|█▊        | 342/1859 [6:55:00<30:27:38, 72.29s/it] 18%|█▊        | 343/1859 [6:56:18<31:05:48, 73.84s/it] 19%|█▊        | 344/1859 [6:57:24<30:06:04, 71.53s/it] 19%|█▊        | 345/1859 [6:58:36<30:09:14, 71.70s/it] 19%|█▊        | 346/1859 [6:59:53<30:44:19, 73.14s/it] 19%|█▊        | 347/1859 [7:01:05<30:36:27, 72.88s/it] 19%|█▊        | 348/1859 [7:02:19<30:47:49, 73.37s/it] 19%|█▉        | 349/1859 [7:03:31<30:35:05, 72.92s/it] 19%|█▉        | 350/1859 [7:04:41<30:12:25, 72.06s/it]                                                       {'loss': 0.7634, 'grad_norm': 0.2319854348897934, 'learning_rate': 9.76760340800827e-05, 'epoch': 0.19}
 19%|█▉        | 350/1859 [7:04:41<30:12:25, 72.06s/it] 19%|█▉        | 351/1859 [7:05:56<30:31:37, 72.88s/it] 19%|█▉        | 352/1859 [7:07:14<31:05:35, 74.28s/it] 19%|█▉        | 353/1859 [7:08:27<30:57:59, 74.02s/it] 19%|█▉        | 354/1859 [7:09:32<29:48:02, 71.28s/it] 19%|█▉        | 355/1859 [7:10:33<28:28:39, 68.16s/it] 19%|█▉        | 356/1859 [7:11:39<28:13:42, 67.61s/it] 19%|█▉        | 357/1859 [7:12:45<28:01:25, 67.17s/it] 19%|█▉        | 358/1859 [7:13:45<27:07:00, 65.04s/it] 19%|█▉        | 359/1859 [7:14:42<26:00:35, 62.42s/it] 19%|█▉        | 360/1859 [7:15:56<27:31:10, 66.09s/it]                                                       {'loss': 0.7561, 'grad_norm': 0.3543745279312134, 'learning_rate': 9.738472571613984e-05, 'epoch': 0.19}
 19%|█▉        | 360/1859 [7:15:56<27:31:10, 66.09s/it] 19%|█▉        | 361/1859 [7:17:04<27:39:55, 66.49s/it] 19%|█▉        | 362/1859 [7:18:23<29:10:33, 70.16s/it] 20%|█▉        | 363/1859 [7:19:37<29:39:56, 71.39s/it] 20%|█▉        | 364/1859 [7:20:49<29:46:47, 71.71s/it] 20%|█▉        | 365/1859 [7:22:23<32:27:11, 78.20s/it] 20%|█▉        | 366/1859 [7:23:30<31:08:39, 75.10s/it] 20%|█▉        | 367/1859 [7:24:40<30:27:04, 73.48s/it] 20%|█▉        | 368/1859 [7:25:47<29:39:01, 71.59s/it] 20%|█▉        | 369/1859 [7:26:58<29:33:33, 71.42s/it] 20%|█▉        | 370/1859 [7:28:30<32:06:38, 77.64s/it]                                                       {'loss': 0.8019, 'grad_norm': 0.40336930751800537, 'learning_rate': 9.70767090057128e-05, 'epoch': 0.2}
 20%|█▉        | 370/1859 [7:28:30<32:06:38, 77.64s/it] 20%|█▉        | 371/1859 [7:29:48<32:04:18, 77.59s/it] 20%|██        | 372/1859 [7:31:01<31:29:09, 76.23s/it] 20%|██        | 373/1859 [7:32:13<30:59:48, 75.09s/it] 20%|██        | 374/1859 [7:33:16<29:22:25, 71.21s/it] 20%|██        | 375/1859 [7:34:33<30:05:57, 73.02s/it] 20%|██        | 376/1859 [7:35:36<28:54:51, 70.19s/it] 20%|██        | 377/1859 [7:36:47<28:56:02, 70.29s/it] 20%|██        | 378/1859 [7:38:11<30:37:09, 74.43s/it] 20%|██        | 379/1859 [7:39:51<33:42:52, 82.01s/it] 20%|██        | 380/1859 [7:40:55<31:27:14, 76.56s/it]                                                       {'loss': 0.7475, 'grad_norm': 0.3750408887863159, 'learning_rate': 9.67520925586931e-05, 'epoch': 0.2}
 20%|██        | 380/1859 [7:40:55<31:27:14, 76.56s/it] 20%|██        | 381/1859 [7:42:02<30:20:35, 73.91s/it] 21%|██        | 382/1859 [7:43:29<31:55:42, 77.82s/it] 21%|██        | 383/1859 [7:45:05<34:09:37, 83.32s/it] 21%|██        | 384/1859 [7:46:24<33:33:19, 81.90s/it] 21%|██        | 385/1859 [7:47:26<31:05:54, 75.95s/it] 21%|██        | 386/1859 [7:48:27<29:12:05, 71.37s/it] 21%|██        | 387/1859 [7:49:47<30:14:00, 73.94s/it] 21%|██        | 388/1859 [7:51:03<30:27:56, 74.56s/it] 21%|██        | 389/1859 [7:52:21<30:54:38, 75.70s/it] 21%|██        | 390/1859 [7:53:19<28:44:26, 70.43s/it]                                                       {'loss': 0.7691, 'grad_norm': 0.26566845178604126, 'learning_rate': 9.64109908382119e-05, 'epoch': 0.21}
 21%|██        | 390/1859 [7:53:19<28:44:26, 70.43s/it] 21%|██        | 391/1859 [7:54:35<29:20:59, 71.98s/it] 21%|██        | 392/1859 [7:56:00<30:57:02, 75.95s/it] 21%|██        | 393/1859 [7:57:27<32:19:28, 79.38s/it] 21%|██        | 394/1859 [7:58:40<31:29:51, 77.40s/it] 21%|██        | 395/1859 [7:59:56<31:16:37, 76.91s/it] 21%|██▏       | 396/1859 [8:01:32<33:36:29, 82.70s/it] 21%|██▏       | 397/1859 [8:02:45<32:24:30, 79.80s/it] 21%|██▏       | 398/1859 [8:04:07<32:34:35, 80.27s/it] 21%|██▏       | 399/1859 [8:05:14<31:01:03, 76.48s/it] 22%|██▏       | 400/1859 [8:06:14<28:58:51, 71.51s/it]                                                       {'loss': 0.768, 'grad_norm': 0.2841324508190155, 'learning_rate': 9.60535241202789e-05, 'epoch': 0.22}
 22%|██▏       | 400/1859 [8:06:14<28:58:51, 71.51s/it] 22%|██▏       | 401/1859 [8:07:29<29:21:06, 72.47s/it] 22%|██▏       | 402/1859 [8:08:43<29:33:29, 73.03s/it] 22%|██▏       | 403/1859 [8:10:28<33:26:27, 82.68s/it] 22%|██▏       | 404/1859 [8:11:57<34:12:05, 84.62s/it] 22%|██▏       | 405/1859 [8:13:03<31:48:26, 78.75s/it] 22%|██▏       | 406/1859 [8:14:32<33:02:57, 81.88s/it] 22%|██▏       | 407/1859 [8:15:49<32:26:15, 80.42s/it] 22%|██▏       | 408/1859 [8:16:54<30:31:37, 75.74s/it] 22%|██▏       | 409/1859 [8:18:08<30:23:16, 75.45s/it] 22%|██▏       | 410/1859 [8:19:22<30:10:50, 74.98s/it]                                                       {'loss': 0.7857, 'grad_norm': 0.4305250942707062, 'learning_rate': 9.567981845137193e-05, 'epoch': 0.22}
 22%|██▏       | 410/1859 [8:19:22<30:10:50, 74.98s/it] 22%|██▏       | 411/1859 [8:20:46<31:11:25, 77.54s/it] 22%|██▏       | 412/1859 [8:21:47<29:13:49, 72.72s/it] 22%|██▏       | 413/1859 [8:22:51<28:09:39, 70.11s/it] 22%|██▏       | 414/1859 [8:24:15<29:45:05, 74.12s/it] 22%|██▏       | 415/1859 [8:25:28<29:39:48, 73.95s/it] 22%|██▏       | 416/1859 [8:26:31<28:19:36, 70.67s/it] 22%|██▏       | 417/1859 [8:27:45<28:42:37, 71.68s/it] 22%|██▏       | 418/1859 [8:28:48<27:38:38, 69.06s/it] 23%|██▎       | 419/1859 [8:29:51<26:52:34, 67.19s/it] 23%|██▎       | 420/1859 [8:31:08<27:58:14, 69.98s/it]                                                       {'loss': 0.7851, 'grad_norm': 0.3783763647079468, 'learning_rate': 9.529000560399163e-05, 'epoch': 0.23}
 23%|██▎       | 420/1859 [8:31:08<27:58:14, 69.98s/it] 23%|██▎       | 421/1859 [8:32:33<29:48:58, 74.64s/it] 23%|██▎       | 422/1859 [8:33:50<30:04:29, 75.34s/it] 23%|██▎       | 423/1859 [8:35:01<29:30:48, 73.99s/it] 23%|██▎       | 424/1859 [8:36:44<32:55:33, 82.60s/it] 23%|██▎       | 425/1859 [8:37:57<31:49:21, 79.89s/it] 23%|██▎       | 426/1859 [8:39:09<30:49:33, 77.44s/it] 23%|██▎       | 427/1859 [8:40:05<28:15:25, 71.04s/it] 23%|██▎       | 428/1859 [8:41:09<27:25:47, 69.01s/it] 23%|██▎       | 429/1859 [8:42:39<29:53:55, 75.27s/it] 23%|██▎       | 430/1859 [8:43:41<28:18:39, 71.32s/it]                                                       {'loss': 0.778, 'grad_norm': 0.3650430738925934, 'learning_rate': 9.488422303019708e-05, 'epoch': 0.23}
 23%|██▎       | 430/1859 [8:43:41<28:18:39, 71.32s/it] 23%|██▎       | 431/1859 [8:44:57<28:46:44, 72.55s/it] 23%|██▎       | 432/1859 [8:45:59<27:33:28, 69.52s/it] 23%|██▎       | 433/1859 [8:47:00<26:30:25, 66.92s/it] 23%|██▎       | 434/1859 [8:48:06<26:19:54, 66.52s/it] 23%|██▎       | 435/1859 [8:49:23<27:36:48, 69.81s/it] 23%|██▎       | 436/1859 [8:50:38<28:15:39, 71.50s/it] 24%|██▎       | 437/1859 [8:51:43<27:26:02, 69.45s/it] 24%|██▎       | 438/1859 [8:52:54<27:36:52, 69.96s/it] 24%|██▎       | 439/1859 [8:53:55<26:26:56, 67.05s/it] 24%|██▎       | 440/1859 [8:55:17<28:14:23, 71.64s/it]                                                       {'loss': 0.7829, 'grad_norm': 0.31614693999290466, 'learning_rate': 9.446261381313875e-05, 'epoch': 0.24}
 24%|██▎       | 440/1859 [8:55:17<28:14:23, 71.64s/it] 24%|██▎       | 441/1859 [8:56:38<29:23:10, 74.61s/it] 24%|██▍       | 442/1859 [8:57:52<29:13:16, 74.24s/it] 24%|██▍       | 443/1859 [8:58:54<27:49:34, 70.74s/it] 24%|██▍       | 444/1859 [9:00:25<30:12:09, 76.84s/it] 24%|██▍       | 445/1859 [9:01:50<31:06:28, 79.20s/it] 24%|██▍       | 446/1859 [9:02:57<29:36:36, 75.44s/it] 24%|██▍       | 447/1859 [9:04:11<29:25:14, 75.01s/it] 24%|██▍       | 448/1859 [9:05:23<29:01:55, 74.07s/it] 24%|██▍       | 449/1859 [9:06:49<30:27:30, 77.77s/it] 24%|██▍       | 450/1859 [9:07:57<29:17:07, 74.82s/it]                                                       {'loss': 0.7713, 'grad_norm': 0.2539416253566742, 'learning_rate': 9.402532661660591e-05, 'epoch': 0.24}
 24%|██▍       | 450/1859 [9:07:57<29:17:07, 74.82s/it] 24%|██▍       | 451/1859 [9:09:15<29:40:14, 75.86s/it] 24%|██▍       | 452/1859 [9:10:55<32:25:16, 82.95s/it] 24%|██▍       | 453/1859 [9:11:59<30:11:01, 77.28s/it] 24%|██▍       | 454/1859 [9:13:12<29:40:19, 76.03s/it] 24%|██▍       | 455/1859 [9:14:26<29:22:28, 75.32s/it] 25%|██▍       | 456/1859 [9:15:32<28:20:11, 72.71s/it] 25%|██▍       | 457/1859 [9:16:38<27:29:30, 70.59s/it] 25%|██▍       | 458/1859 [9:17:47<27:20:35, 70.26s/it] 25%|██▍       | 459/1859 [9:19:09<28:38:34, 73.65s/it] 25%|██▍       | 460/1859 [9:20:31<29:38:10, 76.26s/it]                                                       {'loss': 0.7915, 'grad_norm': 0.43492481112480164, 'learning_rate': 9.35725156326063e-05, 'epoch': 0.25}
 25%|██▍       | 460/1859 [9:20:31<29:38:10, 76.26s/it] 25%|██▍       | 461/1859 [9:22:00<31:00:37, 79.86s/it] 25%|██▍       | 462/1859 [9:23:08<29:40:58, 76.49s/it] 25%|██▍       | 463/1859 [9:24:21<29:11:57, 75.30s/it] 25%|██▍       | 464/1859 [9:25:37<29:14:53, 75.48s/it] 25%|██▌       | 465/1859 [9:26:49<28:49:37, 74.45s/it] 25%|██▌       | 466/1859 [9:28:04<28:55:44, 74.76s/it] 25%|██▌       | 467/1859 [9:29:30<30:14:47, 78.22s/it] 25%|██▌       | 468/1859 [9:30:32<28:15:54, 73.15s/it] 25%|██▌       | 469/1859 [9:31:59<29:49:14, 77.23s/it] 25%|██▌       | 470/1859 [9:33:07<28:47:05, 74.60s/it]                                                       {'loss': 0.7651, 'grad_norm': 0.2828200161457062, 'learning_rate': 9.310434052699634e-05, 'epoch': 0.25}
 25%|██▌       | 470/1859 [9:33:07<28:47:05, 74.60s/it] 25%|██▌       | 471/1859 [9:34:36<30:27:42, 79.01s/it] 25%|██▌       | 472/1859 [9:35:42<28:52:22, 74.94s/it] 25%|██▌       | 473/1859 [9:36:51<28:10:10, 73.17s/it] 25%|██▌       | 474/1859 [9:38:27<30:48:36, 80.08s/it] 26%|██▌       | 475/1859 [9:39:40<30:01:02, 78.08s/it] 26%|██▌       | 476/1859 [9:40:53<29:20:07, 76.36s/it] 26%|██▌       | 477/1859 [9:42:02<28:31:58, 74.33s/it] 26%|██▌       | 478/1859 [9:43:12<28:01:25, 73.05s/it] 26%|██▌       | 479/1859 [9:44:08<25:57:18, 67.71s/it] 26%|██▌       | 480/1859 [9:45:17<26:03:59, 68.05s/it]                                                       {'loss': 0.7847, 'grad_norm': 0.3223974108695984, 'learning_rate': 9.26209663831813e-05, 'epoch': 0.26}
 26%|██▌       | 480/1859 [9:45:17<26:03:59, 68.05s/it] 26%|██▌       | 481/1859 [9:46:42<28:05:10, 73.38s/it] 26%|██▌       | 482/1859 [9:47:59<28:24:59, 74.29s/it] 26%|██▌       | 483/1859 [9:49:28<30:05:36, 78.73s/it] 26%|██▌       | 484/1859 [9:50:47<30:08:54, 78.93s/it] 26%|██▌       | 485/1859 [9:52:20<31:42:49, 83.09s/it] 26%|██▌       | 486/1859 [9:53:56<33:09:44, 86.95s/it] 26%|██▌       | 487/1859 [9:55:08<31:28:53, 82.60s/it] 26%|██▋       | 488/1859 [9:56:42<32:40:10, 85.78s/it] 26%|██▋       | 489/1859 [9:57:56<31:19:29, 82.31s/it] 26%|██▋       | 490/1859 [9:59:13<30:44:12, 80.83s/it]                                                       {'loss': 0.7689, 'grad_norm': 0.38789427280426025, 'learning_rate': 9.212256364390519e-05, 'epoch': 0.26}
 26%|██▋       | 490/1859 [9:59:13<30:44:12, 80.83s/it] 26%|██▋       | 491/1859 [10:00:12<28:11:45, 74.20s/it] 26%|██▋       | 492/1859 [10:01:08<26:09:07, 68.87s/it] 27%|██▋       | 493/1859 [10:02:08<25:03:08, 66.02s/it] 27%|██▋       | 494/1859 [10:03:27<26:32:20, 69.99s/it] 27%|██▋       | 495/1859 [10:04:44<27:18:50, 72.09s/it] 27%|██▋       | 496/1859 [10:06:16<29:33:00, 78.05s/it] 27%|██▋       | 497/1859 [10:07:17<27:35:12, 72.92s/it] 27%|██▋       | 498/1859 [10:08:32<27:48:39, 73.56s/it] 27%|██▋       | 499/1859 [10:09:46<27:50:24, 73.69s/it] 27%|██▋       | 500/1859 [10:10:49<26:33:39, 70.36s/it]                                                        {'loss': 0.7465, 'grad_norm': 0.24766460061073303, 'learning_rate': 9.160930805115095e-05, 'epoch': 0.27}
 27%|██▋       | 500/1859 [10:10:49<26:33:39, 70.36s/it][INFO|trainer.py:4643] 2025-12-04 20:20:44,723 >> 
***** Running Evaluation *****
[INFO|trainer.py:4645] 2025-12-04 20:20:44,723 >>   Num examples = 3305
[INFO|trainer.py:4648] 2025-12-04 20:20:44,723 >>   Batch size = 1

  0%|          | 0/3305 [00:00<?, ?it/s][A
  0%|          | 2/3305 [00:03<1:29:03,  1.62s/it][A
  0%|          | 3/3305 [00:06<2:13:24,  2.42s/it][A
  0%|          | 4/3305 [00:10<2:30:22,  2.73s/it][A
  0%|          | 5/3305 [00:13<2:35:00,  2.82s/it][A
  0%|          | 6/3305 [00:15<2:33:45,  2.80s/it][A
  0%|          | 7/3305 [00:19<2:52:00,  3.13s/it][A
  0%|          | 8/3305 [00:23<3:12:49,  3.51s/it][A
  0%|          | 9/3305 [00:26<3:00:51,  3.29s/it][A
  0%|          | 10/3305 [00:28<2:39:37,  2.91s/it][A
  0%|          | 11/3305 [00:31<2:28:50,  2.71s/it][A
  0%|          | 12/3305 [00:33<2:22:13,  2.59s/it][A
  0%|          | 13/3305 [00:35<2:10:11,  2.37s/it][A
  0%|          | 14/3305 [00:37<2:06:12,  2.30s/it][A
  0%|          | 15/3305 [00:39<2:05:17,  2.28s/it][A
  0%|          | 16/3305 [00:42<2:20:43,  2.57s/it][A
  1%|          | 17/3305 [00:47<2:48:45,  3.08s/it][A
  1%|          | 18/3305 [00:50<2:47:08,  3.05s/it][A
  1%|          | 19/3305 [00:52<2:40:42,  2.93s/it][A
  1%|          | 20/3305 [00:55<2:43:55,  2.99s/it][A
  1%|          | 21/3305 [00:58<2:29:39,  2.73s/it][A
  1%|          | 22/3305 [01:00<2:25:27,  2.66s/it][A
  1%|          | 23/3305 [01:03<2:34:52,  2.83s/it][A
  1%|          | 24/3305 [01:05<2:23:05,  2.62s/it][A
  1%|          | 25/3305 [01:08<2:29:28,  2.73s/it][A
  1%|          | 26/3305 [01:12<2:47:02,  3.06s/it][A
  1%|          | 27/3305 [01:16<2:51:28,  3.14s/it][A
  1%|          | 28/3305 [01:19<2:53:05,  3.17s/it][A
  1%|          | 29/3305 [01:21<2:37:01,  2.88s/it][A
  1%|          | 30/3305 [01:23<2:26:09,  2.68s/it][A
  1%|          | 31/3305 [01:26<2:35:13,  2.84s/it][A
  1%|          | 32/3305 [01:29<2:35:48,  2.86s/it][A
  1%|          | 33/3305 [01:32<2:36:50,  2.88s/it][A
  1%|          | 34/3305 [01:36<2:48:02,  3.08s/it][A
  1%|          | 35/3305 [01:40<3:12:29,  3.53s/it][A
  1%|          | 36/3305 [01:47<4:01:49,  4.44s/it][A
  1%|          | 37/3305 [01:54<4:40:38,  5.15s/it][A
  1%|          | 38/3305 [02:00<5:06:53,  5.64s/it][A
  1%|          | 39/3305 [02:04<4:24:49,  4.87s/it][A
  1%|          | 40/3305 [02:06<3:47:19,  4.18s/it][A
  1%|          | 41/3305 [02:09<3:23:00,  3.73s/it][A
  1%|▏         | 42/3305 [02:12<3:07:23,  3.45s/it][A
  1%|▏         | 43/3305 [02:14<2:51:40,  3.16s/it][A
  1%|▏         | 44/3305 [02:17<2:44:38,  3.03s/it][A
  1%|▏         | 45/3305 [02:19<2:38:04,  2.91s/it][A
  1%|▏         | 46/3305 [02:22<2:36:27,  2.88s/it][A
  1%|▏         | 47/3305 [02:24<2:19:12,  2.56s/it][A
  1%|▏         | 48/3305 [02:26<2:06:07,  2.32s/it][A
  1%|▏         | 49/3305 [02:28<2:01:12,  2.23s/it][A
  2%|▏         | 50/3305 [02:30<2:04:55,  2.30s/it][A
  2%|▏         | 51/3305 [02:33<2:07:33,  2.35s/it][A
  2%|▏         | 52/3305 [02:34<1:57:05,  2.16s/it][A
  2%|▏         | 53/3305 [02:37<1:54:53,  2.12s/it][A
  2%|▏         | 54/3305 [02:40<2:10:34,  2.41s/it][A
  2%|▏         | 55/3305 [02:43<2:23:51,  2.66s/it][A
  2%|▏         | 56/3305 [02:46<2:24:08,  2.66s/it][A
  2%|▏         | 57/3305 [02:49<2:33:16,  2.83s/it][A
  2%|▏         | 58/3305 [02:52<2:43:02,  3.01s/it][A
  2%|▏         | 59/3305 [02:55<2:34:47,  2.86s/it][A
  2%|▏         | 60/3305 [02:57<2:32:22,  2.82s/it][A
  2%|▏         | 61/3305 [03:01<2:39:42,  2.95s/it][A
  2%|▏         | 62/3305 [03:04<2:50:17,  3.15s/it][A
  2%|▏         | 63/3305 [03:08<2:53:45,  3.22s/it][A
  2%|▏         | 64/3305 [03:11<2:49:24,  3.14s/it][A
  2%|▏         | 65/3305 [03:13<2:42:28,  3.01s/it][A
  2%|▏         | 66/3305 [03:16<2:33:39,  2.85s/it][A
  2%|▏         | 67/3305 [03:18<2:25:20,  2.69s/it][A
  2%|▏         | 68/3305 [03:21<2:30:23,  2.79s/it][A
  2%|▏         | 69/3305 [03:23<2:22:49,  2.65s/it][A
  2%|▏         | 70/3305 [03:26<2:15:01,  2.50s/it][A
  2%|▏         | 71/3305 [03:28<2:20:59,  2.62s/it][A
  2%|▏         | 72/3305 [03:32<2:32:43,  2.83s/it][A
  2%|▏         | 73/3305 [03:35<2:39:16,  2.96s/it][A
  2%|▏         | 74/3305 [03:38<2:45:32,  3.07s/it][A
  2%|▏         | 75/3305 [03:42<2:50:07,  3.16s/it][A
  2%|▏         | 76/3305 [03:45<2:49:55,  3.16s/it][A
  2%|▏         | 77/3305 [03:50<3:17:51,  3.68s/it][A
  2%|▏         | 78/3305 [03:56<3:56:31,  4.40s/it][A
  2%|▏         | 79/3305 [04:00<3:50:06,  4.28s/it][A
  2%|▏         | 80/3305 [04:02<3:11:28,  3.56s/it][A
  2%|▏         | 81/3305 [04:05<2:59:44,  3.35s/it][A
  2%|▏         | 82/3305 [04:08<2:54:31,  3.25s/it][A
  3%|▎         | 83/3305 [04:10<2:41:07,  3.00s/it][A
  3%|▎         | 84/3305 [04:14<2:54:39,  3.25s/it][A
  3%|▎         | 85/3305 [04:18<3:00:51,  3.37s/it][A
  3%|▎         | 86/3305 [04:22<3:15:05,  3.64s/it][A
  3%|▎         | 87/3305 [04:27<3:33:16,  3.98s/it][A
  3%|▎         | 88/3305 [04:29<3:07:34,  3.50s/it][A
  3%|▎         | 89/3305 [04:31<2:39:40,  2.98s/it][A
  3%|▎         | 90/3305 [04:33<2:29:10,  2.78s/it][A
  3%|▎         | 91/3305 [04:35<2:22:17,  2.66s/it][A
  3%|▎         | 92/3305 [04:38<2:25:00,  2.71s/it][A
  3%|▎         | 93/3305 [04:41<2:28:36,  2.78s/it][A
  3%|▎         | 94/3305 [04:44<2:21:21,  2.64s/it][A
  3%|▎         | 95/3305 [04:46<2:21:59,  2.65s/it][A
  3%|▎         | 96/3305 [04:50<2:40:48,  3.01s/it][A
  3%|▎         | 97/3305 [04:55<3:11:35,  3.58s/it][A
  3%|▎         | 98/3305 [04:59<3:13:58,  3.63s/it][A
  3%|▎         | 99/3305 [05:01<2:48:22,  3.15s/it][A
  3%|▎         | 100/3305 [05:03<2:39:44,  2.99s/it][A
  3%|▎         | 101/3305 [05:06<2:34:59,  2.90s/it][A
  3%|▎         | 102/3305 [05:11<3:09:11,  3.54s/it][A
  3%|▎         | 103/3305 [05:16<3:38:06,  4.09s/it][A
  3%|▎         | 104/3305 [05:20<3:32:18,  3.98s/it][A
  3%|▎         | 105/3305 [05:25<3:38:07,  4.09s/it][A
  3%|▎         | 106/3305 [05:27<3:06:02,  3.49s/it][A
  3%|▎         | 107/3305 [05:29<2:45:14,  3.10s/it][A
  3%|▎         | 108/3305 [05:31<2:37:14,  2.95s/it][A
  3%|▎         | 109/3305 [05:34<2:30:51,  2.83s/it][A
  3%|▎         | 110/3305 [05:36<2:15:26,  2.54s/it][A
  3%|▎         | 111/3305 [05:38<2:04:14,  2.33s/it][A
  3%|▎         | 112/3305 [05:40<2:01:04,  2.28s/it][A
  3%|▎         | 113/3305 [05:42<2:00:53,  2.27s/it][A
  3%|▎         | 114/3305 [05:45<2:16:40,  2.57s/it][A
  3%|▎         | 115/3305 [05:48<2:17:21,  2.58s/it][A
  4%|▎         | 116/3305 [05:50<2:06:01,  2.37s/it][A
  4%|▎         | 117/3305 [05:54<2:31:54,  2.86s/it][A
  4%|▎         | 118/3305 [05:58<2:54:33,  3.29s/it][A
  4%|▎         | 119/3305 [06:00<2:28:16,  2.79s/it][A
  4%|▎         | 120/3305 [06:03<2:37:21,  2.96s/it][A
  4%|▎         | 121/3305 [06:07<2:57:22,  3.34s/it][A
  4%|▎         | 122/3305 [06:10<2:41:58,  3.05s/it][A
  4%|▎         | 123/3305 [06:11<2:20:47,  2.65s/it][A
  4%|▍         | 124/3305 [06:13<2:10:51,  2.47s/it][A
  4%|▍         | 125/3305 [06:16<2:10:55,  2.47s/it][A
  4%|▍         | 126/3305 [06:18<2:10:04,  2.46s/it][A
  4%|▍         | 127/3305 [06:20<1:59:35,  2.26s/it][A
  4%|▍         | 128/3305 [06:23<2:03:30,  2.33s/it][A
  4%|▍         | 129/3305 [06:26<2:26:10,  2.76s/it][A
  4%|▍         | 130/3305 [06:30<2:46:39,  3.15s/it][A
  4%|▍         | 131/3305 [06:36<3:17:12,  3.73s/it][A
  4%|▍         | 132/3305 [06:40<3:34:54,  4.06s/it][A
  4%|▍         | 133/3305 [06:43<3:10:09,  3.60s/it][A
  4%|▍         | 134/3305 [06:46<2:58:58,  3.39s/it][A
  4%|▍         | 135/3305 [06:48<2:38:57,  3.01s/it][A
  4%|▍         | 136/3305 [06:50<2:17:34,  2.60s/it][A
  4%|▍         | 137/3305 [06:51<2:02:22,  2.32s/it][A
  4%|▍         | 138/3305 [06:54<2:11:16,  2.49s/it][A
  4%|▍         | 139/3305 [06:58<2:28:30,  2.81s/it][A
  4%|▍         | 140/3305 [07:02<2:45:50,  3.14s/it][A
  4%|▍         | 141/3305 [07:04<2:34:11,  2.92s/it][A
  4%|▍         | 142/3305 [07:06<2:18:18,  2.62s/it][A
  4%|▍         | 143/3305 [07:08<2:08:19,  2.44s/it][A
  4%|▍         | 144/3305 [07:10<2:00:45,  2.29s/it][A
  4%|▍         | 145/3305 [07:12<1:55:40,  2.20s/it][A
  4%|▍         | 146/3305 [07:14<1:58:47,  2.26s/it][A
  4%|▍         | 147/3305 [07:16<1:56:31,  2.21s/it][A
  4%|▍         | 148/3305 [07:18<1:42:28,  1.95s/it][A
  5%|▍         | 149/3305 [07:20<1:42:02,  1.94s/it][A
  5%|▍         | 150/3305 [07:24<2:24:01,  2.74s/it][A
  5%|▍         | 151/3305 [07:30<3:04:44,  3.51s/it][A
  5%|▍         | 152/3305 [07:33<3:06:58,  3.56s/it][A
  5%|▍         | 153/3305 [07:37<3:07:29,  3.57s/it][A
  5%|▍         | 154/3305 [07:38<2:33:47,  2.93s/it][A
  5%|▍         | 155/3305 [07:40<2:19:27,  2.66s/it][A
  5%|▍         | 156/3305 [07:43<2:27:03,  2.80s/it][A
  5%|▍         | 157/3305 [07:49<3:10:05,  3.62s/it][A
  5%|▍         | 158/3305 [07:54<3:33:23,  4.07s/it][A
  5%|▍         | 159/3305 [07:56<3:03:12,  3.49s/it][A
  5%|▍         | 160/3305 [07:58<2:43:40,  3.12s/it][A
  5%|▍         | 161/3305 [08:01<2:29:09,  2.85s/it][A
  5%|▍         | 162/3305 [08:03<2:17:22,  2.62s/it][A
  5%|▍         | 163/3305 [08:05<2:15:58,  2.60s/it][A
  5%|▍         | 164/3305 [08:09<2:28:48,  2.84s/it][A
  5%|▍         | 165/3305 [08:13<2:51:37,  3.28s/it][A
  5%|▌         | 166/3305 [08:16<2:50:23,  3.26s/it][A
  5%|▌         | 167/3305 [08:19<2:43:50,  3.13s/it][A
  5%|▌         | 168/3305 [08:22<2:38:29,  3.03s/it][A
  5%|▌         | 169/3305 [08:24<2:31:53,  2.91s/it][A
  5%|▌         | 170/3305 [08:27<2:23:45,  2.75s/it][A
  5%|▌         | 171/3305 [08:30<2:27:15,  2.82s/it][A
  5%|▌         | 172/3305 [08:33<2:34:08,  2.95s/it][A
  5%|▌         | 173/3305 [08:36<2:26:20,  2.80s/it][A
  5%|▌         | 174/3305 [08:38<2:16:58,  2.62s/it][A
  5%|▌         | 175/3305 [08:40<2:16:16,  2.61s/it][A
  5%|▌         | 176/3305 [08:44<2:27:26,  2.83s/it][A
  5%|▌         | 177/3305 [08:48<2:44:40,  3.16s/it][A
  5%|▌         | 178/3305 [08:51<2:40:55,  3.09s/it][A
  5%|▌         | 179/3305 [08:53<2:26:03,  2.80s/it][A
  5%|▌         | 180/3305 [08:56<2:26:20,  2.81s/it][A
  5%|▌         | 181/3305 [08:59<2:44:05,  3.15s/it][A
  6%|▌         | 182/3305 [09:05<3:17:34,  3.80s/it][A
  6%|▌         | 183/3305 [09:10<3:46:32,  4.35s/it][A
  6%|▌         | 184/3305 [09:15<3:46:12,  4.35s/it][A
  6%|▌         | 185/3305 [09:20<3:58:04,  4.58s/it][A
  6%|▌         | 186/3305 [09:25<4:00:47,  4.63s/it][A
  6%|▌         | 187/3305 [09:27<3:27:08,  3.99s/it][A
  6%|▌         | 188/3305 [09:30<3:04:40,  3.55s/it][A
  6%|▌         | 189/3305 [09:32<2:45:16,  3.18s/it][A
  6%|▌         | 190/3305 [09:35<2:38:15,  3.05s/it][A
  6%|▌         | 191/3305 [09:37<2:33:57,  2.97s/it][A
  6%|▌         | 192/3305 [09:39<2:13:04,  2.56s/it][A
  6%|▌         | 193/3305 [09:41<2:02:02,  2.35s/it][A
  6%|▌         | 194/3305 [09:44<2:16:53,  2.64s/it][A
  6%|▌         | 195/3305 [09:47<2:15:28,  2.61s/it][A
  6%|▌         | 196/3305 [09:48<1:59:44,  2.31s/it][A
  6%|▌         | 197/3305 [09:51<1:57:08,  2.26s/it][A
  6%|▌         | 198/3305 [09:53<2:03:12,  2.38s/it][A
  6%|▌         | 199/3305 [09:56<2:10:31,  2.52s/it][A
  6%|▌         | 200/3305 [09:58<2:08:15,  2.48s/it][A
  6%|▌         | 201/3305 [10:01<2:10:55,  2.53s/it][A
  6%|▌         | 202/3305 [10:04<2:16:35,  2.64s/it][A
  6%|▌         | 203/3305 [10:07<2:14:49,  2.61s/it][A
  6%|▌         | 204/3305 [10:12<2:53:06,  3.35s/it][A
  6%|▌         | 205/3305 [10:17<3:28:46,  4.04s/it][A
  6%|▌         | 206/3305 [10:20<3:11:28,  3.71s/it][A
  6%|▋         | 207/3305 [10:23<2:56:40,  3.42s/it][A
  6%|▋         | 208/3305 [10:25<2:36:57,  3.04s/it][A
  6%|▋         | 209/3305 [10:28<2:27:23,  2.86s/it][A
  6%|▋         | 210/3305 [10:30<2:23:56,  2.79s/it][A
  6%|▋         | 211/3305 [10:33<2:18:13,  2.68s/it][A
  6%|▋         | 212/3305 [10:34<2:02:34,  2.38s/it][A
  6%|▋         | 213/3305 [10:37<2:01:48,  2.36s/it][A
  6%|▋         | 214/3305 [10:39<2:00:21,  2.34s/it][A
  7%|▋         | 215/3305 [10:41<1:51:56,  2.17s/it][A
  7%|▋         | 216/3305 [10:43<1:53:23,  2.20s/it][A
  7%|▋         | 217/3305 [10:46<2:13:32,  2.59s/it][A
  7%|▋         | 218/3305 [10:50<2:26:07,  2.84s/it][A
  7%|▋         | 219/3305 [10:54<2:41:26,  3.14s/it][A
  7%|▋         | 220/3305 [10:57<2:39:30,  3.10s/it][A
  7%|▋         | 221/3305 [10:59<2:21:27,  2.75s/it][A
  7%|▋         | 222/3305 [11:01<2:07:33,  2.48s/it][A
  7%|▋         | 223/3305 [11:02<1:51:46,  2.18s/it][A
  7%|▋         | 224/3305 [11:05<2:07:51,  2.49s/it][A
  7%|▋         | 225/3305 [11:09<2:23:54,  2.80s/it][A
  7%|▋         | 226/3305 [11:12<2:30:38,  2.94s/it][A
  7%|▋         | 227/3305 [11:16<2:47:17,  3.26s/it][A
  7%|▋         | 228/3305 [11:19<2:41:04,  3.14s/it][A
  7%|▋         | 229/3305 [11:21<2:33:08,  2.99s/it][A
  7%|▋         | 230/3305 [11:24<2:28:38,  2.90s/it][A
  7%|▋         | 231/3305 [11:26<2:15:13,  2.64s/it][A
  7%|▋         | 232/3305 [11:28<2:03:22,  2.41s/it][A
  7%|▋         | 233/3305 [11:30<1:57:54,  2.30s/it][A
  7%|▋         | 234/3305 [11:33<2:09:12,  2.52s/it][A
  7%|▋         | 235/3305 [11:37<2:22:19,  2.78s/it][A
  7%|▋         | 236/3305 [11:39<2:16:12,  2.66s/it][A
  7%|▋         | 237/3305 [11:41<2:04:05,  2.43s/it][A
  7%|▋         | 238/3305 [11:43<1:57:39,  2.30s/it][A
  7%|▋         | 239/3305 [11:45<1:54:53,  2.25s/it][A
  7%|▋         | 240/3305 [11:48<2:13:42,  2.62s/it][A
  7%|▋         | 241/3305 [11:53<2:45:40,  3.24s/it][A
  7%|▋         | 242/3305 [11:58<3:08:45,  3.70s/it][A
  7%|▋         | 243/3305 [12:02<3:22:21,  3.97s/it][A
  7%|▋         | 244/3305 [12:05<2:58:35,  3.50s/it][A
  7%|▋         | 245/3305 [12:07<2:39:30,  3.13s/it][A
  7%|▋         | 246/3305 [12:09<2:26:35,  2.88s/it][A
  7%|▋         | 247/3305 [12:12<2:21:32,  2.78s/it][A
  8%|▊         | 248/3305 [12:15<2:17:27,  2.70s/it][A
  8%|▊         | 249/3305 [12:17<2:18:14,  2.71s/it][A
  8%|▊         | 250/3305 [12:20<2:15:57,  2.67s/it][A
  8%|▊         | 251/3305 [12:23<2:16:26,  2.68s/it][A
  8%|▊         | 252/3305 [12:25<2:11:45,  2.59s/it][A
  8%|▊         | 253/3305 [12:27<2:03:20,  2.42s/it][A
  8%|▊         | 254/3305 [12:29<1:58:19,  2.33s/it][A
  8%|▊         | 255/3305 [12:32<2:03:54,  2.44s/it][A
  8%|▊         | 256/3305 [12:35<2:17:11,  2.70s/it][A
  8%|▊         | 257/3305 [12:38<2:18:33,  2.73s/it][A
  8%|▊         | 258/3305 [12:40<2:08:36,  2.53s/it][A
  8%|▊         | 259/3305 [12:42<2:07:13,  2.51s/it][A
  8%|▊         | 260/3305 [12:45<2:13:04,  2.62s/it][A
  8%|▊         | 261/3305 [12:49<2:28:42,  2.93s/it][A
  8%|▊         | 262/3305 [12:52<2:37:27,  3.10s/it][A
  8%|▊         | 263/3305 [12:54<2:16:10,  2.69s/it][A
  8%|▊         | 264/3305 [12:57<2:20:00,  2.76s/it][A
  8%|▊         | 265/3305 [13:03<3:03:56,  3.63s/it][A
  8%|▊         | 266/3305 [13:08<3:23:50,  4.02s/it][A
  8%|▊         | 267/3305 [13:13<3:39:37,  4.34s/it][A
  8%|▊         | 268/3305 [13:17<3:40:17,  4.35s/it][A
  8%|▊         | 269/3305 [13:19<2:59:50,  3.55s/it][A
  8%|▊         | 270/3305 [13:21<2:36:15,  3.09s/it][A
  8%|▊         | 271/3305 [13:23<2:24:37,  2.86s/it][A
  8%|▊         | 272/3305 [13:25<2:14:41,  2.66s/it][A
  8%|▊         | 273/3305 [13:28<2:16:37,  2.70s/it][A
  8%|▊         | 274/3305 [13:30<2:10:08,  2.58s/it][A
  8%|▊         | 275/3305 [13:32<1:56:44,  2.31s/it][A
  8%|▊         | 276/3305 [13:34<1:54:50,  2.27s/it][A
  8%|▊         | 277/3305 [13:37<1:59:32,  2.37s/it][A
  8%|▊         | 278/3305 [13:40<2:11:47,  2.61s/it][A
  8%|▊         | 279/3305 [13:43<2:15:19,  2.68s/it][A
  8%|▊         | 280/3305 [13:45<2:05:55,  2.50s/it][A
  9%|▊         | 281/3305 [13:48<2:08:53,  2.56s/it][A
  9%|▊         | 282/3305 [13:50<2:01:03,  2.40s/it][A
  9%|▊         | 283/3305 [13:52<1:59:57,  2.38s/it][A
  9%|▊         | 284/3305 [13:55<2:12:38,  2.63s/it][A
  9%|▊         | 285/3305 [14:00<2:45:44,  3.29s/it][A
  9%|▊         | 286/3305 [14:06<3:18:07,  3.94s/it][A
  9%|▊         | 287/3305 [14:08<3:00:33,  3.59s/it][A
  9%|▊         | 288/3305 [14:10<2:30:11,  2.99s/it][A
  9%|▊         | 289/3305 [14:12<2:13:57,  2.67s/it][A
  9%|▉         | 290/3305 [14:15<2:21:32,  2.82s/it][A
  9%|▉         | 291/3305 [14:17<2:14:33,  2.68s/it][A
  9%|▉         | 292/3305 [14:20<2:09:35,  2.58s/it][A
  9%|▉         | 293/3305 [14:21<1:57:25,  2.34s/it][A
  9%|▉         | 294/3305 [14:24<2:00:22,  2.40s/it][A
  9%|▉         | 295/3305 [14:27<2:15:21,  2.70s/it][A
  9%|▉         | 296/3305 [14:31<2:27:24,  2.94s/it][A
  9%|▉         | 297/3305 [14:35<2:42:29,  3.24s/it][A
  9%|▉         | 298/3305 [14:39<2:59:21,  3.58s/it][A
  9%|▉         | 299/3305 [14:45<3:25:55,  4.11s/it][A
  9%|▉         | 300/3305 [14:48<3:09:04,  3.78s/it][A
  9%|▉         | 301/3305 [14:50<2:48:26,  3.36s/it][A
  9%|▉         | 302/3305 [14:52<2:32:22,  3.04s/it][A
  9%|▉         | 303/3305 [14:55<2:22:21,  2.85s/it][A
  9%|▉         | 304/3305 [14:57<2:21:13,  2.82s/it][A
  9%|▉         | 305/3305 [15:01<2:29:21,  2.99s/it][A
  9%|▉         | 306/3305 [15:03<2:22:42,  2.86s/it][A
  9%|▉         | 307/3305 [15:05<2:08:23,  2.57s/it][A
  9%|▉         | 308/3305 [15:07<2:01:45,  2.44s/it][A
  9%|▉         | 309/3305 [15:10<1:58:29,  2.37s/it][A
  9%|▉         | 310/3305 [15:13<2:06:26,  2.53s/it][A
  9%|▉         | 311/3305 [15:16<2:19:09,  2.79s/it][A
  9%|▉         | 312/3305 [15:19<2:20:12,  2.81s/it][A
  9%|▉         | 313/3305 [15:21<2:18:34,  2.78s/it][A
 10%|▉         | 314/3305 [15:23<2:06:41,  2.54s/it][A
 10%|▉         | 315/3305 [15:26<2:05:05,  2.51s/it][A
 10%|▉         | 316/3305 [15:28<2:05:06,  2.51s/it][A
 10%|▉         | 317/3305 [15:31<1:59:44,  2.40s/it][A
 10%|▉         | 318/3305 [15:33<1:55:57,  2.33s/it][A
 10%|▉         | 319/3305 [15:35<1:54:10,  2.29s/it][A
 10%|▉         | 320/3305 [15:36<1:40:32,  2.02s/it][A
 10%|▉         | 321/3305 [15:38<1:35:20,  1.92s/it][A
 10%|▉         | 322/3305 [15:42<2:03:45,  2.49s/it][A
 10%|▉         | 323/3305 [15:47<2:41:50,  3.26s/it][A
 10%|▉         | 324/3305 [15:51<2:50:31,  3.43s/it][A
 10%|▉         | 325/3305 [15:55<2:58:50,  3.60s/it][A
 10%|▉         | 326/3305 [15:58<2:47:02,  3.36s/it][A
 10%|▉         | 327/3305 [16:00<2:38:09,  3.19s/it][A
 10%|▉         | 328/3305 [16:04<2:41:12,  3.25s/it][A
 10%|▉         | 329/3305 [16:07<2:45:07,  3.33s/it][A
 10%|▉         | 330/3305 [16:10<2:35:35,  3.14s/it][A
 10%|█         | 331/3305 [16:12<2:20:46,  2.84s/it][A
 10%|█         | 332/3305 [16:16<2:35:50,  3.15s/it][A
 10%|█         | 333/3305 [16:19<2:39:50,  3.23s/it][A
 10%|█         | 334/3305 [16:21<2:13:54,  2.70s/it][A
 10%|█         | 335/3305 [16:23<2:07:39,  2.58s/it][A
 10%|█         | 336/3305 [16:26<2:06:08,  2.55s/it][A
 10%|█         | 337/3305 [16:28<2:06:57,  2.57s/it][A
 10%|█         | 338/3305 [16:31<2:13:18,  2.70s/it][A
 10%|█         | 339/3305 [16:35<2:24:28,  2.92s/it][A
 10%|█         | 340/3305 [16:39<2:41:53,  3.28s/it][A
 10%|█         | 341/3305 [16:41<2:25:48,  2.95s/it][A
 10%|█         | 342/3305 [16:43<2:16:52,  2.77s/it][A
 10%|█         | 343/3305 [16:46<2:16:06,  2.76s/it][A
 10%|█         | 344/3305 [16:48<2:07:55,  2.59s/it][A
 10%|█         | 345/3305 [16:50<2:02:57,  2.49s/it][A
 10%|█         | 346/3305 [16:53<2:00:37,  2.45s/it][A
 10%|█         | 347/3305 [16:55<2:00:29,  2.44s/it][A
 11%|█         | 348/3305 [17:01<2:55:49,  3.57s/it][A
 11%|█         | 349/3305 [17:10<4:08:32,  5.04s/it][A
 11%|█         | 350/3305 [17:14<3:53:47,  4.75s/it][A
 11%|█         | 351/3305 [17:17<3:31:30,  4.30s/it][A
 11%|█         | 352/3305 [17:23<3:51:04,  4.69s/it][A
 11%|█         | 353/3305 [17:29<4:17:43,  5.24s/it][A
 11%|█         | 354/3305 [17:33<3:51:04,  4.70s/it][A
 11%|█         | 355/3305 [17:34<3:06:36,  3.80s/it][A
 11%|█         | 356/3305 [17:38<3:09:56,  3.86s/it][A
 11%|█         | 357/3305 [17:43<3:22:20,  4.12s/it][A
 11%|█         | 358/3305 [17:48<3:31:50,  4.31s/it][A
 11%|█         | 359/3305 [17:53<3:47:09,  4.63s/it][A
 11%|█         | 360/3305 [17:56<3:16:48,  4.01s/it][A
 11%|█         | 361/3305 [17:58<2:51:52,  3.50s/it][A
 11%|█         | 362/3305 [18:00<2:21:21,  2.88s/it][A
 11%|█         | 363/3305 [18:02<2:13:03,  2.71s/it][A
 11%|█         | 364/3305 [18:06<2:27:37,  3.01s/it][A
 11%|█         | 365/3305 [18:09<2:26:40,  2.99s/it][A
 11%|█         | 366/3305 [18:11<2:19:08,  2.84s/it][A
 11%|█         | 367/3305 [18:14<2:26:28,  2.99s/it][A
 11%|█         | 368/3305 [18:17<2:19:21,  2.85s/it][A
 11%|█         | 369/3305 [18:20<2:17:10,  2.80s/it][A
 11%|█         | 370/3305 [18:22<2:14:53,  2.76s/it][A
 11%|█         | 371/3305 [18:25<2:15:56,  2.78s/it][A
 11%|█▏        | 372/3305 [18:27<2:01:47,  2.49s/it][A
 11%|█▏        | 373/3305 [18:29<1:52:02,  2.29s/it][A
 11%|█▏        | 374/3305 [18:31<1:50:09,  2.25s/it][A
 11%|█▏        | 375/3305 [18:34<1:57:01,  2.40s/it][A
 11%|█▏        | 376/3305 [18:37<2:03:18,  2.53s/it][A
 11%|█▏        | 377/3305 [18:39<2:02:34,  2.51s/it][A
 11%|█▏        | 378/3305 [18:41<1:57:17,  2.40s/it][A
 11%|█▏        | 379/3305 [18:43<1:54:19,  2.34s/it][A
 11%|█▏        | 380/3305 [18:46<1:58:23,  2.43s/it][A
 12%|█▏        | 381/3305 [18:49<2:05:03,  2.57s/it][A
 12%|█▏        | 382/3305 [18:52<2:06:43,  2.60s/it][A
 12%|█▏        | 383/3305 [18:53<1:56:58,  2.40s/it][A
 12%|█▏        | 384/3305 [18:55<1:48:01,  2.22s/it][A
 12%|█▏        | 385/3305 [18:57<1:44:42,  2.15s/it][A
 12%|█▏        | 386/3305 [18:59<1:39:27,  2.04s/it][A
 12%|█▏        | 387/3305 [19:01<1:36:00,  1.97s/it][A
 12%|█▏        | 388/3305 [19:03<1:31:56,  1.89s/it][A
 12%|█▏        | 389/3305 [19:05<1:37:55,  2.01s/it][A
 12%|█▏        | 390/3305 [19:07<1:42:37,  2.11s/it][A
 12%|█▏        | 391/3305 [19:09<1:43:45,  2.14s/it][A
 12%|█▏        | 392/3305 [19:11<1:42:36,  2.11s/it][A
 12%|█▏        | 393/3305 [19:14<1:43:02,  2.12s/it][A
 12%|█▏        | 394/3305 [19:16<1:48:10,  2.23s/it][A
 12%|█▏        | 395/3305 [19:19<1:56:06,  2.39s/it][A
 12%|█▏        | 396/3305 [19:21<1:52:57,  2.33s/it][A
 12%|█▏        | 397/3305 [19:23<1:44:09,  2.15s/it][A
 12%|█▏        | 398/3305 [19:25<1:47:36,  2.22s/it][A
 12%|█▏        | 399/3305 [19:27<1:46:48,  2.21s/it][A
 12%|█▏        | 400/3305 [19:30<1:55:16,  2.38s/it][A
 12%|█▏        | 401/3305 [19:33<1:57:43,  2.43s/it][A
 12%|█▏        | 402/3305 [19:35<1:56:31,  2.41s/it][A
 12%|█▏        | 403/3305 [19:37<1:50:29,  2.28s/it][A
 12%|█▏        | 404/3305 [19:40<1:54:12,  2.36s/it][A
 12%|█▏        | 405/3305 [19:43<2:05:14,  2.59s/it][A
 12%|█▏        | 406/3305 [19:45<2:00:12,  2.49s/it][A
 12%|█▏        | 407/3305 [19:47<1:58:10,  2.45s/it][A
 12%|█▏        | 408/3305 [19:50<1:56:26,  2.41s/it][A
 12%|█▏        | 409/3305 [19:52<2:02:25,  2.54s/it][A
 12%|█▏        | 410/3305 [19:55<2:02:49,  2.55s/it][A
 12%|█▏        | 411/3305 [19:58<2:02:02,  2.53s/it][A
 12%|█▏        | 412/3305 [19:59<1:50:39,  2.29s/it][A
 12%|█▏        | 413/3305 [20:02<1:51:33,  2.31s/it][A
 13%|█▎        | 414/3305 [20:04<1:47:10,  2.22s/it][A
 13%|█▎        | 415/3305 [20:06<1:50:36,  2.30s/it][A
 13%|█▎        | 416/3305 [20:09<2:05:29,  2.61s/it][A
 13%|█▎        | 417/3305 [20:12<2:11:53,  2.74s/it][A
 13%|█▎        | 418/3305 [20:15<2:09:39,  2.69s/it][A
 13%|█▎        | 419/3305 [20:17<2:05:27,  2.61s/it][A
 13%|█▎        | 420/3305 [20:19<1:51:52,  2.33s/it][A
 13%|█▎        | 421/3305 [20:21<1:46:55,  2.22s/it][A
 13%|█▎        | 422/3305 [20:25<2:04:41,  2.59s/it][A
 13%|█▎        | 423/3305 [20:28<2:09:43,  2.70s/it][A
 13%|█▎        | 424/3305 [20:30<2:07:58,  2.67s/it][A
 13%|█▎        | 425/3305 [20:33<2:10:16,  2.71s/it][A
 13%|█▎        | 426/3305 [20:38<2:48:27,  3.51s/it][A
 13%|█▎        | 427/3305 [20:44<3:22:16,  4.22s/it][A
 13%|█▎        | 428/3305 [20:46<2:48:02,  3.50s/it][A
 13%|█▎        | 429/3305 [20:50<2:49:12,  3.53s/it][A
 13%|█▎        | 430/3305 [20:53<2:44:54,  3.44s/it][A
 13%|█▎        | 431/3305 [20:56<2:35:23,  3.24s/it][A
 13%|█▎        | 432/3305 [20:58<2:26:09,  3.05s/it][A
 13%|█▎        | 433/3305 [21:01<2:27:36,  3.08s/it][A
 13%|█▎        | 434/3305 [21:05<2:37:44,  3.30s/it][A
 13%|█▎        | 435/3305 [21:08<2:37:12,  3.29s/it][A
 13%|█▎        | 436/3305 [21:12<2:40:44,  3.36s/it][A
 13%|█▎        | 437/3305 [21:14<2:21:24,  2.96s/it][A
 13%|█▎        | 438/3305 [21:17<2:16:07,  2.85s/it][A
 13%|█▎        | 439/3305 [21:20<2:19:15,  2.92s/it][A
 13%|█▎        | 440/3305 [21:22<2:11:35,  2.76s/it][A
 13%|█▎        | 441/3305 [21:25<2:13:11,  2.79s/it][A
 13%|█▎        | 442/3305 [21:28<2:23:04,  3.00s/it][A
 13%|█▎        | 443/3305 [21:32<2:32:41,  3.20s/it][A
 13%|█▎        | 444/3305 [21:35<2:30:46,  3.16s/it][A
 13%|█▎        | 445/3305 [21:37<2:11:21,  2.76s/it][A
 13%|█▎        | 446/3305 [21:41<2:28:08,  3.11s/it][A
 14%|█▎        | 447/3305 [21:46<2:53:36,  3.64s/it][A
 14%|█▎        | 448/3305 [21:48<2:30:49,  3.17s/it][A
 14%|█▎        | 449/3305 [21:50<2:13:07,  2.80s/it][A
 14%|█▎        | 450/3305 [21:53<2:18:11,  2.90s/it][A
 14%|█▎        | 451/3305 [21:57<2:33:46,  3.23s/it][A
 14%|█▎        | 452/3305 [22:00<2:31:47,  3.19s/it][A
 14%|█▎        | 453/3305 [22:03<2:29:08,  3.14s/it][A
 14%|█▎        | 454/3305 [22:07<2:34:09,  3.24s/it][A
 14%|█▍        | 455/3305 [22:09<2:29:44,  3.15s/it][A
 14%|█▍        | 456/3305 [22:12<2:20:43,  2.96s/it][A
 14%|█▍        | 457/3305 [22:15<2:19:38,  2.94s/it][A
 14%|█▍        | 458/3305 [22:18<2:23:36,  3.03s/it][A
 14%|█▍        | 459/3305 [22:21<2:21:39,  2.99s/it][A
 14%|█▍        | 460/3305 [22:24<2:25:43,  3.07s/it][A
 14%|█▍        | 461/3305 [22:28<2:29:56,  3.16s/it][A
 14%|█▍        | 462/3305 [22:30<2:19:07,  2.94s/it][A
 14%|█▍        | 463/3305 [22:32<2:09:33,  2.74s/it][A
 14%|█▍        | 464/3305 [22:35<2:08:22,  2.71s/it][A
 14%|█▍        | 465/3305 [22:37<2:04:57,  2.64s/it][A
 14%|█▍        | 466/3305 [22:40<1:58:48,  2.51s/it][A
 14%|█▍        | 467/3305 [22:43<2:07:14,  2.69s/it][A
 14%|█▍        | 468/3305 [22:47<2:30:26,  3.18s/it][A
 14%|█▍        | 469/3305 [22:53<3:04:06,  3.90s/it][A
 14%|█▍        | 470/3305 [22:56<2:51:45,  3.63s/it][A
 14%|█▍        | 471/3305 [22:57<2:19:30,  2.95s/it][A
 14%|█▍        | 472/3305 [22:59<2:02:12,  2.59s/it][A
 14%|█▍        | 473/3305 [23:02<2:17:33,  2.91s/it][A
 14%|█▍        | 474/3305 [23:07<2:42:11,  3.44s/it][A
 14%|█▍        | 475/3305 [23:10<2:38:02,  3.35s/it][A
 14%|█▍        | 476/3305 [23:15<2:58:48,  3.79s/it][A
 14%|█▍        | 477/3305 [23:20<3:18:41,  4.22s/it][A
 14%|█▍        | 478/3305 [23:24<3:13:29,  4.11s/it][A
 14%|█▍        | 479/3305 [23:27<3:01:52,  3.86s/it][A
 15%|█▍        | 480/3305 [23:30<2:40:35,  3.41s/it][A
 15%|█▍        | 481/3305 [23:32<2:24:31,  3.07s/it][A
 15%|█▍        | 482/3305 [23:34<2:09:58,  2.76s/it][A
 15%|█▍        | 483/3305 [23:37<2:08:21,  2.73s/it][A
 15%|█▍        | 484/3305 [23:40<2:11:58,  2.81s/it][A
 15%|█▍        | 485/3305 [23:42<2:01:34,  2.59s/it][A
 15%|█▍        | 486/3305 [23:45<2:06:09,  2.69s/it][A
 15%|█▍        | 487/3305 [23:48<2:11:39,  2.80s/it][A
 15%|█▍        | 488/3305 [23:50<2:02:11,  2.60s/it][A
 15%|█▍        | 489/3305 [23:52<1:57:01,  2.49s/it][A
 15%|█▍        | 490/3305 [23:55<1:55:31,  2.46s/it][A
 15%|█▍        | 491/3305 [23:56<1:47:45,  2.30s/it][A
 15%|█▍        | 492/3305 [23:59<1:54:37,  2.45s/it][A
 15%|█▍        | 493/3305 [24:02<2:04:31,  2.66s/it][A
 15%|█▍        | 494/3305 [24:05<2:08:44,  2.75s/it][A
 15%|█▍        | 495/3305 [24:08<2:01:44,  2.60s/it][A
 15%|█▌        | 496/3305 [24:10<1:57:11,  2.50s/it][A
 15%|█▌        | 497/3305 [24:13<2:08:42,  2.75s/it][A
 15%|█▌        | 498/3305 [24:17<2:19:20,  2.98s/it][A
 15%|█▌        | 499/3305 [24:20<2:21:23,  3.02s/it][A
 15%|█▌        | 500/3305 [24:22<2:09:19,  2.77s/it][A
 15%|█▌        | 501/3305 [24:24<2:04:39,  2.67s/it][A
 15%|█▌        | 502/3305 [24:28<2:12:29,  2.84s/it][A
 15%|█▌        | 503/3305 [24:31<2:17:18,  2.94s/it][A
 15%|█▌        | 504/3305 [24:34<2:18:08,  2.96s/it][A
 15%|█▌        | 505/3305 [24:37<2:18:34,  2.97s/it][A
 15%|█▌        | 506/3305 [24:39<2:10:42,  2.80s/it][A
 15%|█▌        | 507/3305 [24:41<2:01:03,  2.60s/it][A
 15%|█▌        | 508/3305 [24:43<1:53:27,  2.43s/it][A
 15%|█▌        | 509/3305 [24:46<1:59:53,  2.57s/it][A
 15%|█▌        | 510/3305 [24:50<2:08:04,  2.75s/it][A
 15%|█▌        | 511/3305 [24:51<1:50:58,  2.38s/it][A
 15%|█▌        | 512/3305 [24:53<1:37:53,  2.10s/it][A
 16%|█▌        | 513/3305 [24:54<1:35:40,  2.06s/it][A
 16%|█▌        | 514/3305 [24:57<1:48:57,  2.34s/it][A
 16%|█▌        | 515/3305 [25:00<1:52:22,  2.42s/it][A
 16%|█▌        | 516/3305 [25:03<1:52:41,  2.42s/it][A
 16%|█▌        | 517/3305 [25:06<2:03:29,  2.66s/it][A
 16%|█▌        | 518/3305 [25:08<1:55:51,  2.49s/it][A
 16%|█▌        | 519/3305 [25:11<1:58:20,  2.55s/it][A
 16%|█▌        | 520/3305 [25:14<2:08:32,  2.77s/it][A
 16%|█▌        | 521/3305 [25:16<2:03:27,  2.66s/it][A
 16%|█▌        | 522/3305 [25:19<1:58:54,  2.56s/it][A
 16%|█▌        | 523/3305 [25:21<2:02:21,  2.64s/it][A
 16%|█▌        | 524/3305 [25:25<2:19:45,  3.02s/it][A
 16%|█▌        | 525/3305 [25:29<2:26:20,  3.16s/it][A
 16%|█▌        | 526/3305 [25:31<2:17:15,  2.96s/it][A
 16%|█▌        | 527/3305 [25:35<2:33:44,  3.32s/it][A
 16%|█▌        | 528/3305 [25:40<2:52:32,  3.73s/it][A
 16%|█▌        | 529/3305 [25:44<2:48:55,  3.65s/it][A
 16%|█▌        | 530/3305 [25:46<2:27:46,  3.19s/it][A
 16%|█▌        | 531/3305 [25:48<2:19:07,  3.01s/it][A
 16%|█▌        | 532/3305 [25:52<2:26:28,  3.17s/it][A
 16%|█▌        | 533/3305 [25:56<2:34:15,  3.34s/it][A
 16%|█▌        | 534/3305 [25:59<2:31:49,  3.29s/it][A
 16%|█▌        | 535/3305 [26:02<2:36:19,  3.39s/it][A
 16%|█▌        | 536/3305 [26:05<2:31:37,  3.29s/it][A
 16%|█▌        | 537/3305 [26:07<2:11:57,  2.86s/it][A
 16%|█▋        | 538/3305 [26:09<2:03:36,  2.68s/it][A
 16%|█▋        | 539/3305 [26:12<2:01:20,  2.63s/it][A
 16%|█▋        | 540/3305 [26:17<2:28:41,  3.23s/it][A
 16%|█▋        | 541/3305 [26:22<2:53:51,  3.77s/it][A
 16%|█▋        | 542/3305 [26:25<2:44:48,  3.58s/it][A
 16%|█▋        | 543/3305 [26:30<3:05:50,  4.04s/it][A
 16%|█▋        | 544/3305 [26:34<3:11:36,  4.16s/it][A
 16%|█▋        | 545/3305 [26:38<3:02:05,  3.96s/it][A
 17%|█▋        | 546/3305 [26:41<2:47:03,  3.63s/it][A
 17%|█▋        | 547/3305 [26:43<2:27:37,  3.21s/it][A
 17%|█▋        | 548/3305 [26:45<2:08:33,  2.80s/it][A
 17%|█▋        | 549/3305 [26:46<1:52:13,  2.44s/it][A
 17%|█▋        | 550/3305 [26:48<1:45:58,  2.31s/it][A
 17%|█▋        | 551/3305 [26:52<1:57:55,  2.57s/it][A
 17%|█▋        | 552/3305 [26:55<2:10:42,  2.85s/it][A
 17%|█▋        | 553/3305 [26:58<2:10:07,  2.84s/it][A
 17%|█▋        | 554/3305 [27:01<2:08:12,  2.80s/it][A
 17%|█▋        | 555/3305 [27:05<2:30:18,  3.28s/it][A
 17%|█▋        | 556/3305 [27:11<3:07:14,  4.09s/it][A
 17%|█▋        | 557/3305 [27:15<3:03:05,  4.00s/it][A
 17%|█▋        | 558/3305 [27:18<2:56:30,  3.86s/it][A
 17%|█▋        | 559/3305 [27:22<2:52:14,  3.76s/it][A
 17%|█▋        | 560/3305 [27:26<2:51:56,  3.76s/it][A
 17%|█▋        | 561/3305 [27:29<2:45:20,  3.62s/it][A
 17%|█▋        | 562/3305 [27:32<2:36:10,  3.42s/it][A
 17%|█▋        | 563/3305 [27:35<2:32:11,  3.33s/it][A
 17%|█▋        | 564/3305 [27:37<2:14:54,  2.95s/it][A
 17%|█▋        | 565/3305 [27:40<2:11:40,  2.88s/it][A
 17%|█▋        | 566/3305 [27:43<2:15:50,  2.98s/it][A
 17%|█▋        | 567/3305 [27:46<2:14:46,  2.95s/it][A
 17%|█▋        | 568/3305 [27:48<2:08:31,  2.82s/it][A
 17%|█▋        | 569/3305 [27:50<1:57:37,  2.58s/it][A
 17%|█▋        | 570/3305 [27:53<1:56:15,  2.55s/it][A
 17%|█▋        | 571/3305 [27:56<2:09:05,  2.83s/it][A
 17%|█▋        | 572/3305 [27:59<2:08:43,  2.83s/it][A
 17%|█▋        | 573/3305 [28:01<1:56:37,  2.56s/it][A
 17%|█▋        | 574/3305 [28:04<1:57:01,  2.57s/it][A
 17%|█▋        | 575/3305 [28:06<1:59:37,  2.63s/it][A
 17%|█▋        | 576/3305 [28:09<2:00:52,  2.66s/it][A
 17%|█▋        | 577/3305 [28:11<1:53:27,  2.50s/it][A
 17%|█▋        | 578/3305 [28:14<1:50:12,  2.42s/it][A
 18%|█▊        | 579/3305 [28:16<1:49:07,  2.40s/it][A
 18%|█▊        | 580/3305 [28:18<1:47:22,  2.36s/it][A
 18%|█▊        | 581/3305 [28:21<1:53:16,  2.49s/it][A
 18%|█▊        | 582/3305 [28:25<2:13:20,  2.94s/it][A
 18%|█▊        | 583/3305 [28:29<2:31:20,  3.34s/it][A
 18%|█▊        | 584/3305 [28:32<2:30:28,  3.32s/it][A
 18%|█▊        | 585/3305 [28:35<2:20:33,  3.10s/it][A
 18%|█▊        | 586/3305 [28:38<2:16:24,  3.01s/it][A
 18%|█▊        | 587/3305 [28:40<2:11:09,  2.90s/it][A
 18%|█▊        | 588/3305 [28:43<2:00:30,  2.66s/it][A
 18%|█▊        | 589/3305 [28:45<2:00:41,  2.67s/it][A
 18%|█▊        | 590/3305 [28:49<2:11:22,  2.90s/it][A
 18%|█▊        | 591/3305 [28:52<2:11:58,  2.92s/it][A
 18%|█▊        | 592/3305 [28:54<2:03:48,  2.74s/it][A
 18%|█▊        | 593/3305 [28:57<2:05:14,  2.77s/it][A
 18%|█▊        | 594/3305 [29:00<2:04:29,  2.76s/it][A
 18%|█▊        | 595/3305 [29:02<1:56:12,  2.57s/it][A
 18%|█▊        | 596/3305 [29:04<1:52:48,  2.50s/it][A
 18%|█▊        | 597/3305 [29:07<2:00:31,  2.67s/it][A
 18%|█▊        | 598/3305 [29:10<2:07:16,  2.82s/it][A
 18%|█▊        | 599/3305 [29:13<2:08:12,  2.84s/it][A
 18%|█▊        | 600/3305 [29:15<2:00:32,  2.67s/it][A
 18%|█▊        | 601/3305 [29:17<1:47:28,  2.38s/it][A
 18%|█▊        | 602/3305 [29:19<1:40:01,  2.22s/it][A
 18%|█▊        | 603/3305 [29:21<1:39:36,  2.21s/it][A
 18%|█▊        | 604/3305 [29:24<1:45:36,  2.35s/it][A
 18%|█▊        | 605/3305 [29:26<1:47:43,  2.39s/it][A
 18%|█▊        | 606/3305 [29:28<1:43:58,  2.31s/it][A
 18%|█▊        | 607/3305 [29:31<1:41:11,  2.25s/it][A
 18%|█▊        | 608/3305 [29:32<1:36:07,  2.14s/it][A
 18%|█▊        | 609/3305 [29:35<1:35:13,  2.12s/it][A
 18%|█▊        | 610/3305 [29:37<1:45:24,  2.35s/it][A
 18%|█▊        | 611/3305 [29:40<1:52:14,  2.50s/it][A
 19%|█▊        | 612/3305 [29:42<1:44:04,  2.32s/it][A
 19%|█▊        | 613/3305 [29:45<1:55:02,  2.56s/it][A
 19%|█▊        | 614/3305 [29:49<2:05:41,  2.80s/it][A
 19%|█▊        | 615/3305 [29:52<2:13:05,  2.97s/it][A
 19%|█▊        | 616/3305 [29:54<2:06:22,  2.82s/it][A
 19%|█▊        | 617/3305 [29:57<2:04:33,  2.78s/it][A
 19%|█▊        | 618/3305 [30:00<2:05:16,  2.80s/it][A
 19%|█▊        | 619/3305 [30:03<2:08:20,  2.87s/it][A
 19%|█▉        | 620/3305 [30:08<2:35:00,  3.46s/it][A
 19%|█▉        | 621/3305 [30:13<2:54:59,  3.91s/it][A
 19%|█▉        | 622/3305 [30:17<2:53:07,  3.87s/it][A
 19%|█▉        | 623/3305 [30:22<3:11:30,  4.28s/it][A
 19%|█▉        | 624/3305 [30:26<3:14:23,  4.35s/it][A
 19%|█▉        | 625/3305 [30:29<2:45:48,  3.71s/it][A
 19%|█▉        | 626/3305 [30:31<2:26:31,  3.28s/it][A
 19%|█▉        | 627/3305 [30:33<2:16:31,  3.06s/it][A
 19%|█▉        | 628/3305 [30:36<2:07:02,  2.85s/it][A
 19%|█▉        | 629/3305 [30:37<1:51:43,  2.51s/it][A
 19%|█▉        | 630/3305 [30:40<1:55:27,  2.59s/it][A
 19%|█▉        | 631/3305 [30:44<2:11:37,  2.95s/it][A
 19%|█▉        | 632/3305 [30:47<2:09:36,  2.91s/it][A
 19%|█▉        | 633/3305 [30:49<2:03:34,  2.77s/it][A
 19%|█▉        | 634/3305 [30:52<2:01:49,  2.74s/it][A
 19%|█▉        | 635/3305 [30:54<1:51:42,  2.51s/it][A
 19%|█▉        | 636/3305 [30:56<1:45:35,  2.37s/it][A
 19%|█▉        | 637/3305 [31:00<2:10:59,  2.95s/it][A
 19%|█▉        | 638/3305 [31:05<2:35:41,  3.50s/it][A
 19%|█▉        | 639/3305 [31:08<2:29:15,  3.36s/it][A
 19%|█▉        | 640/3305 [31:11<2:21:12,  3.18s/it][A
 19%|█▉        | 641/3305 [31:13<2:04:10,  2.80s/it][A
 19%|█▉        | 642/3305 [31:15<1:54:45,  2.59s/it][A
 19%|█▉        | 643/3305 [31:17<1:52:47,  2.54s/it][A
 19%|█▉        | 644/3305 [31:20<1:49:06,  2.46s/it][A
 20%|█▉        | 645/3305 [31:22<1:48:56,  2.46s/it][A
 20%|█▉        | 646/3305 [31:24<1:48:33,  2.45s/it][A
 20%|█▉        | 647/3305 [31:27<1:47:33,  2.43s/it][A
 20%|█▉        | 648/3305 [31:29<1:44:45,  2.37s/it][A
 20%|█▉        | 649/3305 [31:32<1:45:48,  2.39s/it][A
 20%|█▉        | 650/3305 [31:35<2:04:45,  2.82s/it][A
 20%|█▉        | 651/3305 [31:39<2:19:22,  3.15s/it][A
 20%|█▉        | 652/3305 [31:42<2:17:42,  3.11s/it][A
 20%|█▉        | 653/3305 [31:46<2:22:44,  3.23s/it][A
 20%|█▉        | 654/3305 [31:49<2:16:00,  3.08s/it][A
 20%|█▉        | 655/3305 [31:51<2:03:53,  2.81s/it][A
 20%|█▉        | 656/3305 [31:53<1:59:05,  2.70s/it][A
 20%|█▉        | 657/3305 [31:56<1:59:38,  2.71s/it][A
 20%|█▉        | 658/3305 [31:58<1:52:48,  2.56s/it][A
 20%|█▉        | 659/3305 [32:00<1:45:25,  2.39s/it][A
 20%|█▉        | 660/3305 [32:03<1:45:58,  2.40s/it][A
 20%|██        | 661/3305 [32:06<1:58:30,  2.69s/it][A
 20%|██        | 662/3305 [32:09<2:05:25,  2.85s/it][A
 20%|██        | 663/3305 [32:12<1:59:39,  2.72s/it][A
 20%|██        | 664/3305 [32:17<2:38:06,  3.59s/it][A
 20%|██        | 665/3305 [32:23<3:10:25,  4.33s/it][A
 20%|██        | 666/3305 [32:25<2:37:18,  3.58s/it][A
 20%|██        | 667/3305 [32:27<2:20:19,  3.19s/it][A
 20%|██        | 668/3305 [32:32<2:42:50,  3.71s/it][A
 20%|██        | 669/3305 [32:38<3:10:54,  4.35s/it][A
 20%|██        | 670/3305 [32:41<2:49:53,  3.87s/it][A
 20%|██        | 671/3305 [32:43<2:28:22,  3.38s/it][A
 20%|██        | 672/3305 [32:45<2:06:22,  2.88s/it][A
 20%|██        | 673/3305 [32:46<1:48:21,  2.47s/it][A
 20%|██        | 674/3305 [32:49<1:53:08,  2.58s/it][A
 20%|██        | 675/3305 [32:52<1:57:19,  2.68s/it][A
 20%|██        | 676/3305 [32:55<1:58:10,  2.70s/it][A
 20%|██        | 677/3305 [32:58<2:02:40,  2.80s/it][A
 21%|██        | 678/3305 [33:01<2:09:09,  2.95s/it][A
 21%|██        | 679/3305 [33:03<2:01:53,  2.78s/it][A
 21%|██        | 680/3305 [33:05<1:46:45,  2.44s/it][A
 21%|██        | 681/3305 [33:08<1:58:53,  2.72s/it][A
 21%|██        | 682/3305 [33:13<2:24:03,  3.30s/it][A
 21%|██        | 683/3305 [33:16<2:22:24,  3.26s/it][A
 21%|██        | 684/3305 [33:19<2:16:37,  3.13s/it][A
 21%|██        | 685/3305 [33:22<2:15:30,  3.10s/it][A
 21%|██        | 686/3305 [33:24<2:04:53,  2.86s/it][A
 21%|██        | 687/3305 [33:26<1:48:30,  2.49s/it][A
 21%|██        | 688/3305 [33:27<1:33:59,  2.15s/it][A
 21%|██        | 689/3305 [33:29<1:29:54,  2.06s/it][A
 21%|██        | 690/3305 [33:32<1:34:32,  2.17s/it][A
 21%|██        | 691/3305 [33:34<1:38:32,  2.26s/it][A
 21%|██        | 692/3305 [33:36<1:33:46,  2.15s/it][A
 21%|██        | 693/3305 [33:38<1:32:13,  2.12s/it][A
 21%|██        | 694/3305 [33:40<1:34:53,  2.18s/it][A
 21%|██        | 695/3305 [33:43<1:41:21,  2.33s/it][A
 21%|██        | 696/3305 [33:47<2:01:25,  2.79s/it][A
 21%|██        | 697/3305 [33:51<2:20:15,  3.23s/it][A
 21%|██        | 698/3305 [33:55<2:21:52,  3.27s/it][A
 21%|██        | 699/3305 [33:57<2:15:09,  3.11s/it][A
 21%|██        | 700/3305 [34:01<2:16:54,  3.15s/it][A
 21%|██        | 701/3305 [34:03<2:08:32,  2.96s/it][A
 21%|██        | 702/3305 [34:05<1:58:19,  2.73s/it][A
 21%|██▏       | 703/3305 [34:07<1:47:52,  2.49s/it][A
 21%|██▏       | 704/3305 [34:10<1:47:00,  2.47s/it][A
 21%|██▏       | 705/3305 [34:12<1:46:42,  2.46s/it][A
 21%|██▏       | 706/3305 [34:15<1:49:13,  2.52s/it][A
 21%|██▏       | 707/3305 [34:18<1:53:16,  2.62s/it][A
 21%|██▏       | 708/3305 [34:20<1:46:37,  2.46s/it][A
 21%|██▏       | 709/3305 [34:21<1:37:02,  2.24s/it][A
 21%|██▏       | 710/3305 [34:23<1:27:47,  2.03s/it][A
 22%|██▏       | 711/3305 [34:25<1:27:34,  2.03s/it][A
 22%|██▏       | 712/3305 [34:29<1:56:18,  2.69s/it][A
 22%|██▏       | 713/3305 [34:34<2:28:58,  3.45s/it][A
 22%|██▏       | 714/3305 [34:38<2:23:48,  3.33s/it][A
 22%|██▏       | 715/3305 [34:40<2:12:40,  3.07s/it][A
 22%|██▏       | 716/3305 [34:43<2:10:40,  3.03s/it][A
 22%|██▏       | 717/3305 [34:46<2:11:00,  3.04s/it][A
 22%|██▏       | 718/3305 [34:49<2:05:31,  2.91s/it][A
 22%|██▏       | 719/3305 [34:52<2:15:32,  3.14s/it][A
 22%|██▏       | 720/3305 [34:56<2:23:39,  3.33s/it][A
 22%|██▏       | 721/3305 [34:59<2:22:17,  3.30s/it][A
 22%|██▏       | 722/3305 [35:02<2:15:11,  3.14s/it][A
 22%|██▏       | 723/3305 [35:05<2:09:09,  3.00s/it][A
 22%|██▏       | 724/3305 [35:08<2:08:52,  3.00s/it][A
 22%|██▏       | 725/3305 [35:10<2:02:10,  2.84s/it][A
 22%|██▏       | 726/3305 [35:14<2:12:55,  3.09s/it][A
 22%|██▏       | 727/3305 [35:18<2:21:10,  3.29s/it][A
 22%|██▏       | 728/3305 [35:21<2:17:20,  3.20s/it][A
 22%|██▏       | 729/3305 [35:24<2:16:38,  3.18s/it][A
 22%|██▏       | 730/3305 [35:27<2:15:14,  3.15s/it][A
 22%|██▏       | 731/3305 [35:30<2:17:34,  3.21s/it][A
 22%|██▏       | 732/3305 [35:33<2:07:20,  2.97s/it][A
 22%|██▏       | 733/3305 [35:35<1:57:43,  2.75s/it][A
 22%|██▏       | 734/3305 [35:38<1:59:49,  2.80s/it][A
 22%|██▏       | 735/3305 [35:40<1:55:41,  2.70s/it][A
 22%|██▏       | 736/3305 [35:43<1:59:39,  2.79s/it][A
 22%|██▏       | 737/3305 [35:48<2:23:22,  3.35s/it][A
 22%|██▏       | 738/3305 [35:52<2:35:29,  3.63s/it][A
 22%|██▏       | 739/3305 [35:54<2:18:37,  3.24s/it][A
 22%|██▏       | 740/3305 [35:57<2:04:22,  2.91s/it][A
 22%|██▏       | 741/3305 [35:59<2:01:32,  2.84s/it][A
 22%|██▏       | 742/3305 [36:02<2:02:45,  2.87s/it][A
 22%|██▏       | 743/3305 [36:05<1:56:25,  2.73s/it][A
 23%|██▎       | 744/3305 [36:07<1:51:46,  2.62s/it][A
 23%|██▎       | 745/3305 [36:10<1:50:37,  2.59s/it][A
 23%|██▎       | 746/3305 [36:12<1:48:13,  2.54s/it][A
 23%|██▎       | 747/3305 [36:14<1:42:01,  2.39s/it][A
 23%|██▎       | 748/3305 [36:16<1:34:22,  2.21s/it][A
 23%|██▎       | 749/3305 [36:18<1:32:13,  2.16s/it][A
 23%|██▎       | 750/3305 [36:21<1:40:14,  2.35s/it][A
 23%|██▎       | 751/3305 [36:23<1:43:46,  2.44s/it][A
 23%|██▎       | 752/3305 [36:26<1:52:20,  2.64s/it][A
 23%|██▎       | 753/3305 [36:30<1:58:55,  2.80s/it][A
 23%|██▎       | 754/3305 [36:32<2:00:10,  2.83s/it][A
 23%|██▎       | 755/3305 [36:37<2:18:56,  3.27s/it][A
 23%|██▎       | 756/3305 [36:41<2:30:43,  3.55s/it][A
 23%|██▎       | 757/3305 [36:43<2:16:52,  3.22s/it][A
 23%|██▎       | 758/3305 [36:46<2:07:34,  3.01s/it][A
 23%|██▎       | 759/3305 [36:49<2:11:11,  3.09s/it][A
 23%|██▎       | 760/3305 [36:52<2:09:12,  3.05s/it][A
 23%|██▎       | 761/3305 [36:54<1:57:15,  2.77s/it][A
 23%|██▎       | 762/3305 [36:59<2:18:01,  3.26s/it][A
 23%|██▎       | 763/3305 [37:02<2:18:50,  3.28s/it][A
 23%|██▎       | 764/3305 [37:04<2:01:27,  2.87s/it][A
 23%|██▎       | 765/3305 [37:07<2:04:00,  2.93s/it][A
 23%|██▎       | 766/3305 [37:10<2:01:42,  2.88s/it][A
 23%|██▎       | 767/3305 [37:12<1:53:09,  2.68s/it][A
 23%|██▎       | 768/3305 [37:14<1:50:46,  2.62s/it][A
 23%|██▎       | 769/3305 [37:18<2:05:13,  2.96s/it][A
 23%|██▎       | 770/3305 [37:23<2:22:46,  3.38s/it][A
 23%|██▎       | 771/3305 [37:24<2:04:56,  2.96s/it][A
 23%|██▎       | 772/3305 [37:27<2:05:12,  2.97s/it][A
 23%|██▎       | 773/3305 [37:30<2:04:08,  2.94s/it][A
 23%|██▎       | 774/3305 [37:32<1:49:32,  2.60s/it][A
 23%|██▎       | 775/3305 [37:34<1:40:02,  2.37s/it][A
 23%|██▎       | 776/3305 [37:36<1:30:03,  2.14s/it][A
 24%|██▎       | 777/3305 [37:37<1:26:49,  2.06s/it][A
 24%|██▎       | 778/3305 [37:40<1:37:28,  2.31s/it][A
 24%|██▎       | 779/3305 [37:43<1:41:59,  2.42s/it][A
 24%|██▎       | 780/3305 [37:45<1:35:30,  2.27s/it][A
 24%|██▎       | 781/3305 [37:47<1:34:07,  2.24s/it][A
 24%|██▎       | 782/3305 [37:49<1:34:31,  2.25s/it][A
 24%|██▎       | 783/3305 [37:52<1:35:45,  2.28s/it][A
 24%|██▎       | 784/3305 [37:55<1:42:58,  2.45s/it][A
 24%|██▍       | 785/3305 [37:57<1:47:52,  2.57s/it][A
 24%|██▍       | 786/3305 [38:00<1:46:27,  2.54s/it][A
 24%|██▍       | 787/3305 [38:02<1:41:23,  2.42s/it][A
 24%|██▍       | 788/3305 [38:04<1:36:08,  2.29s/it][A
 24%|██▍       | 789/3305 [38:06<1:36:58,  2.31s/it][A
 24%|██▍       | 790/3305 [38:09<1:34:51,  2.26s/it][A
 24%|██▍       | 791/3305 [38:10<1:25:59,  2.05s/it][A
 24%|██▍       | 792/3305 [38:12<1:20:08,  1.91s/it][A
 24%|██▍       | 793/3305 [38:14<1:28:07,  2.10s/it][A
 24%|██▍       | 794/3305 [38:18<1:48:38,  2.60s/it][A
 24%|██▍       | 795/3305 [38:21<1:53:16,  2.71s/it][A
 24%|██▍       | 796/3305 [38:25<2:05:54,  3.01s/it][A
 24%|██▍       | 797/3305 [38:28<2:08:38,  3.08s/it][A
 24%|██▍       | 798/3305 [38:30<1:57:36,  2.81s/it][A
 24%|██▍       | 799/3305 [38:33<1:53:49,  2.73s/it][A
 24%|██▍       | 800/3305 [38:36<1:59:41,  2.87s/it][A
 24%|██▍       | 801/3305 [38:39<2:02:31,  2.94s/it][A
 24%|██▍       | 802/3305 [38:43<2:17:04,  3.29s/it][A
 24%|██▍       | 803/3305 [38:48<2:38:37,  3.80s/it][A
 24%|██▍       | 804/3305 [38:52<2:40:08,  3.84s/it][A
 24%|██▍       | 805/3305 [38:55<2:33:37,  3.69s/it][A
 24%|██▍       | 806/3305 [38:58<2:22:31,  3.42s/it][A
 24%|██▍       | 807/3305 [39:02<2:26:49,  3.53s/it][A
 24%|██▍       | 808/3305 [39:05<2:26:11,  3.51s/it][A
 24%|██▍       | 809/3305 [39:09<2:22:55,  3.44s/it][A
 25%|██▍       | 810/3305 [39:12<2:26:10,  3.52s/it][A
 25%|██▍       | 811/3305 [39:16<2:32:14,  3.66s/it][A
 25%|██▍       | 812/3305 [39:21<2:50:55,  4.11s/it][A
 25%|██▍       | 813/3305 [39:26<2:51:45,  4.14s/it][A
 25%|██▍       | 814/3305 [39:31<3:08:39,  4.54s/it][A
 25%|██▍       | 815/3305 [39:37<3:23:16,  4.90s/it][A
 25%|██▍       | 816/3305 [39:39<2:52:13,  4.15s/it][A
 25%|██▍       | 817/3305 [39:41<2:20:28,  3.39s/it][A
 25%|██▍       | 818/3305 [39:43<2:07:45,  3.08s/it][A
 25%|██▍       | 819/3305 [39:46<2:06:07,  3.04s/it][A
 25%|██▍       | 820/3305 [39:49<2:00:13,  2.90s/it][A
 25%|██▍       | 821/3305 [39:52<2:04:34,  3.01s/it][A
 25%|██▍       | 822/3305 [39:55<2:03:15,  2.98s/it][A
 25%|██▍       | 823/3305 [39:58<2:02:25,  2.96s/it][A
 25%|██▍       | 824/3305 [40:00<1:50:16,  2.67s/it][A
 25%|██▍       | 825/3305 [40:03<1:52:08,  2.71s/it][A
 25%|██▍       | 826/3305 [40:06<1:54:00,  2.76s/it][A
 25%|██▌       | 827/3305 [40:07<1:40:42,  2.44s/it][A
 25%|██▌       | 828/3305 [40:09<1:35:44,  2.32s/it][A
 25%|██▌       | 829/3305 [40:12<1:39:09,  2.40s/it][A
 25%|██▌       | 830/3305 [40:15<1:41:47,  2.47s/it][A
 25%|██▌       | 831/3305 [40:18<1:48:19,  2.63s/it][A
 25%|██▌       | 832/3305 [40:21<2:02:32,  2.97s/it][A
 25%|██▌       | 833/3305 [40:25<2:10:38,  3.17s/it][A
 25%|██▌       | 834/3305 [40:28<2:08:59,  3.13s/it][A
 25%|██▌       | 835/3305 [40:30<2:00:48,  2.93s/it][A
 25%|██▌       | 836/3305 [40:33<1:50:12,  2.68s/it][A
 25%|██▌       | 837/3305 [40:35<1:44:35,  2.54s/it][A
 25%|██▌       | 838/3305 [40:37<1:39:03,  2.41s/it][A
 25%|██▌       | 839/3305 [40:39<1:36:04,  2.34s/it][A
 25%|██▌       | 840/3305 [40:42<1:49:22,  2.66s/it][A
 25%|██▌       | 841/3305 [40:46<2:02:22,  2.98s/it][A
 25%|██▌       | 842/3305 [40:49<1:59:34,  2.91s/it][A
 26%|██▌       | 843/3305 [40:52<1:58:30,  2.89s/it][A
 26%|██▌       | 844/3305 [40:56<2:19:21,  3.40s/it][A
 26%|██▌       | 845/3305 [41:01<2:29:52,  3.66s/it][A
 26%|██▌       | 846/3305 [41:03<2:12:22,  3.23s/it][A
 26%|██▌       | 847/3305 [41:05<1:58:47,  2.90s/it][A
 26%|██▌       | 848/3305 [41:08<1:59:49,  2.93s/it][A
 26%|██▌       | 849/3305 [41:13<2:27:24,  3.60s/it][A
 26%|██▌       | 850/3305 [41:18<2:47:34,  4.10s/it][A
 26%|██▌       | 851/3305 [41:22<2:36:04,  3.82s/it][A
 26%|██▌       | 852/3305 [41:25<2:28:48,  3.64s/it][A
 26%|██▌       | 853/3305 [41:28<2:25:30,  3.56s/it][A
 26%|██▌       | 854/3305 [41:31<2:13:53,  3.28s/it][A
 26%|██▌       | 855/3305 [41:33<2:03:52,  3.03s/it][A
 26%|██▌       | 856/3305 [41:36<2:00:49,  2.96s/it][A
 26%|██▌       | 857/3305 [41:39<1:55:44,  2.84s/it][A
 26%|██▌       | 858/3305 [41:42<1:57:43,  2.89s/it][A
 26%|██▌       | 859/3305 [41:44<1:57:40,  2.89s/it][A
 26%|██▌       | 860/3305 [41:47<1:56:22,  2.86s/it][A
 26%|██▌       | 861/3305 [41:50<1:49:42,  2.69s/it][A
 26%|██▌       | 862/3305 [41:52<1:46:02,  2.60s/it][A
 26%|██▌       | 863/3305 [41:55<1:47:37,  2.64s/it][A
 26%|██▌       | 864/3305 [41:58<1:52:11,  2.76s/it][A
 26%|██▌       | 865/3305 [42:02<2:05:28,  3.09s/it][A
 26%|██▌       | 866/3305 [42:05<2:16:01,  3.35s/it][A
 26%|██▌       | 867/3305 [42:08<1:59:58,  2.95s/it][A
 26%|██▋       | 868/3305 [42:10<1:59:35,  2.94s/it][A
 26%|██▋       | 869/3305 [42:14<2:10:39,  3.22s/it][A
 26%|██▋       | 870/3305 [42:17<1:59:35,  2.95s/it][A
 26%|██▋       | 871/3305 [42:19<1:48:10,  2.67s/it][A
 26%|██▋       | 872/3305 [42:21<1:42:19,  2.52s/it][A
 26%|██▋       | 873/3305 [42:24<1:45:35,  2.61s/it][A
 26%|██▋       | 874/3305 [42:27<1:53:11,  2.79s/it][A
 26%|██▋       | 875/3305 [42:29<1:50:45,  2.73s/it][A
 27%|██▋       | 876/3305 [42:33<1:57:55,  2.91s/it][A
 27%|██▋       | 877/3305 [42:35<1:53:52,  2.81s/it][A
 27%|██▋       | 878/3305 [42:38<1:51:45,  2.76s/it][A
 27%|██▋       | 879/3305 [42:40<1:45:50,  2.62s/it][A
 27%|██▋       | 880/3305 [42:44<2:01:38,  3.01s/it][A
 27%|██▋       | 881/3305 [42:49<2:23:44,  3.56s/it][A
 27%|██▋       | 882/3305 [42:51<2:08:27,  3.18s/it][A
 27%|██▋       | 883/3305 [42:54<1:59:11,  2.95s/it][A
 27%|██▋       | 884/3305 [42:57<2:05:33,  3.11s/it][A
 27%|██▋       | 885/3305 [43:01<2:14:25,  3.33s/it][A
 27%|██▋       | 886/3305 [43:05<2:17:05,  3.40s/it][A
 27%|██▋       | 887/3305 [43:07<2:06:30,  3.14s/it][A
 27%|██▋       | 888/3305 [43:09<1:48:42,  2.70s/it][A
 27%|██▋       | 889/3305 [43:11<1:41:13,  2.51s/it][A
 27%|██▋       | 890/3305 [43:13<1:40:08,  2.49s/it][A
 27%|██▋       | 891/3305 [43:16<1:46:46,  2.65s/it][A
 27%|██▋       | 892/3305 [43:20<1:54:08,  2.84s/it][A
 27%|██▋       | 893/3305 [43:22<1:52:10,  2.79s/it][A
 27%|██▋       | 894/3305 [43:25<1:50:11,  2.74s/it][A
 27%|██▋       | 895/3305 [43:27<1:39:42,  2.48s/it][A
 27%|██▋       | 896/3305 [43:29<1:38:44,  2.46s/it][A
 27%|██▋       | 897/3305 [43:32<1:44:34,  2.61s/it][A
 27%|██▋       | 898/3305 [43:34<1:40:04,  2.49s/it][A
 27%|██▋       | 899/3305 [43:37<1:45:10,  2.62s/it][A
 27%|██▋       | 900/3305 [43:41<1:52:31,  2.81s/it][A
 27%|██▋       | 901/3305 [43:43<1:46:11,  2.65s/it][A
 27%|██▋       | 902/3305 [43:45<1:41:01,  2.52s/it][A
 27%|██▋       | 903/3305 [43:47<1:31:35,  2.29s/it][A
 27%|██▋       | 904/3305 [43:49<1:27:46,  2.19s/it][A
 27%|██▋       | 905/3305 [43:51<1:28:19,  2.21s/it][A
 27%|██▋       | 906/3305 [43:53<1:30:38,  2.27s/it][A
 27%|██▋       | 907/3305 [43:56<1:31:42,  2.29s/it][A
 27%|██▋       | 908/3305 [43:58<1:24:44,  2.12s/it][A
 28%|██▊       | 909/3305 [43:59<1:14:25,  1.86s/it][A
 28%|██▊       | 910/3305 [44:03<1:43:18,  2.59s/it][A
 28%|██▊       | 911/3305 [44:08<2:15:17,  3.39s/it][A
 28%|██▊       | 912/3305 [44:11<2:07:15,  3.19s/it][A
 28%|██▊       | 913/3305 [44:13<1:54:00,  2.86s/it][A
 28%|██▊       | 914/3305 [44:16<1:50:12,  2.77s/it][A
 28%|██▊       | 915/3305 [44:18<1:44:41,  2.63s/it][A
 28%|██▊       | 916/3305 [44:20<1:33:55,  2.36s/it][A
 28%|██▊       | 917/3305 [44:23<1:38:36,  2.48s/it][A
 28%|██▊       | 918/3305 [44:26<1:50:54,  2.79s/it][A
 28%|██▊       | 919/3305 [44:31<2:11:53,  3.32s/it][A
 28%|██▊       | 920/3305 [44:35<2:29:25,  3.76s/it][A
 28%|██▊       | 921/3305 [44:39<2:31:30,  3.81s/it][A
 28%|██▊       | 922/3305 [44:42<2:23:30,  3.61s/it][A
 28%|██▊       | 923/3305 [44:44<1:59:58,  3.02s/it][A
 28%|██▊       | 924/3305 [44:45<1:39:53,  2.52s/it][A
 28%|██▊       | 925/3305 [44:47<1:31:40,  2.31s/it][A
 28%|██▊       | 926/3305 [44:49<1:30:31,  2.28s/it][A
 28%|██▊       | 927/3305 [44:52<1:33:46,  2.37s/it][A
 28%|██▊       | 928/3305 [44:55<1:42:34,  2.59s/it][A
 28%|██▊       | 929/3305 [44:58<1:40:39,  2.54s/it][A
 28%|██▊       | 930/3305 [45:00<1:36:43,  2.44s/it][A
 28%|██▊       | 931/3305 [45:03<1:46:50,  2.70s/it][A
 28%|██▊       | 932/3305 [45:06<1:46:53,  2.70s/it][A
 28%|██▊       | 933/3305 [45:09<1:47:10,  2.71s/it][A
 28%|██▊       | 934/3305 [45:13<2:13:29,  3.38s/it][A
 28%|██▊       | 935/3305 [45:19<2:36:44,  3.97s/it][A
 28%|██▊       | 936/3305 [45:22<2:28:25,  3.76s/it][A
 28%|██▊       | 937/3305 [45:27<2:39:25,  4.04s/it][A
 28%|██▊       | 938/3305 [45:33<3:00:53,  4.59s/it][A
 28%|██▊       | 939/3305 [45:36<2:48:14,  4.27s/it][A
 28%|██▊       | 940/3305 [45:39<2:35:42,  3.95s/it][A
 28%|██▊       | 941/3305 [45:42<2:15:56,  3.45s/it][A
 29%|██▊       | 942/3305 [45:44<2:04:37,  3.16s/it][A
 29%|██▊       | 943/3305 [45:47<2:06:34,  3.22s/it][A
 29%|██▊       | 944/3305 [45:51<2:08:03,  3.25s/it][A
 29%|██▊       | 945/3305 [45:54<2:08:59,  3.28s/it][A
 29%|██▊       | 946/3305 [45:57<2:06:24,  3.22s/it][A
 29%|██▊       | 947/3305 [46:01<2:14:07,  3.41s/it][A
 29%|██▊       | 948/3305 [46:04<2:08:32,  3.27s/it][A
 29%|██▊       | 949/3305 [46:06<1:57:58,  3.00s/it][A
 29%|██▊       | 950/3305 [46:10<2:00:21,  3.07s/it][A
 29%|██▉       | 951/3305 [46:15<2:29:22,  3.81s/it][A
 29%|██▉       | 952/3305 [46:21<2:55:34,  4.48s/it][A
 29%|██▉       | 953/3305 [46:24<2:36:53,  4.00s/it][A
 29%|██▉       | 954/3305 [46:26<2:16:31,  3.48s/it][A
 29%|██▉       | 955/3305 [46:29<2:01:27,  3.10s/it][A
 29%|██▉       | 956/3305 [46:31<1:52:21,  2.87s/it][A
 29%|██▉       | 957/3305 [46:36<2:16:41,  3.49s/it][A
 29%|██▉       | 958/3305 [46:41<2:36:21,  4.00s/it][A
 29%|██▉       | 959/3305 [46:43<2:09:22,  3.31s/it][A
 29%|██▉       | 960/3305 [46:45<1:59:19,  3.05s/it][A
 29%|██▉       | 961/3305 [46:48<1:54:42,  2.94s/it][A
 29%|██▉       | 962/3305 [46:52<2:03:34,  3.16s/it][A
 29%|██▉       | 963/3305 [46:57<2:25:06,  3.72s/it][A
 29%|██▉       | 964/3305 [47:01<2:27:58,  3.79s/it][A
 29%|██▉       | 965/3305 [47:04<2:19:47,  3.58s/it][A
 29%|██▉       | 966/3305 [47:07<2:20:31,  3.60s/it][A
 29%|██▉       | 967/3305 [47:11<2:23:20,  3.68s/it][A
 29%|██▉       | 968/3305 [47:14<2:17:03,  3.52s/it][A
 29%|██▉       | 969/3305 [47:18<2:21:38,  3.64s/it][A
 29%|██▉       | 970/3305 [47:21<2:07:42,  3.28s/it][A
 29%|██▉       | 971/3305 [47:22<1:43:41,  2.67s/it][A
 29%|██▉       | 972/3305 [47:24<1:33:02,  2.39s/it][A
 29%|██▉       | 973/3305 [47:25<1:26:15,  2.22s/it][A
 29%|██▉       | 974/3305 [47:27<1:22:05,  2.11s/it][A
 30%|██▉       | 975/3305 [47:32<1:46:43,  2.75s/it][A
 30%|██▉       | 976/3305 [47:37<2:16:36,  3.52s/it][A
 30%|██▉       | 977/3305 [47:39<2:05:34,  3.24s/it][A
 30%|██▉       | 978/3305 [47:43<2:03:49,  3.19s/it][A
 30%|██▉       | 979/3305 [47:45<1:57:25,  3.03s/it][A
 30%|██▉       | 980/3305 [47:48<1:54:51,  2.96s/it][A
 30%|██▉       | 981/3305 [47:50<1:46:36,  2.75s/it][A
 30%|██▉       | 982/3305 [47:54<1:53:24,  2.93s/it][A
 30%|██▉       | 983/3305 [47:57<2:00:07,  3.10s/it][A
 30%|██▉       | 984/3305 [47:59<1:48:46,  2.81s/it][A
 30%|██▉       | 985/3305 [48:01<1:40:00,  2.59s/it][A
 30%|██▉       | 986/3305 [48:03<1:31:17,  2.36s/it][A
 30%|██▉       | 987/3305 [48:05<1:30:33,  2.34s/it][A
 30%|██▉       | 988/3305 [48:08<1:31:10,  2.36s/it][A
 30%|██▉       | 989/3305 [48:10<1:28:29,  2.29s/it][A
 30%|██▉       | 990/3305 [48:12<1:28:43,  2.30s/it][A
 30%|██▉       | 991/3305 [48:17<1:53:26,  2.94s/it][A
 30%|███       | 992/3305 [48:21<2:11:52,  3.42s/it][A
 30%|███       | 993/3305 [48:24<1:59:28,  3.10s/it][A
 30%|███       | 994/3305 [48:26<1:54:00,  2.96s/it][A
 30%|███       | 995/3305 [48:30<2:08:22,  3.33s/it][A
 30%|███       | 996/3305 [48:35<2:21:45,  3.68s/it][A
 30%|███       | 997/3305 [48:37<2:05:42,  3.27s/it][A
 30%|███       | 998/3305 [48:40<1:56:38,  3.03s/it][A
 30%|███       | 999/3305 [48:42<1:46:21,  2.77s/it][A
 30%|███       | 1000/3305 [48:44<1:35:20,  2.48s/it][A
 30%|███       | 1001/3305 [48:46<1:28:05,  2.29s/it][A
 30%|███       | 1002/3305 [48:48<1:26:26,  2.25s/it][A
 30%|███       | 1003/3305 [48:50<1:23:21,  2.17s/it][A
 30%|███       | 1004/3305 [48:54<1:44:51,  2.73s/it][A
 30%|███       | 1005/3305 [48:58<2:03:04,  3.21s/it][A
 30%|███       | 1006/3305 [49:00<1:51:29,  2.91s/it][A
 30%|███       | 1007/3305 [49:03<1:51:40,  2.92s/it][A
 30%|███       | 1008/3305 [49:06<1:45:36,  2.76s/it][A
 31%|███       | 1009/3305 [49:07<1:33:29,  2.44s/it][A
 31%|███       | 1010/3305 [49:10<1:34:43,  2.48s/it][A
 31%|███       | 1011/3305 [49:12<1:33:45,  2.45s/it][A
 31%|███       | 1012/3305 [49:14<1:23:54,  2.20s/it][A
 31%|███       | 1013/3305 [49:16<1:17:58,  2.04s/it][A
 31%|███       | 1014/3305 [49:18<1:17:55,  2.04s/it][A
 31%|███       | 1015/3305 [49:20<1:20:34,  2.11s/it][A
 31%|███       | 1016/3305 [49:22<1:24:10,  2.21s/it][A
 31%|███       | 1017/3305 [49:25<1:29:39,  2.35s/it][A
 31%|███       | 1018/3305 [49:28<1:35:10,  2.50s/it][A
 31%|███       | 1019/3305 [49:34<2:15:00,  3.54s/it][A
 31%|███       | 1020/3305 [49:40<2:47:20,  4.39s/it][A
 31%|███       | 1021/3305 [49:43<2:27:08,  3.87s/it][A
 31%|███       | 1022/3305 [49:47<2:35:15,  4.08s/it][A
 31%|███       | 1023/3305 [49:52<2:38:44,  4.17s/it][A
 31%|███       | 1024/3305 [49:54<2:14:36,  3.54s/it][A
 31%|███       | 1025/3305 [49:56<1:54:09,  3.00s/it][A
 31%|███       | 1026/3305 [49:59<2:00:41,  3.18s/it][A
 31%|███       | 1027/3305 [50:05<2:28:00,  3.90s/it][A
 31%|███       | 1028/3305 [50:09<2:29:22,  3.94s/it][A
 31%|███       | 1029/3305 [50:12<2:18:12,  3.64s/it][A
 31%|███       | 1030/3305 [50:15<2:09:23,  3.41s/it][A
 31%|███       | 1031/3305 [50:17<1:57:44,  3.11s/it][A
 31%|███       | 1032/3305 [50:19<1:49:27,  2.89s/it][A
 31%|███▏      | 1033/3305 [50:22<1:44:43,  2.77s/it][A
 31%|███▏      | 1034/3305 [50:25<1:44:20,  2.76s/it][A
 31%|███▏      | 1035/3305 [50:29<2:03:06,  3.25s/it][A
 31%|███▏      | 1036/3305 [50:35<2:30:56,  3.99s/it][A
 31%|███▏      | 1037/3305 [50:38<2:27:24,  3.90s/it][A
 31%|███▏      | 1038/3305 [50:41<2:17:34,  3.64s/it][A
 31%|███▏      | 1039/3305 [50:46<2:31:26,  4.01s/it][A
 31%|███▏      | 1040/3305 [50:51<2:40:12,  4.24s/it][A
 31%|███▏      | 1041/3305 [50:54<2:30:13,  3.98s/it][A
 32%|███▏      | 1042/3305 [50:57<2:14:10,  3.56s/it][A
 32%|███▏      | 1043/3305 [51:00<2:12:32,  3.52s/it][A
 32%|███▏      | 1044/3305 [51:05<2:25:26,  3.86s/it][A
 32%|███▏      | 1045/3305 [51:08<2:18:28,  3.68s/it][A
 32%|███▏      | 1046/3305 [51:12<2:13:01,  3.53s/it][A
 32%|███▏      | 1047/3305 [51:14<2:00:07,  3.19s/it][A
 32%|███▏      | 1048/3305 [51:16<1:46:00,  2.82s/it][A
 32%|███▏      | 1049/3305 [51:21<2:11:24,  3.49s/it][A
 32%|███▏      | 1050/3305 [51:28<2:47:17,  4.45s/it][A
 32%|███▏      | 1051/3305 [51:31<2:34:49,  4.12s/it][A
 32%|███▏      | 1052/3305 [51:34<2:18:20,  3.68s/it][A
 32%|███▏      | 1053/3305 [51:36<2:00:43,  3.22s/it][A
 32%|███▏      | 1054/3305 [51:38<1:51:52,  2.98s/it][A
 32%|███▏      | 1055/3305 [51:41<1:48:02,  2.88s/it][A
 32%|███▏      | 1056/3305 [51:44<1:46:21,  2.84s/it][A
 32%|███▏      | 1057/3305 [51:46<1:39:01,  2.64s/it][A
 32%|███▏      | 1058/3305 [51:48<1:32:32,  2.47s/it][A
 32%|███▏      | 1059/3305 [51:50<1:31:08,  2.43s/it][A
 32%|███▏      | 1060/3305 [51:52<1:26:36,  2.31s/it][A
 32%|███▏      | 1061/3305 [51:55<1:28:30,  2.37s/it][A
 32%|███▏      | 1062/3305 [51:58<1:33:49,  2.51s/it][A
 32%|███▏      | 1063/3305 [51:59<1:25:40,  2.29s/it][A
 32%|███▏      | 1064/3305 [52:01<1:18:57,  2.11s/it][A
 32%|███▏      | 1065/3305 [52:03<1:20:34,  2.16s/it][A
 32%|███▏      | 1066/3305 [52:06<1:21:13,  2.18s/it][A
 32%|███▏      | 1067/3305 [52:08<1:21:16,  2.18s/it][A
 32%|███▏      | 1068/3305 [52:10<1:24:51,  2.28s/it][A
 32%|███▏      | 1069/3305 [52:13<1:29:01,  2.39s/it][A
 32%|███▏      | 1070/3305 [52:16<1:32:42,  2.49s/it][A
 32%|███▏      | 1071/3305 [52:19<1:39:02,  2.66s/it][A
 32%|███▏      | 1072/3305 [52:22<1:47:16,  2.88s/it][A
 32%|███▏      | 1073/3305 [52:25<1:46:21,  2.86s/it][A
 32%|███▏      | 1074/3305 [52:28<1:49:06,  2.93s/it][A
 33%|███▎      | 1075/3305 [52:31<1:51:30,  3.00s/it][A
 33%|███▎      | 1076/3305 [52:34<1:53:14,  3.05s/it][A
 33%|███▎      | 1077/3305 [52:38<1:58:02,  3.18s/it][A
 33%|███▎      | 1078/3305 [52:40<1:51:40,  3.01s/it][A
 33%|███▎      | 1079/3305 [52:45<2:06:13,  3.40s/it][A
 33%|███▎      | 1080/3305 [52:49<2:19:24,  3.76s/it][A
 33%|███▎      | 1081/3305 [52:52<2:03:59,  3.35s/it][A
 33%|███▎      | 1082/3305 [52:54<1:56:25,  3.14s/it][A
 33%|███▎      | 1083/3305 [52:57<1:45:52,  2.86s/it][A
 33%|███▎      | 1084/3305 [52:59<1:40:45,  2.72s/it][A
 33%|███▎      | 1085/3305 [53:02<1:41:10,  2.73s/it][A
 33%|███▎      | 1086/3305 [53:05<1:51:39,  3.02s/it][A
 33%|███▎      | 1087/3305 [53:09<1:55:47,  3.13s/it][A
 33%|███▎      | 1088/3305 [53:11<1:48:16,  2.93s/it][A
 33%|███▎      | 1089/3305 [53:14<1:48:54,  2.95s/it][A
 33%|███▎      | 1090/3305 [53:17<1:45:07,  2.85s/it][A
 33%|███▎      | 1091/3305 [53:19<1:38:17,  2.66s/it][A
 33%|███▎      | 1092/3305 [53:25<2:11:07,  3.55s/it][A
 33%|███▎      | 1093/3305 [53:32<2:53:13,  4.70s/it][A
 33%|███▎      | 1094/3305 [53:35<2:38:12,  4.29s/it][A
 33%|███▎      | 1095/3305 [53:38<2:18:17,  3.75s/it][A
 33%|███▎      | 1096/3305 [53:43<2:29:49,  4.07s/it][A
 33%|███▎      | 1097/3305 [53:48<2:42:50,  4.43s/it][A
 33%|███▎      | 1098/3305 [53:51<2:31:22,  4.12s/it][A
 33%|███▎      | 1099/3305 [53:56<2:34:50,  4.21s/it][A
 33%|███▎      | 1100/3305 [53:59<2:24:32,  3.93s/it][A
 33%|███▎      | 1101/3305 [54:01<2:03:03,  3.35s/it][A
 33%|███▎      | 1102/3305 [54:04<1:52:58,  3.08s/it][A
 33%|███▎      | 1103/3305 [54:07<1:57:41,  3.21s/it][A
 33%|███▎      | 1104/3305 [54:11<2:00:40,  3.29s/it][A
 33%|███▎      | 1105/3305 [54:14<1:58:00,  3.22s/it][A
 33%|███▎      | 1106/3305 [54:18<2:08:21,  3.50s/it][A
 33%|███▎      | 1107/3305 [54:21<2:06:24,  3.45s/it][A
 34%|███▎      | 1108/3305 [54:24<2:01:12,  3.31s/it][A
 34%|███▎      | 1109/3305 [54:27<2:00:12,  3.28s/it][A
 34%|███▎      | 1110/3305 [54:29<1:45:13,  2.88s/it][A
 34%|███▎      | 1111/3305 [54:31<1:36:40,  2.64s/it][A
 34%|███▎      | 1112/3305 [54:34<1:40:51,  2.76s/it][A
 34%|███▎      | 1113/3305 [54:37<1:35:43,  2.62s/it][A
 34%|███▎      | 1114/3305 [54:38<1:25:07,  2.33s/it][A
 34%|███▎      | 1115/3305 [54:41<1:28:55,  2.44s/it][A
 34%|███▍      | 1116/3305 [54:44<1:32:32,  2.54s/it][A
 34%|███▍      | 1117/3305 [54:46<1:31:10,  2.50s/it][A
 34%|███▍      | 1118/3305 [54:48<1:28:40,  2.43s/it][A
 34%|███▍      | 1119/3305 [54:51<1:31:39,  2.52s/it][A
 34%|███▍      | 1120/3305 [54:54<1:33:11,  2.56s/it][A
 34%|███▍      | 1121/3305 [54:56<1:27:32,  2.40s/it][A
 34%|███▍      | 1122/3305 [54:59<1:30:29,  2.49s/it][A
 34%|███▍      | 1123/3305 [55:01<1:34:11,  2.59s/it][A
 34%|███▍      | 1124/3305 [55:04<1:30:00,  2.48s/it][A
 34%|███▍      | 1125/3305 [55:06<1:25:14,  2.35s/it][A
 34%|███▍      | 1126/3305 [55:09<1:35:02,  2.62s/it][A
 34%|███▍      | 1127/3305 [55:12<1:45:51,  2.92s/it][A
 34%|███▍      | 1128/3305 [55:15<1:38:50,  2.72s/it][A
 34%|███▍      | 1129/3305 [55:18<1:41:09,  2.79s/it][A
 34%|███▍      | 1130/3305 [55:21<1:43:19,  2.85s/it][A
 34%|███▍      | 1131/3305 [55:22<1:31:10,  2.52s/it][A
 34%|███▍      | 1132/3305 [55:24<1:24:07,  2.32s/it][A
 34%|███▍      | 1133/3305 [55:26<1:16:06,  2.10s/it][A
 34%|███▍      | 1134/3305 [55:28<1:12:44,  2.01s/it][A
 34%|███▍      | 1135/3305 [55:31<1:26:31,  2.39s/it][A
 34%|███▍      | 1136/3305 [55:35<1:40:50,  2.79s/it][A
 34%|███▍      | 1137/3305 [55:37<1:36:15,  2.66s/it][A
 34%|███▍      | 1138/3305 [55:40<1:42:10,  2.83s/it][A
 34%|███▍      | 1139/3305 [55:44<1:49:48,  3.04s/it][A
 34%|███▍      | 1140/3305 [55:48<2:06:13,  3.50s/it][A
 35%|███▍      | 1141/3305 [55:54<2:26:47,  4.07s/it][A
 35%|███▍      | 1142/3305 [55:57<2:21:10,  3.92s/it][A
 35%|███▍      | 1143/3305 [56:00<2:09:43,  3.60s/it][A
 35%|███▍      | 1144/3305 [56:02<1:51:53,  3.11s/it][A
 35%|███▍      | 1145/3305 [56:04<1:40:39,  2.80s/it][A
 35%|███▍      | 1146/3305 [56:07<1:35:43,  2.66s/it][A
 35%|███▍      | 1147/3305 [56:09<1:30:12,  2.51s/it][A
 35%|███▍      | 1148/3305 [56:10<1:19:03,  2.20s/it][A
 35%|███▍      | 1149/3305 [56:12<1:19:19,  2.21s/it][A
 35%|███▍      | 1150/3305 [56:15<1:24:45,  2.36s/it][A
 35%|███▍      | 1151/3305 [56:18<1:30:05,  2.51s/it][A
 35%|███▍      | 1152/3305 [56:21<1:40:17,  2.79s/it][A
 35%|███▍      | 1153/3305 [56:25<1:43:12,  2.88s/it][A
 35%|███▍      | 1154/3305 [56:27<1:39:31,  2.78s/it][A
 35%|███▍      | 1155/3305 [56:29<1:35:04,  2.65s/it][A
 35%|███▍      | 1156/3305 [56:32<1:31:06,  2.54s/it][A
 35%|███▌      | 1157/3305 [56:34<1:26:45,  2.42s/it][A
 35%|███▌      | 1158/3305 [56:36<1:28:23,  2.47s/it][A
 35%|███▌      | 1159/3305 [56:40<1:39:26,  2.78s/it][A
 35%|███▌      | 1160/3305 [56:43<1:47:04,  3.00s/it][A
 35%|███▌      | 1161/3305 [56:46<1:40:52,  2.82s/it][A
 35%|███▌      | 1162/3305 [56:48<1:38:08,  2.75s/it][A
 35%|███▌      | 1163/3305 [56:51<1:35:51,  2.69s/it][A
 35%|███▌      | 1164/3305 [56:53<1:30:25,  2.53s/it][A
 35%|███▌      | 1165/3305 [56:56<1:28:39,  2.49s/it][A
 35%|███▌      | 1166/3305 [56:57<1:22:38,  2.32s/it][A
 35%|███▌      | 1167/3305 [57:00<1:26:02,  2.41s/it][A
 35%|███▌      | 1168/3305 [57:03<1:35:33,  2.68s/it][A
 35%|███▌      | 1169/3305 [57:07<1:40:18,  2.82s/it][A
 35%|███▌      | 1170/3305 [57:09<1:37:39,  2.74s/it][A
 35%|███▌      | 1171/3305 [57:12<1:36:39,  2.72s/it][A
 35%|███▌      | 1172/3305 [57:15<1:38:55,  2.78s/it][A
 35%|███▌      | 1173/3305 [57:17<1:31:21,  2.57s/it][A
 36%|███▌      | 1174/3305 [57:20<1:33:31,  2.63s/it][A
 36%|███▌      | 1175/3305 [57:23<1:37:00,  2.73s/it][A
 36%|███▌      | 1176/3305 [57:26<1:42:12,  2.88s/it][A
 36%|███▌      | 1177/3305 [57:29<1:47:53,  3.04s/it][A
 36%|███▌      | 1178/3305 [57:32<1:42:40,  2.90s/it][A
 36%|███▌      | 1179/3305 [57:35<1:41:40,  2.87s/it][A
 36%|███▌      | 1180/3305 [57:37<1:35:40,  2.70s/it][A
 36%|███▌      | 1181/3305 [57:40<1:41:38,  2.87s/it][A
 36%|███▌      | 1182/3305 [57:44<1:48:45,  3.07s/it][A
 36%|███▌      | 1183/3305 [57:46<1:40:02,  2.83s/it][A
 36%|███▌      | 1184/3305 [57:49<1:38:40,  2.79s/it][A
 36%|███▌      | 1185/3305 [57:52<1:46:04,  3.00s/it][A
 36%|███▌      | 1186/3305 [57:56<2:00:09,  3.40s/it][A
 36%|███▌      | 1187/3305 [58:00<1:58:20,  3.35s/it][A
 36%|███▌      | 1188/3305 [58:01<1:37:26,  2.76s/it][A
 36%|███▌      | 1189/3305 [58:04<1:33:48,  2.66s/it][A
 36%|███▌      | 1190/3305 [58:07<1:40:18,  2.85s/it][A
 36%|███▌      | 1191/3305 [58:09<1:36:31,  2.74s/it][A
 36%|███▌      | 1192/3305 [58:12<1:34:29,  2.68s/it][A
 36%|███▌      | 1193/3305 [58:14<1:30:31,  2.57s/it][A
 36%|███▌      | 1194/3305 [58:17<1:35:50,  2.72s/it][A
 36%|███▌      | 1195/3305 [58:20<1:34:49,  2.70s/it][A
 36%|███▌      | 1196/3305 [58:22<1:26:55,  2.47s/it][A
 36%|███▌      | 1197/3305 [58:25<1:29:39,  2.55s/it][A
 36%|███▌      | 1198/3305 [58:28<1:37:07,  2.77s/it][A
 36%|███▋      | 1199/3305 [58:30<1:34:40,  2.70s/it][A
 36%|███▋      | 1200/3305 [58:32<1:25:47,  2.45s/it][A
 36%|███▋      | 1201/3305 [58:34<1:21:34,  2.33s/it][A
 36%|███▋      | 1202/3305 [58:36<1:18:17,  2.23s/it][A
 36%|███▋      | 1203/3305 [58:38<1:16:16,  2.18s/it][A
 36%|███▋      | 1204/3305 [58:41<1:25:39,  2.45s/it][A
 36%|███▋      | 1205/3305 [58:45<1:34:11,  2.69s/it][A
 36%|███▋      | 1206/3305 [58:47<1:28:23,  2.53s/it][A
 37%|███▋      | 1207/3305 [58:49<1:25:54,  2.46s/it][A
 37%|███▋      | 1208/3305 [58:51<1:25:16,  2.44s/it][A
 37%|███▋      | 1209/3305 [58:54<1:20:58,  2.32s/it][A
 37%|███▋      | 1210/3305 [58:55<1:15:21,  2.16s/it][A
 37%|███▋      | 1211/3305 [58:57<1:14:51,  2.14s/it][A
 37%|███▋      | 1212/3305 [58:59<1:11:08,  2.04s/it][A
 37%|███▋      | 1213/3305 [59:01<1:11:15,  2.04s/it][A
 37%|███▋      | 1214/3305 [59:03<1:13:07,  2.10s/it][A
 37%|███▋      | 1215/3305 [59:06<1:15:40,  2.17s/it][A
 37%|███▋      | 1216/3305 [59:08<1:19:35,  2.29s/it][A
 37%|███▋      | 1217/3305 [59:11<1:24:14,  2.42s/it][A
 37%|███▋      | 1218/3305 [59:14<1:31:58,  2.64s/it][A
 37%|███▋      | 1219/3305 [59:18<1:39:51,  2.87s/it][A
 37%|███▋      | 1220/3305 [59:21<1:42:17,  2.94s/it][A
 37%|███▋      | 1221/3305 [59:25<1:51:16,  3.20s/it][A
 37%|███▋      | 1222/3305 [59:28<1:57:52,  3.40s/it][A
 37%|███▋      | 1223/3305 [59:30<1:42:39,  2.96s/it][A
 37%|███▋      | 1224/3305 [59:33<1:37:10,  2.80s/it][A
 37%|███▋      | 1225/3305 [59:35<1:34:34,  2.73s/it][A
 37%|███▋      | 1226/3305 [59:38<1:33:27,  2.70s/it][A
 37%|███▋      | 1227/3305 [59:41<1:39:09,  2.86s/it][A
 37%|███▋      | 1228/3305 [59:44<1:37:29,  2.82s/it][A
 37%|███▋      | 1229/3305 [59:46<1:29:47,  2.60s/it][A
 37%|███▋      | 1230/3305 [59:48<1:26:46,  2.51s/it][A
 37%|███▋      | 1231/3305 [59:51<1:29:33,  2.59s/it][A
 37%|███▋      | 1232/3305 [59:54<1:28:19,  2.56s/it][A
 37%|███▋      | 1233/3305 [59:56<1:28:10,  2.55s/it][A
 37%|███▋      | 1234/3305 [1:00:00<1:37:48,  2.83s/it][A
 37%|███▋      | 1235/3305 [1:00:02<1:37:37,  2.83s/it][A
 37%|███▋      | 1236/3305 [1:00:05<1:30:26,  2.62s/it][A
 37%|███▋      | 1237/3305 [1:00:07<1:30:15,  2.62s/it][A
 37%|███▋      | 1238/3305 [1:00:10<1:30:46,  2.64s/it][A
 37%|███▋      | 1239/3305 [1:00:12<1:26:11,  2.50s/it][A
 38%|███▊      | 1240/3305 [1:00:14<1:21:46,  2.38s/it][A
 38%|███▊      | 1241/3305 [1:00:16<1:17:43,  2.26s/it][A
 38%|███▊      | 1242/3305 [1:00:18<1:17:08,  2.24s/it][A
 38%|███▊      | 1243/3305 [1:00:21<1:19:57,  2.33s/it][A
 38%|███▊      | 1244/3305 [1:00:23<1:17:57,  2.27s/it][A
 38%|███▊      | 1245/3305 [1:00:26<1:20:44,  2.35s/it][A
 38%|███▊      | 1246/3305 [1:00:29<1:26:49,  2.53s/it][A
 38%|███▊      | 1247/3305 [1:00:31<1:28:35,  2.58s/it][A
 38%|███▊      | 1248/3305 [1:00:35<1:44:11,  3.04s/it][A
 38%|███▊      | 1249/3305 [1:00:40<2:02:59,  3.59s/it][A
 38%|███▊      | 1250/3305 [1:00:43<1:56:40,  3.41s/it][A
 38%|███▊      | 1251/3305 [1:00:45<1:43:26,  3.02s/it][A
 38%|███▊      | 1252/3305 [1:00:48<1:35:49,  2.80s/it][A
 38%|███▊      | 1253/3305 [1:00:50<1:28:56,  2.60s/it][A
 38%|███▊      | 1254/3305 [1:00:52<1:21:35,  2.39s/it][A
 38%|███▊      | 1255/3305 [1:00:54<1:17:19,  2.26s/it][A
 38%|███▊      | 1256/3305 [1:00:56<1:17:58,  2.28s/it][A
 38%|███▊      | 1257/3305 [1:00:58<1:16:35,  2.24s/it][A
 38%|███▊      | 1258/3305 [1:01:01<1:22:19,  2.41s/it][A
 38%|███▊      | 1259/3305 [1:01:05<1:39:30,  2.92s/it][A
 38%|███▊      | 1260/3305 [1:01:09<1:54:50,  3.37s/it][A
 38%|███▊      | 1261/3305 [1:01:13<1:58:21,  3.47s/it][A
 38%|███▊      | 1262/3305 [1:01:16<1:49:53,  3.23s/it][A
 38%|███▊      | 1263/3305 [1:01:19<1:46:30,  3.13s/it][A
 38%|███▊      | 1264/3305 [1:01:22<1:52:29,  3.31s/it][A
 38%|███▊      | 1265/3305 [1:01:27<2:04:24,  3.66s/it][A
 38%|███▊      | 1266/3305 [1:01:31<2:12:20,  3.89s/it][A
 38%|███▊      | 1267/3305 [1:01:34<2:01:58,  3.59s/it][A
 38%|███▊      | 1268/3305 [1:01:39<2:17:14,  4.04s/it][A
 38%|███▊      | 1269/3305 [1:01:44<2:27:23,  4.34s/it][A
 38%|███▊      | 1270/3305 [1:01:47<2:12:26,  3.90s/it][A
 38%|███▊      | 1271/3305 [1:01:51<2:06:47,  3.74s/it][A
 38%|███▊      | 1272/3305 [1:01:54<2:01:29,  3.59s/it][A
 39%|███▊      | 1273/3305 [1:01:56<1:52:34,  3.32s/it][A
 39%|███▊      | 1274/3305 [1:02:01<2:01:01,  3.58s/it][A
 39%|███▊      | 1275/3305 [1:02:05<2:07:07,  3.76s/it][A
 39%|███▊      | 1276/3305 [1:02:08<1:59:38,  3.54s/it][A
 39%|███▊      | 1277/3305 [1:02:11<1:50:54,  3.28s/it][A
 39%|███▊      | 1278/3305 [1:02:14<1:50:44,  3.28s/it][A
 39%|███▊      | 1279/3305 [1:02:17<1:46:23,  3.15s/it][A
 39%|███▊      | 1280/3305 [1:02:19<1:33:02,  2.76s/it][A
 39%|███▉      | 1281/3305 [1:02:21<1:25:22,  2.53s/it][A
 39%|███▉      | 1282/3305 [1:02:22<1:16:26,  2.27s/it][A
 39%|███▉      | 1283/3305 [1:02:25<1:19:11,  2.35s/it][A
 39%|███▉      | 1284/3305 [1:02:28<1:32:11,  2.74s/it][A
 39%|███▉      | 1285/3305 [1:02:32<1:42:37,  3.05s/it][A
 39%|███▉      | 1286/3305 [1:02:37<1:56:06,  3.45s/it][A
 39%|███▉      | 1287/3305 [1:02:40<2:00:07,  3.57s/it][A
 39%|███▉      | 1288/3305 [1:02:44<2:04:14,  3.70s/it][A
 39%|███▉      | 1289/3305 [1:02:47<1:52:36,  3.35s/it][A
 39%|███▉      | 1290/3305 [1:02:49<1:37:28,  2.90s/it][A
 39%|███▉      | 1291/3305 [1:02:51<1:26:20,  2.57s/it][A
 39%|███▉      | 1292/3305 [1:02:53<1:19:58,  2.38s/it][A
 39%|███▉      | 1293/3305 [1:02:55<1:17:32,  2.31s/it][A
 39%|███▉      | 1294/3305 [1:02:59<1:36:25,  2.88s/it][A
 39%|███▉      | 1295/3305 [1:03:04<1:57:22,  3.50s/it][A
 39%|███▉      | 1296/3305 [1:03:06<1:46:13,  3.17s/it][A
 39%|███▉      | 1297/3305 [1:03:08<1:37:05,  2.90s/it][A
 39%|███▉      | 1298/3305 [1:03:11<1:35:17,  2.85s/it][A
 39%|███▉      | 1299/3305 [1:03:14<1:37:29,  2.92s/it][A
 39%|███▉      | 1300/3305 [1:03:18<1:42:50,  3.08s/it][A
 39%|███▉      | 1301/3305 [1:03:21<1:42:30,  3.07s/it][A
 39%|███▉      | 1302/3305 [1:03:24<1:39:33,  2.98s/it][A
 39%|███▉      | 1303/3305 [1:03:26<1:30:45,  2.72s/it][A
 39%|███▉      | 1304/3305 [1:03:28<1:22:12,  2.47s/it][A
 39%|███▉      | 1305/3305 [1:03:30<1:22:48,  2.48s/it][A
 40%|███▉      | 1306/3305 [1:03:33<1:25:44,  2.57s/it][A
 40%|███▉      | 1307/3305 [1:03:37<1:37:56,  2.94s/it][A
 40%|███▉      | 1308/3305 [1:03:40<1:46:51,  3.21s/it][A
 40%|███▉      | 1309/3305 [1:03:42<1:32:17,  2.77s/it][A
 40%|███▉      | 1310/3305 [1:03:44<1:26:27,  2.60s/it][A
 40%|███▉      | 1311/3305 [1:03:47<1:26:33,  2.60s/it][A
 40%|███▉      | 1312/3305 [1:03:50<1:30:21,  2.72s/it][A
 40%|███▉      | 1313/3305 [1:03:53<1:36:12,  2.90s/it][A
 40%|███▉      | 1314/3305 [1:03:58<1:54:33,  3.45s/it][A
 40%|███▉      | 1315/3305 [1:04:04<2:21:45,  4.27s/it][A
 40%|███▉      | 1316/3305 [1:04:09<2:22:15,  4.29s/it][A
 40%|███▉      | 1317/3305 [1:04:11<2:05:25,  3.79s/it][A
 40%|███▉      | 1318/3305 [1:04:15<2:07:14,  3.84s/it][A
 40%|███▉      | 1319/3305 [1:04:19<2:09:04,  3.90s/it][A
 40%|███▉      | 1320/3305 [1:04:21<1:48:11,  3.27s/it][A
 40%|███▉      | 1321/3305 [1:04:23<1:36:00,  2.90s/it][A
 40%|████      | 1322/3305 [1:04:25<1:28:59,  2.69s/it][A
 40%|████      | 1323/3305 [1:04:30<1:44:49,  3.17s/it][A
 40%|████      | 1324/3305 [1:04:34<1:56:23,  3.53s/it][A
 40%|████      | 1325/3305 [1:04:36<1:42:46,  3.11s/it][A
 40%|████      | 1326/3305 [1:04:39<1:36:08,  2.91s/it][A
 40%|████      | 1327/3305 [1:04:41<1:28:37,  2.69s/it][A
 40%|████      | 1328/3305 [1:04:43<1:29:43,  2.72s/it][A
 40%|████      | 1329/3305 [1:04:46<1:29:04,  2.70s/it][A
 40%|████      | 1330/3305 [1:04:48<1:23:22,  2.53s/it][A
 40%|████      | 1331/3305 [1:04:52<1:39:21,  3.02s/it][A
 40%|████      | 1332/3305 [1:04:57<1:54:41,  3.49s/it][A
 40%|████      | 1333/3305 [1:05:00<1:46:13,  3.23s/it][A
 40%|████      | 1334/3305 [1:05:03<1:47:39,  3.28s/it][A
 40%|████      | 1335/3305 [1:05:06<1:47:14,  3.27s/it][A
 40%|████      | 1336/3305 [1:05:09<1:40:48,  3.07s/it][A
 40%|████      | 1337/3305 [1:05:11<1:30:16,  2.75s/it][A
 40%|████      | 1338/3305 [1:05:14<1:36:36,  2.95s/it][A
 41%|████      | 1339/3305 [1:05:18<1:47:24,  3.28s/it][A
 41%|████      | 1340/3305 [1:05:21<1:42:05,  3.12s/it][A
 41%|████      | 1341/3305 [1:05:23<1:30:43,  2.77s/it][A
 41%|████      | 1342/3305 [1:05:25<1:18:55,  2.41s/it][A
 41%|████      | 1343/3305 [1:05:26<1:10:54,  2.17s/it][A
 41%|████      | 1344/3305 [1:05:28<1:06:25,  2.03s/it][A
 41%|████      | 1345/3305 [1:05:30<1:04:52,  1.99s/it][A
 41%|████      | 1346/3305 [1:05:32<1:07:26,  2.07s/it][A
 41%|████      | 1347/3305 [1:05:36<1:25:13,  2.61s/it][A
 41%|████      | 1348/3305 [1:05:40<1:38:59,  3.03s/it][A
 41%|████      | 1349/3305 [1:05:43<1:41:14,  3.11s/it][A
 41%|████      | 1350/3305 [1:05:46<1:38:47,  3.03s/it][A
 41%|████      | 1351/3305 [1:05:48<1:27:39,  2.69s/it][A
 41%|████      | 1352/3305 [1:05:50<1:25:03,  2.61s/it][A
 41%|████      | 1353/3305 [1:05:53<1:28:37,  2.72s/it][A
 41%|████      | 1354/3305 [1:05:56<1:28:44,  2.73s/it][A
 41%|████      | 1355/3305 [1:05:59<1:26:39,  2.67s/it][A
 41%|████      | 1356/3305 [1:06:02<1:29:28,  2.75s/it][A
 41%|████      | 1357/3305 [1:06:04<1:27:55,  2.71s/it][A
 41%|████      | 1358/3305 [1:06:07<1:25:22,  2.63s/it][A
 41%|████      | 1359/3305 [1:06:09<1:24:01,  2.59s/it][A
 41%|████      | 1360/3305 [1:06:13<1:33:41,  2.89s/it][A
 41%|████      | 1361/3305 [1:06:16<1:35:27,  2.95s/it][A
 41%|████      | 1362/3305 [1:06:18<1:27:35,  2.70s/it][A
 41%|████      | 1363/3305 [1:06:21<1:29:16,  2.76s/it][A
 41%|████▏     | 1364/3305 [1:06:24<1:33:40,  2.90s/it][A
 41%|████▏     | 1365/3305 [1:06:27<1:33:10,  2.88s/it][A
 41%|████▏     | 1366/3305 [1:06:30<1:35:46,  2.96s/it][A
 41%|████▏     | 1367/3305 [1:06:33<1:36:09,  2.98s/it][A
 41%|████▏     | 1368/3305 [1:06:35<1:24:10,  2.61s/it][A
 41%|████▏     | 1369/3305 [1:06:37<1:24:03,  2.61s/it][A
 41%|████▏     | 1370/3305 [1:06:40<1:27:29,  2.71s/it][A
 41%|████▏     | 1371/3305 [1:06:42<1:20:27,  2.50s/it][A
 42%|████▏     | 1372/3305 [1:06:44<1:11:05,  2.21s/it][A
 42%|████▏     | 1373/3305 [1:06:46<1:07:56,  2.11s/it][A
 42%|████▏     | 1374/3305 [1:06:49<1:16:21,  2.37s/it][A
 42%|████▏     | 1375/3305 [1:06:53<1:30:25,  2.81s/it][A
 42%|████▏     | 1376/3305 [1:06:58<1:54:47,  3.57s/it][A
 42%|████▏     | 1377/3305 [1:07:03<2:06:17,  3.93s/it][A
 42%|████▏     | 1378/3305 [1:07:05<1:52:51,  3.51s/it][A
 42%|████▏     | 1379/3305 [1:07:08<1:47:02,  3.33s/it][A
 42%|████▏     | 1380/3305 [1:07:12<1:52:32,  3.51s/it][A
 42%|████▏     | 1381/3305 [1:07:15<1:45:25,  3.29s/it][A
 42%|████▏     | 1382/3305 [1:07:17<1:28:59,  2.78s/it][A
 42%|████▏     | 1383/3305 [1:07:18<1:19:25,  2.48s/it][A
 42%|████▏     | 1384/3305 [1:07:21<1:21:24,  2.54s/it][A
 42%|████▏     | 1385/3305 [1:07:24<1:26:04,  2.69s/it][A
 42%|████▏     | 1386/3305 [1:07:26<1:19:08,  2.47s/it][A
 42%|████▏     | 1387/3305 [1:07:28<1:19:26,  2.49s/it][A
 42%|████▏     | 1388/3305 [1:07:31<1:18:34,  2.46s/it][A
 42%|████▏     | 1389/3305 [1:07:33<1:14:55,  2.35s/it][A
 42%|████▏     | 1390/3305 [1:07:35<1:15:49,  2.38s/it][A
 42%|████▏     | 1391/3305 [1:07:38<1:22:09,  2.58s/it][A
 42%|████▏     | 1392/3305 [1:07:42<1:32:09,  2.89s/it][A
 42%|████▏     | 1393/3305 [1:07:45<1:29:17,  2.80s/it][A
 42%|████▏     | 1394/3305 [1:07:47<1:26:31,  2.72s/it][A
 42%|████▏     | 1395/3305 [1:07:49<1:20:10,  2.52s/it][A
 42%|████▏     | 1396/3305 [1:07:53<1:29:54,  2.83s/it][A
 42%|████▏     | 1397/3305 [1:07:57<1:45:26,  3.32s/it][A
 42%|████▏     | 1398/3305 [1:08:00<1:38:06,  3.09s/it][A
 42%|████▏     | 1399/3305 [1:08:03<1:35:22,  3.00s/it][A
 42%|████▏     | 1400/3305 [1:08:05<1:28:50,  2.80s/it][A
 42%|████▏     | 1401/3305 [1:08:08<1:31:09,  2.87s/it][A
 42%|████▏     | 1402/3305 [1:08:11<1:35:44,  3.02s/it][A
 42%|████▏     | 1403/3305 [1:08:14<1:29:04,  2.81s/it][A
 42%|████▏     | 1404/3305 [1:08:16<1:23:57,  2.65s/it][A
 43%|████▎     | 1405/3305 [1:08:19<1:25:06,  2.69s/it][A
 43%|████▎     | 1406/3305 [1:08:21<1:21:50,  2.59s/it][A
 43%|████▎     | 1407/3305 [1:08:24<1:23:59,  2.65s/it][A
 43%|████▎     | 1408/3305 [1:08:28<1:37:42,  3.09s/it][A
 43%|████▎     | 1409/3305 [1:08:31<1:36:17,  3.05s/it][A
 43%|████▎     | 1410/3305 [1:08:33<1:28:09,  2.79s/it][A
 43%|████▎     | 1411/3305 [1:08:36<1:26:45,  2.75s/it][A
 43%|████▎     | 1412/3305 [1:08:39<1:28:49,  2.82s/it][A
 43%|████▎     | 1413/3305 [1:08:41<1:27:13,  2.77s/it][A
 43%|████▎     | 1414/3305 [1:08:44<1:28:52,  2.82s/it][A
 43%|████▎     | 1415/3305 [1:08:48<1:33:56,  2.98s/it][A
 43%|████▎     | 1416/3305 [1:08:51<1:41:10,  3.21s/it][A
 43%|████▎     | 1417/3305 [1:08:55<1:44:50,  3.33s/it][A
 43%|████▎     | 1418/3305 [1:08:58<1:41:44,  3.24s/it][A
 43%|████▎     | 1419/3305 [1:09:01<1:38:06,  3.12s/it][A
 43%|████▎     | 1420/3305 [1:09:03<1:25:40,  2.73s/it][A
 43%|████▎     | 1421/3305 [1:09:07<1:35:49,  3.05s/it][A
 43%|████▎     | 1422/3305 [1:09:11<1:50:43,  3.53s/it][A
 43%|████▎     | 1423/3305 [1:09:15<1:49:16,  3.48s/it][A
 43%|████▎     | 1424/3305 [1:09:19<1:55:49,  3.69s/it][A
 43%|████▎     | 1425/3305 [1:09:22<1:48:55,  3.48s/it][A
 43%|████▎     | 1426/3305 [1:09:24<1:40:01,  3.19s/it][A
 43%|████▎     | 1427/3305 [1:09:28<1:46:57,  3.42s/it][A
 43%|████▎     | 1428/3305 [1:09:32<1:49:19,  3.49s/it][A
 43%|████▎     | 1429/3305 [1:09:34<1:35:43,  3.06s/it][A
 43%|████▎     | 1430/3305 [1:09:37<1:31:49,  2.94s/it][A
 43%|████▎     | 1431/3305 [1:09:39<1:29:08,  2.85s/it][A
 43%|████▎     | 1432/3305 [1:09:42<1:24:20,  2.70s/it][A
 43%|████▎     | 1433/3305 [1:09:44<1:22:49,  2.65s/it][A
 43%|████▎     | 1434/3305 [1:09:46<1:18:13,  2.51s/it][A
 43%|████▎     | 1435/3305 [1:09:48<1:12:17,  2.32s/it][A
 43%|████▎     | 1436/3305 [1:09:51<1:15:07,  2.41s/it][A
 43%|████▎     | 1437/3305 [1:09:54<1:22:55,  2.66s/it][A
 44%|████▎     | 1438/3305 [1:09:57<1:23:50,  2.69s/it][A
 44%|████▎     | 1439/3305 [1:10:00<1:32:22,  2.97s/it][A
 44%|████▎     | 1440/3305 [1:10:04<1:41:40,  3.27s/it][A
 44%|████▎     | 1441/3305 [1:10:08<1:40:28,  3.23s/it][A
 44%|████▎     | 1442/3305 [1:10:11<1:39:05,  3.19s/it][A
 44%|████▎     | 1443/3305 [1:10:15<1:53:07,  3.65s/it][A
 44%|████▎     | 1444/3305 [1:10:18<1:43:30,  3.34s/it][A
 44%|████▎     | 1445/3305 [1:10:20<1:29:03,  2.87s/it][A
 44%|████▍     | 1446/3305 [1:10:21<1:16:55,  2.48s/it][A
 44%|████▍     | 1447/3305 [1:10:23<1:11:28,  2.31s/it][A
 44%|████▍     | 1448/3305 [1:10:25<1:08:04,  2.20s/it][A
 44%|████▍     | 1449/3305 [1:10:28<1:10:40,  2.28s/it][A
 44%|████▍     | 1450/3305 [1:10:31<1:19:25,  2.57s/it][A
 44%|████▍     | 1451/3305 [1:10:33<1:17:53,  2.52s/it][A
 44%|████▍     | 1452/3305 [1:10:36<1:18:39,  2.55s/it][A
 44%|████▍     | 1453/3305 [1:10:40<1:32:48,  3.01s/it][A
 44%|████▍     | 1454/3305 [1:10:44<1:40:48,  3.27s/it][A
 44%|████▍     | 1455/3305 [1:10:46<1:28:05,  2.86s/it][A
 44%|████▍     | 1456/3305 [1:10:47<1:15:42,  2.46s/it][A
 44%|████▍     | 1457/3305 [1:10:49<1:12:08,  2.34s/it][A
 44%|████▍     | 1458/3305 [1:10:52<1:14:36,  2.42s/it][A
 44%|████▍     | 1459/3305 [1:10:55<1:15:53,  2.47s/it][A
 44%|████▍     | 1460/3305 [1:10:58<1:21:49,  2.66s/it][A
 44%|████▍     | 1461/3305 [1:11:01<1:29:02,  2.90s/it][A
 44%|████▍     | 1462/3305 [1:11:03<1:19:52,  2.60s/it][A
 44%|████▍     | 1463/3305 [1:11:05<1:13:30,  2.39s/it][A
 44%|████▍     | 1464/3305 [1:11:09<1:24:28,  2.75s/it][A
 44%|████▍     | 1465/3305 [1:11:15<1:54:17,  3.73s/it][A
 44%|████▍     | 1466/3305 [1:11:20<2:10:45,  4.27s/it][A
 44%|████▍     | 1467/3305 [1:11:25<2:16:47,  4.47s/it][A
 44%|████▍     | 1468/3305 [1:11:29<2:08:41,  4.20s/it][A
 44%|████▍     | 1469/3305 [1:11:31<1:48:32,  3.55s/it][A
 44%|████▍     | 1470/3305 [1:11:32<1:31:15,  2.98s/it][A
 45%|████▍     | 1471/3305 [1:11:35<1:25:23,  2.79s/it][A
 45%|████▍     | 1472/3305 [1:11:37<1:22:04,  2.69s/it][A
 45%|████▍     | 1473/3305 [1:11:39<1:16:29,  2.51s/it][A
 45%|████▍     | 1474/3305 [1:11:42<1:21:06,  2.66s/it][A
 45%|████▍     | 1475/3305 [1:11:45<1:23:23,  2.73s/it][A
 45%|████▍     | 1476/3305 [1:11:48<1:23:15,  2.73s/it][A
 45%|████▍     | 1477/3305 [1:11:51<1:24:31,  2.77s/it][A
 45%|████▍     | 1478/3305 [1:11:54<1:25:53,  2.82s/it][A
 45%|████▍     | 1479/3305 [1:11:56<1:26:46,  2.85s/it][A
 45%|████▍     | 1480/3305 [1:11:59<1:19:59,  2.63s/it][A
 45%|████▍     | 1481/3305 [1:12:02<1:25:33,  2.81s/it][A
 45%|████▍     | 1482/3305 [1:12:06<1:34:29,  3.11s/it][A
 45%|████▍     | 1483/3305 [1:12:08<1:25:53,  2.83s/it][A
 45%|████▍     | 1484/3305 [1:12:10<1:15:26,  2.49s/it][A
 45%|████▍     | 1485/3305 [1:12:12<1:14:15,  2.45s/it][A
 45%|████▍     | 1486/3305 [1:12:16<1:25:17,  2.81s/it][A
 45%|████▍     | 1487/3305 [1:12:19<1:27:00,  2.87s/it][A
 45%|████▌     | 1488/3305 [1:12:21<1:23:26,  2.76s/it][A
 45%|████▌     | 1489/3305 [1:12:24<1:25:50,  2.84s/it][A
 45%|████▌     | 1490/3305 [1:12:27<1:29:24,  2.96s/it][A
 45%|████▌     | 1491/3305 [1:12:30<1:27:54,  2.91s/it][A
 45%|████▌     | 1492/3305 [1:12:35<1:42:44,  3.40s/it][A
 45%|████▌     | 1493/3305 [1:12:39<1:55:42,  3.83s/it][A
 45%|████▌     | 1494/3305 [1:12:43<1:57:16,  3.89s/it][A
 45%|████▌     | 1495/3305 [1:12:48<2:02:17,  4.05s/it][A
 45%|████▌     | 1496/3305 [1:12:52<2:03:30,  4.10s/it][A
 45%|████▌     | 1497/3305 [1:12:58<2:22:10,  4.72s/it][A
 45%|████▌     | 1498/3305 [1:13:02<2:16:22,  4.53s/it][A
 45%|████▌     | 1499/3305 [1:13:05<1:57:54,  3.92s/it][A
 45%|████▌     | 1500/3305 [1:13:12<2:26:31,  4.87s/it][A
 45%|████▌     | 1501/3305 [1:13:20<2:51:10,  5.69s/it][A
 45%|████▌     | 1502/3305 [1:13:22<2:24:18,  4.80s/it][A
 45%|████▌     | 1503/3305 [1:13:26<2:17:07,  4.57s/it][A
 46%|████▌     | 1504/3305 [1:13:31<2:17:11,  4.57s/it][A
 46%|████▌     | 1505/3305 [1:13:33<1:56:18,  3.88s/it][A
 46%|████▌     | 1506/3305 [1:13:36<1:43:46,  3.46s/it][A
 46%|████▌     | 1507/3305 [1:13:40<1:50:49,  3.70s/it][A
 46%|████▌     | 1508/3305 [1:13:44<1:54:07,  3.81s/it][A
 46%|████▌     | 1509/3305 [1:13:47<1:43:40,  3.46s/it][A
 46%|████▌     | 1510/3305 [1:13:49<1:38:19,  3.29s/it][A
 46%|████▌     | 1511/3305 [1:13:52<1:31:18,  3.05s/it][A
 46%|████▌     | 1512/3305 [1:13:55<1:28:36,  2.97s/it][A
 46%|████▌     | 1513/3305 [1:14:00<1:50:34,  3.70s/it][A
 46%|████▌     | 1514/3305 [1:14:08<2:23:44,  4.82s/it][A
 46%|████▌     | 1515/3305 [1:14:11<2:14:20,  4.50s/it][A
 46%|████▌     | 1516/3305 [1:14:15<2:03:36,  4.15s/it][A
 46%|████▌     | 1517/3305 [1:14:20<2:13:58,  4.50s/it][A
 46%|████▌     | 1518/3305 [1:14:26<2:24:58,  4.87s/it][A
 46%|████▌     | 1519/3305 [1:14:28<2:05:09,  4.20s/it][A
 46%|████▌     | 1520/3305 [1:14:31<1:51:42,  3.75s/it][A
 46%|████▌     | 1521/3305 [1:14:35<1:51:09,  3.74s/it][A
 46%|████▌     | 1522/3305 [1:14:38<1:50:47,  3.73s/it][A
 46%|████▌     | 1523/3305 [1:14:42<1:46:59,  3.60s/it][A
 46%|████▌     | 1524/3305 [1:14:44<1:35:20,  3.21s/it][A
 46%|████▌     | 1525/3305 [1:14:47<1:32:04,  3.10s/it][A
 46%|████▌     | 1526/3305 [1:14:50<1:34:17,  3.18s/it][A
 46%|████▌     | 1527/3305 [1:14:53<1:33:55,  3.17s/it][A
 46%|████▌     | 1528/3305 [1:14:56<1:30:27,  3.05s/it][A
 46%|████▋     | 1529/3305 [1:14:59<1:27:10,  2.95s/it][A
 46%|████▋     | 1530/3305 [1:15:02<1:27:48,  2.97s/it][A
 46%|████▋     | 1531/3305 [1:15:06<1:37:37,  3.30s/it][A
 46%|████▋     | 1532/3305 [1:15:11<1:53:36,  3.84s/it][A
 46%|████▋     | 1533/3305 [1:15:15<1:50:36,  3.75s/it][A
 46%|████▋     | 1534/3305 [1:15:18<1:44:03,  3.53s/it][A
 46%|████▋     | 1535/3305 [1:15:20<1:31:47,  3.11s/it][A
 46%|████▋     | 1536/3305 [1:15:22<1:25:39,  2.91s/it][A
 47%|████▋     | 1537/3305 [1:15:26<1:28:58,  3.02s/it][A
 47%|████▋     | 1538/3305 [1:15:28<1:21:47,  2.78s/it][A
 47%|████▋     | 1539/3305 [1:15:30<1:14:49,  2.54s/it][A
 47%|████▋     | 1540/3305 [1:15:32<1:12:57,  2.48s/it][A
 47%|████▋     | 1541/3305 [1:15:35<1:12:53,  2.48s/it][A
 47%|████▋     | 1542/3305 [1:15:37<1:13:30,  2.50s/it][A
 47%|████▋     | 1543/3305 [1:15:40<1:20:08,  2.73s/it][A
 47%|████▋     | 1544/3305 [1:15:44<1:29:06,  3.04s/it][A
 47%|████▋     | 1545/3305 [1:15:48<1:34:11,  3.21s/it][A
 47%|████▋     | 1546/3305 [1:15:50<1:29:14,  3.04s/it][A
 47%|████▋     | 1547/3305 [1:15:52<1:18:15,  2.67s/it][A
 47%|████▋     | 1548/3305 [1:15:55<1:17:36,  2.65s/it][A
 47%|████▋     | 1549/3305 [1:15:59<1:27:07,  2.98s/it][A
 47%|████▋     | 1550/3305 [1:16:02<1:31:57,  3.14s/it][A
 47%|████▋     | 1551/3305 [1:16:05<1:27:10,  2.98s/it][A
 47%|████▋     | 1552/3305 [1:16:08<1:31:02,  3.12s/it][A
 47%|████▋     | 1553/3305 [1:16:11<1:32:42,  3.17s/it][A
 47%|████▋     | 1554/3305 [1:16:14<1:31:41,  3.14s/it][A
 47%|████▋     | 1555/3305 [1:16:18<1:34:53,  3.25s/it][A
 47%|████▋     | 1556/3305 [1:16:20<1:24:45,  2.91s/it][A
 47%|████▋     | 1557/3305 [1:16:23<1:24:03,  2.89s/it][A
 47%|████▋     | 1558/3305 [1:16:26<1:24:33,  2.90s/it][A
 47%|████▋     | 1559/3305 [1:16:29<1:24:43,  2.91s/it][A
 47%|████▋     | 1560/3305 [1:16:32<1:28:23,  3.04s/it][A
 47%|████▋     | 1561/3305 [1:16:34<1:17:44,  2.67s/it][A
 47%|████▋     | 1562/3305 [1:16:36<1:12:37,  2.50s/it][A
 47%|████▋     | 1563/3305 [1:16:39<1:17:47,  2.68s/it][A
 47%|████▋     | 1564/3305 [1:16:42<1:15:13,  2.59s/it][A
 47%|████▋     | 1565/3305 [1:16:43<1:08:36,  2.37s/it][A
 47%|████▋     | 1566/3305 [1:16:46<1:09:07,  2.38s/it][A
 47%|████▋     | 1567/3305 [1:16:48<1:10:11,  2.42s/it][A
 47%|████▋     | 1568/3305 [1:16:50<1:03:15,  2.19s/it][A
 47%|████▋     | 1569/3305 [1:16:53<1:13:00,  2.52s/it][A
 48%|████▊     | 1570/3305 [1:16:57<1:25:05,  2.94s/it][A
 48%|████▊     | 1571/3305 [1:17:00<1:22:19,  2.85s/it][A
 48%|████▊     | 1572/3305 [1:17:03<1:26:41,  3.00s/it][A
 48%|████▊     | 1573/3305 [1:17:06<1:22:17,  2.85s/it][A
 48%|████▊     | 1574/3305 [1:17:08<1:21:57,  2.84s/it][A
 48%|████▊     | 1575/3305 [1:17:13<1:34:41,  3.28s/it][A
 48%|████▊     | 1576/3305 [1:17:17<1:43:09,  3.58s/it][A
 48%|████▊     | 1577/3305 [1:17:20<1:38:59,  3.44s/it][A
 48%|████▊     | 1578/3305 [1:17:22<1:28:04,  3.06s/it][A
 48%|████▊     | 1579/3305 [1:17:26<1:32:14,  3.21s/it][A
 48%|████▊     | 1580/3305 [1:17:31<1:47:42,  3.75s/it][A
 48%|████▊     | 1581/3305 [1:17:34<1:41:00,  3.52s/it][A
 48%|████▊     | 1582/3305 [1:17:38<1:44:19,  3.63s/it][A
 48%|████▊     | 1583/3305 [1:17:43<1:54:32,  3.99s/it][A
 48%|████▊     | 1584/3305 [1:17:45<1:44:41,  3.65s/it][A
 48%|████▊     | 1585/3305 [1:17:48<1:34:45,  3.31s/it][A
 48%|████▊     | 1586/3305 [1:17:50<1:27:37,  3.06s/it][A
 48%|████▊     | 1587/3305 [1:17:53<1:19:11,  2.77s/it][A
 48%|████▊     | 1588/3305 [1:17:55<1:14:53,  2.62s/it][A
 48%|████▊     | 1589/3305 [1:17:57<1:13:56,  2.59s/it][A
 48%|████▊     | 1590/3305 [1:18:00<1:15:49,  2.65s/it][A
 48%|████▊     | 1591/3305 [1:18:04<1:25:18,  2.99s/it][A
 48%|████▊     | 1592/3305 [1:18:07<1:25:08,  2.98s/it][A
 48%|████▊     | 1593/3305 [1:18:10<1:22:34,  2.89s/it][A
 48%|████▊     | 1594/3305 [1:18:13<1:23:28,  2.93s/it][A
 48%|████▊     | 1595/3305 [1:18:15<1:16:07,  2.67s/it][A
 48%|████▊     | 1596/3305 [1:18:18<1:24:15,  2.96s/it][A
 48%|████▊     | 1597/3305 [1:18:21<1:18:23,  2.75s/it][A
 48%|████▊     | 1598/3305 [1:18:23<1:16:17,  2.68s/it][A
 48%|████▊     | 1599/3305 [1:18:26<1:18:55,  2.78s/it][A
 48%|████▊     | 1600/3305 [1:18:28<1:13:42,  2.59s/it][A
 48%|████▊     | 1601/3305 [1:18:31<1:14:46,  2.63s/it][A
 48%|████▊     | 1602/3305 [1:18:35<1:27:05,  3.07s/it][A
 49%|████▊     | 1603/3305 [1:18:40<1:46:24,  3.75s/it][A
 49%|████▊     | 1604/3305 [1:18:44<1:47:14,  3.78s/it][A
 49%|████▊     | 1605/3305 [1:18:46<1:31:33,  3.23s/it][A
 49%|████▊     | 1606/3305 [1:18:49<1:26:43,  3.06s/it][A
 49%|████▊     | 1607/3305 [1:18:52<1:27:52,  3.11s/it][A
 49%|████▊     | 1608/3305 [1:18:55<1:25:23,  3.02s/it][A
 49%|████▊     | 1609/3305 [1:18:59<1:32:16,  3.26s/it][A
 49%|████▊     | 1610/3305 [1:19:03<1:43:21,  3.66s/it][A
 49%|████▊     | 1611/3305 [1:19:06<1:34:28,  3.35s/it][A
 49%|████▉     | 1612/3305 [1:19:09<1:31:28,  3.24s/it][A
 49%|████▉     | 1613/3305 [1:19:11<1:25:59,  3.05s/it][A
 49%|████▉     | 1614/3305 [1:19:14<1:17:25,  2.75s/it][A
 49%|████▉     | 1615/3305 [1:19:15<1:10:41,  2.51s/it][A
 49%|████▉     | 1616/3305 [1:19:17<1:06:19,  2.36s/it][A
 49%|████▉     | 1617/3305 [1:19:20<1:04:00,  2.28s/it][A
 49%|████▉     | 1618/3305 [1:19:22<1:04:15,  2.29s/it][A
 49%|████▉     | 1619/3305 [1:19:24<1:04:31,  2.30s/it][A
 49%|████▉     | 1620/3305 [1:19:27<1:08:04,  2.42s/it][A
 49%|████▉     | 1621/3305 [1:19:29<1:07:09,  2.39s/it][A
 49%|████▉     | 1622/3305 [1:19:33<1:15:17,  2.68s/it][A
 49%|████▉     | 1623/3305 [1:19:37<1:27:06,  3.11s/it][A
 49%|████▉     | 1624/3305 [1:19:40<1:27:45,  3.13s/it][A
 49%|████▉     | 1625/3305 [1:19:43<1:28:05,  3.15s/it][A
 49%|████▉     | 1626/3305 [1:19:45<1:21:48,  2.92s/it][A
 49%|████▉     | 1627/3305 [1:19:48<1:14:28,  2.66s/it][A
 49%|████▉     | 1628/3305 [1:19:50<1:14:26,  2.66s/it][A
 49%|████▉     | 1629/3305 [1:19:52<1:10:19,  2.52s/it][A
 49%|████▉     | 1630/3305 [1:19:55<1:10:28,  2.52s/it][A
 49%|████▉     | 1631/3305 [1:19:58<1:12:35,  2.60s/it][A
 49%|████▉     | 1632/3305 [1:20:00<1:07:25,  2.42s/it][A
 49%|████▉     | 1633/3305 [1:20:02<1:09:42,  2.50s/it][A
 49%|████▉     | 1634/3305 [1:20:06<1:15:13,  2.70s/it][A
 49%|████▉     | 1635/3305 [1:20:08<1:11:19,  2.56s/it][A
 50%|████▉     | 1636/3305 [1:20:10<1:09:08,  2.49s/it][A
 50%|████▉     | 1637/3305 [1:20:12<1:04:40,  2.33s/it][A
 50%|████▉     | 1638/3305 [1:20:14<1:04:37,  2.33s/it][A
 50%|████▉     | 1639/3305 [1:20:17<1:06:19,  2.39s/it][A
 50%|████▉     | 1640/3305 [1:20:20<1:09:52,  2.52s/it][A
 50%|████▉     | 1641/3305 [1:20:23<1:13:56,  2.67s/it][A
 50%|████▉     | 1642/3305 [1:20:25<1:11:06,  2.57s/it][A
 50%|████▉     | 1643/3305 [1:20:27<1:06:59,  2.42s/it][A
 50%|████▉     | 1644/3305 [1:20:29<1:02:35,  2.26s/it][A
 50%|████▉     | 1645/3305 [1:20:31<1:03:17,  2.29s/it][A
 50%|████▉     | 1646/3305 [1:20:34<1:05:48,  2.38s/it][A
 50%|████▉     | 1647/3305 [1:20:36<1:05:10,  2.36s/it][A
 50%|████▉     | 1648/3305 [1:20:40<1:18:27,  2.84s/it][A
 50%|████▉     | 1649/3305 [1:20:44<1:25:32,  3.10s/it][A
 50%|████▉     | 1650/3305 [1:20:47<1:26:53,  3.15s/it][A
 50%|████▉     | 1651/3305 [1:20:51<1:29:57,  3.26s/it][A
 50%|████▉     | 1652/3305 [1:20:53<1:22:40,  3.00s/it][A
 50%|█████     | 1653/3305 [1:20:55<1:14:34,  2.71s/it][A
 50%|█████     | 1654/3305 [1:20:58<1:12:14,  2.63s/it][A
 50%|█████     | 1655/3305 [1:21:02<1:27:47,  3.19s/it][A
 50%|█████     | 1656/3305 [1:21:07<1:42:47,  3.74s/it][A
 50%|█████     | 1657/3305 [1:21:10<1:39:27,  3.62s/it][A
 50%|█████     | 1658/3305 [1:21:13<1:34:19,  3.44s/it][A
 50%|█████     | 1659/3305 [1:21:16<1:30:22,  3.29s/it][A
 50%|█████     | 1660/3305 [1:21:19<1:25:10,  3.11s/it][A
 50%|█████     | 1661/3305 [1:21:21<1:17:37,  2.83s/it][A
 50%|█████     | 1662/3305 [1:21:24<1:13:18,  2.68s/it][A
 50%|█████     | 1663/3305 [1:21:26<1:08:42,  2.51s/it][A
 50%|█████     | 1664/3305 [1:21:29<1:13:07,  2.67s/it][A
 50%|█████     | 1665/3305 [1:21:34<1:34:12,  3.45s/it][A
 50%|█████     | 1666/3305 [1:21:39<1:49:13,  4.00s/it][A
 50%|█████     | 1667/3305 [1:21:42<1:36:16,  3.53s/it][A
 50%|█████     | 1668/3305 [1:21:45<1:30:28,  3.32s/it][A
 50%|█████     | 1669/3305 [1:21:47<1:26:43,  3.18s/it][A
 51%|█████     | 1670/3305 [1:21:52<1:34:33,  3.47s/it][A
 51%|█████     | 1671/3305 [1:21:57<1:50:06,  4.04s/it][A
 51%|█████     | 1672/3305 [1:22:00<1:38:56,  3.64s/it][A
 51%|█████     | 1673/3305 [1:22:02<1:25:44,  3.15s/it][A
 51%|█████     | 1674/3305 [1:22:04<1:21:53,  3.01s/it][A
 51%|█████     | 1675/3305 [1:22:07<1:18:29,  2.89s/it][A
 51%|█████     | 1676/3305 [1:22:09<1:12:12,  2.66s/it][A
 51%|█████     | 1677/3305 [1:22:13<1:23:20,  3.07s/it][A
 51%|█████     | 1678/3305 [1:22:18<1:35:19,  3.52s/it][A
 51%|█████     | 1679/3305 [1:22:20<1:24:35,  3.12s/it][A
 51%|█████     | 1680/3305 [1:22:23<1:26:48,  3.21s/it][A
 51%|█████     | 1681/3305 [1:22:27<1:29:51,  3.32s/it][A
 51%|█████     | 1682/3305 [1:22:29<1:17:19,  2.86s/it][A
 51%|█████     | 1683/3305 [1:22:31<1:11:24,  2.64s/it][A
 51%|█████     | 1684/3305 [1:22:33<1:06:45,  2.47s/it][A
 51%|█████     | 1685/3305 [1:22:35<1:01:12,  2.27s/it][A
 51%|█████     | 1686/3305 [1:22:37<58:43,  2.18s/it]  [A
 51%|█████     | 1687/3305 [1:22:39<1:02:28,  2.32s/it][A
 51%|█████     | 1688/3305 [1:22:42<1:05:01,  2.41s/it][A
 51%|█████     | 1689/3305 [1:22:44<1:05:01,  2.41s/it][A
 51%|█████     | 1690/3305 [1:22:46<1:01:11,  2.27s/it][A
 51%|█████     | 1691/3305 [1:22:48<58:21,  2.17s/it]  [A
 51%|█████     | 1692/3305 [1:22:51<1:03:54,  2.38s/it][A
 51%|█████     | 1693/3305 [1:22:53<1:02:45,  2.34s/it][A
 51%|█████▏    | 1694/3305 [1:22:56<1:04:01,  2.38s/it][A
 51%|█████▏    | 1695/3305 [1:22:59<1:10:20,  2.62s/it][A
 51%|█████▏    | 1696/3305 [1:23:02<1:14:02,  2.76s/it][A
 51%|█████▏    | 1697/3305 [1:23:05<1:13:28,  2.74s/it][A
 51%|█████▏    | 1698/3305 [1:23:08<1:14:43,  2.79s/it][A
 51%|█████▏    | 1699/3305 [1:23:11<1:19:14,  2.96s/it][A
 51%|█████▏    | 1700/3305 [1:23:15<1:24:25,  3.16s/it][A
 51%|█████▏    | 1701/3305 [1:23:18<1:28:10,  3.30s/it][A
 51%|█████▏    | 1702/3305 [1:23:21<1:22:02,  3.07s/it][A
 52%|█████▏    | 1703/3305 [1:23:25<1:31:15,  3.42s/it][A
 52%|█████▏    | 1704/3305 [1:23:30<1:46:56,  4.01s/it][A
 52%|█████▏    | 1705/3305 [1:23:34<1:42:58,  3.86s/it][A
 52%|█████▏    | 1706/3305 [1:23:37<1:34:55,  3.56s/it][A
 52%|█████▏    | 1707/3305 [1:23:39<1:24:50,  3.19s/it][A
 52%|█████▏    | 1708/3305 [1:23:44<1:34:42,  3.56s/it][A
 52%|█████▏    | 1709/3305 [1:23:48<1:42:13,  3.84s/it][A
 52%|█████▏    | 1710/3305 [1:23:51<1:36:12,  3.62s/it][A
 52%|█████▏    | 1711/3305 [1:23:55<1:35:16,  3.59s/it][A
 52%|█████▏    | 1712/3305 [1:23:56<1:20:19,  3.03s/it][A
 52%|█████▏    | 1713/3305 [1:24:00<1:23:17,  3.14s/it][A
 52%|█████▏    | 1714/3305 [1:24:04<1:29:36,  3.38s/it][A
 52%|█████▏    | 1715/3305 [1:24:07<1:27:00,  3.28s/it][A
 52%|█████▏    | 1716/3305 [1:24:09<1:20:14,  3.03s/it][A
 52%|█████▏    | 1717/3305 [1:24:11<1:09:34,  2.63s/it][A
 52%|█████▏    | 1718/3305 [1:24:14<1:10:17,  2.66s/it][A
 52%|█████▏    | 1719/3305 [1:24:16<1:11:31,  2.71s/it][A
 52%|█████▏    | 1720/3305 [1:24:18<1:04:59,  2.46s/it][A
 52%|█████▏    | 1721/3305 [1:24:20<1:02:30,  2.37s/it][A
 52%|█████▏    | 1722/3305 [1:24:24<1:08:56,  2.61s/it][A
 52%|█████▏    | 1723/3305 [1:24:28<1:25:41,  3.25s/it][A
 52%|█████▏    | 1724/3305 [1:24:35<1:49:04,  4.14s/it][A
 52%|█████▏    | 1725/3305 [1:24:40<1:59:05,  4.52s/it][A
 52%|█████▏    | 1726/3305 [1:24:43<1:47:24,  4.08s/it][A
 52%|█████▏    | 1727/3305 [1:24:45<1:29:54,  3.42s/it][A
 52%|█████▏    | 1728/3305 [1:24:47<1:17:15,  2.94s/it][A
 52%|█████▏    | 1729/3305 [1:24:49<1:08:42,  2.62s/it][A
 52%|█████▏    | 1730/3305 [1:24:54<1:29:17,  3.40s/it][A
 52%|█████▏    | 1731/3305 [1:25:00<1:50:31,  4.21s/it][A
 52%|█████▏    | 1732/3305 [1:25:04<1:50:11,  4.20s/it][A
 52%|█████▏    | 1733/3305 [1:25:09<1:52:40,  4.30s/it][A
 52%|█████▏    | 1734/3305 [1:25:10<1:32:58,  3.55s/it][A
 52%|█████▏    | 1735/3305 [1:25:15<1:38:45,  3.77s/it][A
 53%|█████▎    | 1736/3305 [1:25:20<1:49:20,  4.18s/it][A
 53%|█████▎    | 1737/3305 [1:25:22<1:34:56,  3.63s/it][A
 53%|█████▎    | 1738/3305 [1:25:25<1:24:16,  3.23s/it][A
 53%|█████▎    | 1739/3305 [1:25:27<1:20:26,  3.08s/it][A
 53%|█████▎    | 1740/3305 [1:25:29<1:13:30,  2.82s/it][A
 53%|█████▎    | 1741/3305 [1:25:32<1:12:07,  2.77s/it][A
 53%|█████▎    | 1742/3305 [1:25:36<1:18:36,  3.02s/it][A
 53%|█████▎    | 1743/3305 [1:25:39<1:17:55,  2.99s/it][A
 53%|█████▎    | 1744/3305 [1:25:41<1:15:15,  2.89s/it][A
 53%|█████▎    | 1745/3305 [1:25:44<1:15:53,  2.92s/it][A
 53%|█████▎    | 1746/3305 [1:25:48<1:25:26,  3.29s/it][A
 53%|█████▎    | 1747/3305 [1:25:52<1:29:29,  3.45s/it][A
 53%|█████▎    | 1748/3305 [1:25:54<1:16:22,  2.94s/it][A
 53%|█████▎    | 1749/3305 [1:25:56<1:10:04,  2.70s/it][A
 53%|█████▎    | 1750/3305 [1:26:00<1:16:38,  2.96s/it][A
 53%|█████▎    | 1751/3305 [1:26:03<1:19:43,  3.08s/it][A
 53%|█████▎    | 1752/3305 [1:26:06<1:16:43,  2.96s/it][A
 53%|█████▎    | 1753/3305 [1:26:08<1:14:19,  2.87s/it][A
 53%|█████▎    | 1754/3305 [1:26:11<1:14:12,  2.87s/it][A
 53%|█████▎    | 1755/3305 [1:26:14<1:12:27,  2.80s/it][A
 53%|█████▎    | 1756/3305 [1:26:18<1:22:47,  3.21s/it][A
 53%|█████▎    | 1757/3305 [1:26:22<1:28:28,  3.43s/it][A
 53%|█████▎    | 1758/3305 [1:26:24<1:19:23,  3.08s/it][A
 53%|█████▎    | 1759/3305 [1:26:27<1:15:52,  2.94s/it][A
 53%|█████▎    | 1760/3305 [1:26:30<1:13:45,  2.86s/it][A
 53%|█████▎    | 1761/3305 [1:26:33<1:13:39,  2.86s/it][A
 53%|█████▎    | 1762/3305 [1:26:35<1:14:39,  2.90s/it][A
 53%|█████▎    | 1763/3305 [1:26:38<1:08:00,  2.65s/it][A
 53%|█████▎    | 1764/3305 [1:26:40<1:03:42,  2.48s/it][A
 53%|█████▎    | 1765/3305 [1:26:42<59:25,  2.32s/it]  [A
 53%|█████▎    | 1766/3305 [1:26:44<56:45,  2.21s/it][A
 53%|█████▎    | 1767/3305 [1:26:46<56:16,  2.20s/it][A
 53%|█████▎    | 1768/3305 [1:26:48<59:09,  2.31s/it][A
 54%|█████▎    | 1769/3305 [1:26:52<1:09:56,  2.73s/it][A
 54%|█████▎    | 1770/3305 [1:26:55<1:13:10,  2.86s/it][A
 54%|█████▎    | 1771/3305 [1:26:57<1:03:41,  2.49s/it][A
 54%|█████▎    | 1772/3305 [1:26:59<1:00:31,  2.37s/it][A
 54%|█████▎    | 1773/3305 [1:27:00<54:10,  2.12s/it]  [A
 54%|█████▎    | 1774/3305 [1:27:02<52:37,  2.06s/it][A
 54%|█████▎    | 1775/3305 [1:27:05<58:43,  2.30s/it][A
 54%|█████▎    | 1776/3305 [1:27:09<1:09:33,  2.73s/it][A
 54%|█████▍    | 1777/3305 [1:27:14<1:30:30,  3.55s/it][A
 54%|█████▍    | 1778/3305 [1:27:19<1:39:52,  3.92s/it][A
 54%|█████▍    | 1779/3305 [1:27:21<1:27:28,  3.44s/it][A
 54%|█████▍    | 1780/3305 [1:27:24<1:19:31,  3.13s/it][A
 54%|█████▍    | 1781/3305 [1:27:27<1:19:01,  3.11s/it][A
 54%|█████▍    | 1782/3305 [1:27:30<1:16:27,  3.01s/it][A
 54%|█████▍    | 1783/3305 [1:27:32<1:06:53,  2.64s/it][A
 54%|█████▍    | 1784/3305 [1:27:34<1:02:01,  2.45s/it][A
 54%|█████▍    | 1785/3305 [1:27:35<55:52,  2.21s/it]  [A
 54%|█████▍    | 1786/3305 [1:27:37<54:16,  2.14s/it][A
 54%|█████▍    | 1787/3305 [1:27:40<1:01:29,  2.43s/it][A
 54%|█████▍    | 1788/3305 [1:27:43<1:07:25,  2.67s/it][A
 54%|█████▍    | 1789/3305 [1:27:46<1:05:52,  2.61s/it][A
 54%|█████▍    | 1790/3305 [1:27:49<1:08:53,  2.73s/it][A
 54%|█████▍    | 1791/3305 [1:27:52<1:13:19,  2.91s/it][A
 54%|█████▍    | 1792/3305 [1:27:55<1:14:49,  2.97s/it][A
 54%|█████▍    | 1793/3305 [1:27:58<1:13:30,  2.92s/it][A
 54%|█████▍    | 1794/3305 [1:28:04<1:32:18,  3.67s/it][A
 54%|█████▍    | 1795/3305 [1:28:09<1:45:18,  4.18s/it][A
 54%|█████▍    | 1796/3305 [1:28:11<1:29:27,  3.56s/it][A
 54%|█████▍    | 1797/3305 [1:28:13<1:18:57,  3.14s/it][A
 54%|█████▍    | 1798/3305 [1:28:18<1:27:13,  3.47s/it][A
 54%|█████▍    | 1799/3305 [1:28:24<1:46:43,  4.25s/it][A
 54%|█████▍    | 1800/3305 [1:28:28<1:46:05,  4.23s/it][A
 54%|█████▍    | 1801/3305 [1:28:30<1:32:28,  3.69s/it][A
 55%|█████▍    | 1802/3305 [1:28:33<1:22:53,  3.31s/it][A
 55%|█████▍    | 1803/3305 [1:28:36<1:23:53,  3.35s/it][A
 55%|█████▍    | 1804/3305 [1:28:39<1:22:24,  3.29s/it][A
 55%|█████▍    | 1805/3305 [1:28:43<1:23:43,  3.35s/it][A
 55%|█████▍    | 1806/3305 [1:28:46<1:26:29,  3.46s/it][A
 55%|█████▍    | 1807/3305 [1:28:50<1:25:06,  3.41s/it][A
 55%|█████▍    | 1808/3305 [1:28:55<1:37:32,  3.91s/it][A
 55%|█████▍    | 1809/3305 [1:29:01<1:57:04,  4.70s/it][A
 55%|█████▍    | 1810/3305 [1:29:06<1:53:55,  4.57s/it][A
 55%|█████▍    | 1811/3305 [1:29:09<1:42:30,  4.12s/it][A
 55%|█████▍    | 1812/3305 [1:29:12<1:33:02,  3.74s/it][A
 55%|█████▍    | 1813/3305 [1:29:15<1:29:17,  3.59s/it][A
 55%|█████▍    | 1814/3305 [1:29:18<1:27:31,  3.52s/it][A
 55%|█████▍    | 1815/3305 [1:29:20<1:17:35,  3.12s/it][A
 55%|█████▍    | 1816/3305 [1:29:23<1:14:14,  2.99s/it][A
 55%|█████▍    | 1817/3305 [1:29:26<1:15:07,  3.03s/it][A
 55%|█████▌    | 1818/3305 [1:29:28<1:09:50,  2.82s/it][A
 55%|█████▌    | 1819/3305 [1:29:31<1:07:52,  2.74s/it][A
 55%|█████▌    | 1820/3305 [1:29:34<1:08:00,  2.75s/it][A
 55%|█████▌    | 1821/3305 [1:29:37<1:08:58,  2.79s/it][A
 55%|█████▌    | 1822/3305 [1:29:42<1:25:52,  3.47s/it][A
 55%|█████▌    | 1823/3305 [1:29:46<1:33:09,  3.77s/it][A
 55%|█████▌    | 1824/3305 [1:29:49<1:24:22,  3.42s/it][A
 55%|█████▌    | 1825/3305 [1:29:52<1:21:10,  3.29s/it][A
 55%|█████▌    | 1826/3305 [1:29:55<1:18:29,  3.18s/it][A
 55%|█████▌    | 1827/3305 [1:29:58<1:20:58,  3.29s/it][A
 55%|█████▌    | 1828/3305 [1:30:00<1:12:00,  2.92s/it][A
 55%|█████▌    | 1829/3305 [1:30:03<1:12:55,  2.96s/it][A
 55%|█████▌    | 1830/3305 [1:30:08<1:21:44,  3.32s/it][A
 55%|█████▌    | 1831/3305 [1:30:11<1:21:14,  3.31s/it][A
 55%|█████▌    | 1832/3305 [1:30:14<1:21:50,  3.33s/it][A
 55%|█████▌    | 1833/3305 [1:30:18<1:27:37,  3.57s/it][A
 55%|█████▌    | 1834/3305 [1:30:21<1:23:18,  3.40s/it][A
 56%|█████▌    | 1835/3305 [1:30:24<1:16:20,  3.12s/it][A
 56%|█████▌    | 1836/3305 [1:30:27<1:15:40,  3.09s/it][A
 56%|█████▌    | 1837/3305 [1:30:30<1:15:37,  3.09s/it][A
 56%|█████▌    | 1838/3305 [1:30:32<1:11:11,  2.91s/it][A
 56%|█████▌    | 1839/3305 [1:30:36<1:17:19,  3.16s/it][A
 56%|█████▌    | 1840/3305 [1:30:42<1:39:36,  4.08s/it][A
 56%|█████▌    | 1841/3305 [1:30:48<1:49:50,  4.50s/it][A
 56%|█████▌    | 1842/3305 [1:30:50<1:34:15,  3.87s/it][A
 56%|█████▌    | 1843/3305 [1:30:52<1:21:58,  3.36s/it][A
 56%|█████▌    | 1844/3305 [1:30:57<1:27:41,  3.60s/it][A
 56%|█████▌    | 1845/3305 [1:31:01<1:34:28,  3.88s/it][A
 56%|█████▌    | 1846/3305 [1:31:03<1:18:57,  3.25s/it][A
 56%|█████▌    | 1847/3305 [1:31:05<1:12:45,  2.99s/it][A
 56%|█████▌    | 1848/3305 [1:31:08<1:08:02,  2.80s/it][A
 56%|█████▌    | 1849/3305 [1:31:10<1:03:43,  2.63s/it][A
 56%|█████▌    | 1850/3305 [1:31:13<1:05:55,  2.72s/it][A
 56%|█████▌    | 1851/3305 [1:31:15<1:05:36,  2.71s/it][A
 56%|█████▌    | 1852/3305 [1:31:18<1:05:15,  2.69s/it][A
 56%|█████▌    | 1853/3305 [1:31:21<1:04:43,  2.67s/it][A
 56%|█████▌    | 1854/3305 [1:31:23<1:04:27,  2.67s/it][A
 56%|█████▌    | 1855/3305 [1:31:26<1:06:01,  2.73s/it][A
 56%|█████▌    | 1856/3305 [1:31:29<1:06:47,  2.77s/it][A
 56%|█████▌    | 1857/3305 [1:31:32<1:07:43,  2.81s/it][A
 56%|█████▌    | 1858/3305 [1:31:35<1:09:21,  2.88s/it][A
 56%|█████▌    | 1859/3305 [1:31:39<1:18:48,  3.27s/it][A
 56%|█████▋    | 1860/3305 [1:31:44<1:27:14,  3.62s/it][A
 56%|█████▋    | 1861/3305 [1:31:46<1:15:34,  3.14s/it][A
 56%|█████▋    | 1862/3305 [1:31:48<1:12:44,  3.02s/it][A
 56%|█████▋    | 1863/3305 [1:31:51<1:09:30,  2.89s/it][A
 56%|█████▋    | 1864/3305 [1:31:54<1:11:06,  2.96s/it][A
 56%|█████▋    | 1865/3305 [1:31:58<1:17:51,  3.24s/it][A
 56%|█████▋    | 1866/3305 [1:32:01<1:14:04,  3.09s/it][A
 56%|█████▋    | 1867/3305 [1:32:04<1:13:12,  3.05s/it][A
 57%|█████▋    | 1868/3305 [1:32:06<1:09:09,  2.89s/it][A
 57%|█████▋    | 1869/3305 [1:32:09<1:05:48,  2.75s/it][A
 57%|█████▋    | 1870/3305 [1:32:12<1:10:58,  2.97s/it][A
 57%|█████▋    | 1871/3305 [1:32:16<1:15:09,  3.14s/it][A
 57%|█████▋    | 1872/3305 [1:32:18<1:07:16,  2.82s/it][A
 57%|█████▋    | 1873/3305 [1:32:20<1:00:21,  2.53s/it][A
 57%|█████▋    | 1874/3305 [1:32:22<57:29,  2.41s/it]  [A
 57%|█████▋    | 1875/3305 [1:32:26<1:10:58,  2.98s/it][A
 57%|█████▋    | 1876/3305 [1:32:31<1:22:46,  3.48s/it][A
 57%|█████▋    | 1877/3305 [1:32:33<1:13:53,  3.11s/it][A
 57%|█████▋    | 1878/3305 [1:32:35<1:09:19,  2.91s/it][A
 57%|█████▋    | 1879/3305 [1:32:38<1:06:53,  2.81s/it][A
 57%|█████▋    | 1880/3305 [1:32:42<1:11:54,  3.03s/it][A
 57%|█████▋    | 1881/3305 [1:32:45<1:15:00,  3.16s/it][A
 57%|█████▋    | 1882/3305 [1:32:48<1:12:02,  3.04s/it][A
 57%|█████▋    | 1883/3305 [1:32:51<1:15:32,  3.19s/it][A
 57%|█████▋    | 1884/3305 [1:32:54<1:12:57,  3.08s/it][A
 57%|█████▋    | 1885/3305 [1:32:59<1:22:19,  3.48s/it][A
 57%|█████▋    | 1886/3305 [1:33:03<1:32:33,  3.91s/it][A
 57%|█████▋    | 1887/3305 [1:33:07<1:26:55,  3.68s/it][A
 57%|█████▋    | 1888/3305 [1:33:10<1:24:28,  3.58s/it][A
 57%|█████▋    | 1889/3305 [1:33:14<1:28:00,  3.73s/it][A
 57%|█████▋    | 1890/3305 [1:33:18<1:31:51,  3.90s/it][A
 57%|█████▋    | 1891/3305 [1:33:20<1:18:12,  3.32s/it][A
 57%|█████▋    | 1892/3305 [1:33:22<1:08:16,  2.90s/it][A
 57%|█████▋    | 1893/3305 [1:33:24<1:02:37,  2.66s/it][A
 57%|█████▋    | 1894/3305 [1:33:28<1:08:30,  2.91s/it][A
 57%|█████▋    | 1895/3305 [1:33:32<1:16:24,  3.25s/it][A
 57%|█████▋    | 1896/3305 [1:33:35<1:15:48,  3.23s/it][A
 57%|█████▋    | 1897/3305 [1:33:38<1:16:02,  3.24s/it][A
 57%|█████▋    | 1898/3305 [1:33:41<1:12:09,  3.08s/it][A
 57%|█████▋    | 1899/3305 [1:33:43<1:03:33,  2.71s/it][A
 57%|█████▋    | 1900/3305 [1:33:44<55:35,  2.37s/it]  [A
 58%|█████▊    | 1901/3305 [1:33:46<52:43,  2.25s/it][A
 58%|█████▊    | 1902/3305 [1:33:49<51:55,  2.22s/it][A
 58%|█████▊    | 1903/3305 [1:33:51<52:42,  2.26s/it][A
 58%|█████▊    | 1904/3305 [1:33:53<52:14,  2.24s/it][A
 58%|█████▊    | 1905/3305 [1:33:55<51:57,  2.23s/it][A
 58%|█████▊    | 1906/3305 [1:33:58<52:26,  2.25s/it][A
 58%|█████▊    | 1907/3305 [1:34:00<52:34,  2.26s/it][A
 58%|█████▊    | 1908/3305 [1:34:02<50:12,  2.16s/it][A
 58%|█████▊    | 1909/3305 [1:34:04<47:29,  2.04s/it][A
 58%|█████▊    | 1910/3305 [1:34:06<51:20,  2.21s/it][A
 58%|█████▊    | 1911/3305 [1:34:08<51:29,  2.22s/it][A
 58%|█████▊    | 1912/3305 [1:34:10<49:34,  2.14s/it][A
 58%|█████▊    | 1913/3305 [1:34:12<48:55,  2.11s/it][A
 58%|█████▊    | 1914/3305 [1:34:15<53:55,  2.33s/it][A
 58%|█████▊    | 1915/3305 [1:34:20<1:13:37,  3.18s/it][A
 58%|█████▊    | 1916/3305 [1:34:25<1:23:51,  3.62s/it][A
 58%|█████▊    | 1917/3305 [1:34:28<1:16:09,  3.29s/it][A
 58%|█████▊    | 1918/3305 [1:34:30<1:11:32,  3.09s/it][A
 58%|█████▊    | 1919/3305 [1:34:33<1:07:23,  2.92s/it][A
 58%|█████▊    | 1920/3305 [1:34:36<1:08:39,  2.97s/it][A
 58%|█████▊    | 1921/3305 [1:34:39<1:07:21,  2.92s/it][A
 58%|█████▊    | 1922/3305 [1:34:41<1:03:43,  2.76s/it][A
 58%|█████▊    | 1923/3305 [1:34:45<1:14:27,  3.23s/it][A
 58%|█████▊    | 1924/3305 [1:34:50<1:25:50,  3.73s/it][A
 58%|█████▊    | 1925/3305 [1:34:53<1:17:00,  3.35s/it][A
 58%|█████▊    | 1926/3305 [1:34:56<1:18:05,  3.40s/it][A
 58%|█████▊    | 1927/3305 [1:34:59<1:14:33,  3.25s/it][A
 58%|█████▊    | 1928/3305 [1:35:01<1:06:11,  2.88s/it][A
 58%|█████▊    | 1929/3305 [1:35:03<1:00:08,  2.62s/it][A
 58%|█████▊    | 1930/3305 [1:35:05<53:00,  2.31s/it]  [A
 58%|█████▊    | 1931/3305 [1:35:07<51:50,  2.26s/it][A
 58%|█████▊    | 1932/3305 [1:35:09<51:51,  2.27s/it][A
 58%|█████▊    | 1933/3305 [1:35:12<53:42,  2.35s/it][A
 59%|█████▊    | 1934/3305 [1:35:14<54:43,  2.39s/it][A
 59%|█████▊    | 1935/3305 [1:35:17<54:09,  2.37s/it][A
 59%|█████▊    | 1936/3305 [1:35:19<53:13,  2.33s/it][A
 59%|█████▊    | 1937/3305 [1:35:22<58:14,  2.55s/it][A
 59%|█████▊    | 1938/3305 [1:35:24<55:11,  2.42s/it][A
 59%|█████▊    | 1939/3305 [1:35:26<54:05,  2.38s/it][A
 59%|█████▊    | 1940/3305 [1:35:29<54:00,  2.37s/it][A
 59%|█████▊    | 1941/3305 [1:35:31<52:51,  2.32s/it][A
 59%|█████▉    | 1942/3305 [1:35:34<56:01,  2.47s/it][A
 59%|█████▉    | 1943/3305 [1:35:36<58:22,  2.57s/it][A
 59%|█████▉    | 1944/3305 [1:35:39<55:37,  2.45s/it][A
 59%|█████▉    | 1945/3305 [1:35:41<56:45,  2.50s/it][A
 59%|█████▉    | 1946/3305 [1:35:44<59:30,  2.63s/it][A
 59%|█████▉    | 1947/3305 [1:35:48<1:05:52,  2.91s/it][A
 59%|█████▉    | 1948/3305 [1:35:50<1:05:03,  2.88s/it][A
 59%|█████▉    | 1949/3305 [1:35:54<1:06:14,  2.93s/it][A
 59%|█████▉    | 1950/3305 [1:35:57<1:11:06,  3.15s/it][A
 59%|█████▉    | 1951/3305 [1:36:00<1:06:44,  2.96s/it][A
 59%|█████▉    | 1952/3305 [1:36:03<1:07:03,  2.97s/it][A
 59%|█████▉    | 1953/3305 [1:36:07<1:17:11,  3.43s/it][A
 59%|█████▉    | 1954/3305 [1:36:12<1:23:44,  3.72s/it][A
 59%|█████▉    | 1955/3305 [1:36:16<1:24:57,  3.78s/it][A
 59%|█████▉    | 1956/3305 [1:36:21<1:34:25,  4.20s/it][A
 59%|█████▉    | 1957/3305 [1:36:25<1:34:05,  4.19s/it][A
 59%|█████▉    | 1958/3305 [1:36:27<1:20:33,  3.59s/it][A
 59%|█████▉    | 1959/3305 [1:36:29<1:12:25,  3.23s/it][A
 59%|█████▉    | 1960/3305 [1:36:32<1:07:36,  3.02s/it][A
 59%|█████▉    | 1961/3305 [1:36:35<1:05:15,  2.91s/it][A
 59%|█████▉    | 1962/3305 [1:36:37<1:04:48,  2.90s/it][A
 59%|█████▉    | 1963/3305 [1:36:40<1:03:30,  2.84s/it][A
 59%|█████▉    | 1964/3305 [1:36:43<1:03:44,  2.85s/it][A
 59%|█████▉    | 1965/3305 [1:36:47<1:09:17,  3.10s/it][A
 59%|█████▉    | 1966/3305 [1:36:50<1:09:01,  3.09s/it][A
 60%|█████▉    | 1967/3305 [1:36:52<1:04:24,  2.89s/it][A
 60%|█████▉    | 1968/3305 [1:36:55<1:04:37,  2.90s/it][A
 60%|█████▉    | 1969/3305 [1:36:58<1:03:57,  2.87s/it][A
 60%|█████▉    | 1970/3305 [1:37:01<1:02:27,  2.81s/it][A
 60%|█████▉    | 1971/3305 [1:37:03<56:46,  2.55s/it]  [A
 60%|█████▉    | 1972/3305 [1:37:04<50:32,  2.27s/it][A
 60%|█████▉    | 1973/3305 [1:37:07<55:30,  2.50s/it][A
 60%|█████▉    | 1974/3305 [1:37:10<59:44,  2.69s/it][A
 60%|█████▉    | 1975/3305 [1:37:13<59:46,  2.70s/it][A
 60%|█████▉    | 1976/3305 [1:37:17<1:08:17,  3.08s/it][A
 60%|█████▉    | 1977/3305 [1:37:21<1:15:15,  3.40s/it][A
 60%|█████▉    | 1978/3305 [1:37:24<1:10:26,  3.18s/it][A
 60%|█████▉    | 1979/3305 [1:37:26<1:03:21,  2.87s/it][A
 60%|█████▉    | 1980/3305 [1:37:28<56:26,  2.56s/it]  [A
 60%|█████▉    | 1981/3305 [1:37:31<58:23,  2.65s/it][A
 60%|█████▉    | 1982/3305 [1:37:34<1:01:13,  2.78s/it][A
 60%|██████    | 1983/3305 [1:37:37<1:02:37,  2.84s/it][A
 60%|██████    | 1984/3305 [1:37:41<1:09:56,  3.18s/it][A
 60%|██████    | 1985/3305 [1:37:44<1:10:02,  3.18s/it][A
 60%|██████    | 1986/3305 [1:37:47<1:05:52,  3.00s/it][A
 60%|██████    | 1987/3305 [1:37:48<58:33,  2.67s/it]  [A
 60%|██████    | 1988/3305 [1:37:51<55:46,  2.54s/it][A
 60%|██████    | 1989/3305 [1:37:53<51:40,  2.36s/it][A
 60%|██████    | 1990/3305 [1:37:55<52:32,  2.40s/it][A
 60%|██████    | 1991/3305 [1:37:58<56:56,  2.60s/it][A
 60%|██████    | 1992/3305 [1:38:01<55:47,  2.55s/it][A
 60%|██████    | 1993/3305 [1:38:04<1:00:35,  2.77s/it][A
 60%|██████    | 1994/3305 [1:38:07<1:04:17,  2.94s/it][A
 60%|██████    | 1995/3305 [1:38:10<1:03:02,  2.89s/it][A
 60%|██████    | 1996/3305 [1:38:13<1:05:08,  2.99s/it][A
 60%|██████    | 1997/3305 [1:38:16<1:07:07,  3.08s/it][A
 60%|██████    | 1998/3305 [1:38:20<1:07:31,  3.10s/it][A
 60%|██████    | 1999/3305 [1:38:22<1:02:21,  2.86s/it][A
 61%|██████    | 2000/3305 [1:38:25<1:01:50,  2.84s/it][A
 61%|██████    | 2001/3305 [1:38:27<56:29,  2.60s/it]  [A
 61%|██████    | 2002/3305 [1:38:29<55:16,  2.55s/it][A
 61%|██████    | 2003/3305 [1:38:32<56:21,  2.60s/it][A
 61%|██████    | 2004/3305 [1:38:34<52:38,  2.43s/it][A
 61%|██████    | 2005/3305 [1:38:36<48:23,  2.23s/it][A
 61%|██████    | 2006/3305 [1:38:38<48:28,  2.24s/it][A
 61%|██████    | 2007/3305 [1:38:40<50:20,  2.33s/it][A
 61%|██████    | 2008/3305 [1:38:43<52:26,  2.43s/it][A
 61%|██████    | 2009/3305 [1:38:46<52:05,  2.41s/it][A
 61%|██████    | 2010/3305 [1:38:48<52:13,  2.42s/it][A
 61%|██████    | 2011/3305 [1:38:53<1:09:17,  3.21s/it][A
 61%|██████    | 2012/3305 [1:38:58<1:21:34,  3.79s/it][A
 61%|██████    | 2013/3305 [1:39:01<1:14:53,  3.48s/it][A
 61%|██████    | 2014/3305 [1:39:03<1:08:27,  3.18s/it][A
 61%|██████    | 2015/3305 [1:39:06<1:04:35,  3.00s/it][A
 61%|██████    | 2016/3305 [1:39:09<1:02:07,  2.89s/it][A
 61%|██████    | 2017/3305 [1:39:12<1:03:41,  2.97s/it][A
 61%|██████    | 2018/3305 [1:39:15<1:04:39,  3.01s/it][A
 61%|██████    | 2019/3305 [1:39:18<1:06:47,  3.12s/it][A
 61%|██████    | 2020/3305 [1:39:21<1:07:34,  3.15s/it][A
 61%|██████    | 2021/3305 [1:39:24<1:01:53,  2.89s/it][A
 61%|██████    | 2022/3305 [1:39:26<55:12,  2.58s/it]  [A
 61%|██████    | 2023/3305 [1:39:28<56:19,  2.64s/it][A
 61%|██████    | 2024/3305 [1:39:31<57:04,  2.67s/it][A
 61%|██████▏   | 2025/3305 [1:39:34<58:34,  2.75s/it][A
 61%|██████▏   | 2026/3305 [1:39:39<1:10:46,  3.32s/it][A
 61%|██████▏   | 2027/3305 [1:39:43<1:17:01,  3.62s/it][A
 61%|██████▏   | 2028/3305 [1:39:49<1:33:37,  4.40s/it][A
 61%|██████▏   | 2029/3305 [1:39:56<1:47:15,  5.04s/it][A
 61%|██████▏   | 2030/3305 [1:39:59<1:33:45,  4.41s/it][A
 61%|██████▏   | 2031/3305 [1:40:01<1:22:09,  3.87s/it][A
 61%|██████▏   | 2032/3305 [1:40:03<1:08:46,  3.24s/it][A
 62%|██████▏   | 2033/3305 [1:40:05<1:00:23,  2.85s/it][A
 62%|██████▏   | 2034/3305 [1:40:07<54:29,  2.57s/it]  [A
 62%|██████▏   | 2035/3305 [1:40:09<52:47,  2.49s/it][A
 62%|██████▏   | 2036/3305 [1:40:12<51:01,  2.41s/it][A
 62%|██████▏   | 2037/3305 [1:40:15<56:14,  2.66s/it][A
 62%|██████▏   | 2038/3305 [1:40:18<58:29,  2.77s/it][A
 62%|██████▏   | 2039/3305 [1:40:20<54:23,  2.58s/it][A
 62%|██████▏   | 2040/3305 [1:40:23<56:17,  2.67s/it][A
 62%|██████▏   | 2041/3305 [1:40:26<1:00:54,  2.89s/it][A
 62%|██████▏   | 2042/3305 [1:40:29<58:43,  2.79s/it]  [A
 62%|██████▏   | 2043/3305 [1:40:30<51:45,  2.46s/it][A
 62%|██████▏   | 2044/3305 [1:40:33<51:19,  2.44s/it][A
 62%|██████▏   | 2045/3305 [1:40:35<50:32,  2.41s/it][A
 62%|██████▏   | 2046/3305 [1:40:37<49:15,  2.35s/it][A
 62%|██████▏   | 2047/3305 [1:40:40<50:07,  2.39s/it][A
 62%|██████▏   | 2048/3305 [1:40:43<52:39,  2.51s/it][A
 62%|██████▏   | 2049/3305 [1:40:46<55:50,  2.67s/it][A
 62%|██████▏   | 2050/3305 [1:40:49<58:13,  2.78s/it][A
 62%|██████▏   | 2051/3305 [1:40:52<58:52,  2.82s/it][A
 62%|██████▏   | 2052/3305 [1:40:55<1:03:59,  3.06s/it][A
 62%|██████▏   | 2053/3305 [1:40:59<1:04:59,  3.11s/it][A
 62%|██████▏   | 2054/3305 [1:41:01<1:03:24,  3.04s/it][A
 62%|██████▏   | 2055/3305 [1:41:05<1:08:30,  3.29s/it][A
 62%|██████▏   | 2056/3305 [1:41:08<1:05:00,  3.12s/it][A
 62%|██████▏   | 2057/3305 [1:41:10<59:58,  2.88s/it]  [A
 62%|██████▏   | 2058/3305 [1:41:12<54:31,  2.62s/it][A
 62%|██████▏   | 2059/3305 [1:41:14<50:44,  2.44s/it][A
 62%|██████▏   | 2060/3305 [1:41:17<52:03,  2.51s/it][A
 62%|██████▏   | 2061/3305 [1:41:20<53:39,  2.59s/it][A
 62%|██████▏   | 2062/3305 [1:41:22<52:25,  2.53s/it][A
 62%|██████▏   | 2063/3305 [1:41:25<55:32,  2.68s/it][A
 62%|██████▏   | 2064/3305 [1:41:29<1:03:53,  3.09s/it][A
 62%|██████▏   | 2065/3305 [1:41:33<1:08:30,  3.32s/it][A
 63%|██████▎   | 2066/3305 [1:41:36<1:06:44,  3.23s/it][A
 63%|██████▎   | 2067/3305 [1:41:39<1:02:49,  3.05s/it][A
 63%|██████▎   | 2068/3305 [1:41:42<1:03:19,  3.07s/it][A
 63%|██████▎   | 2069/3305 [1:41:45<1:00:30,  2.94s/it][A
 63%|██████▎   | 2070/3305 [1:41:47<55:57,  2.72s/it]  [A
 63%|██████▎   | 2071/3305 [1:41:49<51:48,  2.52s/it][A
 63%|██████▎   | 2072/3305 [1:41:51<50:14,  2.45s/it][A
 63%|██████▎   | 2073/3305 [1:41:53<48:50,  2.38s/it][A
 63%|██████▎   | 2074/3305 [1:41:56<48:19,  2.36s/it][A
 63%|██████▎   | 2075/3305 [1:41:59<54:25,  2.66s/it][A
 63%|██████▎   | 2076/3305 [1:42:02<56:28,  2.76s/it][A
 63%|██████▎   | 2077/3305 [1:42:04<52:48,  2.58s/it][A
 63%|██████▎   | 2078/3305 [1:42:06<50:21,  2.46s/it][A
 63%|██████▎   | 2079/3305 [1:42:09<49:07,  2.40s/it][A
 63%|██████▎   | 2080/3305 [1:42:12<54:09,  2.65s/it][A
 63%|██████▎   | 2081/3305 [1:42:16<1:00:47,  2.98s/it][A
 63%|██████▎   | 2082/3305 [1:42:18<57:30,  2.82s/it]  [A
 63%|██████▎   | 2083/3305 [1:42:20<52:52,  2.60s/it][A
 63%|██████▎   | 2084/3305 [1:42:23<52:17,  2.57s/it][A
 63%|██████▎   | 2085/3305 [1:42:26<54:48,  2.70s/it][A
 63%|██████▎   | 2086/3305 [1:42:28<54:17,  2.67s/it][A
 63%|██████▎   | 2087/3305 [1:42:33<1:09:01,  3.40s/it][A
 63%|██████▎   | 2088/3305 [1:42:38<1:14:17,  3.66s/it][A
 63%|██████▎   | 2089/3305 [1:42:39<1:02:06,  3.06s/it][A
 63%|██████▎   | 2090/3305 [1:42:41<55:23,  2.74s/it]  [A
 63%|██████▎   | 2091/3305 [1:42:44<55:27,  2.74s/it][A
 63%|██████▎   | 2092/3305 [1:42:48<1:02:06,  3.07s/it][A
 63%|██████▎   | 2093/3305 [1:42:51<1:02:04,  3.07s/it][A
 63%|██████▎   | 2094/3305 [1:42:53<59:21,  2.94s/it]  [A
 63%|██████▎   | 2095/3305 [1:42:56<56:48,  2.82s/it][A
 63%|██████▎   | 2096/3305 [1:42:58<52:34,  2.61s/it][A
 63%|██████▎   | 2097/3305 [1:43:00<50:22,  2.50s/it][A
 63%|██████▎   | 2098/3305 [1:43:03<51:52,  2.58s/it][A
 64%|██████▎   | 2099/3305 [1:43:07<1:01:59,  3.08s/it][A
 64%|██████▎   | 2100/3305 [1:43:10<1:01:21,  3.05s/it][A
 64%|██████▎   | 2101/3305 [1:43:12<54:39,  2.72s/it]  [A
 64%|██████▎   | 2102/3305 [1:43:15<52:27,  2.62s/it][A
 64%|██████▎   | 2103/3305 [1:43:19<1:05:10,  3.25s/it][A
 64%|██████▎   | 2104/3305 [1:43:25<1:18:25,  3.92s/it][A
 64%|██████▎   | 2105/3305 [1:43:27<1:08:47,  3.44s/it][A
 64%|██████▎   | 2106/3305 [1:43:30<1:05:31,  3.28s/it][A
 64%|██████▍   | 2107/3305 [1:43:33<1:05:16,  3.27s/it][A
 64%|██████▍   | 2108/3305 [1:43:37<1:07:26,  3.38s/it][A
 64%|██████▍   | 2109/3305 [1:43:42<1:15:57,  3.81s/it][A
 64%|██████▍   | 2110/3305 [1:43:46<1:20:01,  4.02s/it][A
 64%|██████▍   | 2111/3305 [1:43:51<1:22:09,  4.13s/it][A
 64%|██████▍   | 2112/3305 [1:43:55<1:20:45,  4.06s/it][A
 64%|██████▍   | 2113/3305 [1:43:58<1:14:04,  3.73s/it][A
 64%|██████▍   | 2114/3305 [1:43:59<1:02:48,  3.16s/it][A
 64%|██████▍   | 2115/3305 [1:44:02<58:53,  2.97s/it]  [A
 64%|██████▍   | 2116/3305 [1:44:07<1:08:28,  3.46s/it][A
 64%|██████▍   | 2117/3305 [1:44:11<1:17:18,  3.90s/it][A
 64%|██████▍   | 2118/3305 [1:44:14<1:10:15,  3.55s/it][A
 64%|██████▍   | 2119/3305 [1:44:17<1:05:10,  3.30s/it][A
 64%|██████▍   | 2120/3305 [1:44:20<1:03:12,  3.20s/it][A
 64%|██████▍   | 2121/3305 [1:44:22<55:39,  2.82s/it]  [A
 64%|██████▍   | 2122/3305 [1:44:24<50:15,  2.55s/it][A
 64%|██████▍   | 2123/3305 [1:44:26<48:14,  2.45s/it][A
 64%|██████▍   | 2124/3305 [1:44:28<43:38,  2.22s/it][A
 64%|██████▍   | 2125/3305 [1:44:31<48:09,  2.45s/it][A
 64%|██████▍   | 2126/3305 [1:44:35<56:55,  2.90s/it][A
 64%|██████▍   | 2127/3305 [1:44:37<56:27,  2.88s/it][A
 64%|██████▍   | 2128/3305 [1:44:40<55:11,  2.81s/it][A
 64%|██████▍   | 2129/3305 [1:44:43<53:36,  2.74s/it][A
 64%|██████▍   | 2130/3305 [1:44:45<49:49,  2.54s/it][A
 64%|██████▍   | 2131/3305 [1:44:48<55:04,  2.82s/it][A
 65%|██████▍   | 2132/3305 [1:44:52<1:01:58,  3.17s/it][A
 65%|██████▍   | 2133/3305 [1:44:56<1:08:08,  3.49s/it][A
 65%|██████▍   | 2134/3305 [1:45:01<1:15:34,  3.87s/it][A
 65%|██████▍   | 2135/3305 [1:45:06<1:22:40,  4.24s/it][A
 65%|██████▍   | 2136/3305 [1:45:10<1:16:57,  3.95s/it][A
 65%|██████▍   | 2137/3305 [1:45:14<1:22:25,  4.23s/it][A
 65%|██████▍   | 2138/3305 [1:45:20<1:29:35,  4.61s/it][A
 65%|██████▍   | 2139/3305 [1:45:23<1:19:51,  4.11s/it][A
 65%|██████▍   | 2140/3305 [1:45:25<1:10:09,  3.61s/it][A
 65%|██████▍   | 2141/3305 [1:45:28<1:04:42,  3.34s/it][A
 65%|██████▍   | 2142/3305 [1:45:31<1:00:22,  3.11s/it][A
 65%|██████▍   | 2143/3305 [1:45:34<1:00:34,  3.13s/it][A
 65%|██████▍   | 2144/3305 [1:45:37<1:00:39,  3.13s/it][A
 65%|██████▍   | 2145/3305 [1:45:39<55:44,  2.88s/it]  [A
 65%|██████▍   | 2146/3305 [1:45:42<52:54,  2.74s/it][A
 65%|██████▍   | 2147/3305 [1:45:44<50:10,  2.60s/it][A
 65%|██████▍   | 2148/3305 [1:45:47<50:56,  2.64s/it][A
 65%|██████▌   | 2149/3305 [1:45:49<48:35,  2.52s/it][A
 65%|██████▌   | 2150/3305 [1:45:51<48:10,  2.50s/it][A
 65%|██████▌   | 2151/3305 [1:45:54<46:59,  2.44s/it][A
 65%|██████▌   | 2152/3305 [1:45:56<48:01,  2.50s/it][A
 65%|██████▌   | 2153/3305 [1:45:59<49:18,  2.57s/it][A
 65%|██████▌   | 2154/3305 [1:46:02<49:23,  2.57s/it][A
 65%|██████▌   | 2155/3305 [1:46:04<50:29,  2.63s/it][A
 65%|██████▌   | 2156/3305 [1:46:07<49:05,  2.56s/it][A
 65%|██████▌   | 2157/3305 [1:46:10<52:46,  2.76s/it][A
 65%|██████▌   | 2158/3305 [1:46:13<55:34,  2.91s/it][A
 65%|██████▌   | 2159/3305 [1:46:16<53:18,  2.79s/it][A
 65%|██████▌   | 2160/3305 [1:46:18<52:28,  2.75s/it][A
 65%|██████▌   | 2161/3305 [1:46:21<53:25,  2.80s/it][A
 65%|██████▌   | 2162/3305 [1:46:25<57:26,  3.02s/it][A
 65%|██████▌   | 2163/3305 [1:46:29<1:01:50,  3.25s/it][A
 65%|██████▌   | 2164/3305 [1:46:32<1:01:47,  3.25s/it][A
 66%|██████▌   | 2165/3305 [1:46:34<58:08,  3.06s/it]  [A
 66%|██████▌   | 2166/3305 [1:46:37<56:00,  2.95s/it][A
 66%|██████▌   | 2167/3305 [1:46:41<1:00:25,  3.19s/it][A
 66%|██████▌   | 2168/3305 [1:46:45<1:07:34,  3.57s/it][A
 66%|██████▌   | 2169/3305 [1:46:49<1:08:51,  3.64s/it][A
 66%|██████▌   | 2170/3305 [1:46:52<1:02:34,  3.31s/it][A
 66%|██████▌   | 2171/3305 [1:46:54<58:45,  3.11s/it]  [A
 66%|██████▌   | 2172/3305 [1:46:58<1:00:54,  3.23s/it][A
 66%|██████▌   | 2173/3305 [1:47:02<1:06:22,  3.52s/it][A
 66%|██████▌   | 2174/3305 [1:47:05<1:01:22,  3.26s/it][A
 66%|██████▌   | 2175/3305 [1:47:07<53:40,  2.85s/it]  [A
 66%|██████▌   | 2176/3305 [1:47:11<59:37,  3.17s/it][A
 66%|██████▌   | 2177/3305 [1:47:16<1:10:21,  3.74s/it][A
 66%|██████▌   | 2178/3305 [1:47:19<1:10:18,  3.74s/it][A
 66%|██████▌   | 2179/3305 [1:47:22<1:03:50,  3.40s/it][A
 66%|██████▌   | 2180/3305 [1:47:25<1:00:18,  3.22s/it][A
 66%|██████▌   | 2181/3305 [1:47:27<57:11,  3.05s/it]  [A
 66%|██████▌   | 2182/3305 [1:47:31<57:55,  3.10s/it][A
 66%|██████▌   | 2183/3305 [1:47:35<1:03:55,  3.42s/it][A
 66%|██████▌   | 2184/3305 [1:47:39<1:07:05,  3.59s/it][A
 66%|██████▌   | 2185/3305 [1:47:44<1:13:42,  3.95s/it][A
 66%|██████▌   | 2186/3305 [1:47:48<1:18:32,  4.21s/it][A
 66%|██████▌   | 2187/3305 [1:47:52<1:15:41,  4.06s/it][A
 66%|██████▌   | 2188/3305 [1:47:56<1:12:16,  3.88s/it][A
 66%|██████▌   | 2189/3305 [1:47:58<1:05:35,  3.53s/it][A
 66%|██████▋   | 2190/3305 [1:48:01<1:00:29,  3.26s/it][A
 66%|██████▋   | 2191/3305 [1:48:04<58:09,  3.13s/it]  [A
 66%|██████▋   | 2192/3305 [1:48:06<52:59,  2.86s/it][A
 66%|██████▋   | 2193/3305 [1:48:08<47:51,  2.58s/it][A
 66%|██████▋   | 2194/3305 [1:48:10<46:26,  2.51s/it][A
 66%|██████▋   | 2195/3305 [1:48:12<44:59,  2.43s/it][A
 66%|██████▋   | 2196/3305 [1:48:16<49:04,  2.66s/it][A
 66%|██████▋   | 2197/3305 [1:48:19<53:28,  2.90s/it][A
 67%|██████▋   | 2198/3305 [1:48:21<50:41,  2.75s/it][A
 67%|██████▋   | 2199/3305 [1:48:24<46:37,  2.53s/it][A
 67%|██████▋   | 2200/3305 [1:48:25<41:21,  2.25s/it][A
 67%|██████▋   | 2201/3305 [1:48:27<40:30,  2.20s/it][A
 67%|██████▋   | 2202/3305 [1:48:30<41:46,  2.27s/it][A
 67%|██████▋   | 2203/3305 [1:48:33<49:07,  2.67s/it][A
 67%|██████▋   | 2204/3305 [1:48:37<53:02,  2.89s/it][A
 67%|██████▋   | 2205/3305 [1:48:40<52:53,  2.88s/it][A
 67%|██████▋   | 2206/3305 [1:48:44<59:35,  3.25s/it][A
 67%|██████▋   | 2207/3305 [1:48:47<1:02:11,  3.40s/it][A
 67%|██████▋   | 2208/3305 [1:48:51<1:03:44,  3.49s/it][A
 67%|██████▋   | 2209/3305 [1:48:55<1:06:22,  3.63s/it][A
 67%|██████▋   | 2210/3305 [1:48:58<1:00:20,  3.31s/it][A
 67%|██████▋   | 2211/3305 [1:49:00<55:34,  3.05s/it]  [A
 67%|██████▋   | 2212/3305 [1:49:03<54:21,  2.98s/it][A
 67%|██████▋   | 2213/3305 [1:49:05<48:11,  2.65s/it][A
 67%|██████▋   | 2214/3305 [1:49:06<43:02,  2.37s/it][A
 67%|██████▋   | 2215/3305 [1:49:08<41:08,  2.26s/it][A
 67%|██████▋   | 2216/3305 [1:49:10<38:33,  2.12s/it][A
 67%|██████▋   | 2217/3305 [1:49:13<41:13,  2.27s/it][A
 67%|██████▋   | 2218/3305 [1:49:16<43:22,  2.39s/it][A
 67%|██████▋   | 2219/3305 [1:49:18<44:30,  2.46s/it][A
 67%|██████▋   | 2220/3305 [1:49:22<49:22,  2.73s/it][A
 67%|██████▋   | 2221/3305 [1:49:24<50:22,  2.79s/it][A
 67%|██████▋   | 2222/3305 [1:49:27<50:24,  2.79s/it][A
 67%|██████▋   | 2223/3305 [1:49:31<53:04,  2.94s/it][A
 67%|██████▋   | 2224/3305 [1:49:33<47:51,  2.66s/it][A
 67%|██████▋   | 2225/3305 [1:49:34<42:30,  2.36s/it][A
 67%|██████▋   | 2226/3305 [1:49:36<40:57,  2.28s/it][A
 67%|██████▋   | 2227/3305 [1:49:39<43:28,  2.42s/it][A
 67%|██████▋   | 2228/3305 [1:49:42<45:58,  2.56s/it][A
 67%|██████▋   | 2229/3305 [1:49:44<45:24,  2.53s/it][A
 67%|██████▋   | 2230/3305 [1:49:47<45:16,  2.53s/it][A
 68%|██████▊   | 2231/3305 [1:49:49<43:34,  2.43s/it][A
 68%|██████▊   | 2232/3305 [1:49:51<38:50,  2.17s/it][A
 68%|██████▊   | 2233/3305 [1:49:53<40:49,  2.29s/it][A
 68%|██████▊   | 2234/3305 [1:49:57<51:13,  2.87s/it][A
 68%|██████▊   | 2235/3305 [1:50:02<59:30,  3.34s/it][A
 68%|██████▊   | 2236/3305 [1:50:05<55:40,  3.13s/it][A
 68%|██████▊   | 2237/3305 [1:50:07<53:47,  3.02s/it][A
 68%|██████▊   | 2238/3305 [1:50:10<54:02,  3.04s/it][A
 68%|██████▊   | 2239/3305 [1:50:13<49:43,  2.80s/it][A
 68%|██████▊   | 2240/3305 [1:50:16<51:14,  2.89s/it][A
 68%|██████▊   | 2241/3305 [1:50:19<52:35,  2.97s/it][A
 68%|██████▊   | 2242/3305 [1:50:21<50:06,  2.83s/it][A
 68%|██████▊   | 2243/3305 [1:50:24<47:54,  2.71s/it][A
 68%|██████▊   | 2244/3305 [1:50:26<45:02,  2.55s/it][A
 68%|██████▊   | 2245/3305 [1:50:30<54:40,  3.10s/it][A
 68%|██████▊   | 2246/3305 [1:50:34<57:14,  3.24s/it][A
 68%|██████▊   | 2247/3305 [1:50:36<50:24,  2.86s/it][A
 68%|██████▊   | 2248/3305 [1:50:38<47:45,  2.71s/it][A
 68%|██████▊   | 2249/3305 [1:50:41<47:57,  2.72s/it][A
 68%|██████▊   | 2250/3305 [1:50:43<44:52,  2.55s/it][A
 68%|██████▊   | 2251/3305 [1:50:45<41:21,  2.35s/it][A
 68%|██████▊   | 2252/3305 [1:50:48<42:55,  2.45s/it][A
 68%|██████▊   | 2253/3305 [1:50:51<45:23,  2.59s/it][A
 68%|██████▊   | 2254/3305 [1:50:53<42:12,  2.41s/it][A
 68%|██████▊   | 2255/3305 [1:50:56<44:45,  2.56s/it][A
 68%|██████▊   | 2256/3305 [1:50:59<49:28,  2.83s/it][A
 68%|██████▊   | 2257/3305 [1:51:02<50:54,  2.91s/it][A
 68%|██████▊   | 2258/3305 [1:51:05<48:34,  2.78s/it][A
 68%|██████▊   | 2259/3305 [1:51:06<43:04,  2.47s/it][A
 68%|██████▊   | 2260/3305 [1:51:10<50:40,  2.91s/it][A
 68%|██████▊   | 2261/3305 [1:51:16<1:03:44,  3.66s/it][A
 68%|██████▊   | 2262/3305 [1:51:19<1:02:43,  3.61s/it][A
 68%|██████▊   | 2263/3305 [1:51:22<56:22,  3.25s/it]  [A
 69%|██████▊   | 2264/3305 [1:51:24<50:39,  2.92s/it][A
 69%|██████▊   | 2265/3305 [1:51:26<45:51,  2.65s/it][A
 69%|██████▊   | 2266/3305 [1:51:28<41:32,  2.40s/it][A
 69%|██████▊   | 2267/3305 [1:51:30<39:55,  2.31s/it][A
 69%|██████▊   | 2268/3305 [1:51:33<45:58,  2.66s/it][A
 69%|██████▊   | 2269/3305 [1:51:37<53:13,  3.08s/it][A
 69%|██████▊   | 2270/3305 [1:51:40<52:20,  3.03s/it][A
 69%|██████▊   | 2271/3305 [1:51:43<49:55,  2.90s/it][A
 69%|██████▊   | 2272/3305 [1:51:46<49:48,  2.89s/it][A
 69%|██████▉   | 2273/3305 [1:51:49<50:12,  2.92s/it][A
 69%|██████▉   | 2274/3305 [1:51:52<51:05,  2.97s/it][A
 69%|██████▉   | 2275/3305 [1:51:54<49:04,  2.86s/it][A
 69%|██████▉   | 2276/3305 [1:51:57<46:11,  2.69s/it][A
 69%|██████▉   | 2277/3305 [1:51:59<44:44,  2.61s/it][A
 69%|██████▉   | 2278/3305 [1:52:02<48:35,  2.84s/it][A
 69%|██████▉   | 2279/3305 [1:52:06<53:26,  3.13s/it][A
 69%|██████▉   | 2280/3305 [1:52:11<59:55,  3.51s/it][A
 69%|██████▉   | 2281/3305 [1:52:15<1:04:29,  3.78s/it][A
 69%|██████▉   | 2282/3305 [1:52:17<55:54,  3.28s/it]  [A
 69%|██████▉   | 2283/3305 [1:52:20<52:38,  3.09s/it][A
 69%|██████▉   | 2284/3305 [1:52:24<56:45,  3.34s/it][A
 69%|██████▉   | 2285/3305 [1:52:29<1:09:23,  4.08s/it][A
 69%|██████▉   | 2286/3305 [1:52:34<1:12:09,  4.25s/it][A
 69%|██████▉   | 2287/3305 [1:52:36<1:00:58,  3.59s/it][A
 69%|██████▉   | 2288/3305 [1:52:40<1:04:35,  3.81s/it][A
 69%|██████▉   | 2289/3305 [1:52:45<1:08:39,  4.05s/it][A
 69%|██████▉   | 2290/3305 [1:52:47<58:09,  3.44s/it]  [A
 69%|██████▉   | 2291/3305 [1:52:49<49:27,  2.93s/it][A
 69%|██████▉   | 2292/3305 [1:52:52<48:32,  2.87s/it][A
 69%|██████▉   | 2293/3305 [1:52:56<54:08,  3.21s/it][A
 69%|██████▉   | 2294/3305 [1:52:59<55:28,  3.29s/it][A
 69%|██████▉   | 2295/3305 [1:53:01<50:47,  3.02s/it][A
 69%|██████▉   | 2296/3305 [1:53:04<49:20,  2.93s/it][A
 70%|██████▉   | 2297/3305 [1:53:07<49:43,  2.96s/it][A
 70%|██████▉   | 2298/3305 [1:53:11<53:51,  3.21s/it][A
 70%|██████▉   | 2299/3305 [1:53:15<59:40,  3.56s/it][A
 70%|██████▉   | 2300/3305 [1:53:17<51:32,  3.08s/it][A
 70%|██████▉   | 2301/3305 [1:53:20<48:02,  2.87s/it][A
 70%|██████▉   | 2302/3305 [1:53:24<53:21,  3.19s/it][A
 70%|██████▉   | 2303/3305 [1:53:27<53:14,  3.19s/it][A
 70%|██████▉   | 2304/3305 [1:53:29<49:21,  2.96s/it][A
 70%|██████▉   | 2305/3305 [1:53:32<48:58,  2.94s/it][A
 70%|██████▉   | 2306/3305 [1:53:35<47:36,  2.86s/it][A
 70%|██████▉   | 2307/3305 [1:53:37<45:55,  2.76s/it][A
 70%|██████▉   | 2308/3305 [1:53:39<40:46,  2.45s/it][A
 70%|██████▉   | 2309/3305 [1:53:42<43:57,  2.65s/it][A
 70%|██████▉   | 2310/3305 [1:53:45<46:54,  2.83s/it][A
 70%|██████▉   | 2311/3305 [1:53:48<43:38,  2.63s/it][A
 70%|██████▉   | 2312/3305 [1:53:50<41:30,  2.51s/it][A
 70%|██████▉   | 2313/3305 [1:53:52<40:02,  2.42s/it][A
 70%|███████   | 2314/3305 [1:53:54<38:52,  2.35s/it][A
 70%|███████   | 2315/3305 [1:53:57<38:38,  2.34s/it][A
 70%|███████   | 2316/3305 [1:53:59<40:56,  2.48s/it][A
 70%|███████   | 2317/3305 [1:54:01<38:04,  2.31s/it][A
 70%|███████   | 2318/3305 [1:54:04<38:37,  2.35s/it][A
 70%|███████   | 2319/3305 [1:54:07<41:43,  2.54s/it][A
 70%|███████   | 2320/3305 [1:54:12<55:18,  3.37s/it][A
 70%|███████   | 2321/3305 [1:54:17<1:03:46,  3.89s/it][A
 70%|███████   | 2322/3305 [1:54:19<52:43,  3.22s/it]  [A
 70%|███████   | 2323/3305 [1:54:21<48:53,  2.99s/it][A
 70%|███████   | 2324/3305 [1:54:26<55:53,  3.42s/it][A
 70%|███████   | 2325/3305 [1:54:30<59:58,  3.67s/it][A
 70%|███████   | 2326/3305 [1:54:33<55:33,  3.40s/it][A
 70%|███████   | 2327/3305 [1:54:35<52:16,  3.21s/it][A
 70%|███████   | 2328/3305 [1:54:39<52:17,  3.21s/it][A
 70%|███████   | 2329/3305 [1:54:42<52:33,  3.23s/it][A
 70%|███████   | 2330/3305 [1:54:44<48:51,  3.01s/it][A
 71%|███████   | 2331/3305 [1:54:46<44:17,  2.73s/it][A
 71%|███████   | 2332/3305 [1:54:49<40:58,  2.53s/it][A
 71%|███████   | 2333/3305 [1:54:50<37:16,  2.30s/it][A
 71%|███████   | 2334/3305 [1:54:53<40:51,  2.53s/it][A
 71%|███████   | 2335/3305 [1:54:57<46:36,  2.88s/it][A
 71%|███████   | 2336/3305 [1:55:00<47:13,  2.92s/it][A
 71%|███████   | 2337/3305 [1:55:03<46:18,  2.87s/it][A
 71%|███████   | 2338/3305 [1:55:06<46:09,  2.86s/it][A
 71%|███████   | 2339/3305 [1:55:08<42:15,  2.63s/it][A
 71%|███████   | 2340/3305 [1:55:10<41:12,  2.56s/it][A
 71%|███████   | 2341/3305 [1:55:13<42:25,  2.64s/it][A
 71%|███████   | 2342/3305 [1:55:16<43:28,  2.71s/it][A
 71%|███████   | 2343/3305 [1:55:19<45:59,  2.87s/it][A
 71%|███████   | 2344/3305 [1:55:23<50:00,  3.12s/it][A
 71%|███████   | 2345/3305 [1:55:26<48:56,  3.06s/it][A
 71%|███████   | 2346/3305 [1:55:29<48:19,  3.02s/it][A
 71%|███████   | 2347/3305 [1:55:33<54:34,  3.42s/it][A
 71%|███████   | 2348/3305 [1:55:40<1:12:32,  4.55s/it][A
 71%|███████   | 2349/3305 [1:55:47<1:25:17,  5.35s/it][A
 71%|███████   | 2350/3305 [1:55:50<1:10:43,  4.44s/it][A
 71%|███████   | 2351/3305 [1:55:52<1:02:21,  3.92s/it][A
 71%|███████   | 2352/3305 [1:55:55<56:51,  3.58s/it]  [A
 71%|███████   | 2353/3305 [1:55:58<51:05,  3.22s/it][A
 71%|███████   | 2354/3305 [1:56:01<53:03,  3.35s/it][A
 71%|███████▏  | 2355/3305 [1:56:05<55:09,  3.48s/it][A
 71%|███████▏  | 2356/3305 [1:56:09<54:54,  3.47s/it][A
 71%|███████▏  | 2357/3305 [1:56:12<55:22,  3.51s/it][A
 71%|███████▏  | 2358/3305 [1:56:15<52:09,  3.31s/it][A
 71%|███████▏  | 2359/3305 [1:56:19<55:30,  3.52s/it][A
 71%|███████▏  | 2360/3305 [1:56:23<58:55,  3.74s/it][A
 71%|███████▏  | 2361/3305 [1:56:27<59:01,  3.75s/it][A
 71%|███████▏  | 2362/3305 [1:56:30<53:21,  3.39s/it][A
 71%|███████▏  | 2363/3305 [1:56:35<1:02:29,  3.98s/it][A
 72%|███████▏  | 2364/3305 [1:56:40<1:09:48,  4.45s/it][A
 72%|███████▏  | 2365/3305 [1:56:43<59:34,  3.80s/it]  [A
 72%|███████▏  | 2366/3305 [1:56:45<53:29,  3.42s/it][A
 72%|███████▏  | 2367/3305 [1:56:48<49:35,  3.17s/it][A
 72%|███████▏  | 2368/3305 [1:56:51<49:46,  3.19s/it][A
 72%|███████▏  | 2369/3305 [1:56:54<46:44,  3.00s/it][A
 72%|███████▏  | 2370/3305 [1:56:56<46:00,  2.95s/it][A
 72%|███████▏  | 2371/3305 [1:56:59<43:29,  2.79s/it][A
 72%|███████▏  | 2372/3305 [1:57:01<39:05,  2.51s/it][A
 72%|███████▏  | 2373/3305 [1:57:03<39:19,  2.53s/it][A
 72%|███████▏  | 2374/3305 [1:57:07<45:39,  2.94s/it][A
 72%|███████▏  | 2375/3305 [1:57:10<44:08,  2.85s/it][A
 72%|███████▏  | 2376/3305 [1:57:12<40:18,  2.60s/it][A
 72%|███████▏  | 2377/3305 [1:57:14<37:57,  2.45s/it][A
 72%|███████▏  | 2378/3305 [1:57:16<34:57,  2.26s/it][A
 72%|███████▏  | 2379/3305 [1:57:18<35:38,  2.31s/it][A
 72%|███████▏  | 2380/3305 [1:57:21<38:56,  2.53s/it][A
 72%|███████▏  | 2381/3305 [1:57:26<48:21,  3.14s/it][A
 72%|███████▏  | 2382/3305 [1:57:31<56:50,  3.69s/it][A
 72%|███████▏  | 2383/3305 [1:57:34<56:28,  3.68s/it][A
 72%|███████▏  | 2384/3305 [1:57:37<51:57,  3.38s/it][A
 72%|███████▏  | 2385/3305 [1:57:42<57:17,  3.74s/it][A
 72%|███████▏  | 2386/3305 [1:57:47<1:05:18,  4.26s/it][A
 72%|███████▏  | 2387/3305 [1:57:50<58:43,  3.84s/it]  [A
 72%|███████▏  | 2388/3305 [1:57:53<53:17,  3.49s/it][A
 72%|███████▏  | 2389/3305 [1:57:55<48:34,  3.18s/it][A
 72%|███████▏  | 2390/3305 [1:57:58<46:31,  3.05s/it][A
 72%|███████▏  | 2391/3305 [1:58:01<44:33,  2.93s/it][A
 72%|███████▏  | 2392/3305 [1:58:04<46:54,  3.08s/it][A
 72%|███████▏  | 2393/3305 [1:58:07<45:58,  3.02s/it][A
 72%|███████▏  | 2394/3305 [1:58:09<42:31,  2.80s/it][A
 72%|███████▏  | 2395/3305 [1:58:11<39:56,  2.63s/it][A
 72%|███████▏  | 2396/3305 [1:58:14<40:49,  2.69s/it][A
 73%|███████▎  | 2397/3305 [1:58:17<41:32,  2.74s/it][A
 73%|███████▎  | 2398/3305 [1:58:20<42:08,  2.79s/it][A
 73%|███████▎  | 2399/3305 [1:58:23<44:11,  2.93s/it][A
 73%|███████▎  | 2400/3305 [1:58:27<48:10,  3.19s/it][A
 73%|███████▎  | 2401/3305 [1:58:31<52:32,  3.49s/it][A
 73%|███████▎  | 2402/3305 [1:58:34<47:50,  3.18s/it][A
 73%|███████▎  | 2403/3305 [1:58:36<43:49,  2.92s/it][A
 73%|███████▎  | 2404/3305 [1:58:39<42:01,  2.80s/it][A
 73%|███████▎  | 2405/3305 [1:58:42<45:41,  3.05s/it][A
 73%|███████▎  | 2406/3305 [1:58:46<49:47,  3.32s/it][A
 73%|███████▎  | 2407/3305 [1:58:48<44:39,  2.98s/it][A
 73%|███████▎  | 2408/3305 [1:58:51<42:12,  2.82s/it][A
 73%|███████▎  | 2409/3305 [1:58:53<39:00,  2.61s/it][A
 73%|███████▎  | 2410/3305 [1:58:55<38:38,  2.59s/it][A
 73%|███████▎  | 2411/3305 [1:58:58<39:52,  2.68s/it][A
 73%|███████▎  | 2412/3305 [1:59:01<39:28,  2.65s/it][A
 73%|███████▎  | 2413/3305 [1:59:04<41:07,  2.77s/it][A
 73%|███████▎  | 2414/3305 [1:59:08<45:06,  3.04s/it][A
 73%|███████▎  | 2415/3305 [1:59:10<44:02,  2.97s/it][A
 73%|███████▎  | 2416/3305 [1:59:13<40:12,  2.71s/it][A
 73%|███████▎  | 2417/3305 [1:59:15<38:13,  2.58s/it][A
 73%|███████▎  | 2418/3305 [1:59:18<38:37,  2.61s/it][A
 73%|███████▎  | 2419/3305 [1:59:20<38:24,  2.60s/it][A
 73%|███████▎  | 2420/3305 [1:59:23<38:59,  2.64s/it][A
 73%|███████▎  | 2421/3305 [1:59:25<36:32,  2.48s/it][A
 73%|███████▎  | 2422/3305 [1:59:28<37:35,  2.55s/it][A
 73%|███████▎  | 2423/3305 [1:59:31<41:41,  2.84s/it][A
 73%|███████▎  | 2424/3305 [1:59:34<42:00,  2.86s/it][A
 73%|███████▎  | 2425/3305 [1:59:37<42:47,  2.92s/it][A
 73%|███████▎  | 2426/3305 [1:59:41<47:08,  3.22s/it][A
 73%|███████▎  | 2427/3305 [1:59:43<43:33,  2.98s/it][A
 73%|███████▎  | 2428/3305 [1:59:46<41:56,  2.87s/it][A
 73%|███████▎  | 2429/3305 [1:59:49<42:32,  2.91s/it][A
 74%|███████▎  | 2430/3305 [1:59:52<42:06,  2.89s/it][A
 74%|███████▎  | 2431/3305 [1:59:55<41:05,  2.82s/it][A
 74%|███████▎  | 2432/3305 [1:59:57<38:16,  2.63s/it][A
 74%|███████▎  | 2433/3305 [2:00:00<39:36,  2.73s/it][A
 74%|███████▎  | 2434/3305 [2:00:03<41:31,  2.86s/it][A
 74%|███████▎  | 2435/3305 [2:00:05<38:09,  2.63s/it][A
 74%|███████▎  | 2436/3305 [2:00:07<36:54,  2.55s/it][A
 74%|███████▎  | 2437/3305 [2:00:09<35:09,  2.43s/it][A
 74%|███████▍  | 2438/3305 [2:00:15<49:17,  3.41s/it][A
 74%|███████▍  | 2439/3305 [2:00:21<1:01:47,  4.28s/it][A
 74%|███████▍  | 2440/3305 [2:00:24<54:17,  3.77s/it]  [A
 74%|███████▍  | 2441/3305 [2:00:27<49:32,  3.44s/it][A
 74%|███████▍  | 2442/3305 [2:00:29<43:59,  3.06s/it][A
 74%|███████▍  | 2443/3305 [2:00:32<45:43,  3.18s/it][A
 74%|███████▍  | 2444/3305 [2:00:37<49:56,  3.48s/it][A
 74%|███████▍  | 2445/3305 [2:00:41<53:45,  3.75s/it][A
 74%|███████▍  | 2446/3305 [2:00:44<52:04,  3.64s/it][A
 74%|███████▍  | 2447/3305 [2:00:47<45:55,  3.21s/it][A
 74%|███████▍  | 2448/3305 [2:00:50<45:59,  3.22s/it][A
 74%|███████▍  | 2449/3305 [2:00:53<46:08,  3.23s/it][A
 74%|███████▍  | 2450/3305 [2:00:55<42:10,  2.96s/it][A
 74%|███████▍  | 2451/3305 [2:00:59<43:10,  3.03s/it][A
 74%|███████▍  | 2452/3305 [2:01:03<51:06,  3.59s/it][A
 74%|███████▍  | 2453/3305 [2:01:08<53:24,  3.76s/it][A
 74%|███████▍  | 2454/3305 [2:01:11<52:03,  3.67s/it][A
 74%|███████▍  | 2455/3305 [2:01:15<51:41,  3.65s/it][A
 74%|███████▍  | 2456/3305 [2:01:17<46:34,  3.29s/it][A
 74%|███████▍  | 2457/3305 [2:01:20<46:23,  3.28s/it][A
 74%|███████▍  | 2458/3305 [2:01:23<43:18,  3.07s/it][A
 74%|███████▍  | 2459/3305 [2:01:26<42:38,  3.02s/it][A
 74%|███████▍  | 2460/3305 [2:01:29<44:04,  3.13s/it][A
 74%|███████▍  | 2461/3305 [2:01:33<44:56,  3.20s/it][A
 74%|███████▍  | 2462/3305 [2:01:37<47:56,  3.41s/it][A
 75%|███████▍  | 2463/3305 [2:01:40<49:24,  3.52s/it][A
 75%|███████▍  | 2464/3305 [2:01:43<45:06,  3.22s/it][A
 75%|███████▍  | 2465/3305 [2:01:44<38:05,  2.72s/it][A
 75%|███████▍  | 2466/3305 [2:01:46<34:56,  2.50s/it][A
 75%|███████▍  | 2467/3305 [2:01:48<31:41,  2.27s/it][A
 75%|███████▍  | 2468/3305 [2:01:50<30:57,  2.22s/it][A
 75%|███████▍  | 2469/3305 [2:01:55<42:55,  3.08s/it][A
 75%|███████▍  | 2470/3305 [2:02:01<52:00,  3.74s/it][A
 75%|███████▍  | 2471/3305 [2:02:03<44:48,  3.22s/it][A
 75%|███████▍  | 2472/3305 [2:02:06<44:06,  3.18s/it][A
 75%|███████▍  | 2473/3305 [2:02:09<46:25,  3.35s/it][A
 75%|███████▍  | 2474/3305 [2:02:12<45:08,  3.26s/it][A
 75%|███████▍  | 2475/3305 [2:02:14<39:52,  2.88s/it][A
 75%|███████▍  | 2476/3305 [2:02:17<36:52,  2.67s/it][A
 75%|███████▍  | 2477/3305 [2:02:18<33:12,  2.41s/it][A
 75%|███████▍  | 2478/3305 [2:02:21<34:23,  2.50s/it][A
 75%|███████▌  | 2479/3305 [2:02:24<37:22,  2.72s/it][A
 75%|███████▌  | 2480/3305 [2:02:27<36:26,  2.65s/it][A
 75%|███████▌  | 2481/3305 [2:02:29<33:41,  2.45s/it][A
 75%|███████▌  | 2482/3305 [2:02:32<35:17,  2.57s/it][A
 75%|███████▌  | 2483/3305 [2:02:35<39:03,  2.85s/it][A
 75%|███████▌  | 2484/3305 [2:02:39<41:28,  3.03s/it][A
 75%|███████▌  | 2485/3305 [2:02:42<42:03,  3.08s/it][A
 75%|███████▌  | 2486/3305 [2:02:44<36:37,  2.68s/it][A
 75%|███████▌  | 2487/3305 [2:02:45<30:55,  2.27s/it][A
 75%|███████▌  | 2488/3305 [2:02:47<28:53,  2.12s/it][A
 75%|███████▌  | 2489/3305 [2:02:48<26:56,  1.98s/it][A
 75%|███████▌  | 2490/3305 [2:02:51<28:10,  2.07s/it][A
 75%|███████▌  | 2491/3305 [2:02:53<29:26,  2.17s/it][A
 75%|███████▌  | 2492/3305 [2:02:55<28:22,  2.09s/it][A
 75%|███████▌  | 2493/3305 [2:02:57<27:33,  2.04s/it][A
 75%|███████▌  | 2494/3305 [2:02:59<29:17,  2.17s/it][A
 75%|███████▌  | 2495/3305 [2:03:03<33:39,  2.49s/it][A
 76%|███████▌  | 2496/3305 [2:03:06<38:32,  2.86s/it][A
 76%|███████▌  | 2497/3305 [2:03:08<35:24,  2.63s/it][A
 76%|███████▌  | 2498/3305 [2:03:10<33:04,  2.46s/it][A
 76%|███████▌  | 2499/3305 [2:03:14<36:02,  2.68s/it][A
 76%|███████▌  | 2500/3305 [2:03:17<40:05,  2.99s/it][A
 76%|███████▌  | 2501/3305 [2:03:21<42:34,  3.18s/it][A
 76%|███████▌  | 2502/3305 [2:03:24<41:54,  3.13s/it][A
 76%|███████▌  | 2503/3305 [2:03:26<35:33,  2.66s/it][A
 76%|███████▌  | 2504/3305 [2:03:29<37:08,  2.78s/it][A
 76%|███████▌  | 2505/3305 [2:03:32<38:59,  2.92s/it][A
 76%|███████▌  | 2506/3305 [2:03:34<37:20,  2.80s/it][A
 76%|███████▌  | 2507/3305 [2:03:37<36:10,  2.72s/it][A
 76%|███████▌  | 2508/3305 [2:03:39<33:51,  2.55s/it][A
 76%|███████▌  | 2509/3305 [2:03:42<34:46,  2.62s/it][A
 76%|███████▌  | 2510/3305 [2:03:45<35:07,  2.65s/it][A
 76%|███████▌  | 2511/3305 [2:03:47<33:42,  2.55s/it][A
 76%|███████▌  | 2512/3305 [2:03:49<33:50,  2.56s/it][A
 76%|███████▌  | 2513/3305 [2:03:52<33:44,  2.56s/it][A
 76%|███████▌  | 2514/3305 [2:03:54<30:36,  2.32s/it][A
 76%|███████▌  | 2515/3305 [2:03:56<29:24,  2.23s/it][A
 76%|███████▌  | 2516/3305 [2:03:59<31:52,  2.42s/it][A
 76%|███████▌  | 2517/3305 [2:04:01<32:49,  2.50s/it][A
 76%|███████▌  | 2518/3305 [2:04:03<29:19,  2.24s/it][A
 76%|███████▌  | 2519/3305 [2:04:05<27:37,  2.11s/it][A
 76%|███████▌  | 2520/3305 [2:04:06<25:48,  1.97s/it][A
 76%|███████▋  | 2521/3305 [2:04:09<26:14,  2.01s/it][A
 76%|███████▋  | 2522/3305 [2:04:11<29:49,  2.29s/it][A
 76%|███████▋  | 2523/3305 [2:04:16<37:36,  2.89s/it][A
 76%|███████▋  | 2524/3305 [2:04:20<42:44,  3.28s/it][A
 76%|███████▋  | 2525/3305 [2:04:23<42:09,  3.24s/it][A
 76%|███████▋  | 2526/3305 [2:04:27<46:03,  3.55s/it][A
 76%|███████▋  | 2527/3305 [2:04:32<49:44,  3.84s/it][A
 76%|███████▋  | 2528/3305 [2:04:35<46:03,  3.56s/it][A
 77%|███████▋  | 2529/3305 [2:04:38<46:19,  3.58s/it][A
 77%|███████▋  | 2530/3305 [2:04:41<44:18,  3.43s/it][A
 77%|███████▋  | 2531/3305 [2:04:44<40:10,  3.11s/it][A
 77%|███████▋  | 2532/3305 [2:04:46<34:33,  2.68s/it][A
 77%|███████▋  | 2533/3305 [2:04:48<33:48,  2.63s/it][A
 77%|███████▋  | 2534/3305 [2:04:51<36:22,  2.83s/it][A
 77%|███████▋  | 2535/3305 [2:04:57<48:27,  3.78s/it][A
 77%|███████▋  | 2536/3305 [2:05:04<58:52,  4.59s/it][A
 77%|███████▋  | 2537/3305 [2:05:06<50:47,  3.97s/it][A
 77%|███████▋  | 2538/3305 [2:05:09<47:00,  3.68s/it][A
 77%|███████▋  | 2539/3305 [2:05:12<44:34,  3.49s/it][A
 77%|███████▋  | 2540/3305 [2:05:15<41:41,  3.27s/it][A
 77%|███████▋  | 2541/3305 [2:05:18<41:32,  3.26s/it][A
 77%|███████▋  | 2542/3305 [2:05:21<39:21,  3.10s/it][A
 77%|███████▋  | 2543/3305 [2:05:23<35:50,  2.82s/it][A
 77%|███████▋  | 2544/3305 [2:05:26<36:53,  2.91s/it][A
 77%|███████▋  | 2545/3305 [2:05:31<41:39,  3.29s/it][A
 77%|███████▋  | 2546/3305 [2:05:34<43:25,  3.43s/it][A
 77%|███████▋  | 2547/3305 [2:05:36<37:02,  2.93s/it][A
 77%|███████▋  | 2548/3305 [2:05:39<38:18,  3.04s/it][A
 77%|███████▋  | 2549/3305 [2:05:45<46:38,  3.70s/it][A
 77%|███████▋  | 2550/3305 [2:05:48<45:35,  3.62s/it][A
 77%|███████▋  | 2551/3305 [2:05:52<46:29,  3.70s/it][A
 77%|███████▋  | 2552/3305 [2:05:56<47:47,  3.81s/it][A
 77%|███████▋  | 2553/3305 [2:05:59<45:34,  3.64s/it][A
 77%|███████▋  | 2554/3305 [2:06:03<46:09,  3.69s/it][A
 77%|███████▋  | 2555/3305 [2:06:06<44:02,  3.52s/it][A
 77%|███████▋  | 2556/3305 [2:06:08<38:44,  3.10s/it][A
 77%|███████▋  | 2557/3305 [2:06:10<34:51,  2.80s/it][A
 77%|███████▋  | 2558/3305 [2:06:12<31:04,  2.50s/it][A
 77%|███████▋  | 2559/3305 [2:06:14<29:14,  2.35s/it][A
 77%|███████▋  | 2560/3305 [2:06:17<30:41,  2.47s/it][A
 77%|███████▋  | 2561/3305 [2:06:20<33:23,  2.69s/it][A
 78%|███████▊  | 2562/3305 [2:06:23<33:06,  2.67s/it][A
 78%|███████▊  | 2563/3305 [2:06:26<35:41,  2.89s/it][A
 78%|███████▊  | 2564/3305 [2:06:30<38:13,  3.09s/it][A
 78%|███████▊  | 2565/3305 [2:06:33<38:12,  3.10s/it][A
 78%|███████▊  | 2566/3305 [2:06:36<38:30,  3.13s/it][A
 78%|███████▊  | 2567/3305 [2:06:40<40:33,  3.30s/it][A
 78%|███████▊  | 2568/3305 [2:06:42<37:35,  3.06s/it][A
 78%|███████▊  | 2569/3305 [2:06:45<34:59,  2.85s/it][A
 78%|███████▊  | 2570/3305 [2:06:47<33:38,  2.75s/it][A
 78%|███████▊  | 2571/3305 [2:06:49<32:09,  2.63s/it][A
 78%|███████▊  | 2572/3305 [2:06:53<35:47,  2.93s/it][A
 78%|███████▊  | 2573/3305 [2:06:58<44:41,  3.66s/it][A
 78%|███████▊  | 2574/3305 [2:07:04<51:44,  4.25s/it][A
 78%|███████▊  | 2575/3305 [2:07:09<52:36,  4.32s/it][A
 78%|███████▊  | 2576/3305 [2:07:10<43:36,  3.59s/it][A
 78%|███████▊  | 2577/3305 [2:07:13<38:27,  3.17s/it][A
 78%|███████▊  | 2578/3305 [2:07:15<35:19,  2.92s/it][A
 78%|███████▊  | 2579/3305 [2:07:18<36:02,  2.98s/it][A
 78%|███████▊  | 2580/3305 [2:07:22<39:00,  3.23s/it][A
 78%|███████▊  | 2581/3305 [2:07:25<36:45,  3.05s/it][A
 78%|███████▊  | 2582/3305 [2:07:28<36:40,  3.04s/it][A
 78%|███████▊  | 2583/3305 [2:07:31<36:16,  3.01s/it][A
 78%|███████▊  | 2584/3305 [2:07:33<32:46,  2.73s/it][A
 78%|███████▊  | 2585/3305 [2:07:35<32:13,  2.69s/it][A
 78%|███████▊  | 2586/3305 [2:07:38<31:23,  2.62s/it][A
 78%|███████▊  | 2587/3305 [2:07:40<31:05,  2.60s/it][A
 78%|███████▊  | 2588/3305 [2:07:43<31:10,  2.61s/it][A
 78%|███████▊  | 2589/3305 [2:07:45<30:39,  2.57s/it][A
 78%|███████▊  | 2590/3305 [2:07:48<31:21,  2.63s/it][A
 78%|███████▊  | 2591/3305 [2:07:51<31:20,  2.63s/it][A
 78%|███████▊  | 2592/3305 [2:07:54<33:15,  2.80s/it][A
 78%|███████▊  | 2593/3305 [2:07:58<36:31,  3.08s/it][A
 78%|███████▊  | 2594/3305 [2:08:01<37:12,  3.14s/it][A
 79%|███████▊  | 2595/3305 [2:08:03<33:57,  2.87s/it][A
 79%|███████▊  | 2596/3305 [2:08:06<33:07,  2.80s/it][A
 79%|███████▊  | 2597/3305 [2:08:08<31:35,  2.68s/it][A
 79%|███████▊  | 2598/3305 [2:08:11<32:13,  2.73s/it][A
 79%|███████▊  | 2599/3305 [2:08:14<33:09,  2.82s/it][A
 79%|███████▊  | 2600/3305 [2:08:17<35:10,  2.99s/it][A
 79%|███████▊  | 2601/3305 [2:08:21<35:17,  3.01s/it][A
 79%|███████▊  | 2602/3305 [2:08:22<31:35,  2.70s/it][A
 79%|███████▉  | 2603/3305 [2:08:25<29:39,  2.54s/it][A
 79%|███████▉  | 2604/3305 [2:08:27<29:30,  2.52s/it][A
 79%|███████▉  | 2605/3305 [2:08:30<30:39,  2.63s/it][A
 79%|███████▉  | 2606/3305 [2:08:33<31:09,  2.67s/it][A
 79%|███████▉  | 2607/3305 [2:08:35<28:23,  2.44s/it][A
 79%|███████▉  | 2608/3305 [2:08:38<30:03,  2.59s/it][A
 79%|███████▉  | 2609/3305 [2:08:40<31:00,  2.67s/it][A
 79%|███████▉  | 2610/3305 [2:08:43<30:46,  2.66s/it][A
 79%|███████▉  | 2611/3305 [2:08:46<32:56,  2.85s/it][A
 79%|███████▉  | 2612/3305 [2:08:50<36:41,  3.18s/it][A
 79%|███████▉  | 2613/3305 [2:08:54<36:44,  3.19s/it][A
 79%|███████▉  | 2614/3305 [2:08:56<33:15,  2.89s/it][A
 79%|███████▉  | 2615/3305 [2:08:58<31:31,  2.74s/it][A
 79%|███████▉  | 2616/3305 [2:09:02<36:15,  3.16s/it][A
 79%|███████▉  | 2617/3305 [2:09:08<43:23,  3.78s/it][A
 79%|███████▉  | 2618/3305 [2:09:11<41:29,  3.62s/it][A
 79%|███████▉  | 2619/3305 [2:09:16<46:34,  4.07s/it][A
 79%|███████▉  | 2620/3305 [2:09:21<50:53,  4.46s/it][A
 79%|███████▉  | 2621/3305 [2:09:23<42:20,  3.71s/it][A
 79%|███████▉  | 2622/3305 [2:09:25<36:49,  3.23s/it][A
 79%|███████▉  | 2623/3305 [2:09:27<32:49,  2.89s/it][A
 79%|███████▉  | 2624/3305 [2:09:30<32:48,  2.89s/it][A
 79%|███████▉  | 2625/3305 [2:09:33<33:12,  2.93s/it][A
 79%|███████▉  | 2626/3305 [2:09:36<32:36,  2.88s/it][A
 79%|███████▉  | 2627/3305 [2:09:39<32:25,  2.87s/it][A
 80%|███████▉  | 2628/3305 [2:09:41<29:40,  2.63s/it][A
 80%|███████▉  | 2629/3305 [2:09:44<31:16,  2.78s/it][A
 80%|███████▉  | 2630/3305 [2:09:46<28:45,  2.56s/it][A
 80%|███████▉  | 2631/3305 [2:09:48<27:22,  2.44s/it][A
 80%|███████▉  | 2632/3305 [2:09:51<27:13,  2.43s/it][A
 80%|███████▉  | 2633/3305 [2:09:53<26:19,  2.35s/it][A
 80%|███████▉  | 2634/3305 [2:09:58<33:55,  3.03s/it][A
 80%|███████▉  | 2635/3305 [2:10:03<41:06,  3.68s/it][A
 80%|███████▉  | 2636/3305 [2:10:05<35:12,  3.16s/it][A
 80%|███████▉  | 2637/3305 [2:10:09<38:02,  3.42s/it][A
 80%|███████▉  | 2638/3305 [2:10:16<49:48,  4.48s/it][A
 80%|███████▉  | 2639/3305 [2:10:22<55:12,  4.97s/it][A
 80%|███████▉  | 2640/3305 [2:10:25<50:06,  4.52s/it][A
 80%|███████▉  | 2641/3305 [2:10:28<43:59,  3.98s/it][A
 80%|███████▉  | 2642/3305 [2:10:30<38:09,  3.45s/it][A
 80%|███████▉  | 2643/3305 [2:10:34<38:52,  3.52s/it][A
 80%|████████  | 2644/3305 [2:10:38<42:27,  3.85s/it][A
 80%|████████  | 2645/3305 [2:10:42<40:44,  3.70s/it][A
 80%|████████  | 2646/3305 [2:10:46<42:11,  3.84s/it][A
 80%|████████  | 2647/3305 [2:10:50<43:18,  3.95s/it][A
 80%|████████  | 2648/3305 [2:10:54<41:51,  3.82s/it][A
 80%|████████  | 2649/3305 [2:10:56<36:10,  3.31s/it][A
 80%|████████  | 2650/3305 [2:10:58<32:14,  2.95s/it][A
 80%|████████  | 2651/3305 [2:11:01<31:06,  2.85s/it][A
 80%|████████  | 2652/3305 [2:11:05<36:38,  3.37s/it][A
 80%|████████  | 2653/3305 [2:11:10<40:22,  3.72s/it][A
 80%|████████  | 2654/3305 [2:11:13<37:25,  3.45s/it][A
 80%|████████  | 2655/3305 [2:11:15<34:34,  3.19s/it][A
 80%|████████  | 2656/3305 [2:11:18<32:56,  3.05s/it][A
 80%|████████  | 2657/3305 [2:11:21<33:50,  3.13s/it][A
 80%|████████  | 2658/3305 [2:11:24<33:03,  3.07s/it][A
 80%|████████  | 2659/3305 [2:11:26<30:53,  2.87s/it][A
 80%|████████  | 2660/3305 [2:11:29<31:00,  2.88s/it][A
 81%|████████  | 2661/3305 [2:11:32<31:29,  2.93s/it][A
 81%|████████  | 2662/3305 [2:11:35<31:42,  2.96s/it][A
 81%|████████  | 2663/3305 [2:11:40<38:01,  3.55s/it][A
 81%|████████  | 2664/3305 [2:11:45<39:53,  3.73s/it][A
 81%|████████  | 2665/3305 [2:11:47<35:29,  3.33s/it][A
 81%|████████  | 2666/3305 [2:11:50<34:38,  3.25s/it][A
 81%|████████  | 2667/3305 [2:11:53<32:16,  3.04s/it][A
 81%|████████  | 2668/3305 [2:11:54<28:48,  2.71s/it][A
 81%|████████  | 2669/3305 [2:11:57<27:32,  2.60s/it][A
 81%|████████  | 2670/3305 [2:12:00<28:31,  2.70s/it][A
 81%|████████  | 2671/3305 [2:12:04<32:13,  3.05s/it][A
 81%|████████  | 2672/3305 [2:12:07<34:22,  3.26s/it][A
 81%|████████  | 2673/3305 [2:12:10<31:45,  3.02s/it][A
 81%|████████  | 2674/3305 [2:12:12<29:18,  2.79s/it][A
 81%|████████  | 2675/3305 [2:12:15<28:34,  2.72s/it][A
 81%|████████  | 2676/3305 [2:12:17<28:02,  2.67s/it][A
 81%|████████  | 2677/3305 [2:12:20<27:10,  2.60s/it][A
 81%|████████  | 2678/3305 [2:12:22<27:10,  2.60s/it][A
 81%|████████  | 2679/3305 [2:12:25<27:14,  2.61s/it][A
 81%|████████  | 2680/3305 [2:12:27<26:46,  2.57s/it][A
 81%|████████  | 2681/3305 [2:12:29<24:38,  2.37s/it][A
 81%|████████  | 2682/3305 [2:12:31<23:26,  2.26s/it][A
 81%|████████  | 2683/3305 [2:12:34<25:31,  2.46s/it][A
 81%|████████  | 2684/3305 [2:12:37<26:11,  2.53s/it][A
 81%|████████  | 2685/3305 [2:12:39<24:32,  2.37s/it][A
 81%|████████▏ | 2686/3305 [2:12:42<27:32,  2.67s/it][A
 81%|████████▏ | 2687/3305 [2:12:46<31:54,  3.10s/it][A
 81%|████████▏ | 2688/3305 [2:12:49<29:47,  2.90s/it][A
 81%|████████▏ | 2689/3305 [2:12:51<28:23,  2.77s/it][A
 81%|████████▏ | 2690/3305 [2:12:53<25:21,  2.47s/it][A
 81%|████████▏ | 2691/3305 [2:12:55<23:36,  2.31s/it][A
 81%|████████▏ | 2692/3305 [2:12:58<24:40,  2.42s/it][A
 81%|████████▏ | 2693/3305 [2:13:00<25:25,  2.49s/it][A
 82%|████████▏ | 2694/3305 [2:13:04<27:53,  2.74s/it][A
 82%|████████▏ | 2695/3305 [2:13:07<29:05,  2.86s/it][A
 82%|████████▏ | 2696/3305 [2:13:09<26:13,  2.58s/it][A
 82%|████████▏ | 2697/3305 [2:13:11<25:38,  2.53s/it][A
 82%|████████▏ | 2698/3305 [2:13:14<26:57,  2.66s/it][A
 82%|████████▏ | 2699/3305 [2:13:17<26:46,  2.65s/it][A
 82%|████████▏ | 2700/3305 [2:13:19<26:57,  2.67s/it][A
 82%|████████▏ | 2701/3305 [2:13:23<29:09,  2.90s/it][A
 82%|████████▏ | 2702/3305 [2:13:26<30:22,  3.02s/it][A
 82%|████████▏ | 2703/3305 [2:13:28<28:08,  2.80s/it][A
 82%|████████▏ | 2704/3305 [2:13:32<31:46,  3.17s/it][A
 82%|████████▏ | 2705/3305 [2:13:37<36:10,  3.62s/it][A
 82%|████████▏ | 2706/3305 [2:13:40<33:38,  3.37s/it][A
 82%|████████▏ | 2707/3305 [2:13:43<31:18,  3.14s/it][A
 82%|████████▏ | 2708/3305 [2:13:45<29:03,  2.92s/it][A
 82%|████████▏ | 2709/3305 [2:13:48<29:05,  2.93s/it][A
 82%|████████▏ | 2710/3305 [2:13:51<28:29,  2.87s/it][A
 82%|████████▏ | 2711/3305 [2:13:53<28:18,  2.86s/it][A
 82%|████████▏ | 2712/3305 [2:13:58<32:09,  3.25s/it][A
 82%|████████▏ | 2713/3305 [2:14:03<38:23,  3.89s/it][A
 82%|████████▏ | 2714/3305 [2:14:06<36:55,  3.75s/it][A
 82%|████████▏ | 2715/3305 [2:14:09<32:56,  3.35s/it][A
 82%|████████▏ | 2716/3305 [2:14:11<29:27,  3.00s/it][A
 82%|████████▏ | 2717/3305 [2:14:13<27:06,  2.77s/it][A
 82%|████████▏ | 2718/3305 [2:14:16<26:58,  2.76s/it][A
 82%|████████▏ | 2719/3305 [2:14:18<26:07,  2.67s/it][A
 82%|████████▏ | 2720/3305 [2:14:21<24:38,  2.53s/it][A
 82%|████████▏ | 2721/3305 [2:14:23<23:36,  2.43s/it][A
 82%|████████▏ | 2722/3305 [2:14:26<24:51,  2.56s/it][A
 82%|████████▏ | 2723/3305 [2:14:29<25:33,  2.64s/it][A
 82%|████████▏ | 2724/3305 [2:14:31<25:57,  2.68s/it][A
 82%|████████▏ | 2725/3305 [2:14:37<34:13,  3.54s/it][A
 82%|████████▏ | 2726/3305 [2:14:44<43:43,  4.53s/it][A
 83%|████████▎ | 2727/3305 [2:14:48<42:04,  4.37s/it][A
 83%|████████▎ | 2728/3305 [2:14:51<37:51,  3.94s/it][A
 83%|████████▎ | 2729/3305 [2:14:53<33:09,  3.45s/it][A
 83%|████████▎ | 2730/3305 [2:14:54<26:43,  2.79s/it][A
 83%|████████▎ | 2731/3305 [2:14:56<25:17,  2.64s/it][A
 83%|████████▎ | 2732/3305 [2:14:59<24:39,  2.58s/it][A
 83%|████████▎ | 2733/3305 [2:15:01<22:40,  2.38s/it][A
 83%|████████▎ | 2734/3305 [2:15:04<24:12,  2.54s/it][A
 83%|████████▎ | 2735/3305 [2:15:07<25:23,  2.67s/it][A
 83%|████████▎ | 2736/3305 [2:15:09<23:59,  2.53s/it][A
 83%|████████▎ | 2737/3305 [2:15:11<21:43,  2.30s/it][A
 83%|████████▎ | 2738/3305 [2:15:13<21:19,  2.26s/it][A
 83%|████████▎ | 2739/3305 [2:15:15<20:12,  2.14s/it][A
 83%|████████▎ | 2740/3305 [2:15:17<20:47,  2.21s/it][A
 83%|████████▎ | 2741/3305 [2:15:20<23:19,  2.48s/it][A
 83%|████████▎ | 2742/3305 [2:15:23<24:30,  2.61s/it][A
 83%|████████▎ | 2743/3305 [2:15:26<25:01,  2.67s/it][A
 83%|████████▎ | 2744/3305 [2:15:28<23:41,  2.53s/it][A
 83%|████████▎ | 2745/3305 [2:15:33<31:10,  3.34s/it][A
 83%|████████▎ | 2746/3305 [2:15:40<41:09,  4.42s/it][A
 83%|████████▎ | 2747/3305 [2:15:44<38:38,  4.15s/it][A
 83%|████████▎ | 2748/3305 [2:15:47<36:00,  3.88s/it][A
 83%|████████▎ | 2749/3305 [2:15:51<36:02,  3.89s/it][A
 83%|████████▎ | 2750/3305 [2:15:54<33:32,  3.63s/it][A
 83%|████████▎ | 2751/3305 [2:15:56<30:16,  3.28s/it][A
 83%|████████▎ | 2752/3305 [2:16:00<31:18,  3.40s/it][A
 83%|████████▎ | 2753/3305 [2:16:04<32:28,  3.53s/it][A
 83%|████████▎ | 2754/3305 [2:16:07<32:16,  3.52s/it][A
 83%|████████▎ | 2755/3305 [2:16:12<36:12,  3.95s/it][A
 83%|████████▎ | 2756/3305 [2:16:18<39:55,  4.36s/it][A
 83%|████████▎ | 2757/3305 [2:16:22<38:31,  4.22s/it][A
 83%|████████▎ | 2758/3305 [2:16:25<34:48,  3.82s/it][A
 83%|████████▎ | 2759/3305 [2:16:26<29:40,  3.26s/it][A
 84%|████████▎ | 2760/3305 [2:16:29<26:36,  2.93s/it][A
 84%|████████▎ | 2761/3305 [2:16:31<24:39,  2.72s/it][A
 84%|████████▎ | 2762/3305 [2:16:34<24:33,  2.71s/it][A
 84%|████████▎ | 2763/3305 [2:16:36<25:05,  2.78s/it][A
 84%|████████▎ | 2764/3305 [2:16:40<26:48,  2.97s/it][A
 84%|████████▎ | 2765/3305 [2:16:43<27:24,  3.05s/it][A
 84%|████████▎ | 2766/3305 [2:16:45<24:01,  2.67s/it][A
 84%|████████▎ | 2767/3305 [2:16:47<22:49,  2.54s/it][A
 84%|████████▍ | 2768/3305 [2:16:50<23:27,  2.62s/it][A
 84%|████████▍ | 2769/3305 [2:16:53<23:28,  2.63s/it][A
 84%|████████▍ | 2770/3305 [2:16:55<22:02,  2.47s/it][A
 84%|████████▍ | 2771/3305 [2:16:57<22:20,  2.51s/it][A
 84%|████████▍ | 2772/3305 [2:17:00<23:54,  2.69s/it][A
 84%|████████▍ | 2773/3305 [2:17:03<24:47,  2.80s/it][A
 84%|████████▍ | 2774/3305 [2:17:06<24:04,  2.72s/it][A
 84%|████████▍ | 2775/3305 [2:17:08<23:17,  2.64s/it][A
 84%|████████▍ | 2776/3305 [2:17:11<23:37,  2.68s/it][A
 84%|████████▍ | 2777/3305 [2:17:14<23:59,  2.73s/it][A
 84%|████████▍ | 2778/3305 [2:17:17<24:50,  2.83s/it][A
 84%|████████▍ | 2779/3305 [2:17:20<25:40,  2.93s/it][A
 84%|████████▍ | 2780/3305 [2:17:23<24:13,  2.77s/it][A
 84%|████████▍ | 2781/3305 [2:17:25<23:32,  2.69s/it][A
 84%|████████▍ | 2782/3305 [2:17:29<25:17,  2.90s/it][A
 84%|████████▍ | 2783/3305 [2:17:33<30:23,  3.49s/it][A
 84%|████████▍ | 2784/3305 [2:17:38<32:54,  3.79s/it][A
 84%|████████▍ | 2785/3305 [2:17:41<30:11,  3.48s/it][A
 84%|████████▍ | 2786/3305 [2:17:44<28:28,  3.29s/it][A
 84%|████████▍ | 2787/3305 [2:17:46<26:29,  3.07s/it][A
 84%|████████▍ | 2788/3305 [2:17:49<24:55,  2.89s/it][A
 84%|████████▍ | 2789/3305 [2:17:51<24:26,  2.84s/it][A
 84%|████████▍ | 2790/3305 [2:17:54<23:55,  2.79s/it][A
 84%|████████▍ | 2791/3305 [2:17:55<20:09,  2.35s/it][A
 84%|████████▍ | 2792/3305 [2:17:57<19:08,  2.24s/it][A
 85%|████████▍ | 2793/3305 [2:17:59<18:53,  2.21s/it][A
 85%|████████▍ | 2794/3305 [2:18:01<17:11,  2.02s/it][A
 85%|████████▍ | 2795/3305 [2:18:03<17:24,  2.05s/it][A
 85%|████████▍ | 2796/3305 [2:18:06<19:07,  2.25s/it][A
 85%|████████▍ | 2797/3305 [2:18:08<19:03,  2.25s/it][A
 85%|████████▍ | 2798/3305 [2:18:12<22:32,  2.67s/it][A
 85%|████████▍ | 2799/3305 [2:18:15<25:05,  2.98s/it][A
 85%|████████▍ | 2800/3305 [2:18:17<21:54,  2.60s/it][A
 85%|████████▍ | 2801/3305 [2:18:19<20:03,  2.39s/it][A
 85%|████████▍ | 2802/3305 [2:18:24<26:20,  3.14s/it][A
 85%|████████▍ | 2803/3305 [2:18:31<35:59,  4.30s/it][A
 85%|████████▍ | 2804/3305 [2:18:35<34:43,  4.16s/it][A
 85%|████████▍ | 2805/3305 [2:18:37<30:38,  3.68s/it][A
 85%|████████▍ | 2806/3305 [2:18:42<32:24,  3.90s/it][A
 85%|████████▍ | 2807/3305 [2:18:45<30:09,  3.63s/it][A
 85%|████████▍ | 2808/3305 [2:18:47<26:11,  3.16s/it][A
 85%|████████▍ | 2809/3305 [2:18:49<23:39,  2.86s/it][A
 85%|████████▌ | 2810/3305 [2:18:52<24:32,  2.97s/it][A
 85%|████████▌ | 2811/3305 [2:18:56<27:05,  3.29s/it][A
 85%|████████▌ | 2812/3305 [2:19:00<27:21,  3.33s/it][A
 85%|████████▌ | 2813/3305 [2:19:03<28:06,  3.43s/it][A
 85%|████████▌ | 2814/3305 [2:19:06<26:51,  3.28s/it][A
 85%|████████▌ | 2815/3305 [2:19:08<23:58,  2.94s/it][A
 85%|████████▌ | 2816/3305 [2:19:10<21:47,  2.67s/it][A
 85%|████████▌ | 2817/3305 [2:19:13<20:13,  2.49s/it][A
 85%|████████▌ | 2818/3305 [2:19:15<19:12,  2.37s/it][A
 85%|████████▌ | 2819/3305 [2:19:17<18:05,  2.23s/it][A
 85%|████████▌ | 2820/3305 [2:19:19<18:09,  2.25s/it][A
 85%|████████▌ | 2821/3305 [2:19:22<19:22,  2.40s/it][A
 85%|████████▌ | 2822/3305 [2:19:24<19:39,  2.44s/it][A
 85%|████████▌ | 2823/3305 [2:19:26<18:57,  2.36s/it][A
 85%|████████▌ | 2824/3305 [2:19:28<16:31,  2.06s/it][A
 85%|████████▌ | 2825/3305 [2:19:30<18:14,  2.28s/it][A
 86%|████████▌ | 2826/3305 [2:19:33<19:07,  2.40s/it][A
 86%|████████▌ | 2827/3305 [2:19:35<18:08,  2.28s/it][A
 86%|████████▌ | 2828/3305 [2:19:37<17:51,  2.25s/it][A
 86%|████████▌ | 2829/3305 [2:19:40<20:02,  2.53s/it][A
 86%|████████▌ | 2830/3305 [2:19:44<21:16,  2.69s/it][A
 86%|████████▌ | 2831/3305 [2:19:45<18:58,  2.40s/it][A
 86%|████████▌ | 2832/3305 [2:19:48<18:47,  2.38s/it][A
 86%|████████▌ | 2833/3305 [2:19:50<18:28,  2.35s/it][A
 86%|████████▌ | 2834/3305 [2:19:53<21:04,  2.69s/it][A
 86%|████████▌ | 2835/3305 [2:19:57<23:14,  2.97s/it][A
 86%|████████▌ | 2836/3305 [2:19:59<20:49,  2.66s/it][A
 86%|████████▌ | 2837/3305 [2:20:01<19:31,  2.50s/it][A
 86%|████████▌ | 2838/3305 [2:20:04<19:37,  2.52s/it][A
 86%|████████▌ | 2839/3305 [2:20:06<20:05,  2.59s/it][A
 86%|████████▌ | 2840/3305 [2:20:09<19:42,  2.54s/it][A
 86%|████████▌ | 2841/3305 [2:20:11<18:13,  2.36s/it][A
 86%|████████▌ | 2842/3305 [2:20:12<16:32,  2.14s/it][A
 86%|████████▌ | 2843/3305 [2:20:15<17:16,  2.24s/it][A
 86%|████████▌ | 2844/3305 [2:20:18<19:20,  2.52s/it][A
 86%|████████▌ | 2845/3305 [2:20:21<19:54,  2.60s/it][A
 86%|████████▌ | 2846/3305 [2:20:24<20:45,  2.71s/it][A
 86%|████████▌ | 2847/3305 [2:20:27<21:01,  2.75s/it][A
 86%|████████▌ | 2848/3305 [2:20:29<20:43,  2.72s/it][A
 86%|████████▌ | 2849/3305 [2:20:31<19:21,  2.55s/it][A
 86%|████████▌ | 2850/3305 [2:20:34<19:41,  2.60s/it][A
 86%|████████▋ | 2851/3305 [2:20:37<19:57,  2.64s/it][A
 86%|████████▋ | 2852/3305 [2:20:39<18:08,  2.40s/it][A
 86%|████████▋ | 2853/3305 [2:20:41<17:14,  2.29s/it][A
 86%|████████▋ | 2854/3305 [2:20:43<18:05,  2.41s/it][A
 86%|████████▋ | 2855/3305 [2:20:47<20:51,  2.78s/it][A
 86%|████████▋ | 2856/3305 [2:20:52<26:17,  3.51s/it][A
 86%|████████▋ | 2857/3305 [2:20:57<28:52,  3.87s/it][A
 86%|████████▋ | 2858/3305 [2:20:59<24:56,  3.35s/it][A
 87%|████████▋ | 2859/3305 [2:21:01<22:22,  3.01s/it][A
 87%|████████▋ | 2860/3305 [2:21:04<20:44,  2.80s/it][A
 87%|████████▋ | 2861/3305 [2:21:05<18:16,  2.47s/it][A
 87%|████████▋ | 2862/3305 [2:21:08<18:10,  2.46s/it][A
 87%|████████▋ | 2863/3305 [2:21:10<18:17,  2.48s/it][A
 87%|████████▋ | 2864/3305 [2:21:14<21:21,  2.91s/it][A
 87%|████████▋ | 2865/3305 [2:21:19<24:50,  3.39s/it][A
 87%|████████▋ | 2866/3305 [2:21:22<23:42,  3.24s/it][A
 87%|████████▋ | 2867/3305 [2:21:25<23:08,  3.17s/it][A
 87%|████████▋ | 2868/3305 [2:21:28<22:36,  3.11s/it][A
 87%|████████▋ | 2869/3305 [2:21:33<26:58,  3.71s/it][A
 87%|████████▋ | 2870/3305 [2:21:39<33:07,  4.57s/it][A
 87%|████████▋ | 2871/3305 [2:21:44<32:48,  4.53s/it][A
 87%|████████▋ | 2872/3305 [2:21:46<27:52,  3.86s/it][A
 87%|████████▋ | 2873/3305 [2:21:49<25:01,  3.48s/it][A
 87%|████████▋ | 2874/3305 [2:21:51<23:37,  3.29s/it][A
 87%|████████▋ | 2875/3305 [2:21:54<21:46,  3.04s/it][A
 87%|████████▋ | 2876/3305 [2:21:56<18:58,  2.65s/it][A
 87%|████████▋ | 2877/3305 [2:21:58<18:55,  2.65s/it][A
 87%|████████▋ | 2878/3305 [2:22:01<19:36,  2.76s/it][A
 87%|████████▋ | 2879/3305 [2:22:04<20:09,  2.84s/it][A
 87%|████████▋ | 2880/3305 [2:22:08<21:53,  3.09s/it][A
 87%|████████▋ | 2881/3305 [2:22:11<22:08,  3.13s/it][A
 87%|████████▋ | 2882/3305 [2:22:15<22:47,  3.23s/it][A
 87%|████████▋ | 2883/3305 [2:22:18<22:00,  3.13s/it][A
 87%|████████▋ | 2884/3305 [2:22:20<19:56,  2.84s/it][A
 87%|████████▋ | 2885/3305 [2:22:22<18:40,  2.67s/it][A
 87%|████████▋ | 2886/3305 [2:22:24<17:44,  2.54s/it][A
 87%|████████▋ | 2887/3305 [2:22:27<18:50,  2.70s/it][A
 87%|████████▋ | 2888/3305 [2:22:30<19:17,  2.77s/it][A
 87%|████████▋ | 2889/3305 [2:22:33<18:36,  2.68s/it][A
 87%|████████▋ | 2890/3305 [2:22:36<18:50,  2.72s/it][A
 87%|████████▋ | 2891/3305 [2:22:38<19:05,  2.77s/it][A
 88%|████████▊ | 2892/3305 [2:22:42<19:44,  2.87s/it][A
 88%|████████▊ | 2893/3305 [2:22:44<19:37,  2.86s/it][A
 88%|████████▊ | 2894/3305 [2:22:47<18:53,  2.76s/it][A
 88%|████████▊ | 2895/3305 [2:22:49<18:19,  2.68s/it][A
 88%|████████▊ | 2896/3305 [2:22:52<18:27,  2.71s/it][A
 88%|████████▊ | 2897/3305 [2:22:54<16:36,  2.44s/it][A
 88%|████████▊ | 2898/3305 [2:22:56<14:45,  2.18s/it][A
 88%|████████▊ | 2899/3305 [2:22:57<14:12,  2.10s/it][A
 88%|████████▊ | 2900/3305 [2:23:02<19:02,  2.82s/it][A
 88%|████████▊ | 2901/3305 [2:23:07<24:00,  3.57s/it][A
 88%|████████▊ | 2902/3305 [2:23:09<20:54,  3.11s/it][A
 88%|████████▊ | 2903/3305 [2:23:12<20:49,  3.11s/it][A
 88%|████████▊ | 2904/3305 [2:23:16<20:57,  3.13s/it][A
 88%|████████▊ | 2905/3305 [2:23:19<21:36,  3.24s/it][A
 88%|████████▊ | 2906/3305 [2:23:23<22:25,  3.37s/it][A
 88%|████████▊ | 2907/3305 [2:23:25<20:39,  3.11s/it][A
 88%|████████▊ | 2908/3305 [2:23:29<21:40,  3.28s/it][A
 88%|████████▊ | 2909/3305 [2:23:32<21:50,  3.31s/it][A
 88%|████████▊ | 2910/3305 [2:23:35<20:16,  3.08s/it][A
 88%|████████▊ | 2911/3305 [2:23:38<19:32,  2.98s/it][A
 88%|████████▊ | 2912/3305 [2:23:40<19:11,  2.93s/it][A
 88%|████████▊ | 2913/3305 [2:23:42<17:09,  2.63s/it][A
 88%|████████▊ | 2914/3305 [2:23:44<15:07,  2.32s/it][A
 88%|████████▊ | 2915/3305 [2:23:46<13:38,  2.10s/it][A
 88%|████████▊ | 2916/3305 [2:23:48<13:22,  2.06s/it][A
 88%|████████▊ | 2917/3305 [2:23:49<12:48,  1.98s/it][A
 88%|████████▊ | 2918/3305 [2:23:52<14:38,  2.27s/it][A
 88%|████████▊ | 2919/3305 [2:23:56<16:46,  2.61s/it][A
 88%|████████▊ | 2920/3305 [2:23:58<15:34,  2.43s/it][A
 88%|████████▊ | 2921/3305 [2:24:01<17:08,  2.68s/it][A
 88%|████████▊ | 2922/3305 [2:24:06<21:04,  3.30s/it][A
 88%|████████▊ | 2923/3305 [2:24:09<20:04,  3.15s/it][A
 88%|████████▊ | 2924/3305 [2:24:11<18:22,  2.89s/it][A
 89%|████████▊ | 2925/3305 [2:24:13<17:21,  2.74s/it][A
 89%|████████▊ | 2926/3305 [2:24:17<20:03,  3.17s/it][A
 89%|████████▊ | 2927/3305 [2:24:22<22:38,  3.59s/it][A
 89%|████████▊ | 2928/3305 [2:24:24<19:37,  3.12s/it][A
 89%|████████▊ | 2929/3305 [2:24:28<20:49,  3.32s/it][A
 89%|████████▊ | 2930/3305 [2:24:32<22:23,  3.58s/it][A
 89%|████████▊ | 2931/3305 [2:24:35<20:44,  3.33s/it][A
 89%|████████▊ | 2932/3305 [2:24:38<21:05,  3.39s/it][A
 89%|████████▊ | 2933/3305 [2:24:41<18:59,  3.06s/it][A
 89%|████████▉ | 2934/3305 [2:24:42<16:39,  2.69s/it][A
 89%|████████▉ | 2935/3305 [2:24:44<15:07,  2.45s/it][A
 89%|████████▉ | 2936/3305 [2:24:46<14:10,  2.30s/it][A
 89%|████████▉ | 2937/3305 [2:24:49<14:32,  2.37s/it][A
 89%|████████▉ | 2938/3305 [2:24:52<15:18,  2.50s/it][A
 89%|████████▉ | 2939/3305 [2:24:54<15:41,  2.57s/it][A
 89%|████████▉ | 2940/3305 [2:24:57<15:54,  2.61s/it][A
 89%|████████▉ | 2941/3305 [2:25:00<16:19,  2.69s/it][A
 89%|████████▉ | 2942/3305 [2:25:03<17:10,  2.84s/it][A
 89%|████████▉ | 2943/3305 [2:25:07<18:15,  3.03s/it][A
 89%|████████▉ | 2944/3305 [2:25:11<19:59,  3.32s/it][A
 89%|████████▉ | 2945/3305 [2:25:14<19:32,  3.26s/it][A
 89%|████████▉ | 2946/3305 [2:25:17<19:19,  3.23s/it][A
 89%|████████▉ | 2947/3305 [2:25:21<21:25,  3.59s/it][A
 89%|████████▉ | 2948/3305 [2:25:26<22:50,  3.84s/it][A
 89%|████████▉ | 2949/3305 [2:25:28<20:34,  3.47s/it][A
 89%|████████▉ | 2950/3305 [2:25:31<18:41,  3.16s/it][A
 89%|████████▉ | 2951/3305 [2:25:33<17:54,  3.04s/it][A
 89%|████████▉ | 2952/3305 [2:25:37<18:01,  3.06s/it][A
 89%|████████▉ | 2953/3305 [2:25:40<17:57,  3.06s/it][A
 89%|████████▉ | 2954/3305 [2:25:42<17:27,  2.98s/it][A
 89%|████████▉ | 2955/3305 [2:25:45<16:06,  2.76s/it][A
 89%|████████▉ | 2956/3305 [2:25:48<17:50,  3.07s/it][A
 89%|████████▉ | 2957/3305 [2:25:53<20:15,  3.49s/it][A
 90%|████████▉ | 2958/3305 [2:25:57<21:39,  3.74s/it][A
 90%|████████▉ | 2959/3305 [2:26:02<23:33,  4.09s/it][A
 90%|████████▉ | 2960/3305 [2:26:05<21:45,  3.79s/it][A
 90%|████████▉ | 2961/3305 [2:26:08<20:08,  3.51s/it][A
 90%|████████▉ | 2962/3305 [2:26:11<19:15,  3.37s/it][A
 90%|████████▉ | 2963/3305 [2:26:15<19:56,  3.50s/it][A
 90%|████████▉ | 2964/3305 [2:26:19<20:57,  3.69s/it][A
 90%|████████▉ | 2965/3305 [2:26:23<20:43,  3.66s/it][A
 90%|████████▉ | 2966/3305 [2:26:25<17:57,  3.18s/it][A
 90%|████████▉ | 2967/3305 [2:26:26<15:28,  2.75s/it][A
 90%|████████▉ | 2968/3305 [2:26:28<13:45,  2.45s/it][A
 90%|████████▉ | 2969/3305 [2:26:30<13:00,  2.32s/it][A
 90%|████████▉ | 2970/3305 [2:26:34<16:12,  2.90s/it][A
 90%|████████▉ | 2971/3305 [2:26:41<21:44,  3.91s/it][A
 90%|████████▉ | 2972/3305 [2:26:45<22:01,  3.97s/it][A
 90%|████████▉ | 2973/3305 [2:26:47<19:26,  3.51s/it][A
 90%|████████▉ | 2974/3305 [2:26:50<17:53,  3.24s/it][A
 90%|█████████ | 2975/3305 [2:26:54<18:31,  3.37s/it][A
 90%|█████████ | 2976/3305 [2:26:58<19:38,  3.58s/it][A
 90%|█████████ | 2977/3305 [2:27:00<17:06,  3.13s/it][A
 90%|█████████ | 2978/3305 [2:27:03<16:53,  3.10s/it][A
 90%|█████████ | 2979/3305 [2:27:07<18:20,  3.38s/it][A
 90%|█████████ | 2980/3305 [2:27:10<17:34,  3.25s/it][A
 90%|█████████ | 2981/3305 [2:27:13<16:57,  3.14s/it][A
 90%|█████████ | 2982/3305 [2:27:15<15:29,  2.88s/it][A
 90%|█████████ | 2983/3305 [2:27:17<13:58,  2.60s/it][A
 90%|█████████ | 2984/3305 [2:27:21<16:47,  3.14s/it][A
 90%|█████████ | 2985/3305 [2:27:27<21:01,  3.94s/it][A
 90%|█████████ | 2986/3305 [2:27:30<19:17,  3.63s/it][A
 90%|█████████ | 2987/3305 [2:27:33<17:45,  3.35s/it][A
 90%|█████████ | 2988/3305 [2:27:35<16:02,  3.04s/it][A
 90%|█████████ | 2989/3305 [2:27:37<14:10,  2.69s/it][A
 90%|█████████ | 2990/3305 [2:27:40<14:05,  2.68s/it][A
 90%|█████████ | 2991/3305 [2:27:42<12:58,  2.48s/it][A
 91%|█████████ | 2992/3305 [2:27:43<11:55,  2.29s/it][A
 91%|█████████ | 2993/3305 [2:27:46<13:01,  2.50s/it][A
 91%|█████████ | 2994/3305 [2:27:50<13:58,  2.69s/it][A
 91%|█████████ | 2995/3305 [2:27:52<13:41,  2.65s/it][A
 91%|█████████ | 2996/3305 [2:27:55<13:45,  2.67s/it][A
 91%|█████████ | 2997/3305 [2:27:57<12:42,  2.48s/it][A
 91%|█████████ | 2998/3305 [2:28:01<14:41,  2.87s/it][A
 91%|█████████ | 2999/3305 [2:28:06<17:51,  3.50s/it][A
 91%|█████████ | 3000/3305 [2:28:08<16:55,  3.33s/it][A
 91%|█████████ | 3001/3305 [2:28:12<16:25,  3.24s/it][A
 91%|█████████ | 3002/3305 [2:28:14<15:29,  3.07s/it][A
 91%|█████████ | 3003/3305 [2:28:16<13:56,  2.77s/it][A
 91%|█████████ | 3004/3305 [2:28:19<14:07,  2.81s/it][A
 91%|█████████ | 3005/3305 [2:28:22<13:23,  2.68s/it][A
 91%|█████████ | 3006/3305 [2:28:23<12:10,  2.44s/it][A
 91%|█████████ | 3007/3305 [2:28:26<12:23,  2.50s/it][A
 91%|█████████ | 3008/3305 [2:28:29<12:32,  2.54s/it][A
 91%|█████████ | 3009/3305 [2:28:31<12:24,  2.52s/it][A
 91%|█████████ | 3010/3305 [2:28:34<12:43,  2.59s/it][A
 91%|█████████ | 3011/3305 [2:28:36<12:27,  2.54s/it][A
 91%|█████████ | 3012/3305 [2:28:39<12:20,  2.53s/it][A
 91%|█████████ | 3013/3305 [2:28:41<11:32,  2.37s/it][A
 91%|█████████ | 3014/3305 [2:28:43<10:59,  2.27s/it][A
 91%|█████████ | 3015/3305 [2:28:45<11:04,  2.29s/it][A
 91%|█████████▏| 3016/3305 [2:28:48<11:07,  2.31s/it][A
 91%|█████████▏| 3017/3305 [2:28:50<11:36,  2.42s/it][A
 91%|█████████▏| 3018/3305 [2:28:53<12:21,  2.58s/it][A
 91%|█████████▏| 3019/3305 [2:28:56<12:22,  2.60s/it][A
 91%|█████████▏| 3020/3305 [2:29:01<15:40,  3.30s/it][A
 91%|█████████▏| 3021/3305 [2:29:06<17:50,  3.77s/it][A
 91%|█████████▏| 3022/3305 [2:29:09<17:14,  3.66s/it][A
 91%|█████████▏| 3023/3305 [2:29:13<17:15,  3.67s/it][A
 91%|█████████▏| 3024/3305 [2:29:15<15:29,  3.31s/it][A
 92%|█████████▏| 3025/3305 [2:29:18<14:15,  3.06s/it][A
 92%|█████████▏| 3026/3305 [2:29:20<13:08,  2.83s/it][A
 92%|█████████▏| 3027/3305 [2:29:22<12:07,  2.62s/it][A
 92%|█████████▏| 3028/3305 [2:29:25<12:43,  2.76s/it][A
 92%|█████████▏| 3029/3305 [2:29:29<14:19,  3.11s/it][A
 92%|█████████▏| 3030/3305 [2:29:32<14:13,  3.10s/it][A
 92%|█████████▏| 3031/3305 [2:29:36<15:29,  3.39s/it][A
 92%|█████████▏| 3032/3305 [2:29:41<17:25,  3.83s/it][A
 92%|█████████▏| 3033/3305 [2:29:44<16:07,  3.56s/it][A
 92%|█████████▏| 3034/3305 [2:29:46<14:18,  3.17s/it][A
 92%|█████████▏| 3035/3305 [2:29:48<12:15,  2.72s/it][A
 92%|█████████▏| 3036/3305 [2:29:50<11:30,  2.57s/it][A
 92%|█████████▏| 3037/3305 [2:29:53<11:42,  2.62s/it][A
 92%|█████████▏| 3038/3305 [2:29:55<11:33,  2.60s/it][A
 92%|█████████▏| 3039/3305 [2:29:58<11:06,  2.50s/it][A
 92%|█████████▏| 3040/3305 [2:30:00<10:38,  2.41s/it][A
 92%|█████████▏| 3041/3305 [2:30:02<10:18,  2.34s/it][A
 92%|█████████▏| 3042/3305 [2:30:05<10:22,  2.37s/it][A
 92%|█████████▏| 3043/3305 [2:30:10<14:37,  3.35s/it][A
 92%|█████████▏| 3044/3305 [2:30:17<19:07,  4.40s/it][A
 92%|█████████▏| 3045/3305 [2:30:19<16:29,  3.81s/it][A
 92%|█████████▏| 3046/3305 [2:30:21<13:37,  3.16s/it][A
 92%|█████████▏| 3047/3305 [2:30:24<12:44,  2.96s/it][A
 92%|█████████▏| 3048/3305 [2:30:29<16:20,  3.81s/it][A
 92%|█████████▏| 3049/3305 [2:30:36<19:34,  4.59s/it][A
 92%|█████████▏| 3050/3305 [2:30:38<16:45,  3.94s/it][A
 92%|█████████▏| 3051/3305 [2:30:43<17:33,  4.15s/it][A
 92%|█████████▏| 3052/3305 [2:30:48<19:01,  4.51s/it][A
 92%|█████████▏| 3053/3305 [2:30:51<16:21,  3.89s/it][A
 92%|█████████▏| 3054/3305 [2:30:53<14:38,  3.50s/it][A
 92%|█████████▏| 3055/3305 [2:30:56<13:04,  3.14s/it][A
 92%|█████████▏| 3056/3305 [2:30:58<11:32,  2.78s/it][A
 92%|█████████▏| 3057/3305 [2:31:01<11:50,  2.87s/it][A
 93%|█████████▎| 3058/3305 [2:31:04<12:25,  3.02s/it][A
 93%|█████████▎| 3059/3305 [2:31:06<11:31,  2.81s/it][A
 93%|█████████▎| 3060/3305 [2:31:08<10:06,  2.48s/it][A
 93%|█████████▎| 3061/3305 [2:31:09<08:36,  2.12s/it][A
 93%|█████████▎| 3062/3305 [2:31:12<08:55,  2.20s/it][A
 93%|█████████▎| 3063/3305 [2:31:15<10:30,  2.61s/it][A
 93%|█████████▎| 3064/3305 [2:31:19<11:34,  2.88s/it][A
 93%|█████████▎| 3065/3305 [2:31:22<12:14,  3.06s/it][A
 93%|█████████▎| 3066/3305 [2:31:25<12:26,  3.12s/it][A
 93%|█████████▎| 3067/3305 [2:31:27<11:04,  2.79s/it][A
 93%|█████████▎| 3068/3305 [2:31:29<09:42,  2.46s/it][A
 93%|█████████▎| 3069/3305 [2:31:31<09:17,  2.36s/it][A
 93%|█████████▎| 3070/3305 [2:31:34<09:48,  2.50s/it][A
 93%|█████████▎| 3071/3305 [2:31:37<10:00,  2.57s/it][A
 93%|█████████▎| 3072/3305 [2:31:41<11:39,  3.00s/it][A
 93%|█████████▎| 3073/3305 [2:31:45<13:09,  3.40s/it][A
 93%|█████████▎| 3074/3305 [2:31:47<11:25,  2.97s/it][A
 93%|█████████▎| 3075/3305 [2:31:49<10:31,  2.74s/it][A
 93%|█████████▎| 3076/3305 [2:31:52<10:00,  2.62s/it][A
 93%|█████████▎| 3077/3305 [2:31:54<09:41,  2.55s/it][A
 93%|█████████▎| 3078/3305 [2:31:58<10:37,  2.81s/it][A
 93%|█████████▎| 3079/3305 [2:32:01<11:22,  3.02s/it][A
 93%|█████████▎| 3080/3305 [2:32:03<10:25,  2.78s/it][A
 93%|█████████▎| 3081/3305 [2:32:06<10:13,  2.74s/it][A
 93%|█████████▎| 3082/3305 [2:32:08<10:00,  2.69s/it][A
 93%|█████████▎| 3083/3305 [2:32:11<10:18,  2.79s/it][A
 93%|█████████▎| 3084/3305 [2:32:15<10:40,  2.90s/it][A
 93%|█████████▎| 3085/3305 [2:32:17<09:41,  2.64s/it][A
 93%|█████████▎| 3086/3305 [2:32:19<09:03,  2.48s/it][A
 93%|█████████▎| 3087/3305 [2:32:21<09:06,  2.50s/it][A
 93%|█████████▎| 3088/3305 [2:32:24<09:09,  2.53s/it][A
 93%|█████████▎| 3089/3305 [2:32:27<09:32,  2.65s/it][A
 93%|█████████▎| 3090/3305 [2:32:30<10:16,  2.87s/it][A
 94%|█████████▎| 3091/3305 [2:32:33<10:33,  2.96s/it][A
 94%|█████████▎| 3092/3305 [2:32:37<11:06,  3.13s/it][A
 94%|█████████▎| 3093/3305 [2:32:40<11:18,  3.20s/it][A
 94%|█████████▎| 3094/3305 [2:32:44<11:31,  3.28s/it][A
 94%|█████████▎| 3095/3305 [2:32:46<10:31,  3.01s/it][A
 94%|█████████▎| 3096/3305 [2:32:48<09:36,  2.76s/it][A
 94%|█████████▎| 3097/3305 [2:32:51<08:59,  2.60s/it][A
 94%|█████████▎| 3098/3305 [2:32:53<08:30,  2.47s/it][A
 94%|█████████▍| 3099/3305 [2:32:56<08:59,  2.62s/it][A
 94%|█████████▍| 3100/3305 [2:32:59<09:41,  2.84s/it][A
 94%|█████████▍| 3101/3305 [2:33:01<09:15,  2.72s/it][A
 94%|█████████▍| 3102/3305 [2:33:03<08:02,  2.38s/it][A
 94%|█████████▍| 3103/3305 [2:33:05<07:52,  2.34s/it][A
 94%|█████████▍| 3104/3305 [2:33:10<09:55,  2.96s/it][A
 94%|█████████▍| 3105/3305 [2:33:13<10:22,  3.11s/it][A
 94%|█████████▍| 3106/3305 [2:33:15<09:24,  2.83s/it][A
 94%|█████████▍| 3107/3305 [2:33:17<08:19,  2.52s/it][A
 94%|█████████▍| 3108/3305 [2:33:19<07:42,  2.35s/it][A
 94%|█████████▍| 3109/3305 [2:33:22<07:43,  2.37s/it][A
 94%|█████████▍| 3110/3305 [2:33:25<08:55,  2.74s/it][A
 94%|█████████▍| 3111/3305 [2:33:28<09:23,  2.90s/it][A
 94%|█████████▍| 3112/3305 [2:33:31<08:57,  2.79s/it][A
 94%|█████████▍| 3113/3305 [2:33:34<08:51,  2.77s/it][A
 94%|█████████▍| 3114/3305 [2:33:36<08:37,  2.71s/it][A
 94%|█████████▍| 3115/3305 [2:33:39<08:18,  2.62s/it][A
 94%|█████████▍| 3116/3305 [2:33:41<07:39,  2.43s/it][A
 94%|█████████▍| 3117/3305 [2:33:42<06:57,  2.22s/it][A
 94%|█████████▍| 3118/3305 [2:33:45<06:59,  2.25s/it][A
 94%|█████████▍| 3119/3305 [2:33:48<07:51,  2.54s/it][A
 94%|█████████▍| 3120/3305 [2:33:50<07:48,  2.53s/it][A
 94%|█████████▍| 3121/3305 [2:33:53<07:55,  2.58s/it][A
 94%|█████████▍| 3122/3305 [2:33:56<08:31,  2.80s/it][A
 94%|█████████▍| 3123/3305 [2:33:59<08:04,  2.66s/it][A
 95%|█████████▍| 3124/3305 [2:34:01<07:52,  2.61s/it][A
 95%|█████████▍| 3125/3305 [2:34:04<08:08,  2.71s/it][A
 95%|█████████▍| 3126/3305 [2:34:07<07:54,  2.65s/it][A
 95%|█████████▍| 3127/3305 [2:34:09<07:32,  2.54s/it][A
 95%|█████████▍| 3128/3305 [2:34:11<06:50,  2.32s/it][A
 95%|█████████▍| 3129/3305 [2:34:14<07:10,  2.44s/it][A
 95%|█████████▍| 3130/3305 [2:34:17<07:46,  2.67s/it][A
 95%|█████████▍| 3131/3305 [2:34:20<08:14,  2.84s/it][A
 95%|█████████▍| 3132/3305 [2:34:23<08:14,  2.86s/it][A
 95%|█████████▍| 3133/3305 [2:34:25<07:32,  2.63s/it][A
 95%|█████████▍| 3134/3305 [2:34:27<06:39,  2.34s/it][A
 95%|█████████▍| 3135/3305 [2:34:29<06:16,  2.21s/it][A
 95%|█████████▍| 3136/3305 [2:34:31<06:19,  2.24s/it][A
 95%|█████████▍| 3137/3305 [2:34:33<06:11,  2.21s/it][A
 95%|█████████▍| 3138/3305 [2:34:35<06:19,  2.27s/it][A
 95%|█████████▍| 3139/3305 [2:34:39<07:11,  2.60s/it][A
 95%|█████████▌| 3140/3305 [2:34:42<07:25,  2.70s/it][A
 95%|█████████▌| 3141/3305 [2:34:43<06:24,  2.35s/it][A
 95%|█████████▌| 3142/3305 [2:34:46<06:24,  2.36s/it][A
 95%|█████████▌| 3143/3305 [2:34:49<07:02,  2.61s/it][A
 95%|█████████▌| 3144/3305 [2:34:54<09:15,  3.45s/it][A
 95%|█████████▌| 3145/3305 [2:34:59<10:39,  4.00s/it][A
 95%|█████████▌| 3146/3305 [2:35:01<08:47,  3.32s/it][A
 95%|█████████▌| 3147/3305 [2:35:03<07:14,  2.75s/it][A
 95%|█████████▌| 3148/3305 [2:35:05<06:38,  2.54s/it][A
 95%|█████████▌| 3149/3305 [2:35:07<06:25,  2.47s/it][A
 95%|█████████▌| 3150/3305 [2:35:10<06:32,  2.53s/it][A
 95%|█████████▌| 3151/3305 [2:35:13<07:12,  2.81s/it][A
 95%|█████████▌| 3152/3305 [2:35:16<07:05,  2.78s/it][A
 95%|█████████▌| 3153/3305 [2:35:18<06:16,  2.48s/it][A
 95%|█████████▌| 3154/3305 [2:35:21<06:42,  2.67s/it][A
 95%|█████████▌| 3155/3305 [2:35:25<07:32,  3.01s/it][A
 95%|█████████▌| 3156/3305 [2:35:28<07:29,  3.01s/it][A
 96%|█████████▌| 3157/3305 [2:35:30<07:02,  2.85s/it][A
 96%|█████████▌| 3158/3305 [2:35:34<07:39,  3.13s/it][A
 96%|█████████▌| 3159/3305 [2:35:37<07:57,  3.27s/it][A
 96%|█████████▌| 3160/3305 [2:35:39<06:59,  2.89s/it][A
 96%|█████████▌| 3161/3305 [2:35:42<06:26,  2.69s/it][A
 96%|█████████▌| 3162/3305 [2:35:44<06:00,  2.52s/it][A
 96%|█████████▌| 3163/3305 [2:35:47<06:15,  2.65s/it][A
 96%|█████████▌| 3164/3305 [2:35:50<06:40,  2.84s/it][A
 96%|█████████▌| 3165/3305 [2:35:53<06:31,  2.79s/it][A
 96%|█████████▌| 3166/3305 [2:35:55<06:13,  2.69s/it][A
 96%|█████████▌| 3167/3305 [2:35:57<05:46,  2.51s/it][A
 96%|█████████▌| 3168/3305 [2:35:59<05:20,  2.34s/it][A
 96%|█████████▌| 3169/3305 [2:36:01<04:52,  2.15s/it][A
 96%|█████████▌| 3170/3305 [2:36:04<05:15,  2.33s/it][A
 96%|█████████▌| 3171/3305 [2:36:07<05:46,  2.59s/it][A
 96%|█████████▌| 3172/3305 [2:36:11<07:05,  3.20s/it][A
 96%|█████████▌| 3173/3305 [2:36:17<08:42,  3.96s/it][A
 96%|█████████▌| 3174/3305 [2:36:19<07:27,  3.41s/it][A
 96%|█████████▌| 3175/3305 [2:36:22<06:50,  3.16s/it][A
 96%|█████████▌| 3176/3305 [2:36:24<06:21,  2.96s/it][A
 96%|█████████▌| 3177/3305 [2:36:28<06:26,  3.02s/it][A
 96%|█████████▌| 3178/3305 [2:36:32<07:34,  3.58s/it][A
 96%|█████████▌| 3179/3305 [2:36:36<07:22,  3.51s/it][A
 96%|█████████▌| 3180/3305 [2:36:39<07:10,  3.45s/it][A
 96%|█████████▌| 3181/3305 [2:36:43<07:34,  3.67s/it][A
 96%|█████████▋| 3182/3305 [2:36:48<08:17,  4.05s/it][A
 96%|█████████▋| 3183/3305 [2:36:51<07:15,  3.57s/it][A
 96%|█████████▋| 3184/3305 [2:36:53<06:15,  3.10s/it][A
 96%|█████████▋| 3185/3305 [2:36:55<05:49,  2.91s/it][A
 96%|█████████▋| 3186/3305 [2:36:58<05:34,  2.81s/it][A
 96%|█████████▋| 3187/3305 [2:37:00<05:31,  2.81s/it][A
 96%|█████████▋| 3188/3305 [2:37:03<05:18,  2.72s/it][A
 96%|█████████▋| 3189/3305 [2:37:05<04:46,  2.47s/it][A
 97%|█████████▋| 3190/3305 [2:37:08<05:00,  2.61s/it][A
 97%|█████████▋| 3191/3305 [2:37:11<04:59,  2.63s/it][A
 97%|█████████▋| 3192/3305 [2:37:13<04:51,  2.58s/it][A
 97%|█████████▋| 3193/3305 [2:37:15<04:37,  2.48s/it][A
 97%|█████████▋| 3194/3305 [2:37:17<04:19,  2.34s/it][A
 97%|█████████▋| 3195/3305 [2:37:19<03:52,  2.11s/it][A
 97%|█████████▋| 3196/3305 [2:37:21<04:01,  2.22s/it][A
 97%|█████████▋| 3197/3305 [2:37:23<03:57,  2.20s/it][A
 97%|█████████▋| 3198/3305 [2:37:26<04:01,  2.26s/it][A
 97%|█████████▋| 3199/3305 [2:37:28<03:57,  2.24s/it][A
 97%|█████████▋| 3200/3305 [2:37:34<05:55,  3.39s/it][A
 97%|█████████▋| 3201/3305 [2:37:41<07:44,  4.46s/it][A
 97%|█████████▋| 3202/3305 [2:37:43<06:33,  3.82s/it][A
 97%|█████████▋| 3203/3305 [2:37:46<06:01,  3.54s/it][A
 97%|█████████▋| 3204/3305 [2:37:49<05:27,  3.25s/it][A
 97%|█████████▋| 3205/3305 [2:37:51<04:59,  3.00s/it][A
 97%|█████████▋| 3206/3305 [2:37:55<05:33,  3.37s/it][A
 97%|█████████▋| 3207/3305 [2:38:01<06:22,  3.91s/it][A
 97%|█████████▋| 3208/3305 [2:38:04<06:05,  3.77s/it][A
 97%|█████████▋| 3209/3305 [2:38:07<05:33,  3.47s/it][A
 97%|█████████▋| 3210/3305 [2:38:10<05:06,  3.22s/it][A
 97%|█████████▋| 3211/3305 [2:38:12<04:51,  3.10s/it][A
 97%|█████████▋| 3212/3305 [2:38:14<04:09,  2.68s/it][A
 97%|█████████▋| 3213/3305 [2:38:17<04:17,  2.80s/it][A
 97%|█████████▋| 3214/3305 [2:38:23<05:32,  3.65s/it][A
 97%|█████████▋| 3215/3305 [2:38:29<06:35,  4.39s/it][A
 97%|█████████▋| 3216/3305 [2:38:33<06:26,  4.34s/it][A
 97%|█████████▋| 3217/3305 [2:38:37<06:00,  4.09s/it][A
 97%|█████████▋| 3218/3305 [2:38:39<05:22,  3.71s/it][A
 97%|█████████▋| 3219/3305 [2:38:42<04:39,  3.25s/it][A
 97%|█████████▋| 3220/3305 [2:38:44<04:05,  2.89s/it][A
 97%|█████████▋| 3221/3305 [2:38:46<03:45,  2.69s/it][A
 97%|█████████▋| 3222/3305 [2:38:48<03:28,  2.51s/it][A
 98%|█████████▊| 3223/3305 [2:38:50<03:15,  2.38s/it][A
 98%|█████████▊| 3224/3305 [2:38:52<03:05,  2.29s/it][A
 98%|█████████▊| 3225/3305 [2:38:55<03:15,  2.44s/it][A
 98%|█████████▊| 3226/3305 [2:38:58<03:23,  2.58s/it][A
 98%|█████████▊| 3227/3305 [2:39:01<03:31,  2.71s/it][A
 98%|█████████▊| 3228/3305 [2:39:04<03:49,  2.98s/it][A
 98%|█████████▊| 3229/3305 [2:39:08<03:56,  3.12s/it][A
 98%|█████████▊| 3230/3305 [2:39:10<03:42,  2.96s/it][A
 98%|█████████▊| 3231/3305 [2:39:13<03:33,  2.88s/it][A
 98%|█████████▊| 3232/3305 [2:39:17<03:41,  3.03s/it][A
 98%|█████████▊| 3233/3305 [2:39:19<03:31,  2.94s/it][A
 98%|█████████▊| 3234/3305 [2:39:21<03:08,  2.66s/it][A
 98%|█████████▊| 3235/3305 [2:39:23<02:55,  2.51s/it][A
 98%|█████████▊| 3236/3305 [2:39:26<02:52,  2.50s/it][A
 98%|█████████▊| 3237/3305 [2:39:29<03:04,  2.72s/it][A
 98%|█████████▊| 3238/3305 [2:39:32<03:07,  2.80s/it][A
 98%|█████████▊| 3239/3305 [2:39:34<02:52,  2.61s/it][A
 98%|█████████▊| 3240/3305 [2:39:37<02:54,  2.68s/it][A
 98%|█████████▊| 3241/3305 [2:39:41<03:10,  2.97s/it][A
 98%|█████████▊| 3242/3305 [2:39:43<03:00,  2.86s/it][A
 98%|█████████▊| 3243/3305 [2:39:46<02:44,  2.66s/it][A
 98%|█████████▊| 3244/3305 [2:39:48<02:46,  2.73s/it][A
 98%|█████████▊| 3245/3305 [2:39:51<02:40,  2.67s/it][A
 98%|█████████▊| 3246/3305 [2:39:53<02:25,  2.46s/it][A
 98%|█████████▊| 3247/3305 [2:39:55<02:13,  2.30s/it][A
 98%|█████████▊| 3248/3305 [2:39:57<02:13,  2.34s/it][A
 98%|█████████▊| 3249/3305 [2:40:00<02:13,  2.39s/it][A
 98%|█████████▊| 3250/3305 [2:40:03<02:31,  2.75s/it][A
 98%|█████████▊| 3251/3305 [2:40:08<02:59,  3.33s/it][A
 98%|█████████▊| 3252/3305 [2:40:11<02:42,  3.06s/it][A
 98%|█████████▊| 3253/3305 [2:40:16<03:08,  3.63s/it][A
 98%|█████████▊| 3254/3305 [2:40:21<03:38,  4.28s/it][A
 98%|█████████▊| 3255/3305 [2:40:25<03:21,  4.04s/it][A
 99%|█████████▊| 3256/3305 [2:40:28<03:01,  3.71s/it][A
 99%|█████████▊| 3257/3305 [2:40:30<02:38,  3.29s/it][A
 99%|█████████▊| 3258/3305 [2:40:33<02:26,  3.13s/it][A
 99%|█████████▊| 3259/3305 [2:40:35<02:16,  2.97s/it][A
 99%|█████████▊| 3260/3305 [2:40:39<02:23,  3.19s/it][A
 99%|█████████▊| 3261/3305 [2:40:43<02:36,  3.55s/it][A
 99%|█████████▊| 3262/3305 [2:40:46<02:24,  3.36s/it][A
 99%|█████████▊| 3263/3305 [2:40:50<02:20,  3.33s/it][A
 99%|█████████▉| 3264/3305 [2:40:52<02:04,  3.04s/it][A
 99%|█████████▉| 3265/3305 [2:40:54<01:54,  2.86s/it][A
 99%|█████████▉| 3266/3305 [2:40:57<01:53,  2.90s/it][A
 99%|█████████▉| 3267/3305 [2:41:01<01:52,  2.97s/it][A
 99%|█████████▉| 3268/3305 [2:41:04<01:54,  3.08s/it][A
 99%|█████████▉| 3269/3305 [2:41:07<01:52,  3.12s/it][A
 99%|█████████▉| 3270/3305 [2:41:09<01:36,  2.76s/it][A
 99%|█████████▉| 3271/3305 [2:41:12<01:31,  2.70s/it][A
 99%|█████████▉| 3272/3305 [2:41:16<01:44,  3.17s/it][A
 99%|█████████▉| 3273/3305 [2:41:21<01:56,  3.63s/it][A
 99%|█████████▉| 3274/3305 [2:41:23<01:40,  3.24s/it][A
 99%|█████████▉| 3275/3305 [2:41:26<01:32,  3.08s/it][A
 99%|█████████▉| 3276/3305 [2:41:28<01:25,  2.96s/it][A
 99%|█████████▉| 3277/3305 [2:41:32<01:25,  3.04s/it][A
 99%|█████████▉| 3278/3305 [2:41:37<01:38,  3.65s/it][A
 99%|█████████▉| 3279/3305 [2:41:42<01:49,  4.22s/it][A
 99%|█████████▉| 3280/3305 [2:41:45<01:37,  3.92s/it][A
 99%|█████████▉| 3281/3305 [2:41:48<01:22,  3.45s/it][A
 99%|█████████▉| 3282/3305 [2:41:51<01:14,  3.26s/it][A
 99%|█████████▉| 3283/3305 [2:41:53<01:09,  3.14s/it][A
 99%|█████████▉| 3284/3305 [2:41:56<01:04,  3.07s/it][A
 99%|█████████▉| 3285/3305 [2:41:58<00:55,  2.75s/it][A
 99%|█████████▉| 3286/3305 [2:42:00<00:45,  2.38s/it][A
 99%|█████████▉| 3287/3305 [2:42:03<00:45,  2.54s/it][A
 99%|█████████▉| 3288/3305 [2:42:06<00:48,  2.87s/it][A
100%|█████████▉| 3289/3305 [2:42:09<00:46,  2.89s/it][A
100%|█████████▉| 3290/3305 [2:42:11<00:39,  2.61s/it][A
100%|█████████▉| 3291/3305 [2:42:14<00:36,  2.57s/it][A
100%|█████████▉| 3292/3305 [2:42:16<00:33,  2.61s/it][A
100%|█████████▉| 3293/3305 [2:42:18<00:28,  2.34s/it][A
100%|█████████▉| 3294/3305 [2:42:20<00:24,  2.27s/it][A
100%|█████████▉| 3295/3305 [2:42:22<00:22,  2.21s/it][A
100%|█████████▉| 3296/3305 [2:42:25<00:19,  2.20s/it][A
100%|█████████▉| 3297/3305 [2:42:27<00:17,  2.25s/it][A
100%|█████████▉| 3298/3305 [2:42:29<00:16,  2.31s/it][A
100%|█████████▉| 3299/3305 [2:42:32<00:15,  2.52s/it][A
100%|█████████▉| 3300/3305 [2:42:36<00:13,  2.76s/it][A
100%|█████████▉| 3301/3305 [2:42:38<00:10,  2.59s/it][A
100%|█████████▉| 3302/3305 [2:42:43<00:09,  3.27s/it][A
100%|█████████▉| 3303/3305 [2:42:49<00:08,  4.07s/it][A
100%|█████████▉| 3304/3305 [2:42:51<00:03,  3.68s/it][A
100%|██████████| 3305/3305 [2:42:53<00:00,  3.04s/it][A                                                        
                                                     [A{'eval_loss': 0.7573413848876953, 'eval_runtime': 9777.2351, 'eval_samples_per_second': 0.338, 'eval_steps_per_second': 0.338, 'epoch': 0.27}
 27%|██▋       | 500/1859 [12:53:46<26:33:39, 70.36s/it]
100%|██████████| 3305/3305 [2:42:53<00:00,  3.04s/it][A
                                                     [A[INFO|trainer.py:4309] 2025-12-04 23:03:50,140 >> Saving model checkpoint to saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500
/opt/conda/lib/python3.11/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in Qwen/Qwen2.5-VL-7B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2421] 2025-12-04 23:03:50,232 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-04 23:03:50,235 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-04 23:03:50,237 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/special_tokens_map.json
[2025-12-04 23:03:51,192] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step500 is about to be saved!
[2025-12-04 23:03:51,234] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/global_step500/mp_rank_00_model_states.pt
[2025-12-04 23:03:51,235] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/global_step500/mp_rank_00_model_states.pt...
[2025-12-04 23:03:51,313] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/global_step500/mp_rank_00_model_states.pt.
[2025-12-04 23:03:51,315] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/global_step500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-12-04 23:03:51,639] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/global_step500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-12-04 23:03:51,644] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/global_step500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-12-04 23:03:51,644] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step500 is ready now!
[INFO|image_processing_base.py:253] 2025-12-04 23:03:51,657 >> Image processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/preprocessor_config.json
[INFO|tokenization_utils_base.py:2421] 2025-12-04 23:03:51,659 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-04 23:03:51,660 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-04 23:03:51,662 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/special_tokens_map.json
[INFO|video_processing_utils.py:600] 2025-12-04 23:03:51,800 >> Video processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/video_preprocessor_config.json
[INFO|processing_utils.py:814] 2025-12-04 23:03:51,802 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/chat_template.jinja
 27%|██▋       | 501/1859 [12:54:54<1132:44:11, 3002.84s/it] 27%|██▋       | 502/1859 [12:56:11<801:05:40, 2125.23s/it]  27%|██▋       | 503/1859 [12:57:16<567:36:20, 1506.92s/it] 27%|██▋       | 504/1859 [12:58:29<405:17:52, 1076.81s/it] 27%|██▋       | 505/1859 [12:59:44<291:57:59, 776.28s/it]  27%|██▋       | 506/1859 [13:01:01<212:52:51, 566.42s/it] 27%|██▋       | 507/1859 [13:02:12<157:00:10, 418.06s/it] 27%|██▋       | 508/1859 [13:03:19<117:20:19, 312.67s/it] 27%|██▋       | 509/1859 [13:04:48<92:06:37, 245.63s/it]  27%|██▋       | 510/1859 [13:06:01<72:38:26, 193.85s/it]                                                         {'loss': 0.7586, 'grad_norm': 0.35089820623397827, 'learning_rate': 9.108138058417189e-05, 'epoch': 0.27}
 27%|██▋       | 510/1859 [13:06:01<72:38:26, 193.85s/it] 27%|██▋       | 511/1859 [13:07:07<58:09:11, 155.31s/it] 28%|██▊       | 512/1859 [13:08:18<48:36:53, 129.93s/it] 28%|██▊       | 513/1859 [13:09:29<41:59:06, 112.29s/it] 28%|██▊       | 514/1859 [13:10:54<38:54:01, 104.12s/it] 28%|██▊       | 515/1859 [13:12:22<37:04:50, 99.32s/it]  28%|██▊       | 516/1859 [13:13:33<33:54:17, 90.88s/it] 28%|██▊       | 517/1859 [13:14:52<32:34:31, 87.39s/it] 28%|██▊       | 518/1859 [13:16:36<34:19:30, 92.15s/it] 28%|██▊       | 519/1859 [13:17:51<32:24:08, 87.05s/it] 28%|██▊       | 520/1859 [13:18:55<29:51:19, 80.27s/it]                                                        {'loss': 0.743, 'grad_norm': 0.3363633155822754, 'learning_rate': 9.053896739567655e-05, 'epoch': 0.28}
 28%|██▊       | 520/1859 [13:18:55<29:51:19, 80.27s/it] 28%|██▊       | 521/1859 [13:20:00<28:07:52, 75.69s/it] 28%|██▊       | 522/1859 [13:21:12<27:41:26, 74.56s/it] 28%|██▊       | 523/1859 [13:22:13<26:11:26, 70.57s/it] 28%|██▊       | 524/1859 [13:23:19<25:36:33, 69.06s/it] 28%|██▊       | 525/1859 [13:24:41<27:05:31, 73.11s/it] 28%|██▊       | 526/1859 [13:25:59<27:31:26, 74.33s/it] 28%|██▊       | 527/1859 [13:27:17<27:59:54, 75.67s/it] 28%|██▊       | 528/1859 [13:28:17<26:09:17, 70.74s/it] 28%|██▊       | 529/1859 [13:29:35<26:57:48, 72.98s/it] 29%|██▊       | 530/1859 [13:30:53<27:33:32, 74.65s/it]                                                        {'loss': 0.7825, 'grad_norm': 0.2907903790473938, 'learning_rate': 8.99822597461894e-05, 'epoch': 0.29}
 29%|██▊       | 530/1859 [13:30:53<27:33:32, 74.65s/it] 29%|██▊       | 531/1859 [13:32:04<27:03:35, 73.35s/it] 29%|██▊       | 532/1859 [13:33:06<25:49:22, 70.05s/it] 29%|██▊       | 533/1859 [13:34:14<25:36:14, 69.51s/it] 29%|██▊       | 534/1859 [13:35:31<26:23:08, 71.69s/it] 29%|██▉       | 535/1859 [13:36:52<27:21:01, 74.37s/it] 29%|██▉       | 536/1859 [13:38:09<27:37:21, 75.16s/it] 29%|██▉       | 537/1859 [13:39:12<26:16:12, 71.54s/it] 29%|██▉       | 538/1859 [13:40:27<26:39:00, 72.63s/it] 29%|██▉       | 539/1859 [13:41:42<26:56:36, 73.48s/it] 29%|██▉       | 540/1859 [13:43:01<27:26:28, 74.90s/it]                                                        {'loss': 0.7314, 'grad_norm': 0.3405322730541229, 'learning_rate': 8.94114539366103e-05, 'epoch': 0.29}
 29%|██▉       | 540/1859 [13:43:01<27:26:28, 74.90s/it] 29%|██▉       | 541/1859 [13:44:31<29:06:33, 79.51s/it] 29%|██▉       | 542/1859 [13:45:43<28:19:26, 77.42s/it] 29%|██▉       | 543/1859 [13:47:06<28:52:24, 78.99s/it] 29%|██▉       | 544/1859 [13:48:05<26:40:50, 73.04s/it] 29%|██▉       | 545/1859 [13:49:27<27:37:28, 75.68s/it] 29%|██▉       | 546/1859 [13:50:27<25:50:16, 70.84s/it] 29%|██▉       | 547/1859 [13:51:41<26:14:53, 72.02s/it] 29%|██▉       | 548/1859 [13:52:47<25:29:52, 70.02s/it] 30%|██▉       | 549/1859 [13:54:03<26:11:28, 71.98s/it] 30%|██▉       | 550/1859 [13:55:19<26:33:35, 73.04s/it]                                                        {'loss': 0.74, 'grad_norm': 0.5162564516067505, 'learning_rate': 8.882675123899677e-05, 'epoch': 0.3}
 30%|██▉       | 550/1859 [13:55:19<26:33:35, 73.04s/it] 30%|██▉       | 551/1859 [13:56:43<27:45:48, 76.41s/it] 30%|██▉       | 552/1859 [13:58:05<28:19:09, 78.00s/it] 30%|██▉       | 553/1859 [13:59:15<27:24:39, 75.56s/it] 30%|██▉       | 554/1859 [14:00:28<27:11:19, 75.00s/it] 30%|██▉       | 555/1859 [14:01:38<26:35:38, 73.42s/it] 30%|██▉       | 556/1859 [14:02:45<25:51:43, 71.45s/it] 30%|██▉       | 557/1859 [14:03:57<25:54:04, 71.62s/it] 30%|███       | 558/1859 [14:05:22<27:20:52, 75.67s/it] 30%|███       | 559/1859 [14:06:45<28:08:29, 77.93s/it] 30%|███       | 560/1859 [14:07:46<26:14:15, 72.71s/it]                                                        {'loss': 0.761, 'grad_norm': 0.25761735439300537, 'learning_rate': 8.82283578255935e-05, 'epoch': 0.3}
 30%|███       | 560/1859 [14:07:46<26:14:15, 72.71s/it] 30%|███       | 561/1859 [14:08:58<26:11:23, 72.64s/it] 30%|███       | 562/1859 [14:10:09<25:55:29, 71.96s/it] 30%|███       | 563/1859 [14:11:12<24:59:31, 69.42s/it] 30%|███       | 564/1859 [14:12:22<24:58:19, 69.42s/it] 30%|███       | 565/1859 [14:13:31<24:55:01, 69.32s/it] 30%|███       | 566/1859 [14:14:43<25:13:10, 70.22s/it] 31%|███       | 567/1859 [14:16:02<26:09:57, 72.91s/it] 31%|███       | 568/1859 [14:17:16<26:13:04, 73.11s/it] 31%|███       | 569/1859 [14:18:21<25:18:34, 70.63s/it] 31%|███       | 570/1859 [14:19:37<25:57:25, 72.49s/it]                                                        {'loss': 0.763, 'grad_norm': 0.29681265354156494, 'learning_rate': 8.761648469613369e-05, 'epoch': 0.31}
 31%|███       | 570/1859 [14:19:37<25:57:25, 72.49s/it] 31%|███       | 571/1859 [14:20:41<24:58:30, 69.81s/it] 31%|███       | 572/1859 [14:21:35<23:12:47, 64.93s/it] 31%|███       | 573/1859 [14:22:49<24:09:55, 67.65s/it] 31%|███       | 574/1859 [14:23:53<23:46:05, 66.59s/it] 31%|███       | 575/1859 [14:24:59<23:42:31, 66.47s/it] 31%|███       | 576/1859 [14:26:02<23:17:55, 65.37s/it] 31%|███       | 577/1859 [14:27:20<24:38:49, 69.21s/it] 31%|███       | 578/1859 [14:28:33<25:01:00, 70.31s/it] 31%|███       | 579/1859 [14:29:58<26:37:49, 74.90s/it] 31%|███       | 580/1859 [14:30:53<24:24:04, 68.68s/it]                                                        {'loss': 0.7612, 'grad_norm': 0.4000256657600403, 'learning_rate': 8.699134760343843e-05, 'epoch': 0.31}
 31%|███       | 580/1859 [14:30:53<24:24:04, 68.68s/it] 31%|███▏      | 581/1859 [14:32:18<26:12:17, 73.82s/it] 31%|███▏      | 582/1859 [14:33:23<25:11:05, 71.00s/it] 31%|███▏      | 583/1859 [14:34:29<24:39:38, 69.58s/it] 31%|███▏      | 584/1859 [14:35:41<24:52:18, 70.23s/it] 31%|███▏      | 585/1859 [14:37:08<26:42:20, 75.46s/it] 32%|███▏      | 586/1859 [14:38:25<26:51:04, 75.93s/it] 32%|███▏      | 587/1859 [14:39:50<27:44:52, 78.53s/it] 32%|███▏      | 588/1859 [14:41:03<27:06:42, 76.79s/it] 32%|███▏      | 589/1859 [14:42:09<25:57:41, 73.59s/it] 32%|███▏      | 590/1859 [14:43:08<24:26:00, 69.31s/it]                                                        {'loss': 0.7716, 'grad_norm': 0.30892214179039, 'learning_rate': 8.63531669773401e-05, 'epoch': 0.32}
 32%|███▏      | 590/1859 [14:43:08<24:26:00, 69.31s/it] 32%|███▏      | 591/1859 [14:44:36<26:21:42, 74.84s/it] 32%|███▏      | 592/1859 [14:45:47<25:56:29, 73.71s/it] 32%|███▏      | 593/1859 [14:47:00<25:49:55, 73.46s/it] 32%|███▏      | 594/1859 [14:48:13<25:44:22, 73.25s/it] 32%|███▏      | 595/1859 [14:49:25<25:34:31, 72.84s/it] 32%|███▏      | 596/1859 [14:50:40<25:50:04, 73.64s/it] 32%|███▏      | 597/1859 [14:51:42<24:33:17, 70.05s/it] 32%|███▏      | 598/1859 [14:53:05<25:53:49, 73.93s/it] 32%|███▏      | 599/1859 [14:54:22<26:14:42, 74.99s/it] 32%|███▏      | 600/1859 [14:55:31<25:35:36, 73.18s/it]                                                        {'loss': 0.7814, 'grad_norm': 0.25354745984077454, 'learning_rate': 8.570216784695637e-05, 'epoch': 0.32}
 32%|███▏      | 600/1859 [14:55:31<25:35:36, 73.18s/it] 32%|███▏      | 601/1859 [14:56:38<24:57:08, 71.41s/it] 32%|███▏      | 602/1859 [14:57:55<25:27:04, 72.89s/it] 32%|███▏      | 603/1859 [14:59:11<25:46:15, 73.87s/it] 32%|███▏      | 604/1859 [15:00:22<25:24:56, 72.91s/it] 33%|███▎      | 605/1859 [15:01:26<24:31:25, 70.40s/it] 33%|███▎      | 606/1859 [15:02:36<24:28:33, 70.32s/it] 33%|███▎      | 607/1859 [15:03:40<23:48:40, 68.47s/it] 33%|███▎      | 608/1859 [15:04:50<23:52:12, 68.69s/it] 33%|███▎      | 609/1859 [15:05:55<23:30:20, 67.70s/it] 33%|███▎      | 610/1859 [15:07:00<23:10:33, 66.80s/it]                                                        {'loss': 0.7316, 'grad_norm': 0.36310702562332153, 'learning_rate': 8.50385797613427e-05, 'epoch': 0.33}
 33%|███▎      | 610/1859 [15:07:00<23:10:33, 66.80s/it] 33%|███▎      | 611/1859 [15:08:20<24:30:51, 70.71s/it] 33%|███▎      | 612/1859 [15:09:30<24:28:02, 70.64s/it] 33%|███▎      | 613/1859 [15:10:49<25:17:47, 73.09s/it] 33%|███▎      | 614/1859 [15:12:11<26:13:07, 75.81s/it] 33%|███▎      | 615/1859 [15:13:20<25:32:11, 73.90s/it] 33%|███▎      | 616/1859 [15:14:30<25:07:13, 72.75s/it] 33%|███▎      | 617/1859 [15:15:35<24:17:41, 70.42s/it] 33%|███▎      | 618/1859 [15:16:49<24:35:25, 71.33s/it] 33%|███▎      | 619/1859 [15:17:45<22:56:44, 66.62s/it] 33%|███▎      | 620/1859 [15:19:06<24:29:35, 71.17s/it]                                                        {'loss': 0.7651, 'grad_norm': 0.33040815591812134, 'learning_rate': 8.436263670855096e-05, 'epoch': 0.33}
 33%|███▎      | 620/1859 [15:19:06<24:29:35, 71.17s/it] 33%|███▎      | 621/1859 [15:20:18<24:28:56, 71.19s/it] 33%|███▎      | 622/1859 [15:21:41<25:44:12, 74.90s/it] 34%|███▎      | 623/1859 [15:22:59<25:59:03, 75.68s/it] 34%|███▎      | 624/1859 [15:24:00<24:26:34, 71.25s/it] 34%|███▎      | 625/1859 [15:25:04<23:42:07, 69.15s/it] 34%|███▎      | 626/1859 [15:26:16<24:01:43, 70.16s/it] 34%|███▎      | 627/1859 [15:27:15<22:49:39, 66.70s/it] 34%|███▍      | 628/1859 [15:28:22<22:49:07, 66.73s/it] 34%|███▍      | 629/1859 [15:29:37<23:40:22, 69.29s/it] 34%|███▍      | 630/1859 [15:30:52<24:17:06, 71.14s/it]                                                        {'loss': 0.8285, 'grad_norm': 0.4153576195240021, 'learning_rate': 8.367457703312271e-05, 'epoch': 0.34}
 34%|███▍      | 630/1859 [15:30:52<24:17:06, 71.14s/it] 34%|███▍      | 631/1859 [15:32:07<24:38:12, 72.23s/it] 34%|███▍      | 632/1859 [15:33:17<24:23:41, 71.57s/it] 34%|███▍      | 633/1859 [15:34:31<24:34:09, 72.14s/it] 34%|███▍      | 634/1859 [15:35:49<25:10:32, 73.99s/it] 34%|███▍      | 635/1859 [15:37:15<26:25:15, 77.71s/it] 34%|███▍      | 636/1859 [15:38:09<23:56:40, 70.48s/it] 34%|███▍      | 637/1859 [15:39:14<23:22:27, 68.86s/it] 34%|███▍      | 638/1859 [15:40:26<23:42:13, 69.89s/it] 34%|███▍      | 639/1859 [15:41:44<24:26:03, 72.10s/it] 34%|███▍      | 640/1859 [15:42:47<23:29:21, 69.37s/it]                                                        {'loss': 0.7532, 'grad_norm': 0.24541939795017242, 'learning_rate': 8.297464335204657e-05, 'epoch': 0.34}
 34%|███▍      | 640/1859 [15:42:47<23:29:21, 69.37s/it] 34%|███▍      | 641/1859 [15:44:01<23:58:27, 70.86s/it] 35%|███▍      | 642/1859 [15:44:56<22:19:41, 66.05s/it] 35%|███▍      | 643/1859 [15:46:08<22:55:08, 67.85s/it] 35%|███▍      | 644/1859 [15:47:08<22:06:06, 65.49s/it] 35%|███▍      | 645/1859 [15:48:17<22:25:35, 66.50s/it] 35%|███▍      | 646/1859 [15:49:14<21:25:35, 63.59s/it] 35%|███▍      | 647/1859 [15:50:16<21:20:05, 63.37s/it] 35%|███▍      | 648/1859 [15:51:22<21:34:59, 64.16s/it] 35%|███▍      | 649/1859 [15:52:37<22:37:27, 67.31s/it] 35%|███▍      | 650/1859 [15:53:54<23:32:26, 70.10s/it]                                                        {'loss': 0.7363, 'grad_norm': 0.2690002918243408, 'learning_rate': 8.226308246920888e-05, 'epoch': 0.35}
 35%|███▍      | 650/1859 [15:53:54<23:32:26, 70.10s/it] 35%|███▌      | 651/1859 [15:54:59<23:05:14, 68.80s/it] 35%|███▌      | 652/1859 [15:56:00<22:16:45, 66.45s/it] 35%|███▌      | 653/1859 [15:57:17<23:16:23, 69.47s/it] 35%|███▌      | 654/1859 [15:58:32<23:48:57, 71.15s/it] 35%|███▌      | 655/1859 [16:00:01<25:33:46, 76.43s/it] 35%|███▌      | 656/1859 [16:01:01<23:54:33, 71.55s/it] 35%|███▌      | 657/1859 [16:02:18<24:26:19, 73.19s/it] 35%|███▌      | 658/1859 [16:03:27<24:00:49, 71.98s/it] 35%|███▌      | 659/1859 [16:04:50<25:02:57, 75.15s/it] 36%|███▌      | 660/1859 [16:06:16<26:10:43, 78.60s/it]                                                        {'loss': 0.7559, 'grad_norm': 0.27590152621269226, 'learning_rate': 8.154014528836806e-05, 'epoch': 0.36}
 36%|███▌      | 660/1859 [16:06:16<26:10:43, 78.60s/it] 36%|███▌      | 661/1859 [16:07:31<25:48:47, 77.57s/it] 36%|███▌      | 662/1859 [16:08:49<25:50:11, 77.70s/it] 36%|███▌      | 663/1859 [16:10:26<27:42:00, 83.38s/it] 36%|███▌      | 664/1859 [16:11:48<27:29:46, 82.83s/it] 36%|███▌      | 665/1859 [16:13:07<27:09:50, 81.90s/it] 36%|███▌      | 666/1859 [16:14:18<26:04:04, 78.66s/it] 36%|███▌      | 667/1859 [16:15:20<24:23:16, 73.66s/it] 36%|███▌      | 668/1859 [16:16:39<24:50:47, 75.10s/it] 36%|███▌      | 669/1859 [16:17:44<23:47:34, 71.98s/it] 36%|███▌      | 670/1859 [16:18:37<21:55:54, 66.40s/it]                                                        {'loss': 0.7528, 'grad_norm': 0.3138694763183594, 'learning_rate': 8.08060867246834e-05, 'epoch': 0.36}
 36%|███▌      | 670/1859 [16:18:37<21:55:54, 66.40s/it] 36%|███▌      | 671/1859 [16:19:46<22:11:29, 67.25s/it] 36%|███▌      | 672/1859 [16:20:50<21:48:19, 66.13s/it] 36%|███▌      | 673/1859 [16:21:49<21:07:43, 64.13s/it] 36%|███▋      | 674/1859 [16:22:57<21:28:42, 65.25s/it] 36%|███▋      | 675/1859 [16:23:59<21:10:38, 64.39s/it] 36%|███▋      | 676/1859 [16:25:00<20:49:13, 63.36s/it] 36%|███▋      | 677/1859 [16:26:11<21:29:48, 65.47s/it] 36%|███▋      | 678/1859 [16:27:18<21:38:28, 65.97s/it] 37%|███▋      | 679/1859 [16:28:20<21:16:08, 64.89s/it] 37%|███▋      | 680/1859 [16:29:25<21:12:33, 64.76s/it]                                                        {'loss': 0.7469, 'grad_norm': 0.30212050676345825, 'learning_rate': 8.006116561482931e-05, 'epoch': 0.37}
 37%|███▋      | 680/1859 [16:29:25<21:12:33, 64.76s/it] 37%|███▋      | 681/1859 [16:30:43<22:31:51, 68.85s/it] 37%|███▋      | 682/1859 [16:32:08<24:04:52, 73.66s/it] 37%|███▋      | 683/1859 [16:33:09<22:48:18, 69.81s/it] 37%|███▋      | 684/1859 [16:34:22<23:07:45, 70.86s/it] 37%|███▋      | 685/1859 [16:35:54<25:06:48, 77.01s/it] 37%|███▋      | 686/1859 [16:37:11<25:08:43, 77.17s/it] 37%|███▋      | 687/1859 [16:38:25<24:45:57, 76.07s/it] 37%|███▋      | 688/1859 [16:39:36<24:15:30, 74.58s/it] 37%|███▋      | 689/1859 [16:40:50<24:14:05, 74.57s/it] 37%|███▋      | 690/1859 [16:42:07<24:27:53, 75.34s/it]                                                        {'loss': 0.7605, 'grad_norm': 0.30991867184638977, 'learning_rate': 7.93056446257268e-05, 'epoch': 0.37}
 37%|███▋      | 690/1859 [16:42:07<24:27:53, 75.34s/it] 37%|███▋      | 691/1859 [16:43:11<23:19:17, 71.88s/it] 37%|███▋      | 692/1859 [16:44:13<22:17:35, 68.77s/it] 37%|███▋      | 693/1859 [16:45:28<22:53:18, 70.67s/it] 37%|███▋      | 694/1859 [16:46:29<21:58:30, 67.91s/it] 37%|███▋      | 695/1859 [16:47:46<22:48:03, 70.52s/it] 37%|███▋      | 696/1859 [16:49:03<23:24:48, 72.48s/it] 37%|███▋      | 697/1859 [16:50:36<25:24:41, 78.73s/it] 38%|███▊      | 698/1859 [16:51:42<24:05:34, 74.71s/it] 38%|███▊      | 699/1859 [16:52:40<22:30:13, 69.84s/it] 38%|███▊      | 700/1859 [16:53:46<22:03:40, 68.52s/it]                                                        {'loss': 0.7908, 'grad_norm': 0.2996237277984619, 'learning_rate': 7.85397901619244e-05, 'epoch': 0.38}
 38%|███▊      | 700/1859 [16:53:46<22:03:40, 68.52s/it] 38%|███▊      | 701/1859 [16:54:50<21:39:51, 67.35s/it] 38%|███▊      | 702/1859 [16:56:08<22:40:17, 70.54s/it] 38%|███▊      | 703/1859 [16:57:30<23:42:01, 73.81s/it] 38%|███▊      | 704/1859 [16:58:45<23:51:27, 74.36s/it] 38%|███▊      | 705/1859 [16:59:55<23:22:15, 72.91s/it] 38%|███▊      | 706/1859 [17:01:13<23:54:07, 74.63s/it] 38%|███▊      | 707/1859 [17:02:26<23:40:56, 74.01s/it] 38%|███▊      | 708/1859 [17:03:30<22:42:17, 71.01s/it] 38%|███▊      | 709/1859 [17:04:30<21:36:23, 67.64s/it] 38%|███▊      | 710/1859 [17:05:33<21:10:52, 66.36s/it]                                                        {'loss': 0.7812, 'grad_norm': 0.33034637570381165, 'learning_rate': 7.776387227166117e-05, 'epoch': 0.38}
 38%|███▊      | 710/1859 [17:05:33<21:10:52, 66.36s/it] 38%|███▊      | 711/1859 [17:06:46<21:44:35, 68.18s/it] 38%|███▊      | 712/1859 [17:07:48<21:09:10, 66.39s/it] 38%|███▊      | 713/1859 [17:08:50<20:44:26, 65.15s/it] 38%|███▊      | 714/1859 [17:09:52<20:28:11, 64.36s/it] 38%|███▊      | 715/1859 [17:11:03<21:00:01, 66.09s/it] 39%|███▊      | 716/1859 [17:12:03<20:27:04, 64.41s/it] 39%|███▊      | 717/1859 [17:13:28<22:23:10, 70.57s/it] 39%|███▊      | 718/1859 [17:14:33<21:47:57, 68.78s/it] 39%|███▊      | 719/1859 [17:15:34<21:02:25, 66.44s/it] 39%|███▊      | 720/1859 [17:16:46<21:36:34, 68.30s/it]                                                        {'loss': 0.734, 'grad_norm': 0.3905127942562103, 'learning_rate': 7.697816455164489e-05, 'epoch': 0.39}
 39%|███▊      | 720/1859 [17:16:46<21:36:34, 68.30s/it] 39%|███▉      | 721/1859 [17:18:02<22:18:52, 70.59s/it] 39%|███▉      | 722/1859 [17:18:59<21:00:32, 66.52s/it] 39%|███▉      | 723/1859 [17:20:04<20:51:33, 66.10s/it] 39%|███▉      | 724/1859 [17:21:12<20:57:36, 66.48s/it] 39%|███▉      | 725/1859 [17:22:28<21:53:55, 69.52s/it] 39%|███▉      | 726/1859 [17:23:34<21:28:55, 68.26s/it] 39%|███▉      | 727/1859 [17:24:48<22:04:29, 70.20s/it] 39%|███▉      | 728/1859 [17:26:08<22:55:56, 72.99s/it] 39%|███▉      | 729/1859 [17:27:13<22:07:58, 70.51s/it] 39%|███▉      | 730/1859 [17:28:21<21:51:56, 69.72s/it]                                                        {'loss': 0.7944, 'grad_norm': 0.3887268006801605, 'learning_rate': 7.618294405057892e-05, 'epoch': 0.39}
 39%|███▉      | 730/1859 [17:28:21<21:51:56, 69.72s/it] 39%|███▉      | 731/1859 [17:29:19<20:46:23, 66.30s/it] 39%|███▉      | 732/1859 [17:30:19<20:12:18, 64.54s/it] 39%|███▉      | 733/1859 [17:31:23<20:07:30, 64.34s/it] 39%|███▉      | 734/1859 [17:32:25<19:52:55, 63.62s/it] 40%|███▉      | 735/1859 [17:33:19<18:59:55, 60.85s/it] 40%|███▉      | 736/1859 [17:34:58<22:30:23, 72.15s/it] 40%|███▉      | 737/1859 [17:36:08<22:15:31, 71.42s/it] 40%|███▉      | 738/1859 [17:37:24<22:40:23, 72.81s/it] 40%|███▉      | 739/1859 [17:38:30<22:02:19, 70.84s/it] 40%|███▉      | 740/1859 [17:39:31<21:05:27, 67.85s/it]                                                        {'loss': 0.7672, 'grad_norm': 0.26710134744644165, 'learning_rate': 7.537849117147212e-05, 'epoch': 0.4}
 40%|███▉      | 740/1859 [17:39:31<21:05:27, 67.85s/it] 40%|███▉      | 741/1859 [17:40:46<21:42:35, 69.91s/it] 40%|███▉      | 742/1859 [17:42:01<22:14:21, 71.68s/it] 40%|███▉      | 743/1859 [17:43:00<21:00:04, 67.75s/it] 40%|████      | 744/1859 [17:44:07<20:53:21, 67.45s/it] 40%|████      | 745/1859 [17:45:30<22:20:56, 72.22s/it] 40%|████      | 746/1859 [17:46:51<23:08:28, 74.85s/it] 40%|████      | 747/1859 [17:48:13<23:46:04, 76.95s/it] 40%|████      | 748/1859 [17:49:23<23:07:33, 74.94s/it] 40%|████      | 749/1859 [17:50:28<22:10:31, 71.92s/it] 40%|████      | 750/1859 [17:51:33<21:32:57, 69.95s/it]                                                        {'loss': 0.7207, 'grad_norm': 0.34972062706947327, 'learning_rate': 7.45650895727657e-05, 'epoch': 0.4}
 40%|████      | 750/1859 [17:51:33<21:32:57, 69.95s/it] 40%|████      | 751/1859 [17:52:39<21:06:21, 68.57s/it] 40%|████      | 752/1859 [17:53:50<21:19:16, 69.34s/it] 41%|████      | 753/1859 [17:55:12<22:31:42, 73.33s/it] 41%|████      | 754/1859 [17:56:50<24:45:53, 80.68s/it] 41%|████      | 755/1859 [17:58:22<25:47:48, 84.12s/it] 41%|████      | 756/1859 [17:59:31<24:20:35, 79.45s/it] 41%|████      | 757/1859 [18:01:01<25:18:43, 82.69s/it] 41%|████      | 758/1859 [18:02:10<23:59:40, 78.46s/it] 41%|████      | 759/1859 [18:03:11<22:20:54, 73.14s/it] 41%|████      | 760/1859 [18:04:31<22:57:54, 75.23s/it]                                                        {'loss': 0.7772, 'grad_norm': 0.32299771904945374, 'learning_rate': 7.374302606831239e-05, 'epoch': 0.41}
 41%|████      | 760/1859 [18:04:31<22:57:54, 75.23s/it] 41%|████      | 761/1859 [18:05:31<21:32:54, 70.65s/it] 41%|████      | 762/1859 [18:06:43<21:40:10, 71.11s/it] 41%|████      | 763/1859 [18:07:57<21:55:01, 71.99s/it] 41%|████      | 764/1859 [18:09:08<21:48:24, 71.69s/it] 41%|████      | 765/1859 [18:10:14<21:14:34, 69.90s/it] 41%|████      | 766/1859 [18:11:17<20:38:01, 67.96s/it] 41%|████▏     | 767/1859 [18:12:24<20:31:27, 67.66s/it] 41%|████▏     | 768/1859 [18:13:14<18:53:42, 62.35s/it] 41%|████▏     | 769/1859 [18:14:29<20:03:22, 66.24s/it] 41%|████▏     | 770/1859 [18:16:04<22:34:46, 74.64s/it]                                                        {'loss': 0.728, 'grad_norm': 0.3129415512084961, 'learning_rate': 7.291259052624278e-05, 'epoch': 0.41}
 41%|████▏     | 770/1859 [18:16:04<22:34:46, 74.64s/it] 41%|████▏     | 771/1859 [18:17:25<23:10:56, 76.71s/it] 42%|████▏     | 772/1859 [18:18:22<21:22:00, 70.76s/it] 42%|████▏     | 773/1859 [18:19:41<22:05:35, 73.24s/it] 42%|████▏     | 774/1859 [18:20:49<21:34:23, 71.58s/it] 42%|████▏     | 775/1859 [18:22:04<21:51:08, 72.57s/it] 42%|████▏     | 776/1859 [18:23:12<21:25:40, 71.23s/it] 42%|████▏     | 777/1859 [18:24:30<22:05:16, 73.49s/it] 42%|████▏     | 778/1859 [18:25:54<23:00:54, 76.65s/it] 42%|████▏     | 779/1859 [18:26:59<21:55:06, 73.06s/it] 42%|████▏     | 780/1859 [18:28:08<21:32:53, 71.89s/it]                                                        {'loss': 0.7562, 'grad_norm': 0.291131854057312, 'learning_rate': 7.207407576675499e-05, 'epoch': 0.42}
 42%|████▏     | 780/1859 [18:28:08<21:32:53, 71.89s/it] 42%|████▏     | 781/1859 [18:29:18<21:17:47, 71.12s/it] 42%|████▏     | 782/1859 [18:30:30<21:21:00, 71.37s/it] 42%|████▏     | 783/1859 [18:31:49<22:05:03, 73.89s/it] 42%|████▏     | 784/1859 [18:33:12<22:52:27, 76.60s/it] 42%|████▏     | 785/1859 [18:34:56<25:15:17, 84.65s/it] 42%|████▏     | 786/1859 [18:36:28<25:55:27, 86.98s/it] 42%|████▏     | 787/1859 [18:38:20<28:06:25, 94.39s/it] 42%|████▏     | 788/1859 [18:39:36<26:27:27, 88.93s/it] 42%|████▏     | 789/1859 [18:41:09<26:48:11, 90.18s/it] 42%|████▏     | 790/1859 [18:42:25<25:31:56, 85.98s/it]                                                        {'loss': 0.7658, 'grad_norm': 0.34750837087631226, 'learning_rate': 7.122777745886305e-05, 'epoch': 0.42}
 42%|████▏     | 790/1859 [18:42:25<25:31:56, 85.98s/it] 43%|████▎     | 791/1859 [18:43:51<25:27:47, 85.83s/it] 43%|████▎     | 792/1859 [18:45:25<26:13:52, 88.50s/it] 43%|████▎     | 793/1859 [18:46:35<24:32:55, 82.90s/it] 43%|████▎     | 794/1859 [18:48:04<25:01:16, 84.58s/it] 43%|████▎     | 795/1859 [18:49:34<25:31:18, 86.35s/it] 43%|████▎     | 796/1859 [18:50:43<23:56:21, 81.07s/it] 43%|████▎     | 797/1859 [18:52:04<23:55:48, 81.12s/it] 43%|████▎     | 798/1859 [18:53:18<23:16:07, 78.95s/it] 43%|████▎     | 799/1859 [18:54:28<22:26:57, 76.24s/it] 43%|████▎     | 800/1859 [18:55:49<22:49:16, 77.58s/it]                                                        {'loss': 0.745, 'grad_norm': 0.3323867619037628, 'learning_rate': 7.037399401614104e-05, 'epoch': 0.43}
 43%|████▎     | 800/1859 [18:55:49<22:49:16, 77.58s/it] 43%|████▎     | 801/1859 [18:56:44<20:50:58, 70.94s/it] 43%|████▎     | 802/1859 [18:58:03<21:29:37, 73.21s/it] 43%|████▎     | 803/1859 [18:59:04<20:25:23, 69.62s/it] 43%|████▎     | 804/1859 [19:00:00<19:11:26, 65.48s/it] 43%|████▎     | 805/1859 [19:01:00<18:43:00, 63.93s/it] 43%|████▎     | 806/1859 [19:02:16<19:45:41, 67.56s/it] 43%|████▎     | 807/1859 [19:03:32<20:28:55, 70.09s/it] 43%|████▎     | 808/1859 [19:04:46<20:46:15, 71.15s/it] 44%|████▎     | 809/1859 [19:05:49<20:05:08, 68.86s/it] 44%|████▎     | 810/1859 [19:06:58<20:03:25, 68.83s/it]                                                        {'loss': 0.7084, 'grad_norm': 0.33563709259033203, 'learning_rate': 6.95130264914993e-05, 'epoch': 0.44}
 44%|████▎     | 810/1859 [19:06:58<20:03:25, 68.83s/it] 44%|████▎     | 811/1859 [19:08:27<21:49:54, 75.00s/it] 44%|████▎     | 812/1859 [19:09:28<20:31:38, 70.58s/it] 44%|████▎     | 813/1859 [19:11:00<22:26:15, 77.22s/it] 44%|████▍     | 814/1859 [19:11:56<20:34:11, 70.86s/it] 44%|████▍     | 815/1859 [19:13:24<21:58:40, 75.79s/it] 44%|████▍     | 816/1859 [19:14:31<21:14:37, 73.32s/it] 44%|████▍     | 817/1859 [19:15:55<22:08:14, 76.48s/it] 44%|████▍     | 818/1859 [19:17:22<22:59:32, 79.51s/it] 44%|████▍     | 819/1859 [19:18:37<22:36:48, 78.28s/it] 44%|████▍     | 820/1859 [19:20:01<23:02:15, 79.82s/it]                                                        {'loss': 0.7715, 'grad_norm': 0.2675652503967285, 'learning_rate': 6.86451784710301e-05, 'epoch': 0.44}
 44%|████▍     | 820/1859 [19:20:01<23:02:15, 79.82s/it] 44%|████▍     | 821/1859 [19:21:16<22:37:21, 78.46s/it] 44%|████▍     | 822/1859 [19:22:21<21:25:39, 74.39s/it] 44%|████▍     | 823/1859 [19:23:38<21:39:57, 75.29s/it] 44%|████▍     | 824/1859 [19:24:54<21:40:23, 75.39s/it] 44%|████▍     | 825/1859 [19:26:00<20:50:23, 72.56s/it] 44%|████▍     | 826/1859 [19:27:42<23:24:08, 81.56s/it] 44%|████▍     | 827/1859 [19:28:55<22:35:48, 78.83s/it] 45%|████▍     | 828/1859 [19:30:19<23:01:43, 80.41s/it] 45%|████▍     | 829/1859 [19:31:35<22:40:27, 79.25s/it] 45%|████▍     | 830/1859 [19:32:51<22:19:14, 78.09s/it]                                                        {'loss': 0.7655, 'grad_norm': 0.2973693013191223, 'learning_rate': 6.777075596696003e-05, 'epoch': 0.45}
 45%|████▍     | 830/1859 [19:32:51<22:19:14, 78.09s/it] 45%|████▍     | 831/1859 [19:34:22<23:24:14, 81.96s/it] 45%|████▍     | 832/1859 [19:35:20<21:19:27, 74.75s/it] 45%|████▍     | 833/1859 [19:36:49<22:33:07, 79.13s/it] 45%|████▍     | 834/1859 [19:38:08<22:31:59, 79.14s/it] 45%|████▍     | 835/1859 [19:39:24<22:13:45, 78.15s/it] 45%|████▍     | 836/1859 [19:40:25<20:46:03, 73.08s/it] 45%|████▌     | 837/1859 [19:41:29<19:56:28, 70.24s/it] 45%|████▌     | 838/1859 [19:42:48<20:42:16, 73.00s/it] 45%|████▌     | 839/1859 [19:43:59<20:27:05, 72.18s/it] 45%|████▌     | 840/1859 [19:45:19<21:09:31, 74.75s/it]                                                        {'loss': 0.7543, 'grad_norm': 0.3420419692993164, 'learning_rate': 6.689006730974685e-05, 'epoch': 0.45}
 45%|████▌     | 840/1859 [19:45:19<21:09:31, 74.75s/it] 45%|████▌     | 841/1859 [19:46:35<21:10:30, 74.88s/it] 45%|████▌     | 842/1859 [19:48:01<22:10:35, 78.50s/it] 45%|████▌     | 843/1859 [19:49:06<20:59:25, 74.38s/it] 45%|████▌     | 844/1859 [19:50:13<20:19:15, 72.07s/it] 45%|████▌     | 845/1859 [19:51:33<20:58:02, 74.44s/it] 46%|████▌     | 846/1859 [19:52:43<20:36:43, 73.25s/it] 46%|████▌     | 847/1859 [19:54:03<21:09:10, 75.25s/it] 46%|████▌     | 848/1859 [19:55:15<20:49:36, 74.16s/it] 46%|████▌     | 849/1859 [19:56:26<20:31:01, 73.13s/it] 46%|████▌     | 850/1859 [19:57:21<19:01:48, 67.90s/it]                                                        {'loss': 0.7592, 'grad_norm': 0.3195655047893524, 'learning_rate': 6.600342303935901e-05, 'epoch': 0.46}
 46%|████▌     | 850/1859 [19:57:21<19:01:48, 67.90s/it] 46%|████▌     | 851/1859 [19:58:32<19:15:58, 68.81s/it] 46%|████▌     | 852/1859 [19:59:37<18:53:09, 67.52s/it] 46%|████▌     | 853/1859 [20:00:54<19:40:20, 70.40s/it] 46%|████▌     | 854/1859 [20:01:58<19:08:05, 68.54s/it] 46%|████▌     | 855/1859 [20:03:07<19:10:58, 68.78s/it] 46%|████▌     | 856/1859 [20:04:07<18:25:05, 66.11s/it] 46%|████▌     | 857/1859 [20:05:11<18:11:27, 65.36s/it] 46%|████▌     | 858/1859 [20:06:21<18:35:34, 66.87s/it] 46%|████▌     | 859/1859 [20:07:31<18:50:25, 67.83s/it] 46%|████▋     | 860/1859 [20:09:01<20:40:03, 74.48s/it]                                                        {'loss': 0.7475, 'grad_norm': 0.3478970229625702, 'learning_rate': 6.511113579577607e-05, 'epoch': 0.46}
 46%|████▋     | 860/1859 [20:09:01<20:40:03, 74.48s/it] 46%|████▋     | 861/1859 [20:10:30<21:50:00, 78.76s/it] 46%|████▋     | 862/1859 [20:11:50<21:56:18, 79.22s/it] 46%|████▋     | 863/1859 [20:13:05<21:33:52, 77.94s/it] 46%|████▋     | 864/1859 [20:14:32<22:17:09, 80.63s/it] 47%|████▋     | 865/1859 [20:15:36<20:53:59, 75.69s/it] 47%|████▋     | 866/1859 [20:16:50<20:41:22, 75.01s/it] 47%|████▋     | 867/1859 [20:18:12<21:15:06, 77.12s/it] 47%|████▋     | 868/1859 [20:19:36<21:48:05, 79.20s/it] 47%|████▋     | 869/1859 [20:20:45<20:56:23, 76.15s/it] 47%|████▋     | 870/1859 [20:22:07<21:25:25, 77.98s/it]                                                        {'loss': 0.7689, 'grad_norm': 0.35727930068969727, 'learning_rate': 6.421352020874846e-05, 'epoch': 0.47}
 47%|████▋     | 870/1859 [20:22:07<21:25:25, 77.98s/it] 47%|████▋     | 871/1859 [20:23:22<21:07:36, 76.98s/it] 47%|████▋     | 872/1859 [20:24:32<20:32:53, 74.95s/it] 47%|████▋     | 873/1859 [20:25:35<19:32:49, 71.37s/it] 47%|████▋     | 874/1859 [20:26:37<18:46:29, 68.62s/it] 47%|████▋     | 875/1859 [20:27:49<19:02:20, 69.65s/it] 47%|████▋     | 876/1859 [20:29:06<19:35:03, 71.72s/it] 47%|████▋     | 877/1859 [20:30:13<19:10:09, 70.27s/it] 47%|████▋     | 878/1859 [20:31:21<18:57:49, 69.59s/it] 47%|████▋     | 879/1859 [20:32:50<20:32:35, 75.46s/it] 47%|████▋     | 880/1859 [20:34:04<20:21:48, 74.88s/it]                                                        {'loss': 0.7721, 'grad_norm': 0.3610367774963379, 'learning_rate': 6.331089278685599e-05, 'epoch': 0.47}
 47%|████▋     | 880/1859 [20:34:04<20:21:48, 74.88s/it] 47%|████▋     | 881/1859 [20:35:28<21:08:04, 77.80s/it] 47%|████▋     | 882/1859 [20:36:47<21:13:26, 78.21s/it] 47%|████▋     | 883/1859 [20:38:07<21:17:19, 78.52s/it] 48%|████▊     | 884/1859 [20:39:11<20:07:49, 74.33s/it] 48%|████▊     | 885/1859 [20:40:48<21:55:26, 81.03s/it] 48%|████▊     | 886/1859 [20:42:13<22:15:08, 82.33s/it] 48%|████▊     | 887/1859 [20:43:37<22:22:19, 82.86s/it] 48%|████▊     | 888/1859 [20:44:51<21:34:46, 80.01s/it] 48%|████▊     | 889/1859 [20:46:08<21:19:02, 79.12s/it] 48%|████▊     | 890/1859 [20:47:10<19:59:09, 74.25s/it]                                                        {'loss': 0.7459, 'grad_norm': 0.3468673825263977, 'learning_rate': 6.24035718059034e-05, 'epoch': 0.48}
 48%|████▊     | 890/1859 [20:47:10<19:59:09, 74.25s/it] 48%|████▊     | 891/1859 [20:48:20<19:37:05, 72.96s/it] 48%|████▊     | 892/1859 [20:49:29<19:15:42, 71.71s/it] 48%|████▊     | 893/1859 [20:50:42<19:19:50, 72.04s/it] 48%|████▊     | 894/1859 [20:51:53<19:11:53, 71.62s/it] 48%|████▊     | 895/1859 [20:53:06<19:17:28, 72.04s/it] 48%|████▊     | 896/1859 [20:54:40<21:02:44, 78.68s/it] 48%|████▊     | 897/1859 [20:56:12<22:04:09, 82.59s/it] 48%|████▊     | 898/1859 [20:57:21<21:00:21, 78.69s/it] 48%|████▊     | 899/1859 [20:58:30<20:09:22, 75.59s/it] 48%|████▊     | 900/1859 [20:59:41<19:48:40, 74.37s/it]                                                        {'loss': 0.7533, 'grad_norm': 0.44512465596199036, 'learning_rate': 6.149187719669312e-05, 'epoch': 0.48}
 48%|████▊     | 900/1859 [20:59:41<19:48:40, 74.37s/it] 48%|████▊     | 901/1859 [21:01:05<20:33:49, 77.28s/it] 49%|████▊     | 902/1859 [21:02:12<19:40:39, 74.02s/it] 49%|████▊     | 903/1859 [21:03:35<20:23:09, 76.77s/it] 49%|████▊     | 904/1859 [21:04:45<19:52:06, 74.90s/it] 49%|████▊     | 905/1859 [21:06:04<20:09:34, 76.07s/it] 49%|████▊     | 906/1859 [21:07:31<20:58:28, 79.23s/it] 49%|████▉     | 907/1859 [21:08:59<21:42:10, 82.07s/it] 49%|████▉     | 908/1859 [21:10:18<21:24:18, 81.03s/it] 49%|████▉     | 909/1859 [21:11:35<21:06:17, 79.98s/it] 49%|████▉     | 910/1859 [21:12:43<20:08:10, 76.39s/it]                                                        {'loss': 0.752, 'grad_norm': 0.3123369812965393, 'learning_rate': 6.057613043221436e-05, 'epoch': 0.49}
 49%|████▉     | 910/1859 [21:12:43<20:08:10, 76.39s/it] 49%|████▉     | 911/1859 [21:14:13<21:07:14, 80.20s/it] 49%|████▉     | 912/1859 [21:15:21<20:09:37, 76.64s/it] 49%|████▉     | 913/1859 [21:16:29<19:27:58, 74.08s/it] 49%|████▉     | 914/1859 [21:17:50<20:01:13, 76.27s/it] 49%|████▉     | 915/1859 [21:18:53<18:53:26, 72.04s/it] 49%|████▉     | 916/1859 [21:19:55<18:07:28, 69.19s/it] 49%|████▉     | 917/1859 [21:21:08<18:23:55, 70.31s/it] 49%|████▉     | 918/1859 [21:22:16<18:11:48, 69.62s/it] 49%|████▉     | 919/1859 [21:23:15<17:19:28, 66.35s/it] 49%|████▉     | 920/1859 [21:24:16<16:53:47, 64.78s/it]                                                        {'loss': 0.7244, 'grad_norm': 0.2660283148288727, 'learning_rate': 5.965665441428837e-05, 'epoch': 0.49}
 49%|████▉     | 920/1859 [21:24:16<16:53:47, 64.78s/it] 50%|████▉     | 921/1859 [21:25:26<17:16:22, 66.29s/it] 50%|████▉     | 922/1859 [21:26:30<17:06:06, 65.71s/it] 50%|████▉     | 923/1859 [21:27:39<17:21:51, 66.79s/it] 50%|████▉     | 924/1859 [21:28:35<16:30:41, 63.57s/it] 50%|████▉     | 925/1859 [21:29:55<17:44:55, 68.41s/it] 50%|████▉     | 926/1859 [21:30:59<17:24:05, 67.14s/it] 50%|████▉     | 927/1859 [21:32:33<19:26:06, 75.07s/it] 50%|████▉     | 928/1859 [21:33:33<18:15:17, 70.59s/it] 50%|████▉     | 929/1859 [21:34:39<17:53:28, 69.26s/it] 50%|█████     | 930/1859 [21:35:53<18:13:05, 70.60s/it]                                                        {'loss': 0.7746, 'grad_norm': 0.42888930439949036, 'learning_rate': 5.873377335970988e-05, 'epoch': 0.5}
 50%|█████     | 930/1859 [21:35:53<18:13:05, 70.60s/it] 50%|█████     | 931/1859 [21:37:41<21:05:23, 81.81s/it] 50%|█████     | 932/1859 [21:38:57<20:37:02, 80.07s/it] 50%|█████     | 933/1859 [21:40:07<19:47:55, 76.97s/it] 50%|█████     | 934/1859 [21:41:35<20:37:11, 80.25s/it] 50%|█████     | 935/1859 [21:43:00<21:00:06, 81.82s/it] 50%|█████     | 936/1859 [21:44:00<19:16:00, 75.15s/it] 50%|█████     | 937/1859 [21:45:08<18:42:55, 73.08s/it] 50%|█████     | 938/1859 [21:46:14<18:09:10, 70.96s/it] 51%|█████     | 939/1859 [21:47:46<19:46:51, 77.40s/it] 51%|█████     | 940/1859 [21:49:09<20:08:36, 78.91s/it]                                                        {'loss': 0.7218, 'grad_norm': 0.18650829792022705, 'learning_rate': 5.780781268592494e-05, 'epoch': 0.51}
 51%|█████     | 940/1859 [21:49:09<20:08:36, 78.91s/it] 51%|█████     | 941/1859 [21:50:08<18:37:22, 73.03s/it] 51%|█████     | 942/1859 [21:51:33<19:31:37, 76.66s/it] 51%|█████     | 943/1859 [21:52:44<19:04:18, 74.95s/it] 51%|█████     | 944/1859 [21:54:00<19:08:30, 75.31s/it] 51%|█████     | 945/1859 [21:55:24<19:43:44, 77.71s/it] 51%|█████     | 946/1859 [21:56:48<20:12:34, 79.69s/it] 51%|█████     | 947/1859 [21:57:55<19:14:42, 75.97s/it] 51%|█████     | 948/1859 [21:59:15<19:30:52, 77.12s/it] 51%|█████     | 949/1859 [22:00:49<20:48:42, 82.33s/it] 51%|█████     | 950/1859 [22:01:44<18:39:44, 73.91s/it]                                                        {'loss': 0.733, 'grad_norm': 0.3385385572910309, 'learning_rate': 5.687909889628529e-05, 'epoch': 0.51}
 51%|█████     | 950/1859 [22:01:44<18:39:44, 73.91s/it] 51%|█████     | 951/1859 [22:02:44<17:38:08, 69.92s/it] 51%|█████     | 952/1859 [22:04:06<18:30:55, 73.49s/it] 51%|█████▏    | 953/1859 [22:05:07<17:30:27, 69.57s/it] 51%|█████▏    | 954/1859 [22:06:21<17:49:32, 70.91s/it] 51%|█████▏    | 955/1859 [22:07:30<17:42:07, 70.50s/it] 51%|█████▏    | 956/1859 [22:08:44<17:57:01, 71.56s/it] 51%|█████▏    | 957/1859 [22:10:19<19:38:31, 78.39s/it] 52%|█████▏    | 958/1859 [22:11:58<21:11:47, 84.69s/it] 52%|█████▏    | 959/1859 [22:13:21<21:02:55, 84.19s/it] 52%|█████▏    | 960/1859 [22:14:27<19:40:45, 78.80s/it]                                                        {'loss': 0.7877, 'grad_norm': 0.29471278190612793, 'learning_rate': 5.594795946491994e-05, 'epoch': 0.52}
 52%|█████▏    | 960/1859 [22:14:27<19:40:45, 78.80s/it] 52%|█████▏    | 961/1859 [22:16:02<20:52:44, 83.70s/it] 52%|█████▏    | 962/1859 [22:17:12<19:47:25, 79.43s/it] 52%|█████▏    | 963/1859 [22:18:16<18:38:25, 74.89s/it] 52%|█████▏    | 964/1859 [22:19:37<19:06:14, 76.84s/it] 52%|█████▏    | 965/1859 [22:20:44<18:20:16, 73.84s/it] 52%|█████▏    | 966/1859 [22:22:13<19:24:49, 78.26s/it] 52%|█████▏    | 967/1859 [22:23:17<18:22:07, 74.13s/it] 52%|█████▏    | 968/1859 [22:24:36<18:40:47, 75.47s/it] 52%|█████▏    | 969/1859 [22:25:42<17:56:25, 72.57s/it] 52%|█████▏    | 970/1859 [22:27:21<19:55:03, 80.66s/it]                                                        {'loss': 0.7493, 'grad_norm': 0.28568780422210693, 'learning_rate': 5.5014722721264364e-05, 'epoch': 0.52}
 52%|█████▏    | 970/1859 [22:27:21<19:55:03, 80.66s/it] 52%|█████▏    | 971/1859 [22:28:25<18:36:21, 75.43s/it] 52%|█████▏    | 972/1859 [22:30:00<20:03:38, 81.42s/it] 52%|█████▏    | 973/1859 [22:31:21<19:59:30, 81.23s/it] 52%|█████▏    | 974/1859 [22:32:27<18:53:03, 76.82s/it] 52%|█████▏    | 975/1859 [22:33:48<19:10:56, 78.12s/it] 53%|█████▎    | 976/1859 [22:34:51<18:02:41, 73.57s/it] 53%|█████▎    | 977/1859 [22:36:18<18:58:38, 77.46s/it] 53%|█████▎    | 978/1859 [22:37:31<18:36:10, 76.02s/it] 53%|█████▎    | 979/1859 [22:38:33<17:34:51, 71.92s/it] 53%|█████▎    | 980/1859 [22:39:45<17:33:10, 71.89s/it]                                                        {'loss': 0.7206, 'grad_norm': 0.431152880191803, 'learning_rate': 5.4079717734288115e-05, 'epoch': 0.53}
 53%|█████▎    | 980/1859 [22:39:45<17:33:10, 71.89s/it] 53%|█████▎    | 981/1859 [22:41:00<17:47:44, 72.97s/it] 53%|█████▎    | 982/1859 [22:42:18<18:09:51, 74.56s/it] 53%|█████▎    | 983/1859 [22:43:35<18:16:47, 75.12s/it] 53%|█████▎    | 984/1859 [22:44:46<17:57:02, 73.85s/it] 53%|█████▎    | 985/1859 [22:45:50<17:12:40, 70.89s/it] 53%|█████▎    | 986/1859 [22:47:17<18:23:24, 75.84s/it] 53%|█████▎    | 987/1859 [22:48:20<17:25:08, 71.91s/it] 53%|█████▎    | 988/1859 [22:49:33<17:29:54, 72.32s/it] 53%|█████▎    | 989/1859 [22:50:56<18:15:37, 75.56s/it] 53%|█████▎    | 990/1859 [22:52:08<17:56:11, 74.31s/it]                                                        {'loss': 0.7582, 'grad_norm': 0.31492680311203003, 'learning_rate': 5.3143274196461735e-05, 'epoch': 0.53}
 53%|█████▎    | 990/1859 [22:52:08<17:56:11, 74.31s/it] 53%|█████▎    | 991/1859 [22:53:22<17:54:03, 74.24s/it] 53%|█████▎    | 992/1859 [22:54:40<18:10:37, 75.48s/it] 53%|█████▎    | 993/1859 [22:56:00<18:27:08, 76.71s/it] 53%|█████▎    | 994/1859 [22:57:31<19:27:25, 80.98s/it] 54%|█████▎    | 995/1859 [22:58:48<19:11:40, 79.98s/it] 54%|█████▎    | 996/1859 [22:59:54<18:07:12, 75.59s/it] 54%|█████▎    | 997/1859 [23:01:21<18:54:32, 78.97s/it] 54%|█████▎    | 998/1859 [23:02:35<18:35:54, 77.76s/it] 54%|█████▎    | 999/1859 [23:03:45<17:58:40, 75.26s/it] 54%|█████▍    | 1000/1859 [23:04:52<17:23:16, 72.87s/it]                                                         {'loss': 0.7631, 'grad_norm': 0.2867879569530487, 'learning_rate': 5.220572230750373e-05, 'epoch': 0.54}
 54%|█████▍    | 1000/1859 [23:04:52<17:23:16, 72.87s/it][INFO|trainer.py:4643] 2025-12-05 09:14:48,299 >> 
***** Running Evaluation *****
[INFO|trainer.py:4645] 2025-12-05 09:14:48,299 >>   Num examples = 3305
[INFO|trainer.py:4648] 2025-12-05 09:14:48,299 >>   Batch size = 1

  0%|          | 0/3305 [00:00<?, ?it/s][A
  0%|          | 2/3305 [00:03<1:31:10,  1.66s/it][A
  0%|          | 3/3305 [00:06<2:15:21,  2.46s/it][A
  0%|          | 4/3305 [00:10<2:33:39,  2.79s/it][A
  0%|          | 5/3305 [00:13<2:39:32,  2.90s/it][A
  0%|          | 6/3305 [00:16<2:38:18,  2.88s/it][A
  0%|          | 7/3305 [00:20<2:58:19,  3.24s/it][A
  0%|          | 8/3305 [00:24<3:20:37,  3.65s/it][A
  0%|          | 9/3305 [00:27<3:08:40,  3.43s/it][A
  0%|          | 10/3305 [00:29<2:47:29,  3.05s/it][A
  0%|          | 11/3305 [00:32<2:36:24,  2.85s/it][A
  0%|          | 12/3305 [00:34<2:29:03,  2.72s/it][A
  0%|          | 13/3305 [00:36<2:16:59,  2.50s/it][A
  0%|          | 14/3305 [00:38<2:12:39,  2.42s/it][A
  0%|          | 15/3305 [00:41<2:11:04,  2.39s/it][A
  0%|          | 16/3305 [00:44<2:27:32,  2.69s/it][A
  1%|          | 17/3305 [00:49<2:56:55,  3.23s/it][A
  1%|          | 18/3305 [00:52<2:54:12,  3.18s/it][A
  1%|          | 19/3305 [00:54<2:46:38,  3.04s/it][A
  1%|          | 20/3305 [00:58<2:50:37,  3.12s/it][A
  1%|          | 21/3305 [01:00<2:37:00,  2.87s/it][A
  1%|          | 22/3305 [01:03<2:33:34,  2.81s/it][A
  1%|          | 23/3305 [01:06<2:42:11,  2.97s/it][A
  1%|          | 24/3305 [01:08<2:30:21,  2.75s/it][A
  1%|          | 25/3305 [01:11<2:36:56,  2.87s/it][A
  1%|          | 26/3305 [01:15<2:54:47,  3.20s/it][A
  1%|          | 27/3305 [01:19<2:59:14,  3.28s/it][A
  1%|          | 28/3305 [01:22<3:00:46,  3.31s/it][A
  1%|          | 29/3305 [01:24<2:43:59,  3.00s/it][A
  1%|          | 30/3305 [01:27<2:33:09,  2.81s/it][A
  1%|          | 31/3305 [01:30<2:42:12,  2.97s/it][A
  1%|          | 32/3305 [01:33<2:42:19,  2.98s/it][A
  1%|          | 33/3305 [01:36<2:44:14,  3.01s/it][A
  1%|          | 34/3305 [01:40<2:54:44,  3.21s/it][A
  1%|          | 35/3305 [01:45<3:21:23,  3.70s/it][A
  1%|          | 36/3305 [01:51<4:09:40,  4.58s/it][A
  1%|          | 37/3305 [01:58<4:50:30,  5.33s/it][A
  1%|          | 38/3305 [02:05<5:15:23,  5.79s/it][A
  1%|          | 39/3305 [02:08<4:31:43,  4.99s/it][A
  1%|          | 40/3305 [02:11<3:54:16,  4.31s/it][A
  1%|          | 41/3305 [02:14<3:30:01,  3.86s/it][A
  1%|▏         | 42/3305 [02:17<3:14:37,  3.58s/it][A
  1%|▏         | 43/3305 [02:20<2:59:26,  3.30s/it][A
  1%|▏         | 44/3305 [02:22<2:51:53,  3.16s/it][A
  1%|▏         | 45/3305 [02:25<2:45:11,  3.04s/it][A
  1%|▏         | 46/3305 [02:28<2:42:46,  3.00s/it][A
  1%|▏         | 47/3305 [02:30<2:25:46,  2.68s/it][A
  1%|▏         | 48/3305 [02:32<2:12:43,  2.45s/it][A
  1%|▏         | 49/3305 [02:34<2:07:27,  2.35s/it][A
  2%|▏         | 50/3305 [02:37<2:11:44,  2.43s/it][A
  2%|▏         | 51/3305 [02:39<2:13:51,  2.47s/it][A
  2%|▏         | 52/3305 [02:41<2:02:50,  2.27s/it][A
  2%|▏         | 53/3305 [02:43<2:00:29,  2.22s/it][A
  2%|▏         | 54/3305 [02:46<2:17:44,  2.54s/it][A
  2%|▏         | 55/3305 [02:50<2:31:38,  2.80s/it][A
  2%|▏         | 56/3305 [02:53<2:31:22,  2.80s/it][A
  2%|▏         | 57/3305 [02:56<2:41:15,  2.98s/it][A
  2%|▏         | 58/3305 [03:00<2:49:56,  3.14s/it][A
  2%|▏         | 59/3305 [03:02<2:41:50,  2.99s/it][A
  2%|▏         | 60/3305 [03:05<2:38:46,  2.94s/it][A
  2%|▏         | 61/3305 [03:08<2:46:52,  3.09s/it][A
  2%|▏         | 62/3305 [03:12<2:57:20,  3.28s/it][A
  2%|▏         | 63/3305 [03:16<3:01:07,  3.35s/it][A
  2%|▏         | 64/3305 [03:19<2:56:37,  3.27s/it][A
  2%|▏         | 65/3305 [03:22<2:49:22,  3.14s/it][A
  2%|▏         | 66/3305 [03:24<2:40:21,  2.97s/it][A
  2%|▏         | 67/3305 [03:27<2:32:01,  2.82s/it][A
  2%|▏         | 68/3305 [03:30<2:36:58,  2.91s/it][A
  2%|▏         | 69/3305 [03:32<2:29:27,  2.77s/it][A
  2%|▏         | 70/3305 [03:34<2:21:10,  2.62s/it][A
  2%|▏         | 71/3305 [03:37<2:26:17,  2.71s/it][A
  2%|▏         | 72/3305 [03:41<2:37:30,  2.92s/it][A
  2%|▏         | 73/3305 [03:44<2:44:42,  3.06s/it][A
  2%|▏         | 74/3305 [03:48<2:51:09,  3.18s/it][A
  2%|▏         | 75/3305 [03:51<2:56:22,  3.28s/it][A
  2%|▏         | 76/3305 [03:55<3:02:01,  3.38s/it][A
  2%|▏         | 77/3305 [04:00<3:28:07,  3.87s/it][A
  2%|▏         | 78/3305 [04:06<4:05:55,  4.57s/it][A
  2%|▏         | 79/3305 [04:10<3:58:19,  4.43s/it][A
  2%|▏         | 80/3305 [04:12<3:18:47,  3.70s/it][A
  2%|▏         | 81/3305 [04:15<3:07:50,  3.50s/it][A
  2%|▏         | 82/3305 [04:18<3:02:44,  3.40s/it][A
  3%|▎         | 83/3305 [04:21<2:49:22,  3.15s/it][A
  3%|▎         | 84/3305 [04:25<3:03:31,  3.42s/it][A
  3%|▎         | 85/3305 [04:29<3:09:11,  3.53s/it][A
  3%|▎         | 86/3305 [04:33<3:25:15,  3.83s/it][A
  3%|▎         | 87/3305 [04:38<3:42:58,  4.16s/it][A
  3%|▎         | 88/3305 [04:41<3:16:12,  3.66s/it][A
  3%|▎         | 89/3305 [04:42<2:47:09,  3.12s/it][A
  3%|▎         | 90/3305 [04:45<2:36:11,  2.91s/it][A
  3%|▎         | 91/3305 [04:47<2:29:40,  2.79s/it][A
  3%|▎         | 92/3305 [04:50<2:31:48,  2.83s/it][A
  3%|▎         | 93/3305 [04:53<2:35:02,  2.90s/it][A
  3%|▎         | 94/3305 [04:56<2:27:05,  2.75s/it][A
  3%|▎         | 95/3305 [04:59<2:28:16,  2.77s/it][A
  3%|▎         | 96/3305 [05:03<2:49:40,  3.17s/it][A
  3%|▎         | 97/3305 [05:08<3:18:50,  3.72s/it][A
  3%|▎         | 98/3305 [05:12<3:21:10,  3.76s/it][A
  3%|▎         | 99/3305 [05:14<2:54:43,  3.27s/it][A
  3%|▎         | 100/3305 [05:16<2:46:56,  3.13s/it][A
  3%|▎         | 101/3305 [05:19<2:42:08,  3.04s/it][A
  3%|▎         | 102/3305 [05:25<3:18:25,  3.72s/it][A
  3%|▎         | 103/3305 [05:30<3:45:52,  4.23s/it][A
  3%|▎         | 104/3305 [05:34<3:40:58,  4.14s/it][A
  3%|▎         | 105/3305 [05:39<3:46:41,  4.25s/it][A
  3%|▎         | 106/3305 [05:41<3:13:05,  3.62s/it][A
  3%|▎         | 107/3305 [05:43<2:51:12,  3.21s/it][A
  3%|▎         | 108/3305 [05:46<2:42:03,  3.04s/it][A
  3%|▎         | 109/3305 [05:48<2:36:52,  2.94s/it][A
  3%|▎         | 110/3305 [05:50<2:21:25,  2.66s/it][A
  3%|▎         | 111/3305 [05:52<2:09:57,  2.44s/it][A
  3%|▎         | 112/3305 [05:54<2:06:42,  2.38s/it][A
  3%|▎         | 113/3305 [05:57<2:06:40,  2.38s/it][A
  3%|▎         | 114/3305 [06:00<2:22:45,  2.68s/it][A
  3%|▎         | 115/3305 [06:03<2:23:09,  2.69s/it][A
  4%|▎         | 116/3305 [06:05<2:11:25,  2.47s/it][A
  4%|▎         | 117/3305 [06:09<2:37:49,  2.97s/it][A
  4%|▎         | 118/3305 [06:13<3:00:15,  3.39s/it][A
  4%|▎         | 119/3305 [06:15<2:34:02,  2.90s/it][A
  4%|▎         | 120/3305 [06:19<2:44:05,  3.09s/it][A
  4%|▎         | 121/3305 [06:23<3:04:27,  3.48s/it][A
  4%|▎         | 122/3305 [06:26<2:48:33,  3.18s/it][A
  4%|▎         | 123/3305 [06:27<2:26:42,  2.77s/it][A
  4%|▍         | 124/3305 [06:29<2:16:29,  2.57s/it][A
  4%|▍         | 125/3305 [06:32<2:16:43,  2.58s/it][A
  4%|▍         | 126/3305 [06:35<2:15:39,  2.56s/it][A
  4%|▍         | 127/3305 [06:36<2:04:55,  2.36s/it][A
  4%|▍         | 128/3305 [06:39<2:09:22,  2.44s/it][A
  4%|▍         | 129/3305 [06:43<2:32:37,  2.88s/it][A
  4%|▍         | 130/3305 [06:47<2:54:28,  3.30s/it][A
  4%|▍         | 131/3305 [06:53<3:27:58,  3.93s/it][A
  4%|▍         | 132/3305 [06:58<3:44:05,  4.24s/it][A
  4%|▍         | 133/3305 [07:00<3:18:27,  3.75s/it][A
  4%|▍         | 134/3305 [07:03<3:07:31,  3.55s/it][A
  4%|▍         | 135/3305 [07:06<2:47:35,  3.17s/it][A
  4%|▍         | 136/3305 [07:07<2:25:09,  2.75s/it][A
  4%|▍         | 137/3305 [07:09<2:09:55,  2.46s/it][A
  4%|▍         | 138/3305 [07:12<2:17:52,  2.61s/it][A
  4%|▍         | 139/3305 [07:16<2:35:50,  2.95s/it][A
  4%|▍         | 140/3305 [07:20<2:53:29,  3.29s/it][A
  4%|▍         | 141/3305 [07:22<2:41:00,  3.05s/it][A
  4%|▍         | 142/3305 [07:24<2:24:31,  2.74s/it][A
  4%|▍         | 143/3305 [07:27<2:13:59,  2.54s/it][A
  4%|▍         | 144/3305 [07:29<2:06:10,  2.39s/it][A
  4%|▍         | 145/3305 [07:31<2:00:51,  2.29s/it][A
  4%|▍         | 146/3305 [07:33<2:04:41,  2.37s/it][A
  4%|▍         | 147/3305 [07:35<2:02:21,  2.32s/it][A
  4%|▍         | 148/3305 [07:37<1:47:57,  2.05s/it][A
  5%|▍         | 149/3305 [07:39<1:47:34,  2.05s/it][A
  5%|▍         | 150/3305 [07:44<2:30:48,  2.87s/it][A
  5%|▍         | 151/3305 [07:49<3:11:20,  3.64s/it][A
  5%|▍         | 152/3305 [07:53<3:13:57,  3.69s/it][A
  5%|▍         | 153/3305 [07:57<3:13:36,  3.69s/it][A
  5%|▍         | 154/3305 [07:58<2:39:20,  3.03s/it][A
  5%|▍         | 155/3305 [08:00<2:24:35,  2.75s/it][A
  5%|▍         | 156/3305 [08:03<2:32:17,  2.90s/it][A
  5%|▍         | 157/3305 [08:09<3:16:12,  3.74s/it][A
  5%|▍         | 158/3305 [08:14<3:38:47,  4.17s/it][A
  5%|▍         | 159/3305 [08:17<3:08:03,  3.59s/it][A
  5%|▍         | 160/3305 [08:19<2:48:34,  3.22s/it][A
  5%|▍         | 161/3305 [08:21<2:33:48,  2.94s/it][A
  5%|▍         | 162/3305 [08:23<2:22:01,  2.71s/it][A
  5%|▍         | 163/3305 [08:26<2:20:43,  2.69s/it][A
  5%|▍         | 164/3305 [08:30<2:33:55,  2.94s/it][A
  5%|▍         | 165/3305 [08:34<2:58:39,  3.41s/it][A
  5%|▌         | 166/3305 [08:37<2:56:29,  3.37s/it][A
  5%|▌         | 167/3305 [08:40<2:50:34,  3.26s/it][A
  5%|▌         | 168/3305 [08:43<2:45:14,  3.16s/it][A
  5%|▌         | 169/3305 [08:46<2:38:15,  3.03s/it][A
  5%|▌         | 170/3305 [08:48<2:30:05,  2.87s/it][A
  5%|▌         | 171/3305 [08:52<2:33:49,  2.95s/it][A
  5%|▌         | 172/3305 [08:55<2:40:21,  3.07s/it][A
  5%|▌         | 173/3305 [08:57<2:31:14,  2.90s/it][A
  5%|▌         | 174/3305 [09:00<2:22:24,  2.73s/it][A
  5%|▌         | 175/3305 [09:02<2:21:04,  2.70s/it][A
  5%|▌         | 176/3305 [09:06<2:32:19,  2.92s/it][A
  5%|▌         | 177/3305 [09:10<2:49:11,  3.25s/it][A
  5%|▌         | 178/3305 [09:13<2:45:00,  3.17s/it][A
  5%|▌         | 179/3305 [09:15<2:30:05,  2.88s/it][A
  5%|▌         | 180/3305 [09:18<2:30:21,  2.89s/it][A
  5%|▌         | 181/3305 [09:22<2:48:42,  3.24s/it][A
  6%|▌         | 182/3305 [09:27<3:22:05,  3.88s/it][A
  6%|▌         | 183/3305 [09:33<3:51:47,  4.45s/it][A
  6%|▌         | 184/3305 [09:38<3:51:32,  4.45s/it][A
  6%|▌         | 185/3305 [09:43<4:03:34,  4.68s/it][A
  6%|▌         | 186/3305 [09:48<4:07:04,  4.75s/it][A
  6%|▌         | 187/3305 [09:50<3:33:04,  4.10s/it][A
  6%|▌         | 188/3305 [09:53<3:10:17,  3.66s/it][A
  6%|▌         | 189/3305 [09:55<2:50:22,  3.28s/it][A
  6%|▌         | 190/3305 [09:58<2:42:50,  3.14s/it][A
  6%|▌         | 191/3305 [10:01<2:38:44,  3.06s/it][A
  6%|▌         | 192/3305 [10:03<2:17:34,  2.65s/it][A
  6%|▌         | 193/3305 [10:05<2:06:02,  2.43s/it][A
  6%|▌         | 194/3305 [10:08<2:21:35,  2.73s/it][A
  6%|▌         | 195/3305 [10:11<2:20:06,  2.70s/it][A
  6%|▌         | 196/3305 [10:12<2:04:30,  2.40s/it][A
  6%|▌         | 197/3305 [10:15<2:01:45,  2.35s/it][A
  6%|▌         | 198/3305 [10:17<2:08:30,  2.48s/it][A
  6%|▌         | 199/3305 [10:20<2:15:48,  2.62s/it][A
  6%|▌         | 200/3305 [10:23<2:12:52,  2.57s/it][A
  6%|▌         | 201/3305 [10:26<2:15:28,  2.62s/it][A
  6%|▌         | 202/3305 [10:29<2:21:41,  2.74s/it][A
  6%|▌         | 203/3305 [10:31<2:20:23,  2.72s/it][A
  6%|▌         | 204/3305 [10:37<3:00:48,  3.50s/it][A
  6%|▌         | 205/3305 [10:42<3:36:26,  4.19s/it][A
  6%|▌         | 206/3305 [10:45<3:18:52,  3.85s/it][A
  6%|▋         | 207/3305 [10:48<3:03:48,  3.56s/it][A
  6%|▋         | 208/3305 [10:51<2:43:39,  3.17s/it][A
  6%|▋         | 209/3305 [10:53<2:33:19,  2.97s/it][A
  6%|▋         | 210/3305 [10:56<2:30:32,  2.92s/it][A
  6%|▋         | 211/3305 [10:58<2:23:59,  2.79s/it][A
  6%|▋         | 212/3305 [11:00<2:08:10,  2.49s/it][A
  6%|▋         | 213/3305 [11:03<2:07:53,  2.48s/it][A
  6%|▋         | 214/3305 [11:05<2:05:51,  2.44s/it][A
  7%|▋         | 215/3305 [11:07<1:57:08,  2.27s/it][A
  7%|▋         | 216/3305 [11:09<1:58:14,  2.30s/it][A
  7%|▋         | 217/3305 [11:13<2:18:04,  2.68s/it][A
  7%|▋         | 218/3305 [11:16<2:30:58,  2.93s/it][A
  7%|▋         | 219/3305 [11:20<2:47:07,  3.25s/it][A
  7%|▋         | 220/3305 [11:23<2:45:13,  3.21s/it][A
  7%|▋         | 221/3305 [11:25<2:26:39,  2.85s/it][A
  7%|▋         | 222/3305 [11:27<2:12:24,  2.58s/it][A
  7%|▋         | 223/3305 [11:29<1:56:04,  2.26s/it][A
  7%|▋         | 224/3305 [11:32<2:11:33,  2.56s/it][A
  7%|▋         | 225/3305 [11:36<2:27:25,  2.87s/it][A
  7%|▋         | 226/3305 [11:39<2:37:30,  3.07s/it][A
  7%|▋         | 227/3305 [11:43<2:53:10,  3.38s/it][A
  7%|▋         | 228/3305 [11:46<2:46:45,  3.25s/it][A
  7%|▋         | 229/3305 [11:49<2:39:12,  3.11s/it][A
  7%|▋         | 230/3305 [11:52<2:34:05,  3.01s/it][A
  7%|▋         | 231/3305 [11:54<2:20:22,  2.74s/it][A
  7%|▋         | 232/3305 [11:56<2:07:51,  2.50s/it][A
  7%|▋         | 233/3305 [11:58<2:02:22,  2.39s/it][A
  7%|▋         | 234/3305 [12:01<2:14:50,  2.63s/it][A
  7%|▋         | 235/3305 [12:05<2:27:27,  2.88s/it][A
  7%|▋         | 236/3305 [12:07<2:21:18,  2.76s/it][A
  7%|▋         | 237/3305 [12:09<2:09:21,  2.53s/it][A
  7%|▋         | 238/3305 [12:11<2:02:38,  2.40s/it][A
  7%|▋         | 239/3305 [12:14<1:59:54,  2.35s/it][A
  7%|▋         | 240/3305 [12:17<2:19:16,  2.73s/it][A
  7%|▋         | 241/3305 [12:22<2:50:40,  3.34s/it][A
  7%|▋         | 242/3305 [12:27<3:13:50,  3.80s/it][A
  7%|▋         | 243/3305 [12:32<3:27:41,  4.07s/it][A
  7%|▋         | 244/3305 [12:34<3:04:14,  3.61s/it][A
  7%|▋         | 245/3305 [12:36<2:44:35,  3.23s/it][A
  7%|▋         | 246/3305 [12:39<2:31:44,  2.98s/it][A
  7%|▋         | 247/3305 [12:41<2:26:08,  2.87s/it][A
  8%|▊         | 248/3305 [12:44<2:22:38,  2.80s/it][A
  8%|▊         | 249/3305 [12:47<2:23:50,  2.82s/it][A
  8%|▊         | 250/3305 [12:50<2:21:33,  2.78s/it][A
  8%|▊         | 251/3305 [12:52<2:22:35,  2.80s/it][A
  8%|▊         | 252/3305 [12:55<2:17:40,  2.71s/it][A
  8%|▊         | 253/3305 [12:57<2:08:54,  2.53s/it][A
  8%|▊         | 254/3305 [12:59<2:03:58,  2.44s/it][A
  8%|▊         | 255/3305 [13:02<2:09:17,  2.54s/it][A
  8%|▊         | 256/3305 [13:06<2:23:21,  2.82s/it][A
  8%|▊         | 257/3305 [13:09<2:27:02,  2.89s/it][A
  8%|▊         | 258/3305 [13:11<2:16:22,  2.69s/it][A
  8%|▊         | 259/3305 [13:13<2:14:38,  2.65s/it][A
  8%|▊         | 260/3305 [13:16<2:20:23,  2.77s/it][A
  8%|▊         | 261/3305 [13:20<2:36:37,  3.09s/it][A
  8%|▊         | 262/3305 [13:24<2:44:32,  3.24s/it][A
  8%|▊         | 263/3305 [13:26<2:22:10,  2.80s/it][A
  8%|▊         | 264/3305 [13:29<2:26:17,  2.89s/it][A
  8%|▊         | 265/3305 [13:34<3:10:15,  3.76s/it][A
  8%|▊         | 266/3305 [13:40<3:30:15,  4.15s/it][A
  8%|▊         | 267/3305 [13:45<3:45:47,  4.46s/it][A
  8%|▊         | 268/3305 [13:49<3:45:56,  4.46s/it][A
  8%|▊         | 269/3305 [13:51<3:05:00,  3.66s/it][A
  8%|▊         | 270/3305 [13:53<2:41:27,  3.19s/it][A
  8%|▊         | 271/3305 [13:55<2:29:03,  2.95s/it][A
  8%|▊         | 272/3305 [13:58<2:18:42,  2.74s/it][A
  8%|▊         | 273/3305 [14:01<2:20:26,  2.78s/it][A
  8%|▊         | 274/3305 [14:03<2:13:54,  2.65s/it][A
  8%|▊         | 275/3305 [14:05<2:00:19,  2.38s/it][A
  8%|▊         | 276/3305 [14:07<1:58:24,  2.35s/it][A
  8%|▊         | 277/3305 [14:10<2:03:42,  2.45s/it][A
  8%|▊         | 278/3305 [14:13<2:16:46,  2.71s/it][A
  8%|▊         | 279/3305 [14:16<2:19:57,  2.78s/it][A
  8%|▊         | 280/3305 [14:18<2:10:11,  2.58s/it][A
  9%|▊         | 281/3305 [14:21<2:12:32,  2.63s/it][A
  9%|▊         | 282/3305 [14:23<2:05:01,  2.48s/it][A
  9%|▊         | 283/3305 [14:25<2:04:01,  2.46s/it][A
  9%|▊         | 284/3305 [14:29<2:16:43,  2.72s/it][A
  9%|▊         | 285/3305 [14:34<2:51:43,  3.41s/it][A
  9%|▊         | 286/3305 [14:39<3:23:03,  4.04s/it][A
  9%|▊         | 287/3305 [14:42<3:05:26,  3.69s/it][A
  9%|▊         | 288/3305 [14:44<2:34:16,  3.07s/it][A
  9%|▊         | 289/3305 [14:46<2:18:06,  2.75s/it][A
  9%|▉         | 290/3305 [14:49<2:25:47,  2.90s/it][A
  9%|▉         | 291/3305 [14:51<2:19:11,  2.77s/it][A
  9%|▉         | 292/3305 [14:54<2:14:24,  2.68s/it][A
  9%|▉         | 293/3305 [14:56<2:01:53,  2.43s/it][A
  9%|▉         | 294/3305 [14:58<2:04:38,  2.48s/it][A
  9%|▉         | 295/3305 [15:02<2:19:31,  2.78s/it][A
  9%|▉         | 296/3305 [15:05<2:31:39,  3.02s/it][A
  9%|▉         | 297/3305 [15:09<2:46:52,  3.33s/it][A
  9%|▉         | 298/3305 [15:14<3:05:04,  3.69s/it][A
  9%|▉         | 299/3305 [15:19<3:31:41,  4.23s/it][A
  9%|▉         | 300/3305 [15:22<3:14:08,  3.88s/it][A
  9%|▉         | 301/3305 [15:25<2:53:38,  3.47s/it][A
  9%|▉         | 302/3305 [15:27<2:37:47,  3.15s/it][A
  9%|▉         | 303/3305 [15:30<2:27:11,  2.94s/it][A
  9%|▉         | 304/3305 [15:33<2:25:12,  2.90s/it][A
  9%|▉         | 305/3305 [15:36<2:33:16,  3.07s/it][A
  9%|▉         | 306/3305 [15:39<2:26:22,  2.93s/it][A
  9%|▉         | 307/3305 [15:41<2:12:32,  2.65s/it][A
  9%|▉         | 308/3305 [15:43<2:06:59,  2.54s/it][A
  9%|▉         | 309/3305 [15:45<2:03:58,  2.48s/it][A
  9%|▉         | 310/3305 [15:48<2:11:20,  2.63s/it][A
  9%|▉         | 311/3305 [15:52<2:23:50,  2.88s/it][A
  9%|▉         | 312/3305 [15:55<2:24:07,  2.89s/it][A
  9%|▉         | 313/3305 [15:58<2:22:44,  2.86s/it][A
 10%|▉         | 314/3305 [16:00<2:11:08,  2.63s/it][A
 10%|▉         | 315/3305 [16:02<2:09:02,  2.59s/it][A
 10%|▉         | 316/3305 [16:05<2:10:36,  2.62s/it][A
 10%|▉         | 317/3305 [16:07<2:05:03,  2.51s/it][A
 10%|▉         | 318/3305 [16:09<2:01:00,  2.43s/it][A
 10%|▉         | 319/3305 [16:12<1:59:11,  2.39s/it][A
 10%|▉         | 320/3305 [16:13<1:44:48,  2.11s/it][A
 10%|▉         | 321/3305 [16:15<1:39:19,  2.00s/it][A
 10%|▉         | 322/3305 [16:19<2:08:12,  2.58s/it][A
 10%|▉         | 323/3305 [16:24<2:46:47,  3.36s/it][A
 10%|▉         | 324/3305 [16:28<2:55:28,  3.53s/it][A
 10%|▉         | 325/3305 [16:32<3:03:30,  3.69s/it][A
 10%|▉         | 326/3305 [16:35<2:51:09,  3.45s/it][A
 10%|▉         | 327/3305 [16:38<2:42:23,  3.27s/it][A
 10%|▉         | 328/3305 [16:41<2:45:27,  3.33s/it][A
 10%|▉         | 329/3305 [16:45<2:49:15,  3.41s/it][A
 10%|▉         | 330/3305 [16:47<2:39:26,  3.22s/it][A
 10%|█         | 331/3305 [16:50<2:24:32,  2.92s/it][A
 10%|█         | 332/3305 [16:54<2:40:24,  3.24s/it][A
 10%|█         | 333/3305 [16:57<2:44:04,  3.31s/it][A
 10%|█         | 334/3305 [16:59<2:17:43,  2.78s/it][A
 10%|█         | 335/3305 [17:01<2:11:10,  2.65s/it][A
 10%|█         | 336/3305 [17:04<2:09:37,  2.62s/it][A
 10%|█         | 337/3305 [17:06<2:10:26,  2.64s/it][A
 10%|█         | 338/3305 [17:09<2:17:01,  2.77s/it][A
 10%|█         | 339/3305 [17:13<2:30:29,  3.04s/it][A
 10%|█         | 340/3305 [17:17<2:47:23,  3.39s/it][A
 10%|█         | 341/3305 [17:19<2:30:40,  3.05s/it][A
 10%|█         | 342/3305 [17:22<2:21:49,  2.87s/it][A
 10%|█         | 343/3305 [17:25<2:20:58,  2.86s/it][A
 10%|█         | 344/3305 [17:27<2:12:13,  2.68s/it][A
 10%|█         | 345/3305 [17:29<2:06:50,  2.57s/it][A
 10%|█         | 346/3305 [17:32<2:05:01,  2.54s/it][A
 10%|█         | 347/3305 [17:34<2:04:42,  2.53s/it][A
 11%|█         | 348/3305 [17:41<3:02:38,  3.71s/it][A
 11%|█         | 349/3305 [17:49<4:14:00,  5.16s/it][A
 11%|█         | 350/3305 [17:53<3:59:05,  4.85s/it][A
 11%|█         | 351/3305 [17:57<3:36:26,  4.40s/it][A
 11%|█         | 352/3305 [18:03<3:58:08,  4.84s/it][A
 11%|█         | 353/3305 [18:09<4:24:37,  5.38s/it][A
 11%|█         | 354/3305 [18:13<3:58:14,  4.84s/it][A
 11%|█         | 355/3305 [18:15<3:12:41,  3.92s/it][A
 11%|█         | 356/3305 [18:19<3:17:18,  4.01s/it][A
 11%|█         | 357/3305 [18:24<3:28:52,  4.25s/it][A
 11%|█         | 358/3305 [18:29<3:38:47,  4.45s/it][A
 11%|█         | 359/3305 [18:34<3:52:59,  4.75s/it][A
 11%|█         | 360/3305 [18:37<3:22:21,  4.12s/it][A
 11%|█         | 361/3305 [18:39<2:57:05,  3.61s/it][A
 11%|█         | 362/3305 [18:41<2:26:20,  2.98s/it][A
 11%|█         | 363/3305 [18:43<2:18:29,  2.82s/it][A
 11%|█         | 364/3305 [18:47<2:32:56,  3.12s/it][A
 11%|█         | 365/3305 [18:50<2:31:14,  3.09s/it][A
 11%|█         | 366/3305 [18:52<2:23:37,  2.93s/it][A
 11%|█         | 367/3305 [18:56<2:29:39,  3.06s/it][A
 11%|█         | 368/3305 [18:58<2:22:47,  2.92s/it][A
 11%|█         | 369/3305 [19:01<2:21:00,  2.88s/it][A
 11%|█         | 370/3305 [19:04<2:19:41,  2.86s/it][A
 11%|█         | 371/3305 [19:07<2:19:54,  2.86s/it][A
 11%|█▏        | 372/3305 [19:09<2:05:40,  2.57s/it][A
 11%|█▏        | 373/3305 [19:11<1:56:08,  2.38s/it][A
 11%|█▏        | 374/3305 [19:13<1:54:29,  2.34s/it][A
 11%|█▏        | 375/3305 [19:16<2:02:16,  2.50s/it][A
 11%|█▏        | 376/3305 [19:19<2:06:35,  2.59s/it][A
 11%|█▏        | 377/3305 [19:21<2:04:25,  2.55s/it][A
 11%|█▏        | 378/3305 [19:23<1:59:29,  2.45s/it][A
 11%|█▏        | 379/3305 [19:26<1:56:39,  2.39s/it][A
 11%|█▏        | 380/3305 [19:28<2:01:36,  2.49s/it][A
 12%|█▏        | 381/3305 [19:31<2:08:19,  2.63s/it][A
 12%|█▏        | 382/3305 [19:34<2:09:55,  2.67s/it][A
 12%|█▏        | 383/3305 [19:36<2:00:05,  2.47s/it][A
 12%|█▏        | 384/3305 [19:38<1:52:04,  2.30s/it][A
 12%|█▏        | 385/3305 [19:40<1:48:49,  2.24s/it][A
 12%|█▏        | 386/3305 [19:42<1:43:48,  2.13s/it][A
 12%|█▏        | 387/3305 [19:44<1:40:07,  2.06s/it][A
 12%|█▏        | 388/3305 [19:46<1:36:26,  1.98s/it][A
 12%|█▏        | 389/3305 [19:48<1:42:04,  2.10s/it][A
 12%|█▏        | 390/3305 [19:50<1:47:10,  2.21s/it][A
 12%|█▏        | 391/3305 [19:53<1:49:13,  2.25s/it][A
 12%|█▏        | 392/3305 [19:55<1:47:37,  2.22s/it][A
 12%|█▏        | 393/3305 [19:57<1:47:20,  2.21s/it][A
 12%|█▏        | 394/3305 [20:00<1:52:31,  2.32s/it][A
 12%|█▏        | 395/3305 [20:03<2:00:42,  2.49s/it][A
 12%|█▏        | 396/3305 [20:05<1:57:08,  2.42s/it][A
 12%|█▏        | 397/3305 [20:07<1:48:31,  2.24s/it][A
 12%|█▏        | 398/3305 [20:09<1:51:36,  2.30s/it][A
 12%|█▏        | 399/3305 [20:11<1:50:50,  2.29s/it][A
 12%|█▏        | 400/3305 [20:14<1:59:55,  2.48s/it][A
 12%|█▏        | 401/3305 [20:17<2:02:28,  2.53s/it][A
 12%|█▏        | 402/3305 [20:19<2:01:00,  2.50s/it][A
 12%|█▏        | 403/3305 [20:21<1:54:52,  2.38s/it][A
 12%|█▏        | 404/3305 [20:24<1:59:28,  2.47s/it][A
 12%|█▏        | 405/3305 [20:27<2:11:22,  2.72s/it][A
 12%|█▏        | 406/3305 [20:30<2:05:25,  2.60s/it][A
 12%|█▏        | 407/3305 [20:32<2:03:34,  2.56s/it][A
 12%|█▏        | 408/3305 [20:35<2:01:20,  2.51s/it][A
 12%|█▏        | 409/3305 [20:38<2:07:30,  2.64s/it][A
 12%|█▏        | 410/3305 [20:40<2:08:30,  2.66s/it][A
 12%|█▏        | 411/3305 [20:43<2:07:42,  2.65s/it][A
 12%|█▏        | 412/3305 [20:45<1:55:36,  2.40s/it][A
 12%|█▏        | 413/3305 [20:47<1:56:06,  2.41s/it][A
 13%|█▎        | 414/3305 [20:49<1:51:23,  2.31s/it][A
 13%|█▎        | 415/3305 [20:52<1:57:45,  2.44s/it][A
 13%|█▎        | 416/3305 [20:55<2:11:57,  2.74s/it][A
 13%|█▎        | 417/3305 [20:59<2:18:43,  2.88s/it][A
 13%|█▎        | 418/3305 [21:01<2:16:52,  2.84s/it][A
 13%|█▎        | 419/3305 [21:04<2:12:06,  2.75s/it][A
 13%|█▎        | 420/3305 [21:06<1:57:23,  2.44s/it][A
 13%|█▎        | 421/3305 [21:08<1:51:45,  2.33s/it][A
 13%|█▎        | 422/3305 [21:11<2:10:53,  2.72s/it][A
 13%|█▎        | 423/3305 [21:14<2:15:32,  2.82s/it][A
 13%|█▎        | 424/3305 [21:17<2:13:12,  2.77s/it][A
 13%|█▎        | 425/3305 [21:20<2:15:16,  2.82s/it][A
 13%|█▎        | 426/3305 [21:26<2:55:02,  3.65s/it][A
 13%|█▎        | 427/3305 [21:32<3:28:05,  4.34s/it][A
 13%|█▎        | 428/3305 [21:33<2:53:42,  3.62s/it][A
 13%|█▎        | 429/3305 [21:37<2:54:42,  3.64s/it][A
 13%|█▎        | 430/3305 [21:41<2:51:39,  3.58s/it][A
 13%|█▎        | 431/3305 [21:44<2:42:35,  3.39s/it][A
 13%|█▎        | 432/3305 [21:46<2:32:33,  3.19s/it][A
 13%|█▎        | 433/3305 [21:50<2:33:33,  3.21s/it][A
 13%|█▎        | 434/3305 [21:53<2:43:30,  3.42s/it][A
 13%|█▎        | 435/3305 [21:57<2:43:37,  3.42s/it][A
 13%|█▎        | 436/3305 [22:00<2:46:42,  3.49s/it][A
 13%|█▎        | 437/3305 [22:03<2:26:33,  3.07s/it][A
 13%|█▎        | 438/3305 [22:05<2:21:21,  2.96s/it][A
 13%|█▎        | 439/3305 [22:08<2:24:20,  3.02s/it][A
 13%|█▎        | 440/3305 [22:11<2:16:10,  2.85s/it][A
 13%|█▎        | 441/3305 [22:14<2:18:07,  2.89s/it][A
 13%|█▎        | 442/3305 [22:17<2:28:04,  3.10s/it][A
 13%|█▎        | 443/3305 [22:21<2:37:37,  3.30s/it][A
 13%|█▎        | 444/3305 [22:24<2:35:04,  3.25s/it][A
 13%|█▎        | 445/3305 [22:26<2:15:31,  2.84s/it][A
 13%|█▎        | 446/3305 [22:30<2:33:44,  3.23s/it][A
 14%|█▎        | 447/3305 [22:35<2:59:22,  3.77s/it][A
 14%|█▎        | 448/3305 [22:38<2:36:19,  3.28s/it][A
 14%|█▎        | 449/3305 [22:40<2:18:08,  2.90s/it][A
 14%|█▎        | 450/3305 [22:43<2:23:34,  3.02s/it][A
 14%|█▎        | 451/3305 [22:47<2:39:09,  3.35s/it][A
 14%|█▎        | 452/3305 [22:50<2:36:53,  3.30s/it][A
 14%|█▎        | 453/3305 [22:53<2:34:31,  3.25s/it][A
 14%|█▎        | 454/3305 [22:57<2:38:59,  3.35s/it][A
 14%|█▍        | 455/3305 [23:00<2:34:11,  3.25s/it][A
 14%|█▍        | 456/3305 [23:03<2:25:25,  3.06s/it][A
 14%|█▍        | 457/3305 [23:06<2:24:19,  3.04s/it][A
 14%|█▍        | 458/3305 [23:09<2:31:48,  3.20s/it][A
 14%|█▍        | 459/3305 [23:12<2:28:28,  3.13s/it][A
 14%|█▍        | 460/3305 [23:15<2:32:10,  3.21s/it][A
 14%|█▍        | 461/3305 [23:19<2:35:17,  3.28s/it][A
 14%|█▍        | 462/3305 [23:21<2:24:33,  3.05s/it][A
 14%|█▍        | 463/3305 [23:24<2:14:34,  2.84s/it][A
 14%|█▍        | 464/3305 [23:27<2:13:06,  2.81s/it][A
 14%|█▍        | 465/3305 [23:29<2:09:19,  2.73s/it][A
 14%|█▍        | 466/3305 [23:31<2:04:04,  2.62s/it][A
 14%|█▍        | 467/3305 [23:35<2:12:52,  2.81s/it][A
 14%|█▍        | 468/3305 [23:39<2:37:31,  3.33s/it][A
 14%|█▍        | 469/3305 [23:45<3:11:15,  4.05s/it][A
 14%|█▍        | 470/3305 [23:48<2:57:51,  3.76s/it][A
 14%|█▍        | 471/3305 [23:49<2:24:29,  3.06s/it][A
 14%|█▍        | 472/3305 [23:51<2:06:37,  2.68s/it][A
 14%|█▍        | 473/3305 [23:55<2:22:35,  3.02s/it][A
 14%|█▍        | 474/3305 [24:00<2:47:12,  3.54s/it][A
 14%|█▍        | 475/3305 [24:03<2:44:48,  3.49s/it][A
 14%|█▍        | 476/3305 [24:08<3:06:18,  3.95s/it][A
 14%|█▍        | 477/3305 [24:14<3:26:54,  4.39s/it][A
 14%|█▍        | 478/3305 [24:18<3:20:41,  4.26s/it][A
 14%|█▍        | 479/3305 [24:21<3:07:21,  3.98s/it][A
 15%|█▍        | 480/3305 [24:23<2:45:27,  3.51s/it][A
 15%|█▍        | 481/3305 [24:26<2:29:07,  3.17s/it][A
 15%|█▍        | 482/3305 [24:28<2:14:09,  2.85s/it][A
 15%|█▍        | 483/3305 [24:31<2:13:03,  2.83s/it][A
 15%|█▍        | 484/3305 [24:34<2:16:22,  2.90s/it][A
 15%|█▍        | 485/3305 [24:36<2:05:37,  2.67s/it][A
 15%|█▍        | 486/3305 [24:39<2:10:14,  2.77s/it][A
 15%|█▍        | 487/3305 [24:42<2:15:28,  2.88s/it][A
 15%|█▍        | 488/3305 [24:44<2:05:35,  2.67s/it][A
 15%|█▍        | 489/3305 [24:46<2:00:34,  2.57s/it][A
 15%|█▍        | 490/3305 [24:49<1:59:24,  2.54s/it][A
 15%|█▍        | 491/3305 [24:51<1:51:27,  2.38s/it][A
 15%|█▍        | 492/3305 [24:54<1:59:37,  2.55s/it][A
 15%|█▍        | 493/3305 [24:57<2:09:24,  2.76s/it][A
 15%|█▍        | 494/3305 [25:00<2:13:11,  2.84s/it][A
 15%|█▍        | 495/3305 [25:03<2:05:59,  2.69s/it][A
 15%|█▌        | 496/3305 [25:05<2:00:38,  2.58s/it][A
 15%|█▌        | 497/3305 [25:08<2:12:18,  2.83s/it][A
 15%|█▌        | 498/3305 [25:12<2:22:46,  3.05s/it][A
 15%|█▌        | 499/3305 [25:15<2:25:54,  3.12s/it][A
 15%|█▌        | 500/3305 [25:17<2:14:19,  2.87s/it][A
 15%|█▌        | 501/3305 [25:20<2:09:42,  2.78s/it][A
 15%|█▌        | 502/3305 [25:23<2:18:16,  2.96s/it][A
 15%|█▌        | 503/3305 [25:27<2:22:33,  3.05s/it][A
 15%|█▌        | 504/3305 [25:30<2:23:21,  3.07s/it][A
 15%|█▌        | 505/3305 [25:33<2:23:10,  3.07s/it][A
 15%|█▌        | 506/3305 [25:35<2:16:49,  2.93s/it][A
 15%|█▌        | 507/3305 [25:38<2:06:35,  2.71s/it][A
 15%|█▌        | 508/3305 [25:40<1:58:33,  2.54s/it][A
 15%|█▌        | 509/3305 [25:43<2:04:50,  2.68s/it][A
 15%|█▌        | 510/3305 [25:46<2:12:27,  2.84s/it][A
 15%|█▌        | 511/3305 [25:48<1:54:54,  2.47s/it][A
 15%|█▌        | 512/3305 [25:49<1:41:51,  2.19s/it][A
 16%|█▌        | 513/3305 [25:51<1:39:36,  2.14s/it][A
 16%|█▌        | 514/3305 [25:54<1:52:52,  2.43s/it][A
 16%|█▌        | 515/3305 [25:57<1:55:48,  2.49s/it][A
 16%|█▌        | 516/3305 [25:59<1:56:40,  2.51s/it][A
 16%|█▌        | 517/3305 [26:03<2:07:49,  2.75s/it][A
 16%|█▌        | 518/3305 [26:05<1:59:40,  2.58s/it][A
 16%|█▌        | 519/3305 [26:08<2:02:47,  2.64s/it][A
 16%|█▌        | 520/3305 [26:11<2:13:21,  2.87s/it][A
 16%|█▌        | 521/3305 [26:14<2:08:09,  2.76s/it][A
 16%|█▌        | 522/3305 [26:16<2:03:55,  2.67s/it][A
 16%|█▌        | 523/3305 [26:19<2:07:55,  2.76s/it][A
 16%|█▌        | 524/3305 [26:23<2:25:40,  3.14s/it][A
 16%|█▌        | 525/3305 [26:27<2:31:21,  3.27s/it][A
 16%|█▌        | 526/3305 [26:29<2:21:47,  3.06s/it][A
 16%|█▌        | 527/3305 [26:33<2:38:13,  3.42s/it][A
 16%|█▌        | 528/3305 [26:38<2:56:57,  3.82s/it][A
 16%|█▌        | 529/3305 [26:42<2:53:45,  3.76s/it][A
 16%|█▌        | 530/3305 [26:44<2:31:57,  3.29s/it][A
 16%|█▌        | 531/3305 [26:47<2:23:07,  3.10s/it][A
 16%|█▌        | 532/3305 [26:50<2:31:08,  3.27s/it][A
 16%|█▌        | 533/3305 [26:54<2:39:10,  3.45s/it][A
 16%|█▌        | 534/3305 [26:57<2:36:48,  3.40s/it][A
 16%|█▌        | 535/3305 [27:01<2:41:13,  3.49s/it][A
 16%|█▌        | 536/3305 [27:04<2:36:06,  3.38s/it][A
 16%|█▌        | 537/3305 [27:06<2:15:45,  2.94s/it][A
 16%|█▋        | 538/3305 [27:09<2:07:19,  2.76s/it][A
 16%|█▋        | 539/3305 [27:11<2:04:46,  2.71s/it][A
 16%|█▋        | 540/3305 [27:16<2:32:59,  3.32s/it][A
 16%|█▋        | 541/3305 [27:21<2:58:12,  3.87s/it][A
 16%|█▋        | 542/3305 [27:24<2:49:42,  3.69s/it][A
 16%|█▋        | 543/3305 [27:30<3:10:59,  4.15s/it][A
 16%|█▋        | 544/3305 [27:34<3:16:57,  4.28s/it][A
 16%|█▋        | 545/3305 [27:38<3:07:01,  4.07s/it][A
 17%|█▋        | 546/3305 [27:41<2:51:36,  3.73s/it][A
 17%|█▋        | 547/3305 [27:43<2:31:37,  3.30s/it][A
 17%|█▋        | 548/3305 [27:45<2:12:45,  2.89s/it][A
 17%|█▋        | 549/3305 [27:47<1:56:25,  2.53s/it][A
 17%|█▋        | 550/3305 [27:49<1:49:57,  2.39s/it][A
 17%|█▋        | 551/3305 [27:52<2:02:31,  2.67s/it][A
 17%|█▋        | 552/3305 [27:56<2:14:56,  2.94s/it][A
 17%|█▋        | 553/3305 [27:58<2:13:54,  2.92s/it][A
 17%|█▋        | 554/3305 [28:01<2:12:09,  2.88s/it][A
 17%|█▋        | 555/3305 [28:06<2:38:00,  3.45s/it][A
 17%|█▋        | 556/3305 [28:12<3:14:38,  4.25s/it][A
 17%|█▋        | 557/3305 [28:16<3:10:24,  4.16s/it][A
 17%|█▋        | 558/3305 [28:20<3:03:01,  4.00s/it][A
 17%|█▋        | 559/3305 [28:23<2:57:26,  3.88s/it][A
 17%|█▋        | 560/3305 [28:27<2:56:46,  3.86s/it][A
 17%|█▋        | 561/3305 [28:30<2:49:46,  3.71s/it][A
 17%|█▋        | 562/3305 [28:33<2:40:42,  3.52s/it][A
 17%|█▋        | 563/3305 [28:37<2:36:02,  3.41s/it][A
 17%|█▋        | 564/3305 [28:39<2:18:49,  3.04s/it][A
 17%|█▋        | 565/3305 [28:42<2:15:20,  2.96s/it][A
 17%|█▋        | 566/3305 [28:45<2:19:45,  3.06s/it][A
 17%|█▋        | 567/3305 [28:48<2:19:10,  3.05s/it][A
 17%|█▋        | 568/3305 [28:51<2:12:50,  2.91s/it][A
 17%|█▋        | 569/3305 [28:53<2:01:49,  2.67s/it][A
 17%|█▋        | 570/3305 [28:55<2:00:08,  2.64s/it][A
 17%|█▋        | 571/3305 [28:59<2:14:03,  2.94s/it][A
 17%|█▋        | 572/3305 [29:02<2:13:20,  2.93s/it][A
 17%|█▋        | 573/3305 [29:04<2:00:45,  2.65s/it][A
 17%|█▋        | 574/3305 [29:06<2:01:19,  2.67s/it][A
 17%|█▋        | 575/3305 [29:09<2:03:17,  2.71s/it][A
 17%|█▋        | 576/3305 [29:12<2:05:58,  2.77s/it][A
 17%|█▋        | 577/3305 [29:14<1:58:40,  2.61s/it][A
 17%|█▋        | 578/3305 [29:17<1:55:15,  2.54s/it][A
 18%|█▊        | 579/3305 [29:19<1:53:55,  2.51s/it][A
 18%|█▊        | 580/3305 [29:22<1:51:53,  2.46s/it][A
 18%|█▊        | 581/3305 [29:24<1:57:26,  2.59s/it][A
 18%|█▊        | 582/3305 [29:29<2:17:41,  3.03s/it][A
 18%|█▊        | 583/3305 [29:33<2:35:56,  3.44s/it][A
 18%|█▊        | 584/3305 [29:36<2:34:20,  3.40s/it][A
 18%|█▊        | 585/3305 [29:39<2:25:01,  3.20s/it][A
 18%|█▊        | 586/3305 [29:42<2:20:32,  3.10s/it][A
 18%|█▊        | 587/3305 [29:45<2:15:38,  2.99s/it][A
 18%|█▊        | 588/3305 [29:47<2:04:38,  2.75s/it][A
 18%|█▊        | 589/3305 [29:50<2:05:50,  2.78s/it][A
 18%|█▊        | 590/3305 [29:53<2:16:09,  3.01s/it][A
 18%|█▊        | 591/3305 [29:56<2:16:22,  3.01s/it][A
 18%|█▊        | 592/3305 [29:59<2:08:28,  2.84s/it][A
 18%|█▊        | 593/3305 [30:02<2:10:07,  2.88s/it][A
 18%|█▊        | 594/3305 [30:04<2:09:08,  2.86s/it][A
 18%|█▊        | 595/3305 [30:07<2:00:35,  2.67s/it][A
 18%|█▊        | 596/3305 [30:09<1:57:13,  2.60s/it][A
 18%|█▊        | 597/3305 [30:12<2:04:49,  2.77s/it][A
 18%|█▊        | 598/3305 [30:16<2:12:17,  2.93s/it][A
 18%|█▊        | 599/3305 [30:18<2:12:56,  2.95s/it][A
 18%|█▊        | 600/3305 [30:21<2:04:50,  2.77s/it][A
 18%|█▊        | 601/3305 [30:23<1:51:42,  2.48s/it][A
 18%|█▊        | 602/3305 [30:25<1:44:25,  2.32s/it][A
 18%|█▊        | 603/3305 [30:27<1:44:00,  2.31s/it][A
 18%|█▊        | 604/3305 [30:30<1:49:56,  2.44s/it][A
 18%|█▊        | 605/3305 [30:32<1:51:53,  2.49s/it][A
 18%|█▊        | 606/3305 [30:34<1:48:09,  2.40s/it][A
 18%|█▊        | 607/3305 [30:37<1:44:53,  2.33s/it][A
 18%|█▊        | 608/3305 [30:39<1:39:14,  2.21s/it][A
 18%|█▊        | 609/3305 [30:41<1:38:49,  2.20s/it][A
 18%|█▊        | 610/3305 [30:44<1:49:07,  2.43s/it][A
 18%|█▊        | 611/3305 [30:47<1:55:48,  2.58s/it][A
 19%|█▊        | 612/3305 [30:49<1:48:04,  2.41s/it][A
 19%|█▊        | 613/3305 [30:52<1:59:46,  2.67s/it][A
 19%|█▊        | 614/3305 [30:55<2:10:08,  2.90s/it][A
 19%|█▊        | 615/3305 [30:59<2:17:48,  3.07s/it][A
 19%|█▊        | 616/3305 [31:01<2:11:16,  2.93s/it][A
 19%|█▊        | 617/3305 [31:04<2:10:11,  2.91s/it][A
 19%|█▊        | 618/3305 [31:07<2:10:35,  2.92s/it][A
 19%|█▊        | 619/3305 [31:10<2:14:10,  3.00s/it][A
 19%|█▉        | 620/3305 [31:15<2:41:56,  3.62s/it][A
 19%|█▉        | 621/3305 [31:21<3:03:21,  4.10s/it][A
 19%|█▉        | 622/3305 [31:25<3:01:54,  4.07s/it][A
 19%|█▉        | 623/3305 [31:30<3:20:06,  4.48s/it][A
 19%|█▉        | 624/3305 [31:35<3:22:22,  4.53s/it][A
 19%|█▉        | 625/3305 [31:37<2:52:37,  3.86s/it][A
 19%|█▉        | 626/3305 [31:39<2:32:43,  3.42s/it][A
 19%|█▉        | 627/3305 [31:42<2:21:51,  3.18s/it][A
 19%|█▉        | 628/3305 [31:44<2:11:59,  2.96s/it][A
 19%|█▉        | 629/3305 [31:46<1:56:21,  2.61s/it][A
 19%|█▉        | 630/3305 [31:49<1:59:56,  2.69s/it][A
 19%|█▉        | 631/3305 [31:53<2:16:33,  3.06s/it][A
 19%|█▉        | 632/3305 [31:56<2:14:07,  3.01s/it][A
 19%|█▉        | 633/3305 [31:59<2:07:50,  2.87s/it][A
 19%|█▉        | 634/3305 [32:01<2:06:08,  2.83s/it][A
 19%|█▉        | 635/3305 [32:03<1:55:53,  2.60s/it][A
 19%|█▉        | 636/3305 [32:06<1:50:16,  2.48s/it][A
 19%|█▉        | 637/3305 [32:10<2:16:42,  3.07s/it][A
 19%|█▉        | 638/3305 [32:15<2:41:08,  3.63s/it][A
 19%|█▉        | 639/3305 [32:18<2:34:12,  3.47s/it][A
 19%|█▉        | 640/3305 [32:21<2:25:40,  3.28s/it][A
 19%|█▉        | 641/3305 [32:23<2:08:20,  2.89s/it][A
 19%|█▉        | 642/3305 [32:25<1:58:53,  2.68s/it][A
 19%|█▉        | 643/3305 [32:28<1:57:12,  2.64s/it][A
 19%|█▉        | 644/3305 [32:30<1:53:04,  2.55s/it][A
 20%|█▉        | 645/3305 [32:32<1:52:46,  2.54s/it][A
 20%|█▉        | 646/3305 [32:35<1:52:19,  2.53s/it][A
 20%|█▉        | 647/3305 [32:37<1:51:23,  2.51s/it][A
 20%|█▉        | 648/3305 [32:40<1:49:02,  2.46s/it][A
 20%|█▉        | 649/3305 [32:42<1:50:23,  2.49s/it][A
 20%|█▉        | 650/3305 [32:46<2:08:56,  2.91s/it][A
 20%|█▉        | 651/3305 [32:50<2:23:23,  3.24s/it][A
 20%|█▉        | 652/3305 [32:53<2:21:50,  3.21s/it][A
 20%|█▉        | 653/3305 [32:57<2:27:01,  3.33s/it][A
 20%|█▉        | 654/3305 [33:00<2:19:49,  3.16s/it][A
 20%|█▉        | 655/3305 [33:02<2:07:56,  2.90s/it][A
 20%|█▉        | 656/3305 [33:05<2:02:52,  2.78s/it][A
 20%|█▉        | 657/3305 [33:07<2:03:26,  2.80s/it][A
 20%|█▉        | 658/3305 [33:10<1:56:33,  2.64s/it][A
 20%|█▉        | 659/3305 [33:12<1:49:33,  2.48s/it][A
 20%|█▉        | 660/3305 [33:14<1:50:02,  2.50s/it][A
 20%|██        | 661/3305 [33:18<2:02:06,  2.77s/it][A
 20%|██        | 662/3305 [33:21<2:09:20,  2.94s/it][A
 20%|██        | 663/3305 [33:24<2:03:47,  2.81s/it][A
 20%|██        | 664/3305 [33:29<2:43:56,  3.72s/it][A
 20%|██        | 665/3305 [33:36<3:15:41,  4.45s/it][A
 20%|██        | 666/3305 [33:38<2:43:00,  3.71s/it][A
 20%|██        | 667/3305 [33:40<2:26:16,  3.33s/it][A
 20%|██        | 668/3305 [33:45<2:49:50,  3.86s/it][A
 20%|██        | 669/3305 [33:51<3:17:48,  4.50s/it][A
 20%|██        | 670/3305 [33:54<2:57:18,  4.04s/it][A
 20%|██        | 671/3305 [33:56<2:34:59,  3.53s/it][A
 20%|██        | 672/3305 [33:58<2:12:26,  3.02s/it][A
 20%|██        | 673/3305 [34:00<1:53:42,  2.59s/it][A
 20%|██        | 674/3305 [34:03<1:58:59,  2.71s/it][A
 20%|██        | 675/3305 [34:06<2:03:59,  2.83s/it][A
 20%|██        | 676/3305 [34:09<2:04:14,  2.84s/it][A
 20%|██        | 677/3305 [34:12<2:08:11,  2.93s/it][A
 21%|██        | 678/3305 [34:15<2:13:58,  3.06s/it][A
 21%|██        | 679/3305 [34:18<2:06:38,  2.89s/it][A
 21%|██        | 680/3305 [34:19<1:51:25,  2.55s/it][A
 21%|██        | 681/3305 [34:23<2:03:21,  2.82s/it][A
 21%|██        | 682/3305 [34:28<2:28:41,  3.40s/it][A
 21%|██        | 683/3305 [34:31<2:26:37,  3.36s/it][A
 21%|██        | 684/3305 [34:34<2:18:47,  3.18s/it][A
 21%|██        | 685/3305 [34:37<2:18:12,  3.17s/it][A
 21%|██        | 686/3305 [34:39<2:08:41,  2.95s/it][A
 21%|██        | 687/3305 [34:41<1:52:06,  2.57s/it][A
 21%|██        | 688/3305 [34:42<1:37:36,  2.24s/it][A
 21%|██        | 689/3305 [34:44<1:33:25,  2.14s/it][A
 21%|██        | 690/3305 [34:47<1:38:23,  2.26s/it][A
 21%|██        | 691/3305 [34:49<1:42:12,  2.35s/it][A
 21%|██        | 692/3305 [34:51<1:36:49,  2.22s/it][A
 21%|██        | 693/3305 [34:53<1:34:47,  2.18s/it][A
 21%|██        | 694/3305 [34:56<1:37:47,  2.25s/it][A
 21%|██        | 695/3305 [34:59<1:44:36,  2.40s/it][A
 21%|██        | 696/3305 [35:03<2:05:18,  2.88s/it][A
 21%|██        | 697/3305 [35:07<2:23:56,  3.31s/it][A
 21%|██        | 698/3305 [35:10<2:25:30,  3.35s/it][A
 21%|██        | 699/3305 [35:13<2:19:18,  3.21s/it][A
 21%|██        | 700/3305 [35:17<2:21:13,  3.25s/it][A
 21%|██        | 701/3305 [35:19<2:12:22,  3.05s/it][A
 21%|██        | 702/3305 [35:21<2:01:50,  2.81s/it][A
 21%|██▏       | 703/3305 [35:23<1:51:05,  2.56s/it][A
 21%|██▏       | 704/3305 [35:26<1:51:16,  2.57s/it][A
 21%|██▏       | 705/3305 [35:29<1:50:46,  2.56s/it][A
 21%|██▏       | 706/3305 [35:31<1:53:32,  2.62s/it][A
 21%|██▏       | 707/3305 [35:34<1:57:11,  2.71s/it][A
 21%|██▏       | 708/3305 [35:36<1:51:36,  2.58s/it][A
 21%|██▏       | 709/3305 [35:38<1:41:34,  2.35s/it][A
 21%|██▏       | 710/3305 [35:40<1:31:58,  2.13s/it][A
 22%|██▏       | 711/3305 [35:42<1:31:32,  2.12s/it][A
 22%|██▏       | 712/3305 [35:46<2:00:43,  2.79s/it][A
 22%|██▏       | 713/3305 [35:52<2:34:01,  3.57s/it][A
 22%|██▏       | 714/3305 [35:55<2:28:41,  3.44s/it][A
 22%|██▏       | 715/3305 [35:57<2:16:52,  3.17s/it][A
 22%|██▏       | 716/3305 [36:00<2:15:26,  3.14s/it][A
 22%|██▏       | 717/3305 [36:04<2:15:06,  3.13s/it][A
 22%|██▏       | 718/3305 [36:06<2:09:38,  3.01s/it][A
 22%|██▏       | 719/3305 [36:10<2:19:44,  3.24s/it][A
 22%|██▏       | 720/3305 [36:14<2:27:08,  3.42s/it][A
 22%|██▏       | 721/3305 [36:17<2:25:43,  3.38s/it][A
 22%|██▏       | 722/3305 [36:20<2:19:18,  3.24s/it][A
 22%|██▏       | 723/3305 [36:23<2:13:12,  3.10s/it][A
 22%|██▏       | 724/3305 [36:26<2:12:53,  3.09s/it][A
 22%|██▏       | 725/3305 [36:29<2:06:23,  2.94s/it][A
 22%|██▏       | 726/3305 [36:32<2:18:14,  3.22s/it][A
 22%|██▏       | 727/3305 [36:36<2:26:10,  3.40s/it][A
 22%|██▏       | 728/3305 [36:39<2:21:44,  3.30s/it][A
 22%|██▏       | 729/3305 [36:43<2:20:59,  3.28s/it][A
 22%|██▏       | 730/3305 [36:46<2:19:32,  3.25s/it][A
 22%|██▏       | 731/3305 [36:49<2:21:35,  3.30s/it][A
 22%|██▏       | 732/3305 [36:52<2:11:21,  3.06s/it][A
 22%|██▏       | 733/3305 [36:54<2:01:48,  2.84s/it][A
 22%|██▏       | 734/3305 [36:57<2:03:52,  2.89s/it][A
 22%|██▏       | 735/3305 [37:00<1:59:26,  2.79s/it][A
 22%|██▏       | 736/3305 [37:03<2:04:00,  2.90s/it][A
 22%|██▏       | 737/3305 [37:07<2:27:52,  3.46s/it][A
 22%|██▏       | 738/3305 [37:12<2:40:06,  3.74s/it][A
 22%|██▏       | 739/3305 [37:14<2:23:56,  3.37s/it][A
 22%|██▏       | 740/3305 [37:17<2:09:12,  3.02s/it][A
 22%|██▏       | 741/3305 [37:19<2:05:41,  2.94s/it][A
 22%|██▏       | 742/3305 [37:22<2:08:01,  3.00s/it][A
 22%|██▏       | 743/3305 [37:25<2:01:34,  2.85s/it][A
 23%|██▎       | 744/3305 [37:27<1:56:17,  2.72s/it][A
 23%|██▎       | 745/3305 [37:30<1:54:49,  2.69s/it][A
 23%|██▎       | 746/3305 [37:33<1:52:23,  2.64s/it][A
 23%|██▎       | 747/3305 [37:35<1:45:57,  2.49s/it][A
 23%|██▎       | 748/3305 [37:37<1:38:14,  2.31s/it][A
 23%|██▎       | 749/3305 [37:39<1:35:31,  2.24s/it][A
 23%|██▎       | 750/3305 [37:42<1:43:59,  2.44s/it][A
 23%|██▎       | 751/3305 [37:44<1:47:35,  2.53s/it][A
 23%|██▎       | 752/3305 [37:47<1:56:09,  2.73s/it][A
 23%|██▎       | 753/3305 [37:51<2:03:06,  2.89s/it][A
 23%|██▎       | 754/3305 [37:54<2:03:56,  2.91s/it][A
 23%|██▎       | 755/3305 [37:58<2:23:43,  3.38s/it][A
 23%|██▎       | 756/3305 [38:02<2:35:16,  3.65s/it][A
 23%|██▎       | 757/3305 [38:05<2:20:31,  3.31s/it][A
 23%|██▎       | 758/3305 [38:08<2:12:51,  3.13s/it][A
 23%|██▎       | 759/3305 [38:11<2:15:49,  3.20s/it][A
 23%|██▎       | 760/3305 [38:14<2:13:52,  3.16s/it][A
 23%|██▎       | 761/3305 [38:16<2:01:57,  2.88s/it][A
 23%|██▎       | 762/3305 [38:21<2:22:15,  3.36s/it][A
 23%|██▎       | 763/3305 [38:24<2:23:08,  3.38s/it][A
 23%|██▎       | 764/3305 [38:26<2:05:48,  2.97s/it][A
 23%|██▎       | 765/3305 [38:29<2:07:58,  3.02s/it][A
 23%|██▎       | 766/3305 [38:32<2:05:21,  2.96s/it][A
 23%|██▎       | 767/3305 [38:34<1:56:28,  2.75s/it][A
 23%|██▎       | 768/3305 [38:37<1:54:12,  2.70s/it][A
 23%|██▎       | 769/3305 [38:41<2:09:05,  3.05s/it][A
 23%|██▎       | 770/3305 [38:45<2:26:44,  3.47s/it][A
 23%|██▎       | 771/3305 [38:47<2:08:27,  3.04s/it][A
 23%|██▎       | 772/3305 [38:50<2:08:45,  3.05s/it][A
 23%|██▎       | 773/3305 [38:53<2:07:59,  3.03s/it][A
 23%|██▎       | 774/3305 [38:55<1:53:02,  2.68s/it][A
 23%|██▎       | 775/3305 [38:57<1:44:00,  2.47s/it][A
 23%|██▎       | 776/3305 [38:59<1:33:40,  2.22s/it][A
 24%|██▎       | 777/3305 [39:01<1:30:09,  2.14s/it][A
 24%|██▎       | 778/3305 [39:04<1:40:15,  2.38s/it][A
 24%|██▎       | 779/3305 [39:07<1:44:48,  2.49s/it][A
 24%|██▎       | 780/3305 [39:09<1:38:02,  2.33s/it][A
 24%|██▎       | 781/3305 [39:11<1:36:51,  2.30s/it][A
 24%|██▎       | 782/3305 [39:13<1:37:30,  2.32s/it][A
 24%|██▎       | 783/3305 [39:16<1:38:38,  2.35s/it][A
 24%|██▎       | 784/3305 [39:19<1:46:35,  2.54s/it][A
 24%|██▍       | 785/3305 [39:21<1:51:27,  2.65s/it][A
 24%|██▍       | 786/3305 [39:24<1:50:06,  2.62s/it][A
 24%|██▍       | 787/3305 [39:26<1:44:49,  2.50s/it][A
 24%|██▍       | 788/3305 [39:28<1:39:32,  2.37s/it][A
 24%|██▍       | 789/3305 [39:31<1:40:28,  2.40s/it][A
 24%|██▍       | 790/3305 [39:33<1:38:16,  2.34s/it][A
 24%|██▍       | 791/3305 [39:35<1:29:07,  2.13s/it][A
 24%|██▍       | 792/3305 [39:36<1:23:11,  1.99s/it][A
 24%|██▍       | 793/3305 [39:39<1:31:34,  2.19s/it][A
 24%|██▍       | 794/3305 [39:43<1:52:21,  2.68s/it][A
 24%|██▍       | 795/3305 [39:46<1:56:37,  2.79s/it][A
 24%|██▍       | 796/3305 [39:50<2:09:53,  3.11s/it][A
 24%|██▍       | 797/3305 [39:53<2:12:14,  3.16s/it][A
 24%|██▍       | 798/3305 [39:55<2:01:06,  2.90s/it][A
 24%|██▍       | 799/3305 [39:58<1:57:52,  2.82s/it][A
 24%|██▍       | 800/3305 [40:01<2:03:52,  2.97s/it][A
 24%|██▍       | 801/3305 [40:04<2:06:25,  3.03s/it][A
 24%|██▍       | 802/3305 [40:09<2:21:58,  3.40s/it][A
 24%|██▍       | 803/3305 [40:14<2:42:56,  3.91s/it][A
 24%|██▍       | 804/3305 [40:18<2:44:55,  3.96s/it][A
 24%|██▍       | 805/3305 [40:21<2:38:06,  3.79s/it][A
 24%|██▍       | 806/3305 [40:24<2:26:19,  3.51s/it][A
 24%|██▍       | 807/3305 [40:28<2:31:11,  3.63s/it][A
 24%|██▍       | 808/3305 [40:32<2:31:06,  3.63s/it][A
 24%|██▍       | 809/3305 [40:35<2:29:44,  3.60s/it][A
 25%|██▍       | 810/3305 [40:39<2:31:54,  3.65s/it][A
 25%|██▍       | 811/3305 [40:43<2:37:29,  3.79s/it][A
 25%|██▍       | 812/3305 [40:48<2:55:58,  4.24s/it][A
 25%|██▍       | 813/3305 [40:53<2:56:17,  4.24s/it][A
 25%|██▍       | 814/3305 [40:58<3:14:36,  4.69s/it][A
 25%|██▍       | 815/3305 [41:04<3:28:36,  5.03s/it][A
 25%|██▍       | 816/3305 [41:07<2:57:15,  4.27s/it][A
 25%|██▍       | 817/3305 [41:08<2:24:57,  3.50s/it][A
 25%|██▍       | 818/3305 [41:11<2:11:42,  3.18s/it][A
 25%|██▍       | 819/3305 [41:14<2:09:40,  3.13s/it][A
 25%|██▍       | 820/3305 [41:16<2:03:49,  2.99s/it][A
 25%|██▍       | 821/3305 [41:20<2:08:05,  3.09s/it][A
 25%|██▍       | 822/3305 [41:23<2:05:59,  3.04s/it][A
 25%|██▍       | 823/3305 [41:26<2:05:05,  3.02s/it][A
 25%|██▍       | 824/3305 [41:28<1:53:39,  2.75s/it][A
 25%|██▍       | 825/3305 [41:31<1:55:47,  2.80s/it][A
 25%|██▍       | 826/3305 [41:34<1:57:32,  2.84s/it][A
 25%|██▌       | 827/3305 [41:35<1:43:52,  2.52s/it][A
 25%|██▌       | 828/3305 [41:38<1:40:27,  2.43s/it][A
 25%|██▌       | 829/3305 [41:40<1:44:22,  2.53s/it][A
 25%|██▌       | 830/3305 [41:43<1:46:38,  2.59s/it][A
 25%|██▌       | 831/3305 [41:46<1:52:23,  2.73s/it][A
 25%|██▌       | 832/3305 [41:50<2:07:13,  3.09s/it][A
 25%|██▌       | 833/3305 [41:54<2:15:02,  3.28s/it][A
 25%|██▌       | 834/3305 [41:57<2:13:49,  3.25s/it][A
 25%|██▌       | 835/3305 [41:59<2:04:49,  3.03s/it][A
 25%|██▌       | 836/3305 [42:02<1:54:41,  2.79s/it][A
 25%|██▌       | 837/3305 [42:04<1:48:38,  2.64s/it][A
 25%|██▌       | 838/3305 [42:06<1:42:10,  2.48s/it][A
 25%|██▌       | 839/3305 [42:08<1:39:13,  2.41s/it][A
 25%|██▌       | 840/3305 [42:12<1:52:45,  2.74s/it][A
 25%|██▌       | 841/3305 [42:16<2:05:42,  3.06s/it][A
 25%|██▌       | 842/3305 [42:19<2:04:00,  3.02s/it][A
 26%|██▌       | 843/3305 [42:22<2:02:39,  2.99s/it][A
 26%|██▌       | 844/3305 [42:26<2:24:17,  3.52s/it][A
 26%|██▌       | 845/3305 [42:31<2:34:12,  3.76s/it][A
 26%|██▌       | 846/3305 [42:33<2:16:31,  3.33s/it][A
 26%|██▌       | 847/3305 [42:35<2:03:10,  3.01s/it][A
 26%|██▌       | 848/3305 [42:38<2:04:05,  3.03s/it][A
 26%|██▌       | 849/3305 [42:44<2:31:52,  3.71s/it][A
 26%|██▌       | 850/3305 [42:49<2:52:27,  4.21s/it][A
 26%|██▌       | 851/3305 [42:52<2:41:50,  3.96s/it][A
 26%|██▌       | 852/3305 [42:56<2:34:04,  3.77s/it][A
 26%|██▌       | 853/3305 [42:59<2:30:26,  3.68s/it][A
 26%|██▌       | 854/3305 [43:02<2:18:31,  3.39s/it][A
 26%|██▌       | 855/3305 [43:04<2:08:40,  3.15s/it][A
 26%|██▌       | 856/3305 [43:07<2:05:11,  3.07s/it][A
 26%|██▌       | 857/3305 [43:10<1:59:28,  2.93s/it][A
 26%|██▌       | 858/3305 [43:13<2:01:04,  2.97s/it][A
 26%|██▌       | 859/3305 [43:16<2:01:03,  2.97s/it][A
 26%|██▌       | 860/3305 [43:19<1:59:38,  2.94s/it][A
 26%|██▌       | 861/3305 [43:21<1:53:10,  2.78s/it][A
 26%|██▌       | 862/3305 [43:24<1:48:39,  2.67s/it][A
 26%|██▌       | 863/3305 [43:26<1:50:48,  2.72s/it][A
 26%|██▌       | 864/3305 [43:30<1:55:33,  2.84s/it][A
 26%|██▌       | 865/3305 [43:34<2:08:53,  3.17s/it][A
 26%|██▌       | 866/3305 [43:38<2:19:20,  3.43s/it][A
 26%|██▌       | 867/3305 [43:40<2:03:41,  3.04s/it][A
 26%|██▋       | 868/3305 [43:43<2:04:34,  3.07s/it][A
 26%|██▋       | 869/3305 [43:47<2:15:18,  3.33s/it][A
 26%|██▋       | 870/3305 [43:49<2:03:52,  3.05s/it][A
 26%|██▋       | 871/3305 [43:51<1:52:27,  2.77s/it][A
 26%|██▋       | 872/3305 [43:54<1:46:20,  2.62s/it][A
 26%|██▋       | 873/3305 [43:56<1:49:51,  2.71s/it][A
 26%|██▋       | 874/3305 [44:00<1:57:33,  2.90s/it][A
 26%|██▋       | 875/3305 [44:03<1:55:28,  2.85s/it][A
 27%|██▋       | 876/3305 [44:06<2:02:09,  3.02s/it][A
 27%|██▋       | 877/3305 [44:09<1:57:38,  2.91s/it][A
 27%|██▋       | 878/3305 [44:11<1:56:03,  2.87s/it][A
 27%|██▋       | 879/3305 [44:14<1:49:50,  2.72s/it][A
 27%|██▋       | 880/3305 [44:18<2:06:23,  3.13s/it][A
 27%|██▋       | 881/3305 [44:23<2:29:04,  3.69s/it][A
 27%|██▋       | 882/3305 [44:25<2:12:58,  3.29s/it][A
 27%|██▋       | 883/3305 [44:28<2:03:11,  3.05s/it][A
 27%|██▋       | 884/3305 [44:31<2:10:00,  3.22s/it][A
 27%|██▋       | 885/3305 [44:35<2:18:26,  3.43s/it][A
 27%|██▋       | 886/3305 [44:39<2:21:16,  3.50s/it][A
 27%|██▋       | 887/3305 [44:42<2:10:26,  3.24s/it][A
 27%|██▋       | 888/3305 [44:43<1:52:17,  2.79s/it][A
 27%|██▋       | 889/3305 [44:45<1:45:00,  2.61s/it][A
 27%|██▋       | 890/3305 [44:48<1:42:47,  2.55s/it][A
 27%|██▋       | 891/3305 [44:51<1:49:36,  2.72s/it][A
 27%|██▋       | 892/3305 [44:54<1:56:42,  2.90s/it][A
 27%|██▋       | 893/3305 [44:57<1:55:09,  2.86s/it][A
 27%|██▋       | 894/3305 [45:00<1:53:39,  2.83s/it][A
 27%|██▋       | 895/3305 [45:02<1:43:20,  2.57s/it][A
 27%|██▋       | 896/3305 [45:04<1:42:15,  2.55s/it][A
 27%|██▋       | 897/3305 [45:07<1:47:50,  2.69s/it][A
 27%|██▋       | 898/3305 [45:10<1:43:13,  2.57s/it][A
 27%|██▋       | 899/3305 [45:13<1:48:45,  2.71s/it][A
 27%|██▋       | 900/3305 [45:16<1:55:50,  2.89s/it][A
 27%|██▋       | 901/3305 [45:18<1:49:52,  2.74s/it][A
 27%|██▋       | 902/3305 [45:21<1:44:30,  2.61s/it][A
 27%|██▋       | 903/3305 [45:22<1:34:50,  2.37s/it][A
 27%|██▋       | 904/3305 [45:25<1:31:07,  2.28s/it][A
 27%|██▋       | 905/3305 [45:27<1:31:34,  2.29s/it][A
 27%|██▋       | 906/3305 [45:29<1:33:57,  2.35s/it][A
 27%|██▋       | 907/3305 [45:32<1:34:54,  2.37s/it][A
 27%|██▋       | 908/3305 [45:34<1:27:54,  2.20s/it][A
 28%|██▊       | 909/3305 [45:35<1:17:42,  1.95s/it][A
 28%|██▊       | 910/3305 [45:39<1:47:19,  2.69s/it][A
 28%|██▊       | 911/3305 [45:45<2:19:32,  3.50s/it][A
 28%|██▊       | 912/3305 [45:48<2:10:57,  3.28s/it][A
 28%|██▊       | 913/3305 [45:50<1:57:31,  2.95s/it][A
 28%|██▊       | 914/3305 [45:52<1:53:58,  2.86s/it][A
 28%|██▊       | 915/3305 [45:55<1:48:27,  2.72s/it][A
 28%|██▊       | 916/3305 [45:57<1:37:21,  2.45s/it][A
 28%|██▊       | 917/3305 [45:59<1:41:44,  2.56s/it][A
 28%|██▊       | 918/3305 [46:03<1:54:04,  2.87s/it][A
 28%|██▊       | 919/3305 [46:08<2:16:24,  3.43s/it][A
 28%|██▊       | 920/3305 [46:13<2:34:12,  3.88s/it][A
 28%|██▊       | 921/3305 [46:17<2:35:32,  3.91s/it][A
 28%|██▊       | 922/3305 [46:20<2:27:10,  3.71s/it][A
 28%|██▊       | 923/3305 [46:22<2:03:25,  3.11s/it][A
 28%|██▊       | 924/3305 [46:23<1:43:04,  2.60s/it][A
 28%|██▊       | 925/3305 [46:25<1:34:17,  2.38s/it][A
 28%|██▊       | 926/3305 [46:27<1:33:09,  2.35s/it][A
 28%|██▊       | 927/3305 [46:30<1:36:23,  2.43s/it][A
 28%|██▊       | 928/3305 [46:33<1:45:47,  2.67s/it][A
 28%|██▊       | 929/3305 [46:35<1:43:41,  2.62s/it][A
 28%|██▊       | 930/3305 [46:38<1:41:56,  2.58s/it][A
 28%|██▊       | 931/3305 [46:41<1:51:47,  2.83s/it][A
 28%|██▊       | 932/3305 [46:44<1:51:35,  2.82s/it][A
 28%|██▊       | 933/3305 [46:47<1:51:52,  2.83s/it][A
 28%|██▊       | 934/3305 [46:52<2:18:42,  3.51s/it][A
 28%|██▊       | 935/3305 [46:58<2:42:07,  4.10s/it][A
 28%|██▊       | 936/3305 [47:01<2:33:19,  3.88s/it][A
 28%|██▊       | 937/3305 [47:06<2:45:02,  4.18s/it][A
 28%|██▊       | 938/3305 [47:12<3:06:12,  4.72s/it][A
 28%|██▊       | 939/3305 [47:15<2:53:13,  4.39s/it][A
 28%|██▊       | 940/3305 [47:19<2:40:47,  4.08s/it][A
 28%|██▊       | 941/3305 [47:21<2:20:04,  3.56s/it][A
 29%|██▊       | 942/3305 [47:24<2:08:39,  3.27s/it][A
 29%|██▊       | 943/3305 [47:27<2:10:48,  3.32s/it][A
 29%|██▊       | 944/3305 [47:31<2:12:46,  3.37s/it][A
 29%|██▊       | 945/3305 [47:34<2:13:11,  3.39s/it][A
 29%|██▊       | 946/3305 [47:37<2:10:54,  3.33s/it][A
 29%|██▊       | 947/3305 [47:41<2:19:10,  3.54s/it][A
 29%|██▊       | 948/3305 [47:44<2:12:54,  3.38s/it][A
 29%|██▊       | 949/3305 [47:47<2:02:25,  3.12s/it][A
 29%|██▊       | 950/3305 [47:50<2:05:04,  3.19s/it][A
 29%|██▉       | 951/3305 [47:56<2:36:19,  3.98s/it][A
 29%|██▉       | 952/3305 [48:02<3:01:43,  4.63s/it][A
 29%|██▉       | 953/3305 [48:05<2:42:19,  4.14s/it][A
 29%|██▉       | 954/3305 [48:08<2:21:59,  3.62s/it][A
 29%|██▉       | 955/3305 [48:10<2:06:16,  3.22s/it][A
 29%|██▉       | 956/3305 [48:12<1:57:40,  3.01s/it][A
 29%|██▉       | 957/3305 [48:18<2:23:49,  3.68s/it][A
 29%|██▉       | 958/3305 [48:23<2:42:42,  4.16s/it][A
 29%|██▉       | 959/3305 [48:25<2:14:21,  3.44s/it][A
 29%|██▉       | 960/3305 [48:27<2:05:19,  3.21s/it][A
 29%|██▉       | 961/3305 [48:30<2:00:12,  3.08s/it][A
 29%|██▉       | 962/3305 [48:34<2:08:58,  3.30s/it][A
 29%|██▉       | 963/3305 [48:39<2:30:25,  3.85s/it][A
 29%|██▉       | 964/3305 [48:43<2:33:16,  3.93s/it][A
 29%|██▉       | 965/3305 [48:46<2:25:28,  3.73s/it][A
 29%|██▉       | 966/3305 [48:50<2:27:51,  3.79s/it][A
 29%|██▉       | 967/3305 [48:54<2:29:27,  3.84s/it][A
 29%|██▉       | 968/3305 [48:58<2:22:33,  3.66s/it][A
 29%|██▉       | 969/3305 [49:02<2:27:03,  3.78s/it][A
 29%|██▉       | 970/3305 [49:04<2:12:14,  3.40s/it][A
 29%|██▉       | 971/3305 [49:05<1:47:22,  2.76s/it][A
 29%|██▉       | 972/3305 [49:07<1:37:04,  2.50s/it][A
 29%|██▉       | 973/3305 [49:09<1:29:55,  2.31s/it][A
 29%|██▉       | 974/3305 [49:11<1:25:49,  2.21s/it][A
 30%|██▉       | 975/3305 [49:16<1:52:03,  2.89s/it][A
 30%|██▉       | 976/3305 [49:21<2:21:03,  3.63s/it][A
 30%|██▉       | 977/3305 [49:24<2:09:51,  3.35s/it][A
 30%|██▉       | 978/3305 [49:27<2:07:42,  3.29s/it][A
 30%|██▉       | 979/3305 [49:30<2:01:55,  3.15s/it][A
 30%|██▉       | 980/3305 [49:33<1:59:41,  3.09s/it][A
 30%|██▉       | 981/3305 [49:35<1:51:28,  2.88s/it][A
 30%|██▉       | 982/3305 [49:38<1:58:19,  3.06s/it][A
 30%|██▉       | 983/3305 [49:42<2:04:29,  3.22s/it][A
 30%|██▉       | 984/3305 [49:44<1:53:16,  2.93s/it][A
 30%|██▉       | 985/3305 [49:46<1:44:36,  2.71s/it][A
 30%|██▉       | 986/3305 [49:48<1:35:18,  2.47s/it][A
 30%|██▉       | 987/3305 [49:51<1:34:19,  2.44s/it][A
 30%|██▉       | 988/3305 [49:53<1:34:37,  2.45s/it][A
 30%|██▉       | 989/3305 [49:55<1:31:36,  2.37s/it][A
 30%|██▉       | 990/3305 [49:58<1:32:26,  2.40s/it][A
 30%|██▉       | 991/3305 [50:02<1:57:14,  3.04s/it][A
 30%|███       | 992/3305 [50:07<2:15:36,  3.52s/it][A
 30%|███       | 993/3305 [50:09<2:03:09,  3.20s/it][A
 30%|███       | 994/3305 [50:12<1:57:37,  3.05s/it][A
 30%|███       | 995/3305 [50:17<2:12:51,  3.45s/it][A
 30%|███       | 996/3305 [50:21<2:25:57,  3.79s/it][A
 30%|███       | 997/3305 [50:23<2:08:55,  3.35s/it][A
 30%|███       | 998/3305 [50:26<2:00:04,  3.12s/it][A
 30%|███       | 999/3305 [50:28<1:49:35,  2.85s/it][A
 30%|███       | 1000/3305 [50:30<1:38:28,  2.56s/it][A
 30%|███       | 1001/3305 [50:32<1:30:43,  2.36s/it][A
 30%|███       | 1002/3305 [50:34<1:29:29,  2.33s/it][A
 30%|███       | 1003/3305 [50:36<1:26:27,  2.25s/it][A
 30%|███       | 1004/3305 [50:41<1:48:34,  2.83s/it][A
 30%|███       | 1005/3305 [50:45<2:06:47,  3.31s/it][A
 30%|███       | 1006/3305 [50:47<1:54:51,  3.00s/it][A
 30%|███       | 1007/3305 [50:50<1:54:41,  2.99s/it][A
 30%|███       | 1008/3305 [50:53<1:49:11,  2.85s/it][A
 31%|███       | 1009/3305 [50:55<1:36:49,  2.53s/it][A
 31%|███       | 1010/3305 [50:57<1:37:47,  2.56s/it][A
 31%|███       | 1011/3305 [51:00<1:36:38,  2.53s/it][A
 31%|███       | 1012/3305 [51:01<1:26:41,  2.27s/it][A
 31%|███       | 1013/3305 [51:03<1:20:45,  2.11s/it][A
 31%|███       | 1014/3305 [51:05<1:20:39,  2.11s/it][A
 31%|███       | 1015/3305 [51:07<1:23:06,  2.18s/it][A
 31%|███       | 1016/3305 [51:10<1:26:49,  2.28s/it][A
 31%|███       | 1017/3305 [51:13<1:32:05,  2.41s/it][A
 31%|███       | 1018/3305 [51:16<1:38:09,  2.58s/it][A
 31%|███       | 1019/3305 [51:22<2:18:50,  3.64s/it][A
 31%|███       | 1020/3305 [51:28<2:50:53,  4.49s/it][A
 31%|███       | 1021/3305 [51:31<2:30:31,  3.95s/it][A
 31%|███       | 1022/3305 [51:36<2:38:57,  4.18s/it][A
 31%|███       | 1023/3305 [51:40<2:42:30,  4.27s/it][A
 31%|███       | 1024/3305 [51:42<2:18:43,  3.65s/it][A
 31%|███       | 1025/3305 [51:44<1:57:58,  3.10s/it][A
 31%|███       | 1026/3305 [51:48<2:05:03,  3.29s/it][A
 31%|███       | 1027/3305 [51:54<2:32:31,  4.02s/it][A
 31%|███       | 1028/3305 [51:58<2:33:58,  4.06s/it][A
 31%|███       | 1029/3305 [52:01<2:22:15,  3.75s/it][A
 31%|███       | 1030/3305 [52:04<2:12:54,  3.51s/it][A
 31%|███       | 1031/3305 [52:06<2:01:24,  3.20s/it][A
 31%|███       | 1032/3305 [52:09<1:53:27,  2.99s/it][A
 31%|███▏      | 1033/3305 [52:11<1:48:47,  2.87s/it][A
 31%|███▏      | 1034/3305 [52:14<1:47:42,  2.85s/it][A
 31%|███▏      | 1035/3305 [52:19<2:06:43,  3.35s/it][A
 31%|███▏      | 1036/3305 [52:25<2:34:52,  4.10s/it][A
 31%|███▏      | 1037/3305 [52:28<2:31:17,  4.00s/it][A
 31%|███▏      | 1038/3305 [52:31<2:22:11,  3.76s/it][A
 31%|███▏      | 1039/3305 [52:36<2:36:11,  4.14s/it][A
 31%|███▏      | 1040/3305 [52:41<2:44:55,  4.37s/it][A
 31%|███▏      | 1041/3305 [52:45<2:35:14,  4.11s/it][A
 32%|███▏      | 1042/3305 [52:48<2:18:55,  3.68s/it][A
 32%|███▏      | 1043/3305 [52:51<2:16:21,  3.62s/it][A
 32%|███▏      | 1044/3305 [52:56<2:29:33,  3.97s/it][A
 32%|███▏      | 1045/3305 [52:59<2:22:52,  3.79s/it][A
 32%|███▏      | 1046/3305 [53:03<2:17:56,  3.66s/it][A
 32%|███▏      | 1047/3305 [53:05<2:04:11,  3.30s/it][A
 32%|███▏      | 1048/3305 [53:07<1:49:37,  2.91s/it][A
 32%|███▏      | 1049/3305 [53:12<2:16:21,  3.63s/it][A
 32%|███▏      | 1050/3305 [53:19<2:52:09,  4.58s/it][A
 32%|███▏      | 1051/3305 [53:23<2:39:04,  4.23s/it][A
 32%|███▏      | 1052/3305 [53:25<2:22:20,  3.79s/it][A
 32%|███▏      | 1053/3305 [53:28<2:04:29,  3.32s/it][A
 32%|███▏      | 1054/3305 [53:30<1:55:26,  3.08s/it][A
 32%|███▏      | 1055/3305 [53:33<1:51:45,  2.98s/it][A
 32%|███▏      | 1056/3305 [53:36<1:50:28,  2.95s/it][A
 32%|███▏      | 1057/3305 [53:38<1:43:28,  2.76s/it][A
 32%|███▏      | 1058/3305 [53:40<1:36:32,  2.58s/it][A
 32%|███▏      | 1059/3305 [53:43<1:35:03,  2.54s/it][A
 32%|███▏      | 1060/3305 [53:45<1:30:34,  2.42s/it][A
 32%|███▏      | 1061/3305 [53:47<1:32:30,  2.47s/it][A
 32%|███▏      | 1062/3305 [53:50<1:37:55,  2.62s/it][A
 32%|███▏      | 1063/3305 [53:52<1:29:10,  2.39s/it][A
 32%|███▏      | 1064/3305 [53:54<1:21:51,  2.19s/it][A
 32%|███▏      | 1065/3305 [53:56<1:23:29,  2.24s/it][A
 32%|███▏      | 1066/3305 [53:59<1:24:17,  2.26s/it][Aslurmstepd: error: *** JOB 1543808 ON kn172 CANCELLED AT 2025-12-05T10:08:51 DUE TO TIME LIMIT ***
