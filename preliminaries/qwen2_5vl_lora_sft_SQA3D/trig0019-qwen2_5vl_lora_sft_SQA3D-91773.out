
==========
== CUDA ==
==========

CUDA Version 12.4.1

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

[INFO|2025-11-16 03:52:07] llamafactory.launcher:143 >> Initializing 4 distributed tasks at: 127.0.0.1:56067
W1116 03:52:08.484000 2142862 site-packages/torch/distributed/run.py:792] 
W1116 03:52:08.484000 2142862 site-packages/torch/distributed/run.py:792] *****************************************
W1116 03:52:08.484000 2142862 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1116 03:52:08.484000 2142862 site-packages/torch/distributed/run.py:792] *****************************************
[2025-11-16 03:52:16,231] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-11-16 03:52:16,244] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-11-16 03:52:16,436] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-11-16 03:52:16,660] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2025-11-16 03:52:19,367] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-11-16 03:52:19,367] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-11-16 03:52:19,367] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-11-16 03:52:19,367] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-11-16 03:52:19,371] [INFO] [comm.py:669:init_distributed] cdb=None
[INFO|2025-11-16 03:52:19] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.
[INFO|2025-11-16 03:52:19] llamafactory.hparams.parser:423 >> Process rank: 0, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|hub.py:421] 2025-11-16 03:52:19,698 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2025-11-16 03:52:19,710 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2025-11-16 03:52:19,717 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-11-16 03:52:19,717 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-11-16 03:52:19,717 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-11-16 03:52:19,717 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-11-16 03:52:19,717 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-11-16 03:52:19,717 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-11-16 03:52:19,717 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-11-16 03:52:19,951 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|hub.py:421] 2025-11-16 03:52:19,951 >> Offline mode: forcing local_files_only=True
[INFO|hub.py:421] 2025-11-16 03:52:19,953 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:316] 2025-11-16 03:52:19,954 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2025-11-16 03:52:19,957 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:316] 2025-11-16 03:52:19,959 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2025-11-16 03:52:19,961 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:428] 2025-11-16 03:52:19,969 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:1949] 2025-11-16 03:52:19,969 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2025-11-16 03:52:19,974 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-11-16 03:52:19,974 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-11-16 03:52:19,974 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-11-16 03:52:19,974 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-11-16 03:52:19,974 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-11-16 03:52:19,974 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-11-16 03:52:19,974 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-11-16 03:52:20,142 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:660] 2025-11-16 03:52:20,143 >> Offline mode: forcing local_files_only=True
[INFO|video_processing_utils.py:726] 2025-11-16 03:52:20,146 >> loading configuration file video_preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|video_processing_utils.py:770] 2025-11-16 03:52:20,149 >> Video processor Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:928] 2025-11-16 03:52:20,149 >> Offline mode: forcing local_files_only=True
[INFO|processing_utils.py:1116] 2025-11-16 03:52:20,154 >> loading configuration file processor_config.json from cache at None
[INFO|processing_utils.py:1199] 2025-11-16 03:52:20,457 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2_5_VLProcessor"
}

[INFO|2025-11-16 03:52:20] llamafactory.data.loader:143 >> Loading dataset /scratch/indrisch/huggingface/hub/datasets--cvis-tmu--llamafactory-sqa3d-traces-multiimage-vqa/snapshots/942a5514a2ed64b8ad7ec624ecde74f08884f1c8/data/...
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
Converting format of dataset (num_proc=96): 100%|██████████| 33047/33047 [00:00<?, ? examples/s][INFO|2025-11-16 03:52:20] llamafactory.hparams.parser:423 >> Process rank: 3, world size: 4, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-11-16 03:52:20] llamafactory.hparams.parser:423 >> Process rank: 2, world size: 4, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-11-16 03:52:20] llamafactory.hparams.parser:423 >> Process rank: 1, world size: 4, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
Converting format of dataset (num_proc=96): 33048 examples [00:01,  1.23s/ examples]            Converting format of dataset (num_proc=96): 33051 examples [00:01,  3.77 examples/s][rank3]:[W1116 03:52:22.663498582 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W1116 03:52:22.665236761 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W1116 03:52:22.666423241 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Converting format of dataset (num_proc=96): 33069 examples [00:01, 25.97 examples/s]Converting format of dataset (num_proc=96): 33087 examples [00:01, 49.46 examples/s]Converting format of dataset (num_proc=96): 33104 examples [00:01, 70.07 examples/s]Converting format of dataset (num_proc=96): 33126 examples [00:01, 95.84 examples/s]Converting format of dataset (num_proc=96): 33152 examples [00:01, 127.23 examples/s]Converting format of dataset (num_proc=96): 33179 examples [00:02, 154.70 examples/s]Converting format of dataset (num_proc=96): 33199 examples [00:02, 157.11 examples/s]Converting format of dataset (num_proc=96): 33218 examples [00:02, 161.01 examples/s]Converting format of dataset (num_proc=96): 33246 examples [00:02, 183.50 examples/s]Converting format of dataset (num_proc=96): 33268 examples [00:02, 190.01 examples/s]Converting format of dataset (num_proc=96): 33292 examples [00:02, 199.43 examples/s]Converting format of dataset (num_proc=96): 33317 examples [00:02, 212.37 examples/s]Converting format of dataset (num_proc=96): 33352 examples [00:02, 246.06 examples/s]Converting format of dataset (num_proc=96): 33382 examples [00:02, 259.59 examples/s]Converting format of dataset (num_proc=96): 33413 examples [00:02, 270.48 examples/s]Converting format of dataset (num_proc=96): 33459 examples [00:03, 321.37 examples/s]Converting format of dataset (num_proc=96): 33492 examples [00:03, 323.43 examples/s]Converting format of dataset (num_proc=96): 33535 examples [00:03, 353.36 examples/s]Converting format of dataset (num_proc=96): 33571 examples [00:03, 318.46 examples/s]Converting format of dataset (num_proc=96): 33614 examples [00:03, 348.84 examples/s]Converting format of dataset (num_proc=96): 33681 examples [00:03, 432.50 examples/s]Converting format of dataset (num_proc=96): 33743 examples [00:03, 484.57 examples/s]Converting format of dataset (num_proc=96): 33795 examples [00:03, 424.58 examples/s]Converting format of dataset (num_proc=96): 33845 examples [00:04, 415.08 examples/s]Converting format of dataset (num_proc=96): 33916 examples [00:04, 480.54 examples/s]Converting format of dataset (num_proc=96): 33973 examples [00:04, 500.40 examples/s]Converting format of dataset (num_proc=96): 34039 examples [00:04, 534.56 examples/s]Converting format of dataset (num_proc=96): 34100 examples [00:04, 550.76 examples/s]Converting format of dataset (num_proc=96): 34170 examples [00:04, 572.61 examples/s]Converting format of dataset (num_proc=96): 34275 examples [00:04, 695.40 examples/s]Converting format of dataset (num_proc=96): 34395 examples [00:04, 833.92 examples/s]Converting format of dataset (num_proc=96): 34554 examples [00:04, 1013.05 examples/s]Converting format of dataset (num_proc=96): 34659 examples [00:05, 935.38 examples/s] Converting format of dataset (num_proc=96): 34757 examples [00:05, 936.54 examples/s]Converting format of dataset (num_proc=96): 34852 examples [00:05, 815.99 examples/s]Converting format of dataset (num_proc=96): 34938 examples [00:05, 812.43 examples/s]Converting format of dataset (num_proc=96): 35043 examples [00:05, 868.90 examples/s]Converting format of dataset (num_proc=96): 35133 examples [00:05, 859.67 examples/s]Converting format of dataset (num_proc=96): 35226 examples [00:05, 759.92 examples/s]Converting format of dataset (num_proc=96): 35310 examples [00:05, 667.28 examples/s]Converting format of dataset (num_proc=96): 35389 examples [00:06, 692.20 examples/s]Converting format of dataset (num_proc=96): 35464 examples [00:06, 581.46 examples/s]Converting format of dataset (num_proc=96): 35527 examples [00:06, 513.78 examples/s]Converting format of dataset (num_proc=96): 35643 examples [00:06, 625.96 examples/s]Converting format of dataset (num_proc=96): 35711 examples [00:06, 605.52 examples/s]Converting format of dataset (num_proc=96): 35776 examples [00:06, 586.13 examples/s]Converting format of dataset (num_proc=96): 35838 examples [00:06, 500.55 examples/s]Converting format of dataset (num_proc=96): 35893 examples [00:07, 459.79 examples/s]Converting format of dataset (num_proc=96): 35944 examples [00:07, 464.53 examples/s]Converting format of dataset (num_proc=96): 35993 examples [00:07, 440.66 examples/s]Converting format of dataset (num_proc=96): 36039 examples [00:07, 442.08 examples/s]Converting format of dataset (num_proc=96): 36085 examples [00:07, 435.30 examples/s]Converting format of dataset (num_proc=96): 36140 examples [00:07, 459.64 examples/s]Converting format of dataset (num_proc=96): 36190 examples [00:07, 409.18 examples/s]Converting format of dataset (num_proc=96): 36233 examples [00:07, 326.12 examples/s]Converting format of dataset (num_proc=96): 36300 examples [00:08, 390.92 examples/s]Converting format of dataset (num_proc=96): 36343 examples [00:08, 394.44 examples/s]Converting format of dataset (num_proc=96): 36386 examples [00:08, 399.33 examples/s]Converting format of dataset (num_proc=96): 36433 examples [00:08, 412.46 examples/s]Converting format of dataset (num_proc=96): 36484 examples [00:08, 429.58 examples/s]Converting format of dataset (num_proc=96): 36530 examples [00:08, 388.73 examples/s]Converting format of dataset (num_proc=96): 36602 examples [00:08, 471.05 examples/s]Converting format of dataset (num_proc=96): 36675 examples [00:08, 531.28 examples/s]Converting format of dataset (num_proc=96): 36731 examples [00:08, 515.62 examples/s]Converting format of dataset (num_proc=96): 36803 examples [00:09, 570.92 examples/s]Converting format of dataset (num_proc=96): 36889 examples [00:09, 647.06 examples/s]Converting format of dataset (num_proc=96): 36966 examples [00:09, 670.46 examples/s]Converting format of dataset (num_proc=96): 37091 examples [00:09, 820.18 examples/s]Converting format of dataset (num_proc=96): 37174 examples [00:09, 777.80 examples/s]Converting format of dataset (num_proc=96): 37276 examples [00:09, 841.08 examples/s]Converting format of dataset (num_proc=96): 37440 examples [00:09, 1066.68 examples/s]Converting format of dataset (num_proc=96): 37551 examples [00:09, 967.51 examples/s] Converting format of dataset (num_proc=96): 37721 examples [00:09, 1155.89 examples/s]Converting format of dataset (num_proc=96): 37961 examples [00:10, 1495.94 examples/s]Converting format of dataset (num_proc=96): 38136 examples [00:10, 1554.52 examples/s]Converting format of dataset (num_proc=96): 38370 examples [00:10, 1778.82 examples/s]Converting format of dataset (num_proc=96): 38683 examples [00:10, 2166.31 examples/s]Converting format of dataset (num_proc=96): 39076 examples [00:10, 2677.71 examples/s]Converting format of dataset (num_proc=96): 39577 examples [00:10, 3319.46 examples/s]Converting format of dataset (num_proc=96): 40078 examples [00:10, 3815.13 examples/s]Converting format of dataset (num_proc=96): 40511 examples [00:10, 3967.03 examples/s]Converting format of dataset (num_proc=96): 41140 examples [00:10, 4634.26 examples/s]Converting format of dataset (num_proc=96): 41648 examples [00:10, 4684.77 examples/s]Converting format of dataset (num_proc=96): 42191 examples [00:11, 4899.95 examples/s]Converting format of dataset (num_proc=96): 42724 examples [00:11, 5005.68 examples/s]Converting format of dataset (num_proc=96): 43371 examples [00:11, 5431.63 examples/s]Converting format of dataset (num_proc=96): 43966 examples [00:11, 5547.34 examples/s]Converting format of dataset (num_proc=96): 44523 examples [00:11, 5466.45 examples/s]Converting format of dataset (num_proc=96): 45075 examples [00:11, 5381.40 examples/s]Converting format of dataset (num_proc=96): 45614 examples [00:11, 4757.38 examples/s]Converting format of dataset (num_proc=96): 46105 examples [00:11, 3536.20 examples/s]Converting format of dataset (num_proc=96): 46512 examples [00:12, 2924.62 examples/s]Converting format of dataset (num_proc=96): 46856 examples [00:12, 2945.94 examples/s]Converting format of dataset (num_proc=96): 47186 examples [00:12, 2498.27 examples/s]Converting format of dataset (num_proc=96): 47470 examples [00:12, 2374.52 examples/s]Converting format of dataset (num_proc=96): 47731 examples [00:12, 2245.51 examples/s]Converting format of dataset (num_proc=96): 47971 examples [00:12, 2194.67 examples/s]Converting format of dataset (num_proc=96): 48200 examples [00:13, 2089.97 examples/s]Converting format of dataset (num_proc=96): 48419 examples [00:13, 1672.69 examples/s]Converting format of dataset (num_proc=96): 48600 examples [00:13, 1411.49 examples/s]Converting format of dataset (num_proc=96): 48756 examples [00:13, 1422.79 examples/s]Converting format of dataset (num_proc=96): 48908 examples [00:13, 1416.06 examples/s]Converting format of dataset (num_proc=96): 49060 examples [00:13, 1434.98 examples/s]Converting format of dataset (num_proc=96): 49249 examples [00:13, 1534.11 examples/s]Converting format of dataset (num_proc=96): 49437 examples [00:13, 1623.28 examples/s]Converting format of dataset (num_proc=96): 49607 examples [00:14, 1439.22 examples/s]Converting format of dataset (num_proc=96): 49758 examples [00:14, 1283.25 examples/s]Converting format of dataset (num_proc=96): 49925 examples [00:14, 1368.04 examples/s]Converting format of dataset (num_proc=96): 50070 examples [00:14, 1325.22 examples/s]Converting format of dataset (num_proc=96): 50208 examples [00:14, 1126.05 examples/s]Converting format of dataset (num_proc=96): 50336 examples [00:14, 1150.78 examples/s]Converting format of dataset (num_proc=96): 50457 examples [00:14, 1066.72 examples/s]Converting format of dataset (num_proc=96): 50568 examples [00:15, 888.87 examples/s] Converting format of dataset (num_proc=96): 50664 examples [00:15, 819.98 examples/s]Converting format of dataset (num_proc=96): 50777 examples [00:15, 875.00 examples/s]Converting format of dataset (num_proc=96): 50880 examples [00:15, 905.94 examples/s]Converting format of dataset (num_proc=96): 51020 examples [00:15, 1029.31 examples/s]Converting format of dataset (num_proc=96): 51130 examples [00:15, 1008.69 examples/s]Converting format of dataset (num_proc=96): 51240 examples [00:15, 992.05 examples/s] Converting format of dataset (num_proc=96): 51345 examples [00:15, 1002.48 examples/s]Converting format of dataset (num_proc=96): 51447 examples [00:15, 954.13 examples/s] Converting format of dataset (num_proc=96): 51612 examples [00:16, 1109.40 examples/s]Converting format of dataset (num_proc=96): 51793 examples [00:16, 1281.83 examples/s]Converting format of dataset (num_proc=96): 51925 examples [00:16, 1221.74 examples/s]Converting format of dataset (num_proc=96): 52049 examples [00:16, 1213.12 examples/s]Converting format of dataset (num_proc=96): 52259 examples [00:16, 1448.86 examples/s]Converting format of dataset (num_proc=96): 52446 examples [00:16, 1553.19 examples/s]Converting format of dataset (num_proc=96): 52630 examples [00:16, 1633.24 examples/s]Converting format of dataset (num_proc=96): 52903 examples [00:16, 1927.42 examples/s]Converting format of dataset (num_proc=96): 53204 examples [00:16, 2222.10 examples/s]Converting format of dataset (num_proc=96): 53531 examples [00:17, 2509.17 examples/s]Converting format of dataset (num_proc=96): 53868 examples [00:17, 2726.08 examples/s]Converting format of dataset (num_proc=96): 54192 examples [00:17, 2862.95 examples/s]Converting format of dataset (num_proc=96): 54691 examples [00:17, 3482.04 examples/s]Converting format of dataset (num_proc=96): 55255 examples [00:17, 4118.58 examples/s]Converting format of dataset (num_proc=96): 55814 examples [00:17, 4527.91 examples/s]Converting format of dataset (num_proc=96): 56510 examples [00:17, 5232.62 examples/s]Converting format of dataset (num_proc=96): 57219 examples [00:17, 5775.21 examples/s]Converting format of dataset (num_proc=96): 58094 examples [00:17, 6647.60 examples/s]Converting format of dataset (num_proc=96): 58965 examples [00:17, 7258.34 examples/s]Converting format of dataset (num_proc=96): 59753 examples [00:18, 7419.92 examples/s]Converting format of dataset (num_proc=96): 60563 examples [00:18, 7589.61 examples/s]Converting format of dataset (num_proc=96): 61323 examples [00:18, 7412.41 examples/s]Converting format of dataset (num_proc=96): 62066 examples [00:18, 7093.27 examples/s]Converting format of dataset (num_proc=96): 62782 examples [00:18, 6589.09 examples/s]Converting format of dataset (num_proc=96): 63451 examples [00:18, 5834.75 examples/s]Converting format of dataset (num_proc=96): 64055 examples [00:18, 5272.49 examples/s]Converting format of dataset (num_proc=96): 64602 examples [00:18, 4900.66 examples/s]Converting format of dataset (num_proc=96): 65110 examples [00:19, 3975.85 examples/s]Converting format of dataset (num_proc=96): 65542 examples [00:19, 3160.04 examples/s]Converting format of dataset (num_proc=96): 65900 examples [00:19, 2482.95 examples/s]Converting format of dataset (num_proc=96): 66094 examples [00:20, 1615.79 examples/s]
[rank0]:[W1116 03:52:41.942692213 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
Running tokenizer on dataset (num_proc=96):   0%|          | 0/33047 [00:00<?, ? examples/s]slurmstepd: error: *** JOB 91773 ON trig0019 CANCELLED AT 2025-11-16T04:01:58 DUE TO TIME LIMIT ***

scontrol show job 91773
JobId=91773 JobName=slurm_qwen2_5vl_lora_sft_SQA3D.sh
   UserId=indrisch(3122343) GroupId=indrisch(3122343) MCS_label=N/A
   Priority=369913 Nice=0 Account=def-wangcs QOS=normal
   JobState=COMPLETING Reason=TimeLimit Dependency=(null)
   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:15
   RunTime=00:10:15 TimeLimit=00:10:00 TimeMin=N/A
   SubmitTime=2025-11-16T03:51:27 EligibleTime=2025-11-16T03:51:27
   AccrueTime=2025-11-16T03:51:27
   StartTime=2025-11-16T03:51:43 EndTime=2025-11-16T04:01:58 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2025-11-16T03:51:43 Scheduler=Backfill
   Partition=compute_full_node AllocNode:Sid=trig-login01:1127669
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=trig0019
   BatchHost=trig0019
   NumNodes=1 NumCPUs=96 NumTasks=1 CPUs/Task=96 ReqB:S:C:T=0:0:*:*
   ReqTRES=cpu=96,mem=770000M,node=1,billing=4,gres/gpu=4
   AllocTRES=cpu=96,mem=770000M,node=1,billing=4,gres/gpu=4
   Socks/Node=* NtasksPerN:B:S:C=1:0:*:* CoreSpec=*
   MinCPUsNode=96 MinMemoryNode=770000M MinTmpDiskNode=0
   Features=(null) DelayBoot=00:00:00
   OverSubscribe=NO Contiguous=0 Licenses=(null) Network=(null)
   Command=/scratch/indrisch/LLaMA-Factory/preliminaries/qwen2_5vl_lora_sft_SQA3D/slurm_qwen2_5vl_lora_sft_SQA3D.sh
   WorkDir=/scratch/indrisch/LLaMA-Factory/preliminaries/qwen2_5vl_lora_sft_SQA3D
   Comment=/opt/slurm/bin/sbatch --export=NONE --get-user-env=L slurm_qwen2_5vl_lora_sft_SQA3D.sh 
   StdErr=/scratch/indrisch/LLaMA-Factory/preliminaries/qwen2_5vl_lora_sft_SQA3D/%N-qwen2_5vl_lora_sft_SQA3D-91773.out
   StdIn=/dev/null
   StdOut=/scratch/indrisch/LLaMA-Factory/preliminaries/qwen2_5vl_lora_sft_SQA3D/%N-qwen2_5vl_lora_sft_SQA3D-91773.out
   TresPerNode=gres/gpu:h100:4
   TresPerTask=cpu=96
   

sacct -j 91773
JobID           JobName    Account    Elapsed  MaxVMSize     MaxRSS  SystemCPU    UserCPU ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- ---------- ---------- -------- 
91773        slurm_qwe+ def-wangcs   00:10:15                        29:25.048   13:29:45      0:0 
91773.batch       batch def-wangcs   00:10:17          0 177442112K  29:25.047   13:29:45     0:15 
91773.extern     extern def-wangcs   00:10:18          0        12K  00:00.001   00:00:00      0:0 

