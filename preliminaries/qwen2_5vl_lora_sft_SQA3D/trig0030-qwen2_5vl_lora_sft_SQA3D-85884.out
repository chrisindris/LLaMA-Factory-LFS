
==========
== CUDA ==
==========

CUDA Version 12.4.1

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

[2025-11-13 02:30:32,697] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[INFO|2025-11-13 02:30:35] llamafactory.hparams.parser:423 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.bfloat16
[INFO|hub.py:421] 2025-11-13 02:30:35,637 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2025-11-13 02:30:35,652 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2025-11-13 02:30:35,661 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-11-13 02:30:35,661 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-11-13 02:30:35,661 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-11-13 02:30:35,661 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-11-13 02:30:35,661 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-11-13 02:30:35,661 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-11-13 02:30:35,661 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-11-13 02:30:35,890 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|hub.py:421] 2025-11-13 02:30:35,890 >> Offline mode: forcing local_files_only=True
[INFO|hub.py:421] 2025-11-13 02:30:35,891 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:316] 2025-11-13 02:30:35,894 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2025-11-13 02:30:35,896 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:316] 2025-11-13 02:30:35,899 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2025-11-13 02:30:35,900 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:428] 2025-11-13 02:30:35,907 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:1949] 2025-11-13 02:30:35,907 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2025-11-13 02:30:35,912 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-11-13 02:30:35,912 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-11-13 02:30:35,912 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-11-13 02:30:35,912 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-11-13 02:30:35,912 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-11-13 02:30:35,912 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-11-13 02:30:35,912 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-11-13 02:30:36,089 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:660] 2025-11-13 02:30:36,090 >> Offline mode: forcing local_files_only=True
[INFO|video_processing_utils.py:726] 2025-11-13 02:30:36,093 >> loading configuration file video_preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|video_processing_utils.py:770] 2025-11-13 02:30:36,096 >> Video processor Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:928] 2025-11-13 02:30:36,096 >> Offline mode: forcing local_files_only=True
[INFO|processing_utils.py:1116] 2025-11-13 02:30:36,101 >> loading configuration file processor_config.json from cache at None
[INFO|processing_utils.py:1199] 2025-11-13 02:30:36,420 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2_5_VLProcessor"
}

[INFO|2025-11-13 02:30:36] llamafactory.data.loader:143 >> Loading dataset /scratch/indrisch/huggingface/hub/datasets--cvis-tmu--llamafactory-sqa3d-traces-multiimage-vqa/snapshots/942a5514a2ed64b8ad7ec624ecde74f08884f1c8/data/...
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
Setting num_proc from 16 to 3 for the train split as it only contains 3 shards.

Generating train split: 0 examples [00:00, ? examples/s]
Generating train split: 934 examples [00:00, 2687.00 examples/s]
Generating train split: 2802 examples [00:00, 6130.90 examples/s]
Generating train split: 5604 examples [00:00, 10217.88 examples/s]
Generating train split: 9340 examples [00:00, 15627.99 examples/s]
Generating train split: 11710 examples [00:00, 14891.03 examples/s]
Generating train split: 14512 examples [00:01, 17167.58 examples/s]
Generating train split: 17314 examples [00:01, 16792.89 examples/s]
Generating train split: 20116 examples [00:01, 18778.83 examples/s]
Generating train split: 22918 examples [00:01, 20276.36 examples/s]
Generating train split: 25720 examples [00:01, 17648.47 examples/s]
Generating train split: 28522 examples [00:01, 19237.57 examples/s]
Generating train split: 32186 examples [00:01, 21574.47 examples/s]
Generating train split: 33047 examples [00:02, 15089.45 examples/s]

Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]
Converting format of dataset (num_proc=16):   0%|          | 1/1000 [00:00<09:07,  1.83 examples/s]
Converting format of dataset (num_proc=16):   1%|          | 7/1000 [00:00<01:14, 13.28 examples/s]
Converting format of dataset (num_proc=16):   1%|▏         | 13/1000 [00:00<00:46, 21.14 examples/s]
Converting format of dataset (num_proc=16):   2%|▏         | 17/1000 [00:00<00:39, 25.17 examples/s]
Converting format of dataset (num_proc=16):   2%|▏         | 21/1000 [00:01<00:39, 24.65 examples/s]
Converting format of dataset (num_proc=16):   2%|▎         | 25/1000 [00:01<00:51, 18.85 examples/s]
Converting format of dataset (num_proc=16):   3%|▎         | 28/1000 [00:01<00:51, 18.76 examples/s]
Converting format of dataset (num_proc=16):   3%|▎         | 31/1000 [00:01<00:50, 19.36 examples/s]
Converting format of dataset (num_proc=16):   4%|▎         | 35/1000 [00:01<00:43, 22.14 examples/s]
Converting format of dataset (num_proc=16):   4%|▍         | 38/1000 [00:02<00:49, 19.25 examples/s]
Converting format of dataset (num_proc=16):   4%|▍         | 41/1000 [00:02<00:50, 18.97 examples/s]
Converting format of dataset (num_proc=16):   4%|▍         | 44/1000 [00:02<00:46, 20.63 examples/s]
Converting format of dataset (num_proc=16):   5%|▍         | 47/1000 [00:02<00:47, 20.11 examples/s]
Converting format of dataset (num_proc=16):   5%|▌         | 50/1000 [00:02<00:55, 17.14 examples/s]
Converting format of dataset (num_proc=16):   5%|▌         | 53/1000 [00:02<00:49, 18.99 examples/s]
Converting format of dataset (num_proc=16):   6%|▌         | 56/1000 [00:03<00:54, 17.30 examples/s]
Converting format of dataset (num_proc=16):   6%|▌         | 58/1000 [00:03<00:53, 17.57 examples/s]
Converting format of dataset (num_proc=16):   6%|▌         | 60/1000 [00:03<00:52, 17.82 examples/s]
Converting format of dataset (num_proc=16):   6%|▋         | 65/1000 [00:03<00:41, 22.27 examples/s]
Converting format of dataset (num_proc=16):   7%|▋         | 68/1000 [00:03<00:39, 23.66 examples/s]
Converting format of dataset (num_proc=16):   7%|▋         | 73/1000 [00:03<00:31, 29.04 examples/s]
Converting format of dataset (num_proc=16):   8%|▊         | 77/1000 [00:03<00:32, 28.82 examples/s]
Converting format of dataset (num_proc=16):   8%|▊         | 83/1000 [00:03<00:25, 35.68 examples/s]
Converting format of dataset (num_proc=16):   9%|▊         | 87/1000 [00:04<00:25, 35.26 examples/s]
Converting format of dataset (num_proc=16):   9%|▉         | 91/1000 [00:04<00:28, 31.75 examples/s]
Converting format of dataset (num_proc=16):  10%|▉         | 95/1000 [00:04<00:31, 28.82 examples/s]
Converting format of dataset (num_proc=16):  10%|▉         | 99/1000 [00:04<00:41, 21.63 examples/s]
Converting format of dataset (num_proc=16):  10%|█         | 105/1000 [00:04<00:33, 26.43 examples/s]
Converting format of dataset (num_proc=16):  11%|█         | 110/1000 [00:04<00:29, 30.57 examples/s]
Converting format of dataset (num_proc=16):  12%|█▏        | 115/1000 [00:05<00:28, 31.33 examples/s]
Converting format of dataset (num_proc=16):  12%|█▏        | 121/1000 [00:05<00:24, 36.43 examples/s]
Converting format of dataset (num_proc=16):  13%|█▎        | 128/1000 [00:05<00:20, 43.45 examples/s]
Converting format of dataset (num_proc=16):  13%|█▎        | 134/1000 [00:05<00:28, 30.73 examples/s]
Converting format of dataset (num_proc=16):  14%|█▍        | 138/1000 [00:05<00:29, 28.96 examples/s]
Converting format of dataset (num_proc=16):  14%|█▍        | 142/1000 [00:05<00:33, 25.68 examples/s]
Converting format of dataset (num_proc=16):  15%|█▍        | 147/1000 [00:06<00:29, 29.26 examples/s]
Converting format of dataset (num_proc=16):  15%|█▌        | 152/1000 [00:06<00:25, 33.46 examples/s]
Converting format of dataset (num_proc=16):  16%|█▌        | 156/1000 [00:06<00:26, 31.44 examples/s]
Converting format of dataset (num_proc=16):  16%|█▌        | 160/1000 [00:06<00:33, 25.08 examples/s]
Converting format of dataset (num_proc=16):  16%|█▋        | 163/1000 [00:06<00:41, 20.26 examples/s]
Converting format of dataset (num_proc=16):  17%|█▋        | 166/1000 [00:06<00:38, 21.43 examples/s]
Converting format of dataset (num_proc=16):  17%|█▋        | 173/1000 [00:07<00:27, 29.65 examples/s]
Converting format of dataset (num_proc=16):  18%|█▊        | 177/1000 [00:07<00:32, 25.41 examples/s]
Converting format of dataset (num_proc=16):  18%|█▊        | 184/1000 [00:07<00:28, 29.07 examples/s]
Converting format of dataset (num_proc=16):  19%|█▉        | 188/1000 [00:07<00:32, 24.70 examples/s]
Converting format of dataset (num_proc=16):  19%|█▉        | 191/1000 [00:07<00:32, 25.13 examples/s]
Converting format of dataset (num_proc=16):  20%|█▉        | 195/1000 [00:07<00:28, 27.80 examples/s]
Converting format of dataset (num_proc=16):  20%|██        | 201/1000 [00:08<00:22, 34.82 examples/s]
Converting format of dataset (num_proc=16):  20%|██        | 205/1000 [00:08<00:22, 35.01 examples/s]
Converting format of dataset (num_proc=16):  21%|██        | 212/1000 [00:08<00:20, 38.97 examples/s]
Converting format of dataset (num_proc=16):  22%|██▏       | 217/1000 [00:08<00:20, 38.42 examples/s]
Converting format of dataset (num_proc=16):  22%|██▏       | 223/1000 [00:08<00:21, 36.23 examples/s]
Converting format of dataset (num_proc=16):  23%|██▎       | 227/1000 [00:08<00:22, 34.92 examples/s]
Converting format of dataset (num_proc=16):  23%|██▎       | 231/1000 [00:08<00:23, 32.72 examples/s]
Converting format of dataset (num_proc=16):  24%|██▎       | 235/1000 [00:09<00:27, 27.65 examples/s]
Converting format of dataset (num_proc=16):  24%|██▍       | 238/1000 [00:09<00:30, 25.03 examples/s]
Converting format of dataset (num_proc=16):  24%|██▍       | 241/1000 [00:09<00:30, 25.07 examples/s]
Converting format of dataset (num_proc=16):  24%|██▍       | 244/1000 [00:09<00:31, 23.82 examples/s]
Converting format of dataset (num_proc=16):  25%|██▍       | 247/1000 [00:09<00:31, 23.63 examples/s]
Converting format of dataset (num_proc=16):  25%|██▌       | 254/1000 [00:09<00:25, 29.83 examples/s]
Converting format of dataset (num_proc=16):  26%|██▌       | 259/1000 [00:09<00:23, 30.98 examples/s]
Converting format of dataset (num_proc=16):  27%|██▋       | 267/1000 [00:10<00:18, 39.05 examples/s]
Converting format of dataset (num_proc=16):  27%|██▋       | 271/1000 [00:10<00:20, 36.34 examples/s]
Converting format of dataset (num_proc=16):  28%|██▊       | 276/1000 [00:10<00:21, 34.05 examples/s]
Converting format of dataset (num_proc=16):  28%|██▊       | 280/1000 [00:10<00:20, 34.84 examples/s]
Converting format of dataset (num_proc=16):  28%|██▊       | 284/1000 [00:10<00:25, 28.30 examples/s]
Converting format of dataset (num_proc=16):  29%|██▉       | 288/1000 [00:10<00:25, 27.56 examples/s]
Converting format of dataset (num_proc=16):  29%|██▉       | 293/1000 [00:10<00:22, 31.05 examples/s]
Converting format of dataset (num_proc=16):  30%|██▉       | 297/1000 [00:11<00:24, 28.45 examples/s]
Converting format of dataset (num_proc=16):  30%|███       | 300/1000 [00:11<00:29, 23.67 examples/s]
Converting format of dataset (num_proc=16):  30%|███       | 304/1000 [00:11<00:26, 26.46 examples/s]
Converting format of dataset (num_proc=16):  31%|███       | 307/1000 [00:11<00:28, 24.21 examples/s]
Converting format of dataset (num_proc=16):  31%|███       | 310/1000 [00:11<00:27, 25.20 examples/s]
Converting format of dataset (num_proc=16):  31%|███▏      | 314/1000 [00:11<00:24, 27.79 examples/s]
Converting format of dataset (num_proc=16):  32%|███▏      | 317/1000 [00:11<00:24, 27.97 examples/s]
Converting format of dataset (num_proc=16):  32%|███▏      | 322/1000 [00:12<00:20, 33.35 examples/s]
Converting format of dataset (num_proc=16):  33%|███▎      | 326/1000 [00:12<00:19, 33.97 examples/s]
Converting format of dataset (num_proc=16):  33%|███▎      | 331/1000 [00:12<00:21, 30.79 examples/s]
Converting format of dataset (num_proc=16):  34%|███▎      | 335/1000 [00:12<00:22, 29.66 examples/s]
Converting format of dataset (num_proc=16):  34%|███▍      | 342/1000 [00:12<00:17, 36.88 examples/s]
Converting format of dataset (num_proc=16):  35%|███▍      | 346/1000 [00:12<00:20, 31.89 examples/s]
Converting format of dataset (num_proc=16):  35%|███▌      | 350/1000 [00:13<00:25, 25.24 examples/s]
Converting format of dataset (num_proc=16):  36%|███▌      | 355/1000 [00:13<00:24, 26.05 examples/s]
Converting format of dataset (num_proc=16):  36%|███▌      | 359/1000 [00:13<00:25, 25.22 examples/s]
Converting format of dataset (num_proc=16):  37%|███▋      | 366/1000 [00:13<00:21, 28.98 examples/s]
Converting format of dataset (num_proc=16):  37%|███▋      | 373/1000 [00:13<00:17, 35.10 examples/s]
Converting format of dataset (num_proc=16):  38%|███▊      | 377/1000 [00:13<00:19, 32.57 examples/s]
Converting format of dataset (num_proc=16):  38%|███▊      | 383/1000 [00:14<00:17, 34.54 examples/s]
Converting format of dataset (num_proc=16):  39%|███▉      | 388/1000 [00:14<00:16, 37.10 examples/s]
Converting format of dataset (num_proc=16):  39%|███▉      | 394/1000 [00:14<00:15, 40.27 examples/s]
Converting format of dataset (num_proc=16):  40%|████      | 400/1000 [00:14<00:13, 43.59 examples/s]
Converting format of dataset (num_proc=16):  40%|████      | 405/1000 [00:14<00:17, 34.02 examples/s]
Converting format of dataset (num_proc=16):  41%|████      | 410/1000 [00:14<00:17, 33.88 examples/s]
Converting format of dataset (num_proc=16):  42%|████▏     | 415/1000 [00:14<00:15, 36.66 examples/s]
Converting format of dataset (num_proc=16):  42%|████▏     | 422/1000 [00:14<00:13, 42.11 examples/s]
Converting format of dataset (num_proc=16):  43%|████▎     | 427/1000 [00:15<00:14, 39.05 examples/s]
Converting format of dataset (num_proc=16):  43%|████▎     | 433/1000 [00:15<00:13, 41.48 examples/s]
Converting format of dataset (num_proc=16):  44%|████▍     | 438/1000 [00:15<00:14, 39.01 examples/s]
Converting format of dataset (num_proc=16):  44%|████▍     | 444/1000 [00:15<00:13, 41.09 examples/s]
Converting format of dataset (num_proc=16):  45%|████▍     | 449/1000 [00:15<00:14, 38.60 examples/s]
Converting format of dataset (num_proc=16):  46%|████▌     | 455/1000 [00:15<00:12, 43.36 examples/s]
Converting format of dataset (num_proc=16):  46%|████▌     | 460/1000 [00:15<00:13, 40.37 examples/s]
Converting format of dataset (num_proc=16):  46%|████▋     | 465/1000 [00:16<00:12, 41.19 examples/s]
Converting format of dataset (num_proc=16):  47%|████▋     | 470/1000 [00:16<00:15, 34.86 examples/s]
Converting format of dataset (num_proc=16):  47%|████▋     | 474/1000 [00:16<00:15, 34.56 examples/s]
Converting format of dataset (num_proc=16):  48%|████▊     | 479/1000 [00:16<00:14, 36.41 examples/s]
Converting format of dataset (num_proc=16):  49%|████▊     | 487/1000 [00:16<00:12, 42.22 examples/s]
Converting format of dataset (num_proc=16):  49%|████▉     | 492/1000 [00:16<00:14, 35.56 examples/s]
Converting format of dataset (num_proc=16):  50%|████▉     | 497/1000 [00:16<00:13, 38.32 examples/s]
Converting format of dataset (num_proc=16):  50%|█████     | 502/1000 [00:17<00:15, 31.92 examples/s]
Converting format of dataset (num_proc=16):  51%|█████     | 508/1000 [00:17<00:14, 34.98 examples/s]
Converting format of dataset (num_proc=16):  51%|█████     | 512/1000 [00:17<00:13, 35.92 examples/s]
Converting format of dataset (num_proc=16):  52%|█████▏    | 518/1000 [00:17<00:12, 39.44 examples/s]
Converting format of dataset (num_proc=16):  52%|█████▎    | 525/1000 [00:17<00:10, 43.78 examples/s]
Converting format of dataset (num_proc=16):  53%|█████▎    | 531/1000 [00:17<00:10, 44.56 examples/s]
Converting format of dataset (num_proc=16):  54%|█████▎    | 536/1000 [00:17<00:10, 45.87 examples/s]
Converting format of dataset (num_proc=16):  54%|█████▍    | 541/1000 [00:18<00:13, 34.77 examples/s]
Converting format of dataset (num_proc=16):  55%|█████▍    | 546/1000 [00:18<00:13, 33.08 examples/s]
Converting format of dataset (num_proc=16):  55%|█████▌    | 551/1000 [00:18<00:12, 36.49 examples/s]
Converting format of dataset (num_proc=16):  56%|█████▌    | 557/1000 [00:18<00:10, 41.75 examples/s]
Converting format of dataset (num_proc=16):  56%|█████▌    | 562/1000 [00:18<00:12, 34.20 examples/s]
Converting format of dataset (num_proc=16):  57%|█████▋    | 572/1000 [00:18<00:09, 46.02 examples/s]
Converting format of dataset (num_proc=16):  58%|█████▊    | 578/1000 [00:19<00:12, 33.05 examples/s]
Converting format of dataset (num_proc=16):  58%|█████▊    | 583/1000 [00:19<00:13, 31.42 examples/s]
Converting format of dataset (num_proc=16):  59%|█████▊    | 587/1000 [00:19<00:13, 31.68 examples/s]
Converting format of dataset (num_proc=16):  59%|█████▉    | 592/1000 [00:19<00:11, 34.68 examples/s]
Converting format of dataset (num_proc=16):  60%|█████▉    | 596/1000 [00:19<00:13, 30.87 examples/s]
Converting format of dataset (num_proc=16):  61%|██████    | 608/1000 [00:19<00:08, 44.33 examples/s]
Converting format of dataset (num_proc=16):  61%|██████▏   | 614/1000 [00:20<00:08, 46.43 examples/s]
Converting format of dataset (num_proc=16):  62%|██████▏   | 619/1000 [00:20<00:10, 36.73 examples/s]
Converting format of dataset (num_proc=16):  63%|██████▎   | 626/1000 [00:20<00:08, 42.84 examples/s]
Converting format of dataset (num_proc=16):  63%|██████▎   | 631/1000 [00:20<00:09, 38.45 examples/s]
Converting format of dataset (num_proc=16):  64%|██████▎   | 636/1000 [00:20<00:09, 39.21 examples/s]
Converting format of dataset (num_proc=16):  64%|██████▍   | 641/1000 [00:20<00:10, 32.76 examples/s]
Converting format of dataset (num_proc=16):  65%|██████▍   | 647/1000 [00:20<00:09, 37.62 examples/s]
Converting format of dataset (num_proc=16):  65%|██████▌   | 652/1000 [00:21<00:11, 29.71 examples/s]
Converting format of dataset (num_proc=16):  66%|██████▌   | 657/1000 [00:21<00:10, 31.58 examples/s]
Converting format of dataset (num_proc=16):  66%|██████▌   | 661/1000 [00:21<00:13, 24.31 examples/s]
Converting format of dataset (num_proc=16):  66%|██████▋   | 665/1000 [00:21<00:12, 27.02 examples/s]
Converting format of dataset (num_proc=16):  67%|██████▋   | 669/1000 [00:21<00:12, 27.15 examples/s]
Converting format of dataset (num_proc=16):  67%|██████▋   | 674/1000 [00:22<00:10, 29.77 examples/s]
Converting format of dataset (num_proc=16):  68%|██████▊   | 679/1000 [00:22<00:09, 33.59 examples/s]
Converting format of dataset (num_proc=16):  68%|██████▊   | 685/1000 [00:22<00:08, 36.77 examples/s]
Converting format of dataset (num_proc=16):  69%|██████▉   | 690/1000 [00:22<00:08, 36.38 examples/s]
Converting format of dataset (num_proc=16):  70%|██████▉   | 695/1000 [00:22<00:07, 39.17 examples/s]
Converting format of dataset (num_proc=16):  70%|███████   | 700/1000 [00:22<00:09, 32.07 examples/s]
Converting format of dataset (num_proc=16):  70%|███████   | 704/1000 [00:22<00:08, 33.60 examples/s]
Converting format of dataset (num_proc=16):  71%|███████   | 711/1000 [00:22<00:06, 41.38 examples/s]
Converting format of dataset (num_proc=16):  72%|███████▏  | 717/1000 [00:23<00:08, 35.32 examples/s]
Converting format of dataset (num_proc=16):  72%|███████▎  | 725/1000 [00:23<00:07, 38.34 examples/s]
Converting format of dataset (num_proc=16):  73%|███████▎  | 730/1000 [00:23<00:08, 32.67 examples/s]
Converting format of dataset (num_proc=16):  74%|███████▎  | 735/1000 [00:23<00:07, 34.40 examples/s]
Converting format of dataset (num_proc=16):  74%|███████▍  | 742/1000 [00:23<00:07, 35.97 examples/s]
Converting format of dataset (num_proc=16):  75%|███████▍  | 746/1000 [00:24<00:07, 33.69 examples/s]
Converting format of dataset (num_proc=16):  75%|███████▌  | 753/1000 [00:24<00:06, 38.83 examples/s]
Converting format of dataset (num_proc=16):  76%|███████▌  | 759/1000 [00:24<00:05, 40.63 examples/s]
Converting format of dataset (num_proc=16):  76%|███████▋  | 764/1000 [00:24<00:05, 41.80 examples/s]
Converting format of dataset (num_proc=16):  77%|███████▋  | 769/1000 [00:24<00:05, 40.29 examples/s]
Converting format of dataset (num_proc=16):  78%|███████▊  | 776/1000 [00:24<00:04, 45.35 examples/s]
Converting format of dataset (num_proc=16):  78%|███████▊  | 783/1000 [00:24<00:04, 49.90 examples/s]
Converting format of dataset (num_proc=16):  79%|███████▉  | 789/1000 [00:24<00:04, 48.44 examples/s]
Converting format of dataset (num_proc=16):  80%|███████▉  | 796/1000 [00:24<00:04, 50.70 examples/s]
Converting format of dataset (num_proc=16):  80%|████████  | 802/1000 [00:25<00:04, 43.35 examples/s]
Converting format of dataset (num_proc=16):  81%|████████  | 807/1000 [00:25<00:04, 44.30 examples/s]
Converting format of dataset (num_proc=16):  81%|████████▏ | 813/1000 [00:25<00:04, 42.66 examples/s]
Converting format of dataset (num_proc=16):  82%|████████▏ | 821/1000 [00:25<00:03, 50.95 examples/s]
Converting format of dataset (num_proc=16):  83%|████████▎ | 827/1000 [00:25<00:03, 47.50 examples/s]
Converting format of dataset (num_proc=16):  83%|████████▎ | 833/1000 [00:25<00:03, 49.67 examples/s]
Converting format of dataset (num_proc=16):  84%|████████▍ | 839/1000 [00:25<00:03, 46.37 examples/s]
Converting format of dataset (num_proc=16):  84%|████████▍ | 844/1000 [00:26<00:03, 43.80 examples/s]
Converting format of dataset (num_proc=16):  85%|████████▌ | 851/1000 [00:26<00:03, 44.57 examples/s]
Converting format of dataset (num_proc=16):  86%|████████▌ | 856/1000 [00:26<00:03, 44.82 examples/s]
Converting format of dataset (num_proc=16):  86%|████████▋ | 863/1000 [00:26<00:03, 45.02 examples/s]
Converting format of dataset (num_proc=16):  87%|████████▋ | 870/1000 [00:26<00:02, 47.70 examples/s]
Converting format of dataset (num_proc=16):  88%|████████▊ | 875/1000 [00:26<00:02, 43.71 examples/s]
Converting format of dataset (num_proc=16):  88%|████████▊ | 881/1000 [00:26<00:02, 47.50 examples/s]
Converting format of dataset (num_proc=16):  89%|████████▉ | 888/1000 [00:27<00:02, 49.63 examples/s]
Converting format of dataset (num_proc=16):  89%|████████▉ | 894/1000 [00:27<00:02, 52.19 examples/s]
Converting format of dataset (num_proc=16):  90%|█████████ | 900/1000 [00:27<00:02, 48.03 examples/s]
Converting format of dataset (num_proc=16):  90%|█████████ | 905/1000 [00:27<00:02, 43.04 examples/s]
Converting format of dataset (num_proc=16):  91%|█████████ | 911/1000 [00:27<00:01, 45.04 examples/s]
Converting format of dataset (num_proc=16):  92%|█████████▏| 916/1000 [00:27<00:01, 44.60 examples/s]
Converting format of dataset (num_proc=16):  92%|█████████▏| 921/1000 [00:27<00:02, 37.08 examples/s]
Converting format of dataset (num_proc=16):  93%|█████████▎| 929/1000 [00:27<00:01, 42.86 examples/s]
Converting format of dataset (num_proc=16):  94%|█████████▎| 935/1000 [00:28<00:01, 39.53 examples/s]
Converting format of dataset (num_proc=16):  94%|█████████▍| 940/1000 [00:28<00:01, 41.52 examples/s]
Converting format of dataset (num_proc=16):  94%|█████████▍| 945/1000 [00:28<00:01, 30.53 examples/s]
Converting format of dataset (num_proc=16):  95%|█████████▌| 950/1000 [00:28<00:01, 32.64 examples/s]
Converting format of dataset (num_proc=16):  96%|█████████▌| 955/1000 [00:28<00:01, 30.99 examples/s]
Converting format of dataset (num_proc=16):  96%|█████████▌| 959/1000 [00:28<00:01, 32.22 examples/s]
Converting format of dataset (num_proc=16):  96%|█████████▋| 963/1000 [00:29<00:01, 29.09 examples/s]
Converting format of dataset (num_proc=16):  97%|█████████▋| 967/1000 [00:29<00:01, 30.71 examples/s]
Converting format of dataset (num_proc=16):  97%|█████████▋| 971/1000 [00:29<00:01, 26.72 examples/s]
Converting format of dataset (num_proc=16):  98%|█████████▊| 978/1000 [00:29<00:00, 33.95 examples/s]
Converting format of dataset (num_proc=16):  98%|█████████▊| 982/1000 [00:29<00:00, 33.49 examples/s]
Converting format of dataset (num_proc=16):  99%|█████████▊| 986/1000 [00:30<00:00, 20.60 examples/s]
Converting format of dataset (num_proc=16):  99%|█████████▉| 989/1000 [00:30<00:00, 16.44 examples/s]
Converting format of dataset (num_proc=16):  99%|█████████▉| 992/1000 [00:30<00:00, 15.12 examples/s]
Converting format of dataset (num_proc=16):  99%|█████████▉| 994/1000 [00:30<00:00, 15.48 examples/s]
Converting format of dataset (num_proc=16): 100%|█████████▉| 997/1000 [00:30<00:00, 17.32 examples/s]
Converting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:31<00:00,  8.91 examples/s]
Converting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:31<00:00, 31.43 examples/s]

Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:4009] 2025-11-13 02:33:41,139 >> Token indices sequence length is longer than the specified maximum sequence length for this model (293741 > 131072). Running this sequence through the model will result in indexing errors
[WARNING|tokenization_utils_base.py:4009] 2025-11-13 02:35:39,934 >> Token indices sequence length is longer than the specified maximum sequence length for this model (348500 > 131072). Running this sequence through the model will result in indexing errors
[WARNING|tokenization_utils_base.py:4009] 2025-11-13 02:36:15,114 >> Token indices sequence length is longer than the specified maximum sequence length for this model (401226 > 131072). Running this sequence through the model will result in indexing errors
[WARNING|tokenization_utils_base.py:4009] 2025-11-13 02:38:43,839 >> Token indices sequence length is longer than the specified maximum sequence length for this model (315043 > 131072). Running this sequence through the model will result in indexing errors

Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [07:51<?, ? examples/s]
Traceback (most recent call last):
  File "/opt/conda/bin/llamafactory-cli", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/app/src/llamafactory/cli.py", line 24, in main
    launcher.launch()
  File "/app/src/llamafactory/launcher.py", line 152, in launch
    run_exp()
  File "/app/src/llamafactory/train/tuner.py", line 110, in run_exp
    _training_function(config={"args": args, "callbacks": callbacks})
  File "/app/src/llamafactory/train/tuner.py", line 72, in _training_function
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
  File "/app/src/llamafactory/train/sft/workflow.py", line 51, in run_sft
    dataset_module = get_dataset(template, model_args, data_args, training_args, stage="sft", **tokenizer_module)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/src/llamafactory/data/loader.py", line 315, in get_dataset
    dataset = _get_preprocessed_dataset(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/src/llamafactory/data/loader.py", line 256, in _get_preprocessed_dataset
    dataset = dataset.map(
              ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/datasets/arrow_dataset.py", line 560, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/datasets/arrow_dataset.py", line 3309, in map
    for rank, done, content in iflatmap_unordered(
  File "/opt/conda/lib/python3.11/site-packages/datasets/utils/py_utils.py", line 619, in iflatmap_unordered
    raise RuntimeError(
RuntimeError: One of the subprocesses has abruptly died during map operation.To debug the error, disable multiprocessing.
INFO:    Cleanup error: while stopping driver for /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Core/apptainer/1.3.5/var/apptainer/mnt/session/final: fuse-overlayfs exited: fuse: reading device: Software caused connection abort
slurmstepd: error: Detected 2 oom_kill events in StepId=85884.batch. Some of the step tasks have been OOM Killed.

scontrol show job 85884
JobId=85884 JobName=slurm_qwen2_5vl_lora_sft_SQA3D.sh
   UserId=indrisch(3122343) GroupId=indrisch(3122343) MCS_label=N/A
   Priority=329653 Nice=0 Account=def-wangcs QOS=normal
   JobState=COMPLETING Reason=OutOfMemory Dependency=(null)
   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:125
   RunTime=00:08:55 TimeLimit=00:25:00 TimeMin=N/A
   SubmitTime=2025-11-13T02:30:12 EligibleTime=2025-11-13T02:30:12
   AccrueTime=2025-11-13T02:30:12
   StartTime=2025-11-13T02:30:13 EndTime=2025-11-13T02:39:08 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2025-11-13T02:30:13 Scheduler=Main
   Partition=compute AllocNode:Sid=trig-login01:731173
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=trig0030
   BatchHost=trig0030
   NumNodes=1 NumCPUs=4 NumTasks=1 CPUs/Task=4 ReqB:S:C:T=0:0:*:*
   ReqTRES=cpu=4,mem=192500M,node=1,billing=1,gres/gpu=1
   AllocTRES=cpu=4,mem=192500M,node=1,billing=1,gres/gpu=1
   Socks/Node=* NtasksPerN:B:S:C=1:0:*:* CoreSpec=*
   MinCPUsNode=4 MinMemoryNode=192500M MinTmpDiskNode=0
   Features=(null) DelayBoot=00:00:00
   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)
   Command=./slurm_qwen2_5vl_lora_sft_SQA3D.sh
   WorkDir=/scratch/indrisch/LLaMA-Factory/preliminaries/qwen2_5vl_lora_sft_SQA3D
   Comment=/opt/slurm/bin/sbatch --export=NONE --get-user-env=L ./slurm_qwen2_5vl_lora_sft_SQA3D.sh 
   StdErr=/scratch/indrisch/LLaMA-Factory/preliminaries/qwen2_5vl_lora_sft_SQA3D/%N-qwen2_5vl_lora_sft_SQA3D-85884.out
   StdIn=/dev/null
   StdOut=/scratch/indrisch/LLaMA-Factory/preliminaries/qwen2_5vl_lora_sft_SQA3D/%N-qwen2_5vl_lora_sft_SQA3D-85884.out
   TresPerNode=gres/gpu:h100:1
   TresPerTask=cpu=4
   

sacct -j 85884
JobID           JobName    Account    Elapsed  MaxVMSize     MaxRSS  SystemCPU    UserCPU ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- ---------- ---------- -------- 
85884        slurm_qwe+ def-wangcs   00:08:55                         00:00:00   00:00:00      0:0 
85884.batch       batch def-wangcs   00:08:55                         00:00:00   00:00:00      0:0 
85884.extern     extern def-wangcs   00:08:55                         00:00:00   00:00:00      0:0 


kernel messages produced during job executions:
[Nov13 02:36] llamafactory-cl invoked oom-killer: gfp_mask=0xcc0(GFP_KERNEL), order=0, oom_score_adj=0
[  +0.000009] CPU: 15 PID: 205736 Comm: llamafactory-cl Tainted: P           OE     -------  ---  5.14.0-570.25.1.el9_6.x86_64 #1
[  +0.000003] Hardware name: Lenovo ThinkSystem SD665-N V3/SB27B41167, BIOS QGE140H-8.21 05/13/2025
[  +0.000002] Call Trace:
[  +0.000001]  <TASK>
[  +0.000002]  dump_stack_lvl+0x34/0x48
[  +0.000006]  dump_header+0x4a/0x213
[  +0.000003]  oom_kill_process.cold+0xb/0x10
[  +0.000003]  out_of_memory+0xef/0x2c0
[  +0.000003]  ? srso_alias_return_thunk+0x5/0xfbef5
[  +0.000005]  mem_cgroup_out_of_memory+0x131/0x150
[  +0.000005]  try_charge_memcg+0x742/0x800
[  +0.000002]  ? srso_alias_return_thunk+0x5/0xfbef5
[  +0.000002]  ? alloc_pages_mpol+0x93/0x1e0
[  +0.000004]  ? srso_alias_return_thunk+0x5/0xfbef5
[  +0.000003]  __mem_cgroup_charge+0x3e/0xb0
[  +0.000003]  do_anonymous_page+0x7d/0x3e0
[  +0.000003]  __handle_mm_fault+0x2fe/0x650
[  +0.000005]  handle_mm_fault+0xe9/0x250
[  +0.000003]  do_user_addr_fault+0x154/0x620
[  +0.000004]  exc_page_fault+0x62/0x150
[  +0.000004]  asm_exc_page_fault+0x22/0x30
[  +0.000002] RIP: 0033:0x1458efb7019b
[  +0.000005] Code: Unable to access opcode bytes at 0x1458efb70171.
[  +0.000001] RSP: 002b:00001455d8ad1c00 EFLAGS: 00010297
[  +0.000002] RAX: 00001454a73fa240 RBX: 000000000f915d80 RCX: 000000000f915d80
[  +0.000001] RDX: 00001455d8ad1ce0 RSI: 00001453ae29c240 RDI: 000000000ca5eb70
[  +0.000001] RBP: 00001455d8ad1c40 R08: 0000000000000001 R09: 0000000000000002
[  +0.000001] R10: 0000000000000001 R11: 0000000000000001 R12: 00001455d8ad1ce0
[  +0.000001] R13: 000000000f915d71 R14: 00007ffd36d5e2b0 R15: 0000000000000000
[  +0.000004]  </TASK>
[  +0.000004] memory: usage 197120000kB, limit 197120000kB, failcnt 324243
[  +0.000001] swap: usage 0kB, limit 9007199254740988kB, failcnt 0
[  +0.000001] Memory cgroup stats for /system.slice/slurmstepd.scope/job_85884:
[  +0.000223] anon 201315725312
[  +0.000002] file 2199552
[  +0.000001] kernel 532955136
[  +0.000000] kernel_stack 1884160
[  +0.000001] pagetables 439529472
[  +0.000000] sec_pagetables 0
[  +0.000001] percpu 3193232
[  +0.000000] sock 0
[  +0.000001] vmalloc 8192
[  +0.000000] shmem 1249280
[  +0.000001] zswap 0
[  +0.000000] zswapped 0
[  +0.000000] file_mapped 45056
[  +0.000001] file_dirty 0
[  +0.000000] file_writeback 0
[  +0.000001] swapcached 0
[  +0.000000] anon_thp 23777509376
[  +0.000001] file_thp 0
[  +0.000000] shmem_thp 0
[  +0.000001] inactive_anon 122844549120
[  +0.000001] active_anon 78472368128
[  +0.000000] inactive_file 0
[  +0.000001] active_file 950272
[  +0.000000] unevictable 0
[  +0.000001] slab_reclaimable 64941056
[  +0.000000] slab_unreclaimable 22588144
[  +0.000001] slab 87529200
[  +0.000000] workingset_refault_anon 0
[  +0.000001] workingset_refault_file 24804
[  +0.000000] workingset_activate_anon 0
[  +0.000001] workingset_activate_file 1713
[  +0.000000] workingset_restore_anon 0
[  +0.000001] workingset_restore_file 798
[  +0.000000] workingset_nodereclaim 40
[  +0.000001] pgscan 1610363
[  +0.000000] pgsteal 1543928
[  +0.000001] pgscan_kswapd 16512
[  +0.000000] pgscan_direct 1593851
[  +0.000001] pgscan_khugepaged 0
[  +0.000000] pgsteal_kswapd 16512
[  +0.000001] pgsteal_direct 1527416
[  +0.000001] pgsteal_khugepaged 0
[  +0.000000] pgfault 80886020
[  +0.000001] pgmajfault 4068
[  +0.000000] pgrefill 110822
[  +0.000000] pgactivate 32653
[  +0.000001] pgdeactivate 0
[  +0.000000] pglazyfree 556189
[  +0.000001] pglazyfreed 0
[  +0.000000] zswpin 0
[  +0.000001] zswpout 0
[  +0.000000] zswpwb 0
[  +0.000001] thp_fault_alloc 57271
[  +0.000000] thp_collapse_alloc 16
[  +0.000001] thp_swpout 0
[  +0.000000] thp_swpout_fallback 0
[  +0.000000] Tasks state (memory values in pages):
[  +0.000001] [  pid  ]   uid  tgid total_vm      rss pgtables_bytes swapents oom_score_adj name
[  +0.000002] [ 205004]     0 205004    55341      384    49152        0             0 sleep
[  +0.000004] [ 205000]     0 205000    75370     6110   151552        0         -1000 slurmstepd
[  +0.000003] [ 205110] 3122343 205110    55666      768    65536        0             0 slurm_script
[  +0.000002] [ 205135] 3122343 205135   467835     5321   229376        0             0 starter
[  +0.000002] [ 205154] 3122343 205154  4978007   202521  3670016        0             0 llamafactory-cl
[  +0.000002] [ 205169] 3122343 205169   172581     9026   172032        0             0 squashfuse_ll
[  +0.000002] [ 205176] 3122343 205176     6524     5760    94208        0             0 fuse-overlayfs
[  +0.000001] [ 205349] 3122343 205349  4156990    80762  1929216        0             0 python
[  +0.000003] [ 205458] 3122343 205458  4124218    79237   995328        0             0 python
[  +0.000001] [ 205459] 3122343 205459  4124218    79237   995328        0             0 python
[  +0.000002] [ 205460] 3122343 205460  4124218    79237   995328        0             0 python
[  +0.000002] [ 205462] 3122343 205462  4124218    79429   995328        0             0 python
[  +0.000001] [ 205564] 3122343 205564  5390238   605256  5935104        0             0 llamafactory-cl
[  +0.000002] [ 205565] 3122343 205565  5690840   865191  8024064        0             0 llamafactory-cl
[  +0.000002] [ 205566] 3122343 205566  8107024  3187475 26624000        0             0 llamafactory-cl
[  +0.000002] [ 205567] 3122343 205567  6017871   955967  8757248        0             0 llamafactory-cl
[  +0.000001] [ 205568] 3122343 205568  8314159  3179355 26558464        0             0 llamafactory-cl
[  +0.000002] [ 205569] 3122343 205569  7098899  2235518 18972672        0             0 llamafactory-cl
[  +0.000001] [ 205570] 3122343 205570  6476133  1676401 14491648        0             0 llamafactory-cl
[  +0.000002] [ 205571] 3122343 205571 12676646  6664090 54517760        0             0 llamafactory-cl
[  +0.000002] [ 205572] 3122343 205572 11053760  6157351 50438144        0             0 llamafactory-cl
[  +0.000001] [ 205573] 3122343 205573  7181593  1968703 16818176        0             0 llamafactory-cl
[  +0.000002] [ 205574] 3122343 205574  6493238  1586325 13803520        0             0 llamafactory-cl
[  +0.000002] [ 205575] 3122343 205575 10150735  5324586 43741184        0             0 llamafactory-cl
[  +0.000001] [ 205576] 3122343 205576  6902905  1666612 14385152        0             0 llamafactory-cl
[  +0.000002] [ 205577] 3122343 205577 16949388 10131794 82284544        0             0 llamafactory-cl
[  +0.000002] [ 205578] 3122343 205578  6647833  1838390 15785984        0             0 llamafactory-cl
[  +0.000001] [ 205579] 3122343 205579  7914628  3130344 26177536        0             0 llamafactory-cl
[  +0.000003] [ 205583] 3122343 205583  4978015   193869  2310144        0             0 llamafactory-cl
[  +0.000002] [ 205106]     0 205106    75408     5954   155648        0         -1000 slurmstepd
[  +0.000002] oom-kill:constraint=CONSTRAINT_MEMCG,nodemask=(null),cpuset=task_0,mems_allowed=0-3,oom_memcg=/system.slice/slurmstepd.scope/job_85884,task_memcg=/system.slice/slurmstepd.scope/job_85884/step_batch/user/task_0,task=llamafactory-cl,pid=205577,uid=3122343
[  +0.000018] Memory cgroup out of memory: Killed process 205577 (llamafactory-cl) total-vm:67797552kB, anon-rss:40526192kB, file-rss:984kB, shmem-rss:0kB, UID:3122343 pgtables:80356kB oom_score_adj:0
[  +0.077348] llamafactory-cl invoked oom-killer: gfp_mask=0xcc0(GFP_KERNEL), order=0, oom_score_adj=0
[  +0.000006] CPU: 14 PID: 205735 Comm: llamafactory-cl Tainted: P           OE     -------  ---  5.14.0-570.25.1.el9_6.x86_64 #1
[  +0.000003] Hardware name: Lenovo ThinkSystem SD665-N V3/SB27B41167, BIOS QGE140H-8.21 05/13/2025
[  +0.000002] Call Trace:
[  +0.000003]  <TASK>
[  +0.000003]  dump_stack_lvl+0x34/0x48
[  +0.000006]  dump_header+0x4a/0x213
[  +0.000003]  oom_kill_process.cold+0xb/0x10
[  +0.000003]  out_of_memory+0xef/0x2c0
[  +0.000003]  ? srso_alias_return_thunk+0x5/0xfbef5
[  +0.000005]  mem_cgroup_out_of_memory+0x131/0x150
[  +0.000004]  try_charge_memcg+0x742/0x800
[  +0.000004]  ? srso_alias_return_thunk+0x5/0xfbef5
[  +0.000002]  ? alloc_pages_mpol+0x93/0x1e0
[  +0.000003]  ? srso_alias_return_thunk+0x5/0xfbef5
[  +0.000003]  __mem_cgroup_charge+0x3e/0xb0
[  +0.000002]  do_anonymous_page+0x7d/0x3e0
[  +0.000004]  __handle_mm_fault+0x2fe/0x650
[  +0.000005]  handle_mm_fault+0xe9/0x250
[  +0.000003]  do_user_addr_fault+0x154/0x620
[  +0.000004]  exc_page_fault+0x62/0x150
[  +0.000004]  asm_exc_page_fault+0x22/0x30
[  +0.000003] RIP: 0033:0x1458efb7019b
[  +0.000006] Code: Unable to access opcode bytes at 0x1458efb70171.
[  +0.000001] RSP: 002b:00001455cf891c00 EFLAGS: 00010293
[  +0.000002] RAX: 0000145468fa2c40 RBX: 000000000f915d80 RCX: 000000000f915d80
[  +0.000001] RDX: 00001455cf891ce0 RSI: 000014536fe44c40 RDI: 000000000e4a80f0
[  +0.000001] RBP: 00001455cf891c40 R08: 0000000000000001 R09: 0000000000000002
[  +0.000001] R10: 0000000000000001 R11: 0000000000000001 R12: 00001455cf891ce0
[  +0.000001] R13: 000000000f915d71 R14: 00007ffd36d5e2b0 R15: 0000000000000000
[  +0.000004]  </TASK>
[  +0.000001] memory: usage 197120000kB, limit 197120000kB, failcnt 325051
[  +0.000001] swap: usage 0kB, limit 9007199254740988kB, failcnt 0
[  +0.000001] Memory cgroup stats for /system.slice/slurmstepd.scope/job_85884:
[  +0.000132] anon 201279692800
[  +0.000002] file 2256896
[  +0.000000] kernel 532865024
[  +0.000001] kernel_stack 1835008
[  +0.000000] pagetables 439500800
[  +0.000001] sec_pagetables 0
[  +0.000000] percpu 3193232
[  +0.000001] sock 0
[  +0.000000] vmalloc 8192
[  +0.000000] shmem 1249280
[  +0.000001] zswap 0
[  +0.000000] zswapped 0
[  +0.000001] file_mapped 45056
[  +0.000000] file_dirty 0
[  +0.000001] file_writeback 0
[  +0.000000] swapcached 0
[  +0.000001] anon_thp 23764926464
[  +0.000000] file_thp 0
[  +0.000001] shmem_thp 0
[  +0.000000] inactive_anon 200458059776
[  +0.000001] active_anon 858861568
[  +0.000000] inactive_file 0
[  +0.000001] active_file 1007616
[  +0.000000] unevictable 0
[  +0.000001] slab_reclaimable 64937768
[  +0.000001] slab_unreclaimable 22578976
[  +0.000000] slab 87516744
[  +0.000000] workingset_refault_anon 0
[  +0.000001] workingset_refault_file 24848
[  +0.000000] workingset_activate_anon 0
[  +0.000001] workingset_activate_file 1713
[  +0.000000] workingset_restore_anon 0
[  +0.000001] workingset_restore_file 798
[  +0.000000] workingset_nodereclaim 40
[  +0.000001] pgscan 1610385
[  +0.000000] pgsteal 1543950
[  +0.000001] pgscan_kswapd 16512
[  +0.000000] pgscan_direct 1593873
[  +0.000001] pgscan_khugepaged 0
[  +0.000000] pgsteal_kswapd 16512
[  +0.000001] pgsteal_direct 1527438
[  +0.000000] pgsteal_khugepaged 0
[  +0.000000] pgfault 80886021
[  +0.000001] pgmajfault 4069
[  +0.000000] pgrefill 397625
[  +0.000001] pgactivate 32653
[  +0.000000] pgdeactivate 0
[  +0.000001] pglazyfree 556189
[  +0.000000] pglazyfreed 0
[  +0.000001] zswpin 0
[  +0.000000] zswpout 0
[  +0.000000] zswpwb 0
[  +0.000001] thp_fault_alloc 57271
[  +0.000000] thp_collapse_alloc 16
[  +0.000001] thp_swpout 0
[  +0.000000] thp_swpout_fallback 0
[  +0.000001] Tasks state (memory values in pages):
[  +0.000000] [  pid  ]   uid  tgid total_vm      rss pgtables_bytes swapents oom_score_adj name
[  +0.000002] [ 205004]     0 205004    55341      384    49152        0             0 sleep
[  +0.000003] [ 205000]     0 205000    75370     6110   151552        0         -1000 slurmstepd
[  +0.000003] [ 205110] 3122343 205110    55666      768    65536        0             0 slurm_script
[  +0.000002] [ 205135] 3122343 205135   467835     5321   229376        0             0 starter
[  +0.000002] [ 205154] 3122343 205154  4978007   202521  3670016        0             0 llamafactory-cl
[  +0.000002] [ 205169] 3122343 205169   172581     9026   172032        0             0 squashfuse_ll
[  +0.000002] [ 205176] 3122343 205176     6524     5760    94208        0             0 fuse-overlayfs
[  +0.000002] [ 205349] 3122343 205349  4156990    80762  1929216        0             0 python
[  +0.000001] [ 205458] 3122343 205458  4124218    79237   995328        0             0 python
[  +0.000002] [ 205459] 3122343 205459  4124218    79237   995328        0             0 python
[  +0.000002] [ 205460] 3122343 205460  4124218    79237   995328        0             0 python
[  +0.000001] [ 205462] 3122343 205462  4124218    79429   995328        0             0 python
[  +0.000002] [ 205564] 3122343 205564  5390238   605256  5935104        0             0 llamafactory-cl
[  +0.000001] [ 205565] 3122343 205565  5690840   865191  8024064        0             0 llamafactory-cl
[  +0.000002] [ 205566] 3122343 205566  8107024  3187475 26624000        0             0 llamafactory-cl
[  +0.000001] [ 205567] 3122343 205567  6017871   955967  8757248        0             0 llamafactory-cl
[  +0.000002] [ 205568] 3122343 205568  8314159  3179355 26558464        0             0 llamafactory-cl
[  +0.000002] [ 205569] 3122343 205569  7098899  2235518 18972672        0             0 llamafactory-cl
[  +0.000001] [ 205570] 3122343 205570  6476133  1676401 14491648        0             0 llamafactory-cl
[  +0.000002] [ 205571] 3122343 205571 12676646  6664090 54517760        0             0 llamafactory-cl
[  +0.000002] [ 205572] 3122343 205572 11053760  6157351 50438144        0             0 llamafactory-cl
[  +0.000001] [ 205573] 3122343 205573  7181593  1968703 16818176        0             0 llamafactory-cl
[  +0.000002] [ 205574] 3122343 205574  6493238  1586325 13803520        0             0 llamafactory-cl
[  +0.000002] [ 205575] 3122343 205575 10150735  5324586 43741184        0             0 llamafactory-cl
[  +0.000001] [ 205576] 3122343 205576  6902905  1666612 14385152        0             0 llamafactory-cl
[  +0.000002] [ 205578] 3122343 205578  6647833  1838390 15785984        0             0 llamafactory-cl
[  +0.000001] [ 205579] 3122343 205579  7914628  3130344 26177536        0             0 llamafactory-cl
[  +0.000003] [ 205583] 3122343 205583  4978015   193869  2310144        0             0 llamafactory-cl
[  +0.000002] [ 205106]     0 205106    75408     5954   155648        0         -1000 slurmstepd
[  +0.000001] oom-kill:constraint=CONSTRAINT_MEMCG,nodemask=(null),cpuset=task_0,mems_allowed=0-3,oom_memcg=/system.slice/slurmstepd.scope/job_85884,task_memcg=/system.slice/slurmstepd.scope/job_85884/step_batch/user/task_0,task=llamafactory-cl,pid=205571,uid=3122343
[  +0.000021] Memory cgroup out of memory: Killed process 205571 (llamafactory-cl) total-vm:50706584kB, anon-rss:26654608kB, file-rss:1752kB, shmem-rss:0kB, UID:3122343 pgtables:53240kB oom_score_adj:0
[  +3.637205] oom_reaper: reaped process 205577 (llamafactory-cl), now anon-rss:0kB, file-rss:984kB, shmem-rss:0kB
[  +1.041088] oom_reaper: reaped process 205571 (llamafactory-cl), now anon-rss:200kB, file-rss:1752kB, shmem-rss:0kB

