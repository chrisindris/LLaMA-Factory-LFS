
==========
== CUDA ==
==========

CUDA Version 12.4.1

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

[2025-11-13 03:29:54,522] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[INFO|2025-11-13 03:29:57] llamafactory.hparams.parser:423 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.bfloat16
[INFO|hub.py:421] 2025-11-13 03:29:57,437 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2025-11-13 03:29:57,453 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2025-11-13 03:29:57,463 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-11-13 03:29:57,463 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-11-13 03:29:57,463 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-11-13 03:29:57,463 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-11-13 03:29:57,463 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-11-13 03:29:57,463 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-11-13 03:29:57,463 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-11-13 03:29:57,660 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|hub.py:421] 2025-11-13 03:29:57,660 >> Offline mode: forcing local_files_only=True
[INFO|hub.py:421] 2025-11-13 03:29:57,661 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:316] 2025-11-13 03:29:57,663 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2025-11-13 03:29:57,666 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:316] 2025-11-13 03:29:57,669 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2025-11-13 03:29:57,670 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:428] 2025-11-13 03:29:57,679 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:1949] 2025-11-13 03:29:57,679 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2025-11-13 03:29:57,683 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-11-13 03:29:57,683 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-11-13 03:29:57,683 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-11-13 03:29:57,683 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-11-13 03:29:57,683 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-11-13 03:29:57,683 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-11-13 03:29:57,683 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-11-13 03:29:57,843 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:660] 2025-11-13 03:29:57,844 >> Offline mode: forcing local_files_only=True
[INFO|video_processing_utils.py:726] 2025-11-13 03:29:57,847 >> loading configuration file video_preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|video_processing_utils.py:770] 2025-11-13 03:29:57,850 >> Video processor Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:928] 2025-11-13 03:29:57,850 >> Offline mode: forcing local_files_only=True
[INFO|processing_utils.py:1116] 2025-11-13 03:29:57,856 >> loading configuration file processor_config.json from cache at None
[INFO|processing_utils.py:1199] 2025-11-13 03:29:58,137 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2_5_VLProcessor"
}

[INFO|2025-11-13 03:29:58] llamafactory.data.loader:143 >> Loading dataset /scratch/indrisch/huggingface/hub/datasets--cvis-tmu--llamafactory-sqa3d-traces-multiimage-vqa/snapshots/942a5514a2ed64b8ad7ec624ecde74f08884f1c8/data/...
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
Converting format of dataset (num_proc=64):   0%|          | 0/100 [00:00<?, ? examples/s]Converting format of dataset (num_proc=64):   1%|          | 1/100 [00:01<01:43,  1.04s/ examples]Converting format of dataset (num_proc=64):   6%|▌         | 6/100 [00:01<00:13,  6.78 examples/s]Converting format of dataset (num_proc=64):  13%|█▎        | 13/100 [00:01<00:05, 15.79 examples/s]Converting format of dataset (num_proc=64):  19%|█▉        | 19/100 [00:01<00:03, 22.40 examples/s]Converting format of dataset (num_proc=64):  25%|██▌       | 25/100 [00:01<00:02, 29.27 examples/s]Converting format of dataset (num_proc=64):  31%|███       | 31/100 [00:01<00:02, 32.61 examples/s]Converting format of dataset (num_proc=64):  36%|███▌      | 36/100 [00:01<00:01, 35.88 examples/s]Converting format of dataset (num_proc=64):  42%|████▏     | 42/100 [00:01<00:01, 40.21 examples/s]Converting format of dataset (num_proc=64):  52%|█████▏    | 52/100 [00:01<00:00, 54.44 examples/s]Converting format of dataset (num_proc=64):  60%|██████    | 60/100 [00:02<00:00, 60.81 examples/s]Converting format of dataset (num_proc=64):  67%|██████▋   | 67/100 [00:02<00:00, 55.82 examples/s]Converting format of dataset (num_proc=64):  74%|███████▍  | 74/100 [00:02<00:00, 45.09 examples/s]Converting format of dataset (num_proc=64):  80%|████████  | 80/100 [00:02<00:00, 39.21 examples/s]Converting format of dataset (num_proc=64):  85%|████████▌ | 85/100 [00:02<00:00, 35.43 examples/s]Converting format of dataset (num_proc=64):  91%|█████████ | 91/100 [00:02<00:00, 37.18 examples/s]Converting format of dataset (num_proc=64):  96%|█████████▌| 96/100 [00:03<00:00, 26.73 examples/s]Converting format of dataset (num_proc=64): 100%|██████████| 100/100 [00:03<00:00, 20.04 examples/s]Converting format of dataset (num_proc=64): 100%|██████████| 100/100 [00:04<00:00, 24.63 examples/s]
Running tokenizer on dataset (num_proc=64):   0%|          | 0/100 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=64):   1%|          | 1/100 [06:54<11:24:03, 414.58s/ examples]Running tokenizer on dataset (num_proc=64):   2%|▏         | 2/100 [06:58<4:42:36, 173.03s/ examples] Running tokenizer on dataset (num_proc=64):   3%|▎         | 3/100 [07:08<2:39:29, 98.66s/ examples] Running tokenizer on dataset (num_proc=64):   4%|▍         | 4/100 [08:50<2:39:36, 99.75s/ examples]Running tokenizer on dataset (num_proc=64):   5%|▌         | 5/100 [08:51<1:41:30, 64.11s/ examples]Running tokenizer on dataset (num_proc=64):   6%|▌         | 6/100 [09:56<1:41:16, 64.64s/ examples]Running tokenizer on dataset (num_proc=64):   7%|▋         | 7/100 [10:18<1:18:43, 50.80s/ examples]Running tokenizer on dataset (num_proc=64):   9%|▉         | 9/100 [11:46<1:12:00, 47.48s/ examples]Running tokenizer on dataset (num_proc=64):  10%|█         | 10/100 [12:13<1:03:16, 42.18s/ examples][WARNING|tokenization_utils_base.py:4009] 2025-11-13 03:43:45,233 >> Token indices sequence length is longer than the specified maximum sequence length for this model (140872 > 131072). Running this sequence through the model will result in indexing errors
Running tokenizer on dataset (num_proc=64):  11%|█         | 11/100 [13:43<1:21:29, 54.94s/ examples]Running tokenizer on dataset (num_proc=64):  12%|█▏        | 12/100 [14:31<1:17:53, 53.11s/ examples]Running tokenizer on dataset (num_proc=64):  13%|█▎        | 13/100 [15:04<1:08:31, 47.26s/ examples]Running tokenizer on dataset (num_proc=64):  14%|█▍        | 14/100 [15:05<48:48, 34.05s/ examples]  Running tokenizer on dataset (num_proc=64):  15%|█▌        | 15/100 [15:12<37:07, 26.21s/ examples][WARNING|tokenization_utils_base.py:4009] 2025-11-13 03:45:31,016 >> Token indices sequence length is longer than the specified maximum sequence length for this model (147891 > 131072). Running this sequence through the model will result in indexing errors
Running tokenizer on dataset (num_proc=64):  16%|█▌        | 16/100 [15:30<32:58, 23.56s/ examples]Running tokenizer on dataset (num_proc=64):  16%|█▌        | 16/100 [15:40<1:22:16, 58.76s/ examples]
Traceback (most recent call last):
  File "/opt/conda/bin/llamafactory-cli", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/app/src/llamafactory/cli.py", line 24, in main
    launcher.launch()
  File "/app/src/llamafactory/launcher.py", line 152, in launch
    run_exp()
  File "/app/src/llamafactory/train/tuner.py", line 110, in run_exp
    _training_function(config={"args": args, "callbacks": callbacks})
  File "/app/src/llamafactory/train/tuner.py", line 72, in _training_function
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
  File "/app/src/llamafactory/train/sft/workflow.py", line 51, in run_sft
    dataset_module = get_dataset(template, model_args, data_args, training_args, stage="sft", **tokenizer_module)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/src/llamafactory/data/loader.py", line 315, in get_dataset
    dataset = _get_preprocessed_dataset(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/src/llamafactory/data/loader.py", line 256, in _get_preprocessed_dataset
    dataset = dataset.map(
              ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/datasets/arrow_dataset.py", line 560, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/datasets/arrow_dataset.py", line 3309, in map
    for rank, done, content in iflatmap_unordered(
  File "/opt/conda/lib/python3.11/site-packages/datasets/utils/py_utils.py", line 619, in iflatmap_unordered
    raise RuntimeError(
RuntimeError: One of the subprocesses has abruptly died during map operation.To debug the error, disable multiprocessing.
INFO:    Cleanup error: while stopping driver for /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Core/apptainer/1.3.5/var/apptainer/mnt/session/final: fuse-overlayfs exited: fuse: reading device: Software caused connection abort
slurmstepd: error: Detected 1 oom_kill event in StepId=85905.batch. Some of the step tasks have been OOM Killed.

scontrol show job 85905
JobId=85905 JobName=slurm_qwen2_5vl_lora_sft_SQA3D.sh
   UserId=indrisch(3122343) GroupId=indrisch(3122343) MCS_label=N/A
   Priority=329653 Nice=0 Account=def-wangcs QOS=normal
   JobState=COMPLETING Reason=OutOfMemory Dependency=(null)
   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:125
   RunTime=00:16:14 TimeLimit=00:25:00 TimeMin=N/A
   SubmitTime=2025-11-13T03:29:32 EligibleTime=2025-11-13T03:29:32
   AccrueTime=2025-11-13T03:29:32
   StartTime=2025-11-13T03:29:33 EndTime=2025-11-13T03:45:47 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2025-11-13T03:29:33 Scheduler=Main
   Partition=compute AllocNode:Sid=trig-login01:731173
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=trig0023
   BatchHost=trig0023
   NumNodes=1 NumCPUs=4 NumTasks=1 CPUs/Task=4 ReqB:S:C:T=0:0:*:*
   ReqTRES=cpu=4,mem=192500M,node=1,billing=1,gres/gpu=1
   AllocTRES=cpu=4,mem=192500M,node=1,billing=1,gres/gpu=1
   Socks/Node=* NtasksPerN:B:S:C=1:0:*:* CoreSpec=*
   MinCPUsNode=4 MinMemoryNode=192500M MinTmpDiskNode=0
   Features=(null) DelayBoot=00:00:00
   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)
   Command=./slurm_qwen2_5vl_lora_sft_SQA3D.sh
   WorkDir=/scratch/indrisch/LLaMA-Factory/preliminaries/qwen2_5vl_lora_sft_SQA3D
   Comment=/opt/slurm/bin/sbatch --export=NONE --get-user-env=L ./slurm_qwen2_5vl_lora_sft_SQA3D.sh 
   StdErr=/scratch/indrisch/LLaMA-Factory/preliminaries/qwen2_5vl_lora_sft_SQA3D/%N-qwen2_5vl_lora_sft_SQA3D-85905.out
   StdIn=/dev/null
   StdOut=/scratch/indrisch/LLaMA-Factory/preliminaries/qwen2_5vl_lora_sft_SQA3D/%N-qwen2_5vl_lora_sft_SQA3D-85905.out
   TresPerNode=gres/gpu:h100:1
   TresPerTask=cpu=4
   

sacct -j 85905
JobID           JobName    Account    Elapsed  MaxVMSize     MaxRSS  SystemCPU    UserCPU ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- ---------- ---------- -------- 
85905        slurm_qwe+ def-wangcs   00:16:14                         00:00:00   00:00:00      0:0 
85905.batch       batch def-wangcs   00:16:14                         00:00:00   00:00:00      0:0 
85905.extern     extern def-wangcs   00:16:14                         00:00:00   00:00:00      0:0 


kernel messages produced during job executions:
[Nov13 03:42] llamafactory-cl invoked oom-killer: gfp_mask=0xcc0(GFP_KERNEL), order=0, oom_score_adj=0
[  +0.000009] CPU: 13 PID: 681919 Comm: llamafactory-cl Tainted: P           OE     -------  ---  5.14.0-570.25.1.el9_6.x86_64 #1
[  +0.000003] Hardware name: Lenovo ThinkSystem SD665-N V3/SB27B41167, BIOS QGE140H-8.21 05/13/2025
[  +0.000002] Call Trace:
[  +0.000002]  <TASK>
[  +0.000002]  dump_stack_lvl+0x34/0x48
[  +0.000007]  dump_header+0x4a/0x213
[  +0.000003]  oom_kill_process.cold+0xb/0x10
[  +0.000002]  out_of_memory+0xef/0x2c0
[  +0.000004]  ? srso_alias_return_thunk+0x5/0xfbef5
[  +0.000004]  mem_cgroup_out_of_memory+0x131/0x150
[  +0.000005]  try_charge_memcg+0x742/0x800
[  +0.000003]  ? srso_alias_return_thunk+0x5/0xfbef5
[  +0.000002]  ? alloc_pages_mpol+0x93/0x1e0
[  +0.000003]  ? srso_alias_return_thunk+0x5/0xfbef5
[  +0.000003]  __mem_cgroup_charge+0x3e/0xb0
[  +0.000003]  do_anonymous_page+0x7d/0x3e0
[  +0.000003]  __handle_mm_fault+0x2fe/0x650
[  +0.000005]  handle_mm_fault+0xe9/0x250
[  +0.000003]  do_user_addr_fault+0x154/0x620
[  +0.000004]  exc_page_fault+0x62/0x150
[  +0.000004]  asm_exc_page_fault+0x22/0x30
[  +0.000002] RIP: 0033:0x15112737019b
[  +0.000006] Code: Unable to access opcode bytes at 0x151127370171.
[  +0.000000] RSP: 002b:00007ffd079d7e20 EFLAGS: 00010297
[  +0.000002] RAX: 0000150bd8319240 RBX: 0000000000032880 RCX: 0000000000032880
[  +0.000002] RDX: 00007ffd079d7f00 RSI: 0000150d10b54040 RDI: 000000000000f770
[  +0.000000] RBP: 00007ffd079d7e60 R08: 0000000000000230 R09: 00007ffd079d8708
[  +0.000001] R10: 00007ffd079d7ed0 R11: 00007ffd079d7f80 R12: 00007ffd079d7f00
[  +0.000001] R13: 0000000000032871 R14: 00007ffd079d8150 R15: 0000000000000188
[  +0.000005]  </TASK>
[  +0.000003] memory: usage 197120000kB, limit 197120000kB, failcnt 411813
[  +0.000002] swap: usage 0kB, limit 9007199254740988kB, failcnt 0
[  +0.000001] Memory cgroup stats for /system.slice/slurmstepd.scope/job_85905:
[  +0.000343] anon 201065140224
[  +0.000002] file 3338240
[  +0.000000] kernel 782348288
[  +0.000001] kernel_stack 4571136
[  +0.000000] pagetables 544542720
[  +0.000001] sec_pagetables 0
[  +0.000000] percpu 3268496
[  +0.000001] sock 0
[  +0.000000] vmalloc 8192
[  +0.000001] shmem 1249280
[  +0.000000] zswap 0
[  +0.000000] zswapped 0
[  +0.000001] file_mapped 49152
[  +0.000001] file_dirty 0
[  +0.000000] file_writeback 0
[  +0.000001] swapcached 0
[  +0.000000] anon_thp 408944640
[  +0.000001] file_thp 0
[  +0.000000] shmem_thp 0
[  +0.000000] inactive_anon 178239713280
[  +0.000001] active_anon 22826635264
[  +0.000000] inactive_file 802816
[  +0.000001] active_file 1335296
[  +0.000000] unevictable 0
[  +0.000001] slab_reclaimable 151266072
[  +0.000000] slab_unreclaimable 76496000
[  +0.000001] slab 227762072
[  +0.000001] workingset_refault_anon 0
[  +0.000000] workingset_refault_file 12590
[  +0.000001] workingset_activate_anon 0
[  +0.000000] workingset_activate_file 4446
[  +0.000001] workingset_restore_anon 0
[  +0.000000] workingset_restore_file 315
[  +0.000001] workingset_nodereclaim 41088
[  +0.000001] pgscan 3415209
[  +0.000000] pgsteal 3347455
[  +0.000001] pgscan_kswapd 1449856
[  +0.000000] pgscan_direct 1965289
[  +0.000000] pgscan_khugepaged 64
[  +0.000001] pgsteal_kswapd 1449750
[  +0.000000] pgsteal_direct 1897641
[  +0.000001] pgsteal_khugepaged 64
[  +0.000000] pgfault 127940875
[  +0.000001] pgmajfault 3863
[  +0.000000] pgrefill 685803
[  +0.000001] pgactivate 40086
[  +0.000000] pgdeactivate 0
[  +0.000001] pglazyfree 0
[  +0.000000] pglazyfreed 0
[  +0.000001] zswpin 0
[  +0.000000] zswpout 0
[  +0.000001] zswpwb 0
[  +0.000001] thp_fault_alloc 355
[  +0.000000] thp_collapse_alloc 0
[  +0.000001] thp_swpout 0
[  +0.000000] thp_swpout_fallback 0
[  +0.000001] Tasks state (memory values in pages):
[  +0.000000] [  pid  ]   uid  tgid total_vm      rss pgtables_bytes swapents oom_score_adj name
[  +0.000002] [ 681346]     0 681346    55341      384    49152        0             0 sleep
[  +0.000003] [ 681342]     0 681342    75371     6125   151552        0         -1000 slurmstepd
[  +0.000003] [ 681452] 3122343 681452    55666      768    69632        0             0 slurm_script
[  +0.000002] [ 681476] 3122343 681476   467899     4979   225280        0             0 starter
[  +0.000002] [ 681494] 3122343 681494  4933950   202505  3686400        0             0 llamafactory-cl
[  +0.000002] [ 681507] 3122343 681507   172581     5650   176128        0             0 squashfuse_ll
[  +0.000002] [ 681514] 3122343 681514     6523     5760    90112        0             0 fuse-overlayfs
[  +0.000002] [ 681597] 3122343 681597  4156990    80140  1925120        0             0 python
[  +0.000002] [ 681758] 3122343 681758  4124218    79008   995328        0             0 python
[  +0.000002] [ 681759] 3122343 681759  4124218    78816   995328        0             0 python
[  +0.000001] [ 681760] 3122343 681760  4124218    79008   995328        0             0 python
[  +0.000002] [ 681762] 3122343 681762  4124218    78816   995328        0             0 python
[  +0.000002] [ 681899] 3122343 681899  5007512   274605  3268608        0             0 llamafactory-cl
[  +0.000002] [ 681900] 3122343 681900  5156124   422776  4296704        0             0 llamafactory-cl
[  +0.000001] [ 681901] 3122343 681901  8300515  2755254 23142400        0             0 llamafactory-cl
[  +0.000002] [ 681902] 3122343 681902  6794632  1802910 15548416        0             0 llamafactory-cl
[  +0.000002] [ 681903] 3122343 681903  8139272  3115389 26034176        0             0 llamafactory-cl
[  +0.000001] [ 681904] 3122343 681904  5032581   299340  3469312        0             0 llamafactory-cl
[  +0.000002] [ 681905] 3122343 681905  5495402   748408  7016448        0             0 llamafactory-cl
[  +0.000002] [ 681906] 3122343 681906  5050088   298481  3465216        0             0 llamafactory-cl
[  +0.000001] [ 681907] 3122343 681907  5439281   685048  6524928        0             0 llamafactory-cl
[  +0.000002] [ 681908] 3122343 681908  5007513   274299  3268608        0             0 llamafactory-cl
[  +0.000002] [ 681909] 3122343 681909  6589041  1855441 15921152        0             0 llamafactory-cl
[  +0.000002] [ 681910] 3122343 681910  5582166   742876  6987776        0             0 llamafactory-cl
[  +0.000001] [ 681911] 3122343 681911  5016646   283780  3342336        0             0 llamafactory-cl
[  +0.000002] [ 681912] 3122343 681912  5115658   382264  3969024        0             0 llamafactory-cl
[  +0.000001] [ 681913] 3122343 681913  5100210   366648  3846144        0             0 llamafactory-cl
[  +0.000002] [ 681914] 3122343 681914  5007189   273956  3268608        0             0 llamafactory-cl
[  +0.000002] [ 681915] 3122343 681915  5053343   320281  3633152        0             0 llamafactory-cl
[  +0.000002] [ 681916] 3122343 681916  5018068   284953  3354624        0             0 llamafactory-cl
[  +0.000001] [ 681917] 3122343 681917  6101797  1346744 11845632        0             0 llamafactory-cl
[  +0.000002] [ 681918] 3122343 681918  5475910   723960  6840320        0             0 llamafactory-cl
[  +0.000002] [ 681919] 3122343 681919  7193340  2401408 20766720        0             0 llamafactory-cl
[  +0.000002] [ 681920] 3122343 681920  7195011  2194138 18644992        0             0 llamafactory-cl
[  +0.000001] [ 681921] 3122343 681921  4988119   254896  3108864        0             0 llamafactory-cl
[  +0.000002] [ 681922] 3122343 681922  5376052   557304  5484544        0             0 llamafactory-cl
[  +0.000002] [ 681923] 3122343 681923  7435866  2340763 19832832        0             0 llamafactory-cl
[  +0.000001] [ 681924] 3122343 681924  5025060   292868  3424256        0             0 llamafactory-cl
[  +0.000002] [ 681925] 3122343 681925  5117724   383928  3989504        0             0 llamafactory-cl
[  +0.000002] [ 681926] 3122343 681926  7283073  2479209 20926464        0             0 llamafactory-cl
[  +0.000001] [ 681927] 3122343 681927  8641905  3313935 27619328        0             0 llamafactory-cl
[  +0.000002] [ 681928] 3122343 681928  5386669   617208  5971968        0             0 llamafactory-cl
[  +0.000002] [ 681929] 3122343 681929  5096862   363384  3817472        0             0 llamafactory-cl
[  +0.000002] [ 681930] 3122343 681930  5035087   302165  3489792        0             0 llamafactory-cl
[  +0.000001] [ 681931] 3122343 681931  5010049   277080  3289088        0             0 llamafactory-cl
[  +0.000002] [ 681932] 3122343 681932  5098219   330163  3723264        0             0 llamafactory-cl
[  +0.000002] [ 681933] 3122343 681933  5051133   318159  3616768        0             0 llamafactory-cl
[  +0.000001] [ 681934] 3122343 681934  5095225   361848  3805184        0             0 llamafactory-cl
[  +0.000002] [ 681935] 3122343 681935  7979462  2561470 21594112        0             0 llamafactory-cl
[  +0.000002] [ 681936] 3122343 681936  7439045  2322510 19681280        0             0 llamafactory-cl
[  +0.000002] [ 681937] 3122343 681937  5000815   268125  3219456        0             0 llamafactory-cl
[  +0.000001] [ 681938] 3122343 681938  5060805   328769  3706880        0             0 llamafactory-cl
[  +0.000002] [ 681939] 3122343 681939  5019049   286628  3371008        0             0 llamafactory-cl
[  +0.000002] [ 681940] 3122343 681940  5220310   449400  4620288        0             0 llamafactory-cl
[  +0.000001] [ 681941] 3122343 681941  5049592   317003  3620864        0             0 llamafactory-cl
[  +0.000002] [ 681942] 3122343 681942  7718579  2642260 22257664        0             0 llamafactory-cl
[  +0.000002] [ 681943] 3122343 681943  7709285  2774784 23298048        0             0 llamafactory-cl
[  +0.000001] [ 681944] 3122343 681944  5019214   286190  3375104        0             0 llamafactory-cl
[  +0.000002] [ 681945] 3122343 681945  5030842   298797  3469312        0             0 llamafactory-cl
[  +0.000002] [ 681946] 3122343 681946  5447700   694200  6598656        0             0 llamafactory-cl
[  +0.000002] [ 681947] 3122343 681947  5087960   355813  3928064        0             0 llamafactory-cl
[  +0.000001] [ 681948] 3122343 681948  6983046  2183181 18558976        0             0 llamafactory-cl
[  +0.000002] [ 681949] 3122343 681949  5046941   314606  3600384        0             0 llamafactory-cl
[  +0.000001] [ 681950] 3122343 681950  5061806   329973  3715072        0             0 llamafactory-cl
[  +0.000002] [ 681951] 3122343 681951  5078501   346237  3850240        0             0 llamafactory-cl
[  +0.000002] [ 681952] 3122343 681952  5776878  1043733  9404416        0             0 llamafactory-cl
[  +0.000002] [ 681953] 3122343 681953  5113716   380088  3952640        0             0 llamafactory-cl
[  +0.000001] [ 681954] 3122343 681954  5007459   274393  3276800        0             0 llamafactory-cl
[  +0.000002] [ 681955] 3122343 681955  5393861   531960  5287936        0             0 llamafactory-cl
[  +0.000002] [ 681956] 3122343 681956  4980722   248305  3059712        0             0 llamafactory-cl
[  +0.000001] [ 681957] 3122343 681957  5578243   845765  7827456        0             0 llamafactory-cl
[  +0.000002] [ 681958] 3122343 681958  4979269   247130  3047424        0             0 llamafactory-cl
[  +0.000002] [ 681959] 3122343 681959  4985482   252756  3096576        0             0 llamafactory-cl
[  +0.000001] [ 681960] 3122343 681960  5164405   431160  4452352        0             0 llamafactory-cl
[  +0.000002] [ 681961] 3122343 681961  5128102   394680  4067328        0             0 llamafactory-cl
[  +0.000002] [ 681962] 3122343 681962  7445587  2678655 22528000        0             0 llamafactory-cl
[  +0.000005] [ 681966] 3122343 681966  4929295   194427  2383872        0             0 llamafactory-cl
[  +0.000002] [ 681448]     0 681448    75408     5988   155648        0         -1000 slurmstepd
[  +0.000002] oom-kill:constraint=CONSTRAINT_MEMCG,nodemask=(null),cpuset=task_0,mems_allowed=0-3,oom_memcg=/system.slice/slurmstepd.scope/job_85905,task_memcg=/system.slice/slurmstepd.scope/job_85905/step_batch/user/task_0,task=llamafactory-cl,pid=681927,uid=3122343
[  +0.000022] Memory cgroup out of memory: Killed process 681927 (llamafactory-cl) total-vm:34567620kB, anon-rss:13254756kB, file-rss:984kB, shmem-rss:0kB, UID:3122343 pgtables:26972kB oom_score_adj:0
[  +2.695041] oom_reaper: reaped process 681927 (llamafactory-cl), now anon-rss:752kB, file-rss:984kB, shmem-rss:0kB

