
==========
== CUDA ==
==========

CUDA Version 12.4.1

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

[2025-11-13 02:19:59,772] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[INFO|2025-11-13 02:20:02] llamafactory.hparams.parser:423 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.bfloat16
[INFO|hub.py:421] 2025-11-13 02:20:02,152 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2025-11-13 02:20:02,159 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2025-11-13 02:20:02,164 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-11-13 02:20:02,164 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-11-13 02:20:02,164 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-11-13 02:20:02,164 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-11-13 02:20:02,164 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-11-13 02:20:02,164 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-11-13 02:20:02,164 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-11-13 02:20:02,369 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|hub.py:421] 2025-11-13 02:20:02,369 >> Offline mode: forcing local_files_only=True
[INFO|hub.py:421] 2025-11-13 02:20:02,370 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:316] 2025-11-13 02:20:02,371 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2025-11-13 02:20:02,372 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:316] 2025-11-13 02:20:02,375 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2025-11-13 02:20:02,376 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:428] 2025-11-13 02:20:02,381 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:1949] 2025-11-13 02:20:02,381 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2025-11-13 02:20:02,385 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-11-13 02:20:02,385 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-11-13 02:20:02,385 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-11-13 02:20:02,385 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-11-13 02:20:02,385 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-11-13 02:20:02,385 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-11-13 02:20:02,385 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-11-13 02:20:02,560 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:660] 2025-11-13 02:20:02,562 >> Offline mode: forcing local_files_only=True
[INFO|video_processing_utils.py:726] 2025-11-13 02:20:02,564 >> loading configuration file video_preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|video_processing_utils.py:770] 2025-11-13 02:20:02,567 >> Video processor Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:928] 2025-11-13 02:20:02,567 >> Offline mode: forcing local_files_only=True
[INFO|processing_utils.py:1116] 2025-11-13 02:20:02,571 >> loading configuration file processor_config.json from cache at None
[INFO|processing_utils.py:1199] 2025-11-13 02:20:02,887 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2_5_VLProcessor"
}

[INFO|2025-11-13 02:20:02] llamafactory.data.loader:143 >> Loading dataset /scratch/indrisch/huggingface/hub/datasets--cvis-tmu--llamafactory-sqa3d-traces-multiimage-vqa/snapshots/57a4793962153e37fb77a681f2a8d0b685539997/data/...
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
Converting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 1001 examples [00:00,  2.31 examples/s]           Converting format of dataset (num_proc=16): 1005 examples [00:00, 11.54 examples/s]Converting format of dataset (num_proc=16): 1012 examples [00:00, 25.79 examples/s]Converting format of dataset (num_proc=16): 1017 examples [00:00, 29.95 examples/s]Converting format of dataset (num_proc=16): 1024 examples [00:00, 39.96 examples/s]Converting format of dataset (num_proc=16): 1030 examples [00:01, 32.61 examples/s]Converting format of dataset (num_proc=16): 1037 examples [00:01, 37.79 examples/s]Converting format of dataset (num_proc=16): 1042 examples [00:01, 30.77 examples/s]Converting format of dataset (num_proc=16): 1046 examples [00:01, 27.46 examples/s]Converting format of dataset (num_proc=16): 1055 examples [00:01, 39.21 examples/s]Converting format of dataset (num_proc=16): 1062 examples [00:01, 45.19 examples/s]Converting format of dataset (num_proc=16): 1068 examples [00:02, 42.50 examples/s]Converting format of dataset (num_proc=16): 1074 examples [00:02, 46.07 examples/s]Converting format of dataset (num_proc=16): 1080 examples [00:02, 43.03 examples/s]Converting format of dataset (num_proc=16): 1087 examples [00:02, 48.09 examples/s]Converting format of dataset (num_proc=16): 1093 examples [00:02, 36.24 examples/s]Converting format of dataset (num_proc=16): 1098 examples [00:02, 38.12 examples/s]Converting format of dataset (num_proc=16): 1104 examples [00:02, 37.45 examples/s]Converting format of dataset (num_proc=16): 1111 examples [00:03, 44.30 examples/s]Converting format of dataset (num_proc=16): 1116 examples [00:03, 42.26 examples/s]Converting format of dataset (num_proc=16): 1124 examples [00:03, 50.75 examples/s]Converting format of dataset (num_proc=16): 1132 examples [00:03, 55.77 examples/s]Converting format of dataset (num_proc=16): 1141 examples [00:03, 62.38 examples/s]Converting format of dataset (num_proc=16): 1149 examples [00:03, 53.46 examples/s]Converting format of dataset (num_proc=16): 1156 examples [00:03, 52.89 examples/s]Converting format of dataset (num_proc=16): 1170 examples [00:04, 67.20 examples/s]Converting format of dataset (num_proc=16): 1177 examples [00:04, 66.13 examples/s]Converting format of dataset (num_proc=16): 1186 examples [00:04, 56.28 examples/s]Converting format of dataset (num_proc=16): 1195 examples [00:04, 63.27 examples/s]Converting format of dataset (num_proc=16): 1203 examples [00:04, 65.74 examples/s]Converting format of dataset (num_proc=16): 1212 examples [00:04, 63.70 examples/s]Converting format of dataset (num_proc=16): 1219 examples [00:04, 64.22 examples/s]Converting format of dataset (num_proc=16): 1230 examples [00:04, 74.49 examples/s]Converting format of dataset (num_proc=16): 1239 examples [00:05, 69.99 examples/s]Converting format of dataset (num_proc=16): 1247 examples [00:05, 62.41 examples/s]Converting format of dataset (num_proc=16): 1258 examples [00:05, 70.86 examples/s]Converting format of dataset (num_proc=16): 1276 examples [00:05, 95.38 examples/s]Converting format of dataset (num_proc=16): 1287 examples [00:05, 93.11 examples/s]Converting format of dataset (num_proc=16): 1297 examples [00:05, 77.05 examples/s]Converting format of dataset (num_proc=16): 1306 examples [00:05, 75.54 examples/s]Converting format of dataset (num_proc=16): 1316 examples [00:06, 74.90 examples/s]Converting format of dataset (num_proc=16): 1325 examples [00:06, 77.20 examples/s]Converting format of dataset (num_proc=16): 1336 examples [00:06, 64.83 examples/s]Converting format of dataset (num_proc=16): 1349 examples [00:06, 77.52 examples/s]Converting format of dataset (num_proc=16): 1358 examples [00:06, 69.23 examples/s]Converting format of dataset (num_proc=16): 1371 examples [00:06, 82.14 examples/s]Converting format of dataset (num_proc=16): 1385 examples [00:06, 87.19 examples/s]Converting format of dataset (num_proc=16): 1395 examples [00:07, 83.96 examples/s]Converting format of dataset (num_proc=16): 1404 examples [00:07, 83.03 examples/s]Converting format of dataset (num_proc=16): 1415 examples [00:07, 83.93 examples/s]Converting format of dataset (num_proc=16): 1429 examples [00:07, 95.94 examples/s]Converting format of dataset (num_proc=16): 1442 examples [00:07, 103.59 examples/s]Converting format of dataset (num_proc=16): 1454 examples [00:07, 96.29 examples/s] Converting format of dataset (num_proc=16): 1467 examples [00:07, 104.02 examples/s]Converting format of dataset (num_proc=16): 1480 examples [00:07, 105.34 examples/s]Converting format of dataset (num_proc=16): 1498 examples [00:07, 118.12 examples/s]Converting format of dataset (num_proc=16): 1510 examples [00:08, 116.10 examples/s]Converting format of dataset (num_proc=16): 1522 examples [00:08, 111.99 examples/s]Converting format of dataset (num_proc=16): 1536 examples [00:08, 115.94 examples/s]Converting format of dataset (num_proc=16): 1549 examples [00:08, 96.57 examples/s] Converting format of dataset (num_proc=16): 1566 examples [00:08, 113.67 examples/s]Converting format of dataset (num_proc=16): 1596 examples [00:08, 160.82 examples/s]Converting format of dataset (num_proc=16): 1615 examples [00:08, 127.76 examples/s]Converting format of dataset (num_proc=16): 1633 examples [00:09, 102.13 examples/s]Converting format of dataset (num_proc=16): 1646 examples [00:09, 91.47 examples/s] Converting format of dataset (num_proc=16): 1659 examples [00:09, 87.46 examples/s]Converting format of dataset (num_proc=16): 1669 examples [00:09, 68.45 examples/s]Converting format of dataset (num_proc=16): 1680 examples [00:09, 71.56 examples/s]Converting format of dataset (num_proc=16): 1694 examples [00:10, 81.69 examples/s]Converting format of dataset (num_proc=16): 1708 examples [00:10, 92.78 examples/s]Converting format of dataset (num_proc=16): 1719 examples [00:10, 95.67 examples/s]Converting format of dataset (num_proc=16): 1731 examples [00:10, 100.97 examples/s]Converting format of dataset (num_proc=16): 1746 examples [00:10, 110.62 examples/s]Converting format of dataset (num_proc=16): 1758 examples [00:10, 104.73 examples/s]Converting format of dataset (num_proc=16): 1771 examples [00:10, 107.96 examples/s]Converting format of dataset (num_proc=16): 1784 examples [00:10, 110.69 examples/s]Converting format of dataset (num_proc=16): 1813 examples [00:10, 153.27 examples/s]Converting format of dataset (num_proc=16): 1842 examples [00:11, 185.19 examples/s]Converting format of dataset (num_proc=16): 1861 examples [00:11, 164.16 examples/s]Converting format of dataset (num_proc=16): 1881 examples [00:11, 167.34 examples/s]Converting format of dataset (num_proc=16): 1902 examples [00:11, 144.55 examples/s]Converting format of dataset (num_proc=16): 1919 examples [00:11, 103.51 examples/s]Converting format of dataset (num_proc=16): 1934 examples [00:12, 91.42 examples/s] Converting format of dataset (num_proc=16): 1946 examples [00:12, 95.93 examples/s]Converting format of dataset (num_proc=16): 1959 examples [00:12, 72.82 examples/s]Converting format of dataset (num_proc=16): 1975 examples [00:12, 73.20 examples/s]Converting format of dataset (num_proc=16): 1985 examples [00:12, 70.90 examples/s]Converting format of dataset (num_proc=16): 1995 examples [00:13, 62.19 examples/s]Converting format of dataset (num_proc=16): 2000 examples [00:13, 75.44 examples/s]
Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:01<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:01<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:01<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:01<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:01<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:02<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:02<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:02<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:02<?, ? examples/s]
multiprocess.pool.RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
                    ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/datasets/utils/py_utils.py", line 586, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/opt/conda/lib/python3.11/site-packages/datasets/arrow_dataset.py", line 3674, in _map_single
    for i, batch in iter_outputs(shard_iterable):
  File "/opt/conda/lib/python3.11/site-packages/datasets/arrow_dataset.py", line 3624, in iter_outputs
    yield i, apply_function(example, i, offset=offset)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/datasets/arrow_dataset.py", line 3547, in apply_function
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/src/llamafactory/data/processor/supervised.py", line 99, in preprocess_dataset
    input_ids, labels = self._encode_data_example(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/src/llamafactory/data/processor/supervised.py", line 43, in _encode_data_example
    messages = self.template.mm_plugin.process_messages(prompt + response, images, videos, audios, self.processor)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/src/llamafactory/data/mm_plugin.py", line 1499, in process_messages
    self._validate_messages(messages, images, videos, audios)
  File "/app/src/llamafactory/data/mm_plugin.py", line 202, in _validate_messages
    raise ValueError(
ValueError: The number of images does not match the number of <image> tokens in [{'content': "What is on top of the bathroom cabinet that is on my 11 o'clock?", 'role': 'user'}, {'content': '\nGot it, let\'s try to figure out what\'s on top of the bathroom cabinet at 11 o\'clock. First, let\'s visualize the bathroom setup. The cabinet is likely a white, two-drawer unit. At 11 o\'clock (top-left area), we need to check that spot. Looking at the images, especially the one showing the cabinet area, there\'s a mirror above the sink, but wait, no—wait, the question is about the cabinet. Wait, maybe the "cabinet" here is the storage unit. Wait, no, the sink has a cabinet below? Wait, no, the main cabinet is the white drawers. Wait, in the image where the camera shows the sink and cabinet, the top of the cabinet—wait, no, maybe the mirror? Wait, no, let\'s check again. Wait, the user says "bathroom cabinet that is on my 11 o\'clock". So 11 o\'clock would be the upper left area relative to the sink. Looking at the images, there\'s a mirror mounted on the wall above the sink area. Is that part of the cabinet setup? Wait, no, maybe the cabinet is the white drawers under the sink. Wait, no, the top of that cabinet—wait, no, in some images, there\'s a mirror reflected or mounted. Wait, no, let\'s check the first few images. The shower curtain is colorful, then the sink area. Wait, the correct answer: looking at the images, the object on top of the bathroom cabinet (the white drawer unit) at 11 o\'clock is a mirror. Wait, no, wait—the mirror is on the wall above the sink. Wait, maybe I misread. Wait, no, let\'s check again. Wait, the user\'s question: "What is on top of the bathroom cabinet that is on my 11 o\'clock?" So first, identify the cabinet. The white drawer unit near 11 o\'clock (top-left). What\'s on top? Looking at the images, there\'s a mirror mounted on the wall above that area. Wait, yes! In the reflection or mounted, there\'s a mirror. So the answer is a mirror.\n\n\nTo determine what is on top of the bathroom cabinet at 11 o’clock, analyze the spatial layout and elements in the images:  \n\n1. Locate the “bathroom cabinet” in question: A white, two - drawer storage unit is visible near the top - left (11 o’clock) area relative to other fixtures like the sink and shower.  \n2. Examine objects above this cabinet: In the images, a **mirror** is mounted on the wall above this cabinet area.  \n\n\nMirror', 'role': 'assistant'}].
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/bin/llamafactory-cli", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/app/src/llamafactory/cli.py", line 24, in main
    launcher.launch()
  File "/app/src/llamafactory/launcher.py", line 152, in launch
    run_exp()
  File "/app/src/llamafactory/train/tuner.py", line 110, in run_exp
    _training_function(config={"args": args, "callbacks": callbacks})
  File "/app/src/llamafactory/train/tuner.py", line 72, in _training_function
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
  File "/app/src/llamafactory/train/sft/workflow.py", line 51, in run_sft
    dataset_module = get_dataset(template, model_args, data_args, training_args, stage="sft", **tokenizer_module)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/src/llamafactory/data/loader.py", line 315, in get_dataset
    dataset = _get_preprocessed_dataset(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/app/src/llamafactory/data/loader.py", line 256, in _get_preprocessed_dataset
    dataset = dataset.map(
              ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/datasets/arrow_dataset.py", line 560, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/datasets/arrow_dataset.py", line 3309, in map
    for rank, done, content in iflatmap_unordered(
  File "/opt/conda/lib/python3.11/site-packages/datasets/utils/py_utils.py", line 626, in iflatmap_unordered
    [async_result.get(timeout=0.05) for async_result in async_results]
  File "/opt/conda/lib/python3.11/site-packages/datasets/utils/py_utils.py", line 626, in <listcomp>
    [async_result.get(timeout=0.05) for async_result in async_results]
     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/multiprocess/pool.py", line 774, in get
    raise self._value
ValueError: The number of images does not match the number of <image> tokens in [{'content': "What is on top of the bathroom cabinet that is on my 11 o'clock?", 'role': 'user'}, {'content': '\nGot it, let\'s try to figure out what\'s on top of the bathroom cabinet at 11 o\'clock. First, let\'s visualize the bathroom setup. The cabinet is likely a white, two-drawer unit. At 11 o\'clock (top-left area), we need to check that spot. Looking at the images, especially the one showing the cabinet area, there\'s a mirror above the sink, but wait, no—wait, the question is about the cabinet. Wait, maybe the "cabinet" here is the storage unit. Wait, no, the sink has a cabinet below? Wait, no, the main cabinet is the white drawers. Wait, in the image where the camera shows the sink and cabinet, the top of the cabinet—wait, no, maybe the mirror? Wait, no, let\'s check again. Wait, the user says "bathroom cabinet that is on my 11 o\'clock". So 11 o\'clock would be the upper left area relative to the sink. Looking at the images, there\'s a mirror mounted on the wall above the sink area. Is that part of the cabinet setup? Wait, no, maybe the cabinet is the white drawers under the sink. Wait, no, the top of that cabinet—wait, no, in some images, there\'s a mirror reflected or mounted. Wait, no, let\'s check the first few images. The shower curtain is colorful, then the sink area. Wait, the correct answer: looking at the images, the object on top of the bathroom cabinet (the white drawer unit) at 11 o\'clock is a mirror. Wait, no, wait—the mirror is on the wall above the sink. Wait, maybe I misread. Wait, no, let\'s check again. Wait, the user\'s question: "What is on top of the bathroom cabinet that is on my 11 o\'clock?" So first, identify the cabinet. The white drawer unit near 11 o\'clock (top-left). What\'s on top? Looking at the images, there\'s a mirror mounted on the wall above that area. Wait, yes! In the reflection or mounted, there\'s a mirror. So the answer is a mirror.\n\n\nTo determine what is on top of the bathroom cabinet at 11 o’clock, analyze the spatial layout and elements in the images:  \n\n1. Locate the “bathroom cabinet” in question: A white, two - drawer storage unit is visible near the top - left (11 o’clock) area relative to other fixtures like the sink and shower.  \n2. Examine objects above this cabinet: In the images, a **mirror** is mounted on the wall above this cabinet area.  \n\n\nMirror', 'role': 'assistant'}].

scontrol show job 85878
JobId=85878 JobName=slurm_qwen2_5vl_lora_sft_SQA3D.sh
   UserId=indrisch(3122343) GroupId=indrisch(3122343) MCS_label=N/A
   Priority=329653 Nice=0 Account=def-wangcs QOS=normal
   JobState=COMPLETING Reason=NonZeroExitCode Dependency=(null)
   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=1:0
   RunTime=00:00:34 TimeLimit=00:25:00 TimeMin=N/A
   SubmitTime=2025-11-13T02:19:45 EligibleTime=2025-11-13T02:19:45
   AccrueTime=2025-11-13T02:19:45
   StartTime=2025-11-13T02:19:46 EndTime=2025-11-13T02:20:20 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2025-11-13T02:19:46 Scheduler=Main
   Partition=compute AllocNode:Sid=trig-login01:731173
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=trig0011
   BatchHost=trig0011
   NumNodes=1 NumCPUs=4 NumTasks=1 CPUs/Task=4 ReqB:S:C:T=0:0:*:*
   ReqTRES=cpu=4,mem=192500M,node=1,billing=1,gres/gpu=1
   AllocTRES=cpu=4,mem=192500M,node=1,billing=1,gres/gpu=1
   Socks/Node=* NtasksPerN:B:S:C=1:0:*:* CoreSpec=*
   MinCPUsNode=4 MinMemoryNode=192500M MinTmpDiskNode=0
   Features=(null) DelayBoot=00:00:00
   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)
   Command=./slurm_qwen2_5vl_lora_sft_SQA3D.sh
   WorkDir=/scratch/indrisch/LLaMA-Factory/preliminaries/qwen2_5vl_lora_sft_SQA3D
   Comment=/opt/slurm/bin/sbatch --export=NONE --get-user-env=L ./slurm_qwen2_5vl_lora_sft_SQA3D.sh 
   StdErr=/scratch/indrisch/LLaMA-Factory/preliminaries/qwen2_5vl_lora_sft_SQA3D/%N-qwen2_5vl_lora_sft_SQA3D-85878.out
   StdIn=/dev/null
   StdOut=/scratch/indrisch/LLaMA-Factory/preliminaries/qwen2_5vl_lora_sft_SQA3D/%N-qwen2_5vl_lora_sft_SQA3D-85878.out
   TresPerNode=gres/gpu:h100:1
   TresPerTask=cpu=4
   

sacct -j 85878
JobID           JobName    Account    Elapsed  MaxVMSize     MaxRSS  SystemCPU    UserCPU ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- ---------- ---------- -------- 
85878        slurm_qwe+ def-wangcs   00:00:34                         00:00:00   00:00:00      0:0 
85878.batch       batch def-wangcs   00:00:34                         00:00:00   00:00:00      0:0 
85878.extern     extern def-wangcs   00:00:34                         00:00:00   00:00:00      0:0 

