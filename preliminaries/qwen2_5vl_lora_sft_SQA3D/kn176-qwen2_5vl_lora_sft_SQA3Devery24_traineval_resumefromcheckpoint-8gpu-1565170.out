
==========
== CUDA ==
==========

CUDA Version 12.4.1

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

[INFO|2025-12-09 01:29:16] llamafactory.launcher:143 >> Initializing 8 distributed tasks at: 127.0.0.1:42755
W1209 01:29:18.017000 3082853 site-packages/torch/distributed/run.py:792] 
W1209 01:29:18.017000 3082853 site-packages/torch/distributed/run.py:792] *****************************************
W1209 01:29:18.017000 3082853 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1209 01:29:18.017000 3082853 site-packages/torch/distributed/run.py:792] *****************************************
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
[2025-12-09 01:29:30,676] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-09 01:29:30,676] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-09 01:29:30,676] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-09 01:29:30,676] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-09 01:29:30,676] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-09 01:29:30,676] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-09 01:29:30,676] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-09 01:29:30,677] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2025-12-09 01:29:37,582] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-09 01:29:37,590] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-09 01:29:37,611] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-09 01:29:37,624] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-09 01:29:37,625] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-12-09 01:29:37,629] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-09 01:29:37,754] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-09 01:29:37,805] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-09 01:29:37,907] [INFO] [comm.py:669:init_distributed] cdb=None
[INFO|2025-12-09 01:29:39] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.
[INFO|2025-12-09 01:29:39] llamafactory.hparams.parser:423 >> Process rank: 0, world size: 8, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|hub.py:421] 2025-12-09 01:29:39,089 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2025-12-09 01:29:39,119 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2025-12-09 01:29:39,130 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-12-09 01:29:39,130 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-12-09 01:29:39,130 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-12-09 01:29:39,131 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-09 01:29:39,131 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-09 01:29:39,131 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-12-09 01:29:39,131 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-12-09 01:29:39,518 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|hub.py:421] 2025-12-09 01:29:39,519 >> Offline mode: forcing local_files_only=True
[INFO|hub.py:421] 2025-12-09 01:29:39,520 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:316] 2025-12-09 01:29:39,521 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2025-12-09 01:29:39,525 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:316] 2025-12-09 01:29:39,530 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2025-12-09 01:29:39,531 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:428] 2025-12-09 01:29:39,542 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:1949] 2025-12-09 01:29:39,542 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2025-12-09 01:29:39,547 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-12-09 01:29:39,547 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-12-09 01:29:39,547 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-12-09 01:29:39,547 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-09 01:29:39,547 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-09 01:29:39,547 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-12-09 01:29:39,547 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-12-09 01:29:39,747 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:660] 2025-12-09 01:29:39,750 >> Offline mode: forcing local_files_only=True
[INFO|video_processing_utils.py:726] 2025-12-09 01:29:39,752 >> loading configuration file video_preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|video_processing_utils.py:770] 2025-12-09 01:29:39,758 >> Video processor Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:928] 2025-12-09 01:29:39,758 >> Offline mode: forcing local_files_only=True
[INFO|processing_utils.py:1116] 2025-12-09 01:29:39,765 >> loading configuration file processor_config.json from cache at None
[INFO|processing_utils.py:1199] 2025-12-09 01:29:40,099 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2_5_VLProcessor"
}

[INFO|2025-12-09 01:29:40] llamafactory.data.loader:143 >> Loading dataset /scratch/indrisch/huggingface/hub/datasets--cvis-tmu--llamafactory-sqa3d-traces-multiimage-vqa/snapshots/ce5c54adc1608d1726730c9ff334e65b6dd70e46/data/...
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
[INFO|2025-12-09 01:29:40] llamafactory.hparams.parser:423 >> Process rank: 3, world size: 8, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-12-09 01:29:40] llamafactory.hparams.parser:423 >> Process rank: 4, world size: 8, device: cuda:4, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-12-09 01:29:40] llamafactory.hparams.parser:423 >> Process rank: 7, world size: 8, device: cuda:7, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-12-09 01:29:40] llamafactory.hparams.parser:423 >> Process rank: 1, world size: 8, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-12-09 01:29:40] llamafactory.hparams.parser:423 >> Process rank: 6, world size: 8, device: cuda:6, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-12-09 01:29:40] llamafactory.hparams.parser:423 >> Process rank: 2, world size: 8, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-12-09 01:29:40] llamafactory.hparams.parser:423 >> Process rank: 5, world size: 8, device: cuda:5, distributed training: True, compute dtype: torch.bfloat16
[rank3]:[W1209 01:29:41.110637285 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank4]:[W1209 01:29:41.123670582 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W1209 01:29:41.216251836 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W1209 01:29:41.219627605 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank7]:[W1209 01:29:41.224070358 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank5]:[W1209 01:29:41.237992162 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank0]:[W1209 01:29:41.258829347 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank6]:[W1209 01:29:41.270106300 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
training example:
input_ids:
[151644, 8948, 271, 262, 1446, 525, 264, 16585, 11129, 4142, 11528, 17847, 369, 27979, 647, 47522, 389, 264, 3175, 29519, 6109, 6839, 3941, 5248, 5335, 382, 262, 26149, 510, 262, 68252, 1283, 419, 1943, 11, 498, 686, 5258, 451, 6109, 5335, 438, 366, 1805, 29, 9492, 13, 18877, 1105, 438, 2155, 6194, 315, 279, 83490, 6109, 13, 28634, 311, 5335, 553, 220, 16, 5980, 1922, 304, 1973, 315, 11094, 25, 508, 16, 1125, 508, 17, 1125, 4593, 11, 508, 45, 29562, 262, 3596, 35304, 1718, 8575, 38439, 3298, 14017, 510, 262, 7288, 7854, 264, 12966, 220, 18, 35, 10502, 1614, 3941, 6194, 25, 3754, 6171, 3941, 5335, 11, 23583, 8674, 6249, 17040, 11, 323, 990, 5452, 14, 509, 8957, 14688, 21060, 369, 7990, 55916, 624, 262, 7288, 72077, 409, 849, 292, 3793, 26087, 983, 847, 1290, 97089, 17996, 4065, 14, 29898, 484, 32511, 56111, 7612, 1825, 35978, 28455, 448, 5091, 311, 279, 64171, 14, 8092, 13, 1416, 279, 58385, 374, 54761, 11, 1638, 311, 279, 1429, 38219, 8622, 1651, 323, 1977, 773, 624, 262, 7288, 5443, 1172, 9434, 5904, 26, 653, 537, 17023, 63133, 3565, 476, 17188, 389, 9250, 6540, 7797, 6770, 1633, 43898, 3793, 624, 262, 7288, 3197, 5248, 11178, 3000, 11, 8830, 553, 279, 1850, 2432, 311, 27979, 17133, 488, 4274, 1835, 3941, 6194, 13, 1416, 2058, 54761, 1283, 13295, 678, 5335, 11, 1584, 429, 9355, 382, 262, 30990, 320, 327, 32739, 1378, 10010, 11, 304, 419, 1973, 982, 262, 366, 26865, 397, 262, 14822, 14319, 29208, 32711, 304, 220, 18, 4142, 16, 15, 2805, 7354, 13, 356, 632, 5904, 448, 2168, 14937, 320, 68, 1302, 2572, 65750, 18, 5669, 3100, 7579, 18010, 1290, 315, 8718, 26, 508, 16, 5669, 1852, 1894, 11, 42396, 64212, 2823, 63594, 714, 4583, 320, 2640, 37294, 17, 20, 15, 11211, 7241, 5871, 568, 7036, 894, 17796, 8957, 14, 2969, 26745, 487, 323, 1246, 498, 19673, 432, 624, 262, 690, 26865, 397, 262, 366, 9217, 397, 262, 362, 3175, 63594, 4226, 1172, 11, 892, 1231, 387, 510, 262, 7288, 1416, 14016, 25, 3776, 37328, 4226, 11, 4504, 320, 68, 1302, 2572, 1036, 17, 32511, 476, 7414, 14, 2753, 320, 68, 1302, 2572, 1036, 9693, 854, 4292, 262, 7288, 1416, 537, 14016, 25, 362, 11652, 476, 1378, 624, 262, 1416, 4226, 537, 9434, 11, 421, 1052, 374, 902, 2797, 5904, 11, 476, 421, 279, 4226, 374, 35197, 54761, 25, 1036, 33260, 8253, 854, 624, 262, 690, 9217, 1339, 262, 62947, 609, 43797, 50, 510, 262, 7288, 3155, 4183, 13153, 279, 3405, 476, 1140, 5335, 26, 653, 4183, 2550, 4718, 476, 4960, 14158, 624, 262, 7288, 84468, 4285, 1894, 5036, 448, 518, 1429, 825, 22739, 26087, 4145, 3446, 838, 4322, 1574, 58689, 7256, 854, 4292, 262, 7288, 2823, 12966, 3941, 6194, 26, 421, 6194, 28295, 11, 62552, 279, 11299, 15432, 5904, 323, 5185, 432, 304, 366, 26865, 29816, 257, 151645, 198, 151644, 872, 198, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 30, 220, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151645, 198, 151644, 77091, 198, 198, 32462, 432, 11, 1077, 594, 1430, 311, 7071, 700, 1128, 594, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 62410, 13, 5512, 11, 1077, 594, 50087, 279, 14852, 6505, 13, 576, 21921, 374, 4363, 264, 4158, 11, 1378, 1737, 95112, 4982, 13, 2411, 220, 16, 16, 297, 62410, 320, 3481, 7950, 3082, 701, 582, 1184, 311, 1779, 429, 7702, 13, 21131, 518, 279, 5335, 11, 5310, 279, 825, 9027, 279, 21921, 3082, 11, 1052, 594, 264, 17846, 3403, 279, 19309, 11, 714, 3783, 11, 902, 2293, 11489, 11, 279, 3405, 374, 911, 279, 21921, 13, 13824, 11, 7196, 279, 330, 66, 75512, 1, 1588, 374, 279, 5819, 4982, 13, 13824, 11, 902, 11, 279, 19309, 702, 264, 21921, 3685, 30, 13824, 11, 902, 11, 279, 1887, 21921, 374, 279, 4158, 61607, 13, 13824, 11, 304, 279, 2168, 1380, 279, 6249, 4933, 279, 19309, 323, 21921, 11, 279, 1909, 315, 279, 21921, 2293, 11489, 11, 902, 11, 7196, 279, 17846, 30, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 2727, 330, 65, 77832, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 3263, 2055, 220, 16, 16, 297, 62410, 1035, 387, 279, 8416, 2115, 3082, 8674, 311, 279, 19309, 13, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 279, 19309, 3082, 13, 2160, 429, 949, 315, 279, 21921, 6505, 30, 13824, 11, 902, 11, 7196, 279, 21921, 374, 279, 4158, 61607, 1212, 279, 19309, 13, 13824, 11, 902, 11, 279, 1909, 315, 429, 21921, 2293, 11489, 11, 902, 11, 304, 1045, 5335, 11, 1052, 594, 264, 17846, 25911, 476, 21810, 13, 13824, 11, 902, 11, 1077, 594, 1779, 279, 1156, 2421, 5335, 13, 576, 17196, 45766, 374, 33866, 11, 1221, 279, 19309, 3082, 13, 13824, 11, 279, 4396, 4226, 25, 3330, 518, 279, 5335, 11, 279, 1633, 389, 1909, 315, 279, 14852, 21921, 320, 1782, 4158, 26482, 4982, 8, 518, 220, 16, 16, 297, 62410, 374, 264, 17846, 13, 13824, 11, 902, 11, 3783, 21677, 17846, 374, 389, 279, 7002, 3403, 279, 19309, 13, 13824, 11, 7196, 358, 5786, 878, 13, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 594, 3405, 25, 330, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 7521, 2055, 1156, 11, 10542, 279, 21921, 13, 576, 4158, 26482, 4982, 3143, 220, 16, 16, 297, 62410, 320, 3481, 7950, 568, 3555, 594, 389, 1909, 30, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 429, 3082, 13, 13824, 11, 9834, 0, 758, 279, 21844, 476, 21810, 11, 1052, 594, 264, 17846, 13, 2055, 279, 4226, 374, 264, 17846, 4192, 1249, 8253, 1128, 374, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 527, 20666, 11, 23643, 279, 27979, 6789, 323, 5424, 304, 279, 5335, 25, 18611, 16, 13, 95210, 279, 1036, 65, 77832, 21921, 854, 304, 3405, 25, 362, 4158, 11, 1378, 481, 26482, 5819, 4982, 374, 9434, 3143, 279, 1909, 481, 2115, 320, 16, 16, 297, 527, 20666, 8, 3082, 8674, 311, 1008, 37664, 1075, 279, 19309, 323, 17196, 13, 2303, 17, 13, 1374, 19991, 6171, 3403, 419, 21921, 25, 758, 279, 5335, 11, 264, 3070, 73225, 334, 374, 21810, 389, 279, 7002, 3403, 419, 21921, 3082, 13, 79226, 54216, 151645, 198]
inputs:
<|im_start|>system

    You are a careful vision–language assistant for spatial VQA on a single indoor scene shown across multiple images.

    INPUT:
    Immediately after this message, you will receive N scene images as <image> tags. Treat them as different views of the SAME scene. Refer to images by 1-based index in order of appearance: [1], [2], …, [N].

    REASONING PRINCIPLES:
    • Build a consistent 3D mental model across views: track objects across images, infer relative camera pose, and use scale/occlusion/shadows for depth cues.
    • Interpret deictic terms (“to my right/left/in front/behind”) EGOCENTRICALLY with respect to the narrator/agent. If the viewpoint is ambiguous, default to the most informative central view and say so.
    • Use only visible evidence; do not invent unseen details or rely on external knowledge beyond basic object/color terms.
    • When multiple candidates exist, resolve by the best match to spatial phrase + salience across views. If still ambiguous after checking all images, state that clearly.

    OUTPUT (exactly two blocks, in this order):
    <think>
    Step-by-step reasoning in 3–10 short steps. Cite evidence with image indices (e.g., “[3]: light wood desk right of monitor; [1]: same color, confirms”). Be concise but complete (aim ≤250 tokens unless necessary). Note any occlusion/ambiguity and how you resolved it.
    </think>
    <answer>
    A single concise answer only, which may be:
    • If sufficient: One-word answer, Count (e.g., “2”) or Yes/No (e.g., “yes”).
    • If not sufficient: A sentence or two.
    If answer not visible, if there is no clear evidence, or if the answer is genuinely ambiguous: “cannot determine”.
    </answer>

    STYLE & RULES:
    • Do NOT repeat the question or list images; do NOT output JSON or extra sections.
    • Prefer simple color names with at most one modifier (“light/dark/pale/beige”).
    • Be consistent across views; if views disagree, prioritize the clearest evidence and note it in <think>.
    <|im_end|>
<|im_start|>user
What is on top of the bathroom cabinet that is on my 11 o'clock? <|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|im_end|>
<|im_start|>assistant

Got it, let's try to figure out what's on top of the bathroom cabinet at 11 o'clock. First, let's visualize the bathroom setup. The cabinet is likely a white, two-drawer unit. At 11 o'clock (top-left area), we need to check that spot. Looking at the images, especially the one showing the cabinet area, there's a mirror above the sink, but wait, no—wait, the question is about the cabinet. Wait, maybe the "cabinet" here is the storage unit. Wait, no, the sink has a cabinet below? Wait, no, the main cabinet is the white drawers. Wait, in the image where the camera shows the sink and cabinet, the top of the cabinet—wait, no, maybe the mirror? Wait, no, let's check again. Wait, the user says "bathroom cabinet that is on my 11 o'clock". So 11 o'clock would be the upper left area relative to the sink. Looking at the images, there's a mirror mounted on the wall above the sink area. Is that part of the cabinet setup? Wait, no, maybe the cabinet is the white drawers under the sink. Wait, no, the top of that cabinet—wait, no, in some images, there's a mirror reflected or mounted. Wait, no, let's check the first few images. The shower curtain is colorful, then the sink area. Wait, the correct answer: looking at the images, the object on top of the bathroom cabinet (the white drawer unit) at 11 o'clock is a mirror. Wait, no, wait—the mirror is on the wall above the sink. Wait, maybe I misread. Wait, no, let's check again. Wait, the user's question: "What is on top of the bathroom cabinet that is on my 11 o'clock?" So first, identify the cabinet. The white drawer unit near 11 o'clock (top-left). What's on top? Looking at the images, there's a mirror mounted on the wall above that area. Wait, yes! In the reflection or mounted, there's a mirror. So the answer is a mirror.


To determine what is on top of the bathroom cabinet at 11 o’clock, analyze the spatial layout and elements in the images:  

1. Locate the “bathroom cabinet” in question: A white, two - drawer storage unit is visible near the top - left (11 o’clock) area relative to other fixtures like the sink and shower.  
2. Examine objects above this cabinet: In the images, a **mirror** is mounted on the wall above this cabinet area.  


Mirror<|im_end|>

label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 198, 32462, 432, 11, 1077, 594, 1430, 311, 7071, 700, 1128, 594, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 62410, 13, 5512, 11, 1077, 594, 50087, 279, 14852, 6505, 13, 576, 21921, 374, 4363, 264, 4158, 11, 1378, 1737, 95112, 4982, 13, 2411, 220, 16, 16, 297, 62410, 320, 3481, 7950, 3082, 701, 582, 1184, 311, 1779, 429, 7702, 13, 21131, 518, 279, 5335, 11, 5310, 279, 825, 9027, 279, 21921, 3082, 11, 1052, 594, 264, 17846, 3403, 279, 19309, 11, 714, 3783, 11, 902, 2293, 11489, 11, 279, 3405, 374, 911, 279, 21921, 13, 13824, 11, 7196, 279, 330, 66, 75512, 1, 1588, 374, 279, 5819, 4982, 13, 13824, 11, 902, 11, 279, 19309, 702, 264, 21921, 3685, 30, 13824, 11, 902, 11, 279, 1887, 21921, 374, 279, 4158, 61607, 13, 13824, 11, 304, 279, 2168, 1380, 279, 6249, 4933, 279, 19309, 323, 21921, 11, 279, 1909, 315, 279, 21921, 2293, 11489, 11, 902, 11, 7196, 279, 17846, 30, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 2727, 330, 65, 77832, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 3263, 2055, 220, 16, 16, 297, 62410, 1035, 387, 279, 8416, 2115, 3082, 8674, 311, 279, 19309, 13, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 279, 19309, 3082, 13, 2160, 429, 949, 315, 279, 21921, 6505, 30, 13824, 11, 902, 11, 7196, 279, 21921, 374, 279, 4158, 61607, 1212, 279, 19309, 13, 13824, 11, 902, 11, 279, 1909, 315, 429, 21921, 2293, 11489, 11, 902, 11, 304, 1045, 5335, 11, 1052, 594, 264, 17846, 25911, 476, 21810, 13, 13824, 11, 902, 11, 1077, 594, 1779, 279, 1156, 2421, 5335, 13, 576, 17196, 45766, 374, 33866, 11, 1221, 279, 19309, 3082, 13, 13824, 11, 279, 4396, 4226, 25, 3330, 518, 279, 5335, 11, 279, 1633, 389, 1909, 315, 279, 14852, 21921, 320, 1782, 4158, 26482, 4982, 8, 518, 220, 16, 16, 297, 62410, 374, 264, 17846, 13, 13824, 11, 902, 11, 3783, 21677, 17846, 374, 389, 279, 7002, 3403, 279, 19309, 13, 13824, 11, 7196, 358, 5786, 878, 13, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 594, 3405, 25, 330, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 7521, 2055, 1156, 11, 10542, 279, 21921, 13, 576, 4158, 26482, 4982, 3143, 220, 16, 16, 297, 62410, 320, 3481, 7950, 568, 3555, 594, 389, 1909, 30, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 429, 3082, 13, 13824, 11, 9834, 0, 758, 279, 21844, 476, 21810, 11, 1052, 594, 264, 17846, 13, 2055, 279, 4226, 374, 264, 17846, 4192, 1249, 8253, 1128, 374, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 527, 20666, 11, 23643, 279, 27979, 6789, 323, 5424, 304, 279, 5335, 25, 18611, 16, 13, 95210, 279, 1036, 65, 77832, 21921, 854, 304, 3405, 25, 362, 4158, 11, 1378, 481, 26482, 5819, 4982, 374, 9434, 3143, 279, 1909, 481, 2115, 320, 16, 16, 297, 527, 20666, 8, 3082, 8674, 311, 1008, 37664, 1075, 279, 19309, 323, 17196, 13, 2303, 17, 13, 1374, 19991, 6171, 3403, 419, 21921, 25, 758, 279, 5335, 11, 264, 3070, 73225, 334, 374, 21810, 389, 279, 7002, 3403, 419, 21921, 3082, 13, 79226, 54216, 151645, 198]
labels:

Got it, let's try to figure out what's on top of the bathroom cabinet at 11 o'clock. First, let's visualize the bathroom setup. The cabinet is likely a white, two-drawer unit. At 11 o'clock (top-left area), we need to check that spot. Looking at the images, especially the one showing the cabinet area, there's a mirror above the sink, but wait, no—wait, the question is about the cabinet. Wait, maybe the "cabinet" here is the storage unit. Wait, no, the sink has a cabinet below? Wait, no, the main cabinet is the white drawers. Wait, in the image where the camera shows the sink and cabinet, the top of the cabinet—wait, no, maybe the mirror? Wait, no, let's check again. Wait, the user says "bathroom cabinet that is on my 11 o'clock". So 11 o'clock would be the upper left area relative to the sink. Looking at the images, there's a mirror mounted on the wall above the sink area. Is that part of the cabinet setup? Wait, no, maybe the cabinet is the white drawers under the sink. Wait, no, the top of that cabinet—wait, no, in some images, there's a mirror reflected or mounted. Wait, no, let's check the first few images. The shower curtain is colorful, then the sink area. Wait, the correct answer: looking at the images, the object on top of the bathroom cabinet (the white drawer unit) at 11 o'clock is a mirror. Wait, no, wait—the mirror is on the wall above the sink. Wait, maybe I misread. Wait, no, let's check again. Wait, the user's question: "What is on top of the bathroom cabinet that is on my 11 o'clock?" So first, identify the cabinet. The white drawer unit near 11 o'clock (top-left). What's on top? Looking at the images, there's a mirror mounted on the wall above that area. Wait, yes! In the reflection or mounted, there's a mirror. So the answer is a mirror.


To determine what is on top of the bathroom cabinet at 11 o’clock, analyze the spatial layout and elements in the images:  

1. Locate the “bathroom cabinet” in question: A white, two - drawer storage unit is visible near the top - left (11 o’clock) area relative to other fixtures like the sink and shower.  
2. Examine objects above this cabinet: In the images, a **mirror** is mounted on the wall above this cabinet area.  


Mirror<|im_end|>

[INFO|hub.py:421] 2025-12-09 01:29:46,427 >> Offline mode: forcing local_files_only=True
[INFO|configuration_utils.py:765] 2025-12-09 01:29:46,429 >> loading configuration file config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
[INFO|configuration_utils.py:839] 2025-12-09 01:29:46,433 >> Model config Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 128000,
  "max_window_layers": 28,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "_name_or_path": "Qwen/Qwen2.5-VL-7B-Instruct",
    "architectures": [
      "Qwen2_5_VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "dtype": "bfloat16",
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "layer_types": [
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention"
    ],
    "max_position_embeddings": 128000,
    "max_window_layers": 28,
    "model_type": "qwen2_5_vl_text",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "use_cache": true,
    "use_sliding_window": false,
    "vision_token_id": 151654,
    "vocab_size": 152064
  },
  "tie_word_embeddings": false,
  "transformers_version": "4.57.0",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 3584,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

[INFO|2025-12-09 01:29:46] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
[INFO|2025-12-09 01:29:46] llamafactory.model.model_utils.liger_kernel:143 >> Liger kernel has been applied to the model.
[INFO|hub.py:421] 2025-12-09 01:29:46,893 >> Offline mode: forcing local_files_only=True
[WARNING|logging.py:328] 2025-12-09 01:29:46,896 >> `torch_dtype` is deprecated! Use `dtype` instead!
[INFO|hub.py:421] 2025-12-09 01:29:46,896 >> Offline mode: forcing local_files_only=True
[INFO|modeling_utils.py:4840] 2025-12-09 01:29:46,897 >> Offline mode: forcing local_files_only=True
[INFO|modeling_utils.py:1176] 2025-12-09 01:29:46,901 >> loading weights file model.safetensors from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/model.safetensors.index.json
[INFO|modeling_utils.py:2345] 2025-12-09 01:29:46,906 >> Instantiating Qwen2_5_VLForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:986] 2025-12-09 01:29:46,927 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

`torch_dtype` is deprecated! Use `dtype` instead!
[INFO|modeling_utils.py:2345] 2025-12-09 01:29:46,939 >> Instantiating Qwen2_5_VisionTransformerPretrainedModel model under default dtype torch.bfloat16.
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO|modeling_utils.py:2345] 2025-12-09 01:29:47,038 >> Instantiating Qwen2_5_VLTextModel model under default dtype torch.bfloat16.
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:01<00:04,  1.02s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:01<00:04,  1.02s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:01<00:04,  1.02s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:01<00:04,  1.02s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:01<00:04,  1.02s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:01<00:04,  1.04s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:01<00:04,  1.04s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:01<00:04,  1.04s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:02<00:03,  1.11s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:02<00:03,  1.11s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:02<00:03,  1.11s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:02<00:03,  1.11s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:02<00:03,  1.11s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:02<00:03,  1.11s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:02<00:03,  1.11s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:02<00:03,  1.11s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.04it/s]Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.03it/s]Loading checkpoint shards:  60%|██████    | 3/5 [00:03<00:01,  1.02it/s]Loading checkpoint shards:  60%|██████    | 3/5 [00:03<00:01,  1.01it/s]Loading checkpoint shards:  60%|██████    | 3/5 [00:03<00:02,  1.00s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:03<00:02,  1.03s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:03<00:02,  1.03s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:03<00:02,  1.04s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:04<00:00,  1.01it/s]Loading checkpoint shards:  80%|████████  | 4/5 [00:04<00:00,  1.02it/s]Loading checkpoint shards:  80%|████████  | 4/5 [00:04<00:00,  1.01it/s]Loading checkpoint shards:  80%|████████  | 4/5 [00:04<00:00,  1.00it/s]Loading checkpoint shards:  80%|████████  | 4/5 [00:04<00:00,  1.02it/s]Loading checkpoint shards:  80%|████████  | 4/5 [00:04<00:00,  1.00it/s]Loading checkpoint shards:  80%|████████  | 4/5 [00:04<00:00,  1.02it/s]Loading checkpoint shards:  80%|████████  | 4/5 [00:04<00:01,  1.00s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.34it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.35it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.15it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.36it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.33it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.15it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.36it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.15it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.34it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.15it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.33it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.15it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.34it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.15it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.15it/s]
[INFO|configuration_utils.py:941] 2025-12-09 01:29:51,768 >> loading configuration file generation_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/generation_config.json
[INFO|configuration_utils.py:986] 2025-12-09 01:29:51,768 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 1e-06
}

[INFO|dynamic_module_utils.py:423] 2025-12-09 01:29:51,770 >> Could not locate the custom_generate/generate.py inside Qwen/Qwen2.5-VL-7B-Instruct.
[INFO|2025-12-09 01:29:51] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2025-12-09 01:29:51] llamafactory.model.model_utils.attention:143 >> Using FlashAttention-2 for faster training and inference.
[INFO|2025-12-09 01:29:51] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.
[INFO|2025-12-09 01:29:51] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA
[INFO|2025-12-09 01:29:52] llamafactory.model.adapter:143 >> Loaded adapter(s): /project/aip-wangcs/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-466/
[INFO|2025-12-09 01:29:52] llamafactory.model.loader:143 >> trainable params: 20,185,088 || all params: 8,312,351,744 || trainable%: 0.2428
name: base_model.model.model.visual.patch_embed.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.ln_q.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.0.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.0.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.embed_tokens.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.lm_head.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
[INFO|trainer.py:749] 2025-12-09 01:29:52,917 >> Using auto half precision backend
[WARNING|2025-12-09 01:29:52] llamafactory.train.callbacks:154 >> Previous trainer log in this folder will be deleted.
[WARNING|trainer.py:982] 2025-12-09 01:29:52,921 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
[DEBUG|trainer.py:2373] 2025-12-09 01:29:53,548 >> Currently training with a batch size of: 2
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 8. Using DeepSpeed's value.
[INFO|deepspeed.py:383] 2025-12-09 01:29:53,600 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Currently training with a batch size of: 2
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Currently training with a batch size of: 2
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Currently training with a batch size of: 2
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Currently training with a batch size of: 2
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Currently training with a batch size of: 2
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Currently training with a batch size of: 2
Currently training with a batch size of: 2
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Using /tmp/.cache/torch_extensions as PyTorch extensions root...
Creating extension directory /tmp/.cache/torch_extensions/cpu_adam...
Detected CUDA files, patching ldflags
Emitting ninja build file /tmp/.cache/torch_extensions/cpu_adam/build.ninja...
/opt/conda/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module cpu_adam...
Using envvar MAX_JOBS (16) as the number of workers...
Using /tmp/.cache/torch_extensions as PyTorch extensions root...
Using /tmp/.cache/torch_extensions as PyTorch extensions root...
Using /tmp/.cache/torch_extensions as PyTorch extensions root...
Using /tmp/.cache/torch_extensions as PyTorch extensions root...Using /tmp/.cache/torch_extensions as PyTorch extensions root...
Using /tmp/.cache/torch_extensions as PyTorch extensions root...

Using /tmp/.cache/torch_extensions as PyTorch extensions root...
[1/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.11/site-packages/torch/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o 
[2/3] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.11/site-packages/torch/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o 
[3/3] c++ cpu_adam.o cpu_adam_impl.o -shared -lcurand -L/usr/local/cuda/lib64 -L/opt/conda/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so
Loading extension module cpu_adam...
Time to load cpu_adam op: 28.573143243789673 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-12-09 01:30:23,415] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.9, git-hash=unknown, git-branch=unknown
[2025-12-09 01:30:23,415] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
Loading extension module cpu_adam...
Loading extension module cpu_adam...
Loading extension module cpu_adam...
Loading extension module cpu_adam...
Loading extension module cpu_adam...
Loading extension module cpu_adam...
Time to load cpu_adam op: 27.272539615631104 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
Loading extension module cpu_adam...
[2025-12-09 01:30:23,484] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
Time to load cpu_adam op: 27.415388584136963 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-12-09 01:30:23,484] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
Time to load cpu_adam op: 27.181132316589355 seconds
Time to load cpu_adam op: 27.215517282485962 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
Time to load cpu_adam op: 27.175885677337646 secondsTime to load cpu_adam op: 27.484580516815186 seconds

Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-12-09 01:30:23,485] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
[2025-12-09 01:30:23,485] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
[2025-12-09 01:30:23,486] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
[2025-12-09 01:30:23,486] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
Time to load cpu_adam op: 27.28463125228882 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-12-09 01:30:23,487] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
[2025-12-09 01:30:26,198] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-12-09 01:30:26,202] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-12-09 01:30:26,202] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-12-09 01:30:26,228] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2025-12-09 01:30:26,229] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2025-12-09 01:30:26,229] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-12-09 01:30:26,229] [INFO] [stage_1_and_2.py:150:__init__] Reduce bucket size 500000000
[2025-12-09 01:30:26,229] [INFO] [stage_1_and_2.py:151:__init__] Allgather bucket size 500000000
[2025-12-09 01:30:26,229] [INFO] [stage_1_and_2.py:152:__init__] CPU Offload: True
[2025-12-09 01:30:26,229] [INFO] [stage_1_and_2.py:153:__init__] Round robin gradient partitioning: True
***** Running training *****
  Num examples = 29,742
  Num Epochs = 2
  Instantaneous batch size per device = 2
***** Running training *****
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Gradient Accumulation steps = 8
  Total optimization steps = 466
  Num examples = 29,742
  Num Epochs = 2
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Gradient Accumulation steps = 8
  Total optimization steps = 466
***** Running training *****
  Num examples = 29,742
  Num Epochs = 2
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Gradient Accumulation steps = 8
  Total optimization steps = 466
***** Running training *****
  Num examples = 29,742
  Num Epochs = 2
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Gradient Accumulation steps = 8
  Total optimization steps = 466
***** Running training *****
  Num examples = 29,742
  Num Epochs = 2
***** Running training *****
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Num examples = 29,742
  Gradient Accumulation steps = 8
  Num Epochs = 2
  Total optimization steps = 466
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Gradient Accumulation steps = 8
  Total optimization steps = 466
  Number of trainable parameters = 20,185,088
  Number of trainable parameters = 20,185,088
***** Running training *****
  Num examples = 29,742
  Num Epochs = 2
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Gradient Accumulation steps = 8
  Total optimization steps = 466
  Number of trainable parameters = 20,185,088
  Number of trainable parameters = 20,185,088
  Number of trainable parameters = 20,185,088
  Number of trainable parameters = 20,185,088
  Number of trainable parameters = 20,185,088
[2025-12-09 01:30:26,591] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-12-09 01:30:26,592] [INFO] [utils.py:782:see_memory_usage] MA 15.48 GB         Max_MA 15.48 GB         CA 15.49 GB         Max_CA 15 GB 
[2025-12-09 01:30:26,592] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 31.43 GB, percent = 1.6%
[2025-12-09 01:30:26,818] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-12-09 01:30:26,820] [INFO] [utils.py:782:see_memory_usage] MA 15.48 GB         Max_MA 15.48 GB         CA 15.49 GB         Max_CA 15 GB 
[2025-12-09 01:30:26,820] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 31.44 GB, percent = 1.6%
[2025-12-09 01:30:26,821] [INFO] [stage_1_and_2.py:557:__init__] optimizer state initialized
[2025-12-09 01:30:27,037] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-12-09 01:30:27,038] [INFO] [utils.py:782:see_memory_usage] MA 15.48 GB         Max_MA 15.48 GB         CA 15.49 GB         Max_CA 15 GB 
[2025-12-09 01:30:27,038] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 31.44 GB, percent = 1.6%
[2025-12-09 01:30:27,039] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-12-09 01:30:27,039] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-12-09 01:30:27,039] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-12-09 01:30:27,039] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2025-12-09 01:30:27,044] [INFO] [config.py:1003:print] DeepSpeedEngine configuration:
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   amp_enabled .................. False
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   amp_params ................... False
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   bfloat16_enabled ............. True
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   bfloat16_immediate_grad_update  True
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   checkpoint_parallel_write_pipeline  False
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   checkpoint_tag_validation_enabled  True
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   checkpoint_tag_validation_fail  False
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x15521d232910>
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   communication_data_type ...... None
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   curriculum_enabled_legacy .... False
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   curriculum_params_legacy ..... False
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   data_efficiency_enabled ...... False
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   dataloader_drop_last ......... False
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   disable_allgather ............ False
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   dump_state ................... False
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   dynamic_loss_scale_args ...... None
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   eigenvalue_enabled ........... False
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   eigenvalue_gas_boundary_resolution  1
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   eigenvalue_layer_num ......... 0
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   eigenvalue_max_iter .......... 100
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   eigenvalue_stability ......... 1e-06
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   eigenvalue_tol ............... 0.01
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   eigenvalue_verbose ........... False
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   elasticity_enabled ........... False
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   fp16_auto_cast ............... None
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   fp16_enabled ................. False
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   fp16_master_weights_and_gradients  False
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   global_rank .................. 0
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   grad_accum_dtype ............. None
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   gradient_accumulation_steps .. 8
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   gradient_clipping ............ 1.0
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   gradient_predivide_factor .... 1.0
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   graph_harvesting ............. False
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   initial_dynamic_scale ........ 1
[2025-12-09 01:30:27,045] [INFO] [config.py:1007:print]   load_universal_checkpoint .... False
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   loss_scale ................... 1.0
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   memory_breakdown ............. False
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   mics_hierarchial_params_gather  False
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   mics_shard_size .............. -1
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   optimizer_legacy_fusion ...... False
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   optimizer_name ............... None
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   optimizer_params ............. None
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   pld_enabled .................. False
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   pld_params ................... False
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   prescale_gradients ........... False
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   scheduler_name ............... None
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   scheduler_params ............. None
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   seq_parallel_communication_data_type  torch.float32
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   sparse_attention ............. None
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   sparse_gradients_enabled ..... False
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   steps_per_print .............. inf
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   timers_config ................ enabled=True synchronized=True
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   train_batch_size ............. 128
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   train_micro_batch_size_per_gpu  2
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   use_data_before_expert_parallel_  False
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   use_node_local_storage ....... False
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   wall_clock_breakdown ......... False
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   weight_quantization_config ... None
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   world_size ................... 8
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   zero_allow_untested_optimizer  True
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=True zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   zero_enabled ................. True
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   zero_force_ds_cpu_optimizer .. True
[2025-12-09 01:30:27,046] [INFO] [config.py:1007:print]   zero_optimization_stage ...... 2
[2025-12-09 01:30:27,046] [INFO] [config.py:993:print_user_config]   json = {
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 2, 
    "gradient_accumulation_steps": 8, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": false, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true, 
        "round_robin_gradients": true
    }, 
    "steps_per_print": inf
}
[INFO|trainer.py:2519] 2025-12-09 01:30:27,047 >> ***** Running training *****
[INFO|trainer.py:2520] 2025-12-09 01:30:27,047 >>   Num examples = 29,742
[INFO|trainer.py:2521] 2025-12-09 01:30:27,047 >>   Num Epochs = 2
[INFO|trainer.py:2522] 2025-12-09 01:30:27,047 >>   Instantaneous batch size per device = 2
[INFO|trainer.py:2525] 2025-12-09 01:30:27,047 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2526] 2025-12-09 01:30:27,047 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2527] 2025-12-09 01:30:27,048 >>   Total optimization steps = 466
[INFO|trainer.py:2528] 2025-12-09 01:30:27,051 >>   Number of trainable parameters = 20,185,088
[INFO|integration_utils.py:867] 2025-12-09 01:30:27,054 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Tracking run with wandb version 0.23.0
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /project/aip-wangcs/indrisch/LLaMA-Factory/wandb/wandb/offline-run-20251209_013027-1usp441a
  0%|          | 0/466 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /tmp/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /tmp/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /tmp/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /tmp/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /tmp/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /tmp/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /tmp/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /tmp/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
  0%|          | 1/466 [02:05<16:14:34, 125.75s/it]  0%|          | 2/466 [04:27<17:23:40, 134.96s/it]  1%|          | 3/466 [06:19<16:00:22, 124.46s/it]  1%|          | 4/466 [08:07<15:08:33, 117.99s/it]  1%|          | 5/466 [10:03<15:02:15, 117.43s/it]  1%|▏         | 6/466 [11:47<14:24:58, 112.82s/it]  2%|▏         | 7/466 [13:39<14:20:58, 112.54s/it]  2%|▏         | 8/466 [15:30<14:16:13, 112.17s/it]  2%|▏         | 9/466 [17:20<14:08:24, 111.39s/it]  2%|▏         | 10/466 [19:30<14:50:56, 117.23s/it]                                                    {'loss': 0.727, 'grad_norm': 0.16117005050182343, 'learning_rate': 1.9148936170212766e-05, 'epoch': 0.04}
  2%|▏         | 10/466 [19:30<14:50:56, 117.23s/it]  2%|▏         | 11/466 [21:21<14:33:13, 115.15s/it]  3%|▎         | 12/466 [23:11<14:20:52, 113.77s/it]  3%|▎         | 13/466 [25:19<14:50:32, 117.95s/it]  3%|▎         | 14/466 [27:00<14:09:44, 112.80s/it]  3%|▎         | 15/466 [29:04<14:34:41, 116.37s/it]  3%|▎         | 16/466 [31:09<14:51:31, 118.87s/it]  4%|▎         | 17/466 [32:51<14:10:43, 113.68s/it]  4%|▍         | 18/466 [34:50<14:21:51, 115.43s/it]  4%|▍         | 19/466 [36:44<14:15:52, 114.88s/it]  4%|▍         | 20/466 [38:40<14:15:49, 115.13s/it]                                                    {'loss': 0.7137, 'grad_norm': 0.1703653335571289, 'learning_rate': 4.0425531914893614e-05, 'epoch': 0.09}
  4%|▍         | 20/466 [38:40<14:15:49, 115.13s/it]  5%|▍         | 21/466 [40:23<13:48:36, 111.72s/it]  5%|▍         | 22/466 [42:27<14:13:04, 115.28s/it]  5%|▍         | 23/466 [44:29<14:25:15, 117.19s/it]  5%|▌         | 24/466 [46:33<14:39:02, 119.33s/it]  5%|▌         | 25/466 [48:28<14:27:20, 118.00s/it]  6%|▌         | 26/466 [50:29<14:32:15, 118.94s/it]  6%|▌         | 27/466 [52:16<14:03:37, 115.30s/it]  6%|▌         | 28/466 [54:01<13:39:09, 112.21s/it]  6%|▌         | 29/466 [56:08<14:11:01, 116.85s/it]  6%|▋         | 30/466 [58:10<14:20:24, 118.40s/it]                                                    {'loss': 0.7165, 'grad_norm': 0.18988622725009918, 'learning_rate': 6.170212765957447e-05, 'epoch': 0.13}
  6%|▋         | 30/466 [58:10<14:20:24, 118.40s/it]  7%|▋         | 31/466 [1:00:05<14:10:07, 117.26s/it]  7%|▋         | 32/466 [1:01:59<14:01:03, 116.28s/it]  7%|▋         | 33/466 [1:03:59<14:06:38, 117.32s/it]  7%|▋         | 34/466 [1:05:52<13:55:29, 116.04s/it]  8%|▊         | 35/466 [1:07:33<13:22:00, 111.65s/it]  8%|▊         | 36/466 [1:09:27<13:24:50, 112.30s/it]  8%|▊         | 37/466 [1:11:24<13:32:09, 113.59s/it]  8%|▊         | 38/466 [1:13:11<13:16:38, 111.68s/it]  8%|▊         | 39/466 [1:15:07<13:23:35, 112.92s/it]  9%|▊         | 40/466 [1:17:03<13:28:47, 113.92s/it]                                                      {'loss': 0.7153, 'grad_norm': 0.20202799141407013, 'learning_rate': 8.297872340425533e-05, 'epoch': 0.17}
  9%|▊         | 40/466 [1:17:03<13:28:47, 113.92s/it]  9%|▉         | 41/466 [1:19:11<13:57:36, 118.25s/it]  9%|▉         | 42/466 [1:21:07<13:51:03, 117.60s/it]  9%|▉         | 43/466 [1:23:03<13:44:17, 116.92s/it]  9%|▉         | 44/466 [1:25:03<13:48:41, 117.82s/it] 10%|▉         | 45/466 [1:26:39<13:02:11, 111.48s/it] 10%|▉         | 46/466 [1:28:33<13:04:59, 112.14s/it] 10%|█         | 47/466 [1:30:30<13:12:58, 113.55s/it] 10%|█         | 48/466 [1:32:41<13:47:17, 118.75s/it] 11%|█         | 49/466 [1:34:41<13:48:55, 119.27s/it] 11%|█         | 50/466 [1:36:34<13:34:18, 117.45s/it]                                                      {'loss': 0.7113, 'grad_norm': 0.1805143803358078, 'learning_rate': 9.999437835313409e-05, 'epoch': 0.22}
 11%|█         | 50/466 [1:36:34<13:34:18, 117.45s/it] 11%|█         | 51/466 [1:38:49<14:08:17, 122.64s/it] 11%|█         | 52/466 [1:40:46<13:53:43, 120.83s/it] 11%|█▏        | 53/466 [1:42:44<13:45:31, 119.93s/it] 12%|█▏        | 54/466 [1:44:25<13:05:34, 114.40s/it] 12%|█▏        | 55/466 [1:46:12<12:48:16, 112.16s/it] 12%|█▏        | 56/466 [1:48:17<13:11:45, 115.87s/it] 12%|█▏        | 57/466 [1:50:25<13:36:02, 119.71s/it] 12%|█▏        | 58/466 [1:52:21<13:26:15, 118.57s/it] 13%|█▎        | 59/466 [1:54:30<13:44:46, 121.59s/it] 13%|█▎        | 60/466 [1:56:20<13:19:38, 118.17s/it]                                                      {'loss': 0.7268, 'grad_norm': 0.19170646369457245, 'learning_rate': 9.979775341323098e-05, 'epoch': 0.26}
 13%|█▎        | 60/466 [1:56:20<13:19:38, 118.17s/it] 13%|█▎        | 61/466 [1:58:42<14:05:26, 125.25s/it] 13%|█▎        | 62/466 [2:00:27<13:22:17, 119.15s/it] 14%|█▎        | 63/466 [2:02:15<12:58:46, 115.95s/it] 14%|█▎        | 64/466 [2:04:09<12:52:41, 115.33s/it] 14%|█▍        | 65/466 [2:06:25<13:33:09, 121.67s/it] 14%|█▍        | 66/466 [2:08:22<13:20:29, 120.07s/it] 14%|█▍        | 67/466 [2:10:04<12:41:53, 114.57s/it] 15%|█▍        | 68/466 [2:12:05<12:53:39, 116.63s/it] 15%|█▍        | 69/466 [2:14:01<12:50:18, 116.42s/it] 15%|█▌        | 70/466 [2:16:02<12:56:58, 117.72s/it]                                                      {'loss': 0.7035, 'grad_norm': 0.16100099682807922, 'learning_rate': 9.932130896943477e-05, 'epoch': 0.3}
 15%|█▌        | 70/466 [2:16:02<12:56:58, 117.72s/it] 15%|█▌        | 71/466 [2:17:54<12:44:08, 116.07s/it] 15%|█▌        | 72/466 [2:19:42<12:26:20, 113.66s/it] 16%|█▌        | 73/466 [2:21:46<12:44:50, 116.77s/it] 16%|█▌        | 74/466 [2:23:29<12:15:31, 112.58s/it] 16%|█▌        | 75/466 [2:25:50<13:09:11, 121.10s/it] 16%|█▋        | 76/466 [2:27:36<12:38:09, 116.64s/it] 17%|█▋        | 77/466 [2:29:29<12:29:27, 115.60s/it] 17%|█▋        | 78/466 [2:31:21<12:20:49, 114.56s/it] 17%|█▋        | 79/466 [2:33:10<12:07:02, 112.72s/it] 17%|█▋        | 80/466 [2:34:59<11:59:17, 111.81s/it]                                                      {'loss': 0.7239, 'grad_norm': 0.1938387155532837, 'learning_rate': 9.856772221978671e-05, 'epoch': 0.34}
 17%|█▋        | 80/466 [2:34:59<11:59:17, 111.81s/it] 17%|█▋        | 81/466 [2:36:39<11:33:54, 108.14s/it] 18%|█▊        | 82/466 [2:38:24<11:26:09, 107.21s/it] 18%|█▊        | 83/466 [2:40:34<12:08:21, 114.10s/it] 18%|█▊        | 84/466 [2:42:27<12:03:21, 113.62s/it] 18%|█▊        | 85/466 [2:44:07<11:36:22, 109.66s/it] 18%|█▊        | 86/466 [2:46:08<11:56:39, 113.16s/it] 19%|█▊        | 87/466 [2:48:03<11:58:13, 113.70s/it] 19%|█▉        | 88/466 [2:49:54<11:50:57, 112.85s/it] 19%|█▉        | 89/466 [2:51:36<11:28:31, 109.58s/it] 19%|█▉        | 90/466 [2:53:29<11:32:34, 110.52s/it]                                                      {'loss': 0.716, 'grad_norm': 0.19429484009742737, 'learning_rate': 9.754122765793305e-05, 'epoch': 0.39}
 19%|█▉        | 90/466 [2:53:29<11:32:34, 110.52s/it] 20%|█▉        | 91/466 [2:55:09<11:11:27, 107.43s/it] 20%|█▉        | 92/466 [2:56:46<10:49:09, 104.14s/it] 20%|█▉        | 93/466 [2:58:28<10:44:15, 103.63s/it] 20%|██        | 94/466 [3:00:22<11:02:20, 106.83s/it] 20%|██        | 95/466 [3:02:26<11:31:39, 111.86s/it] 21%|██        | 96/466 [3:04:02<11:00:17, 107.07s/it] 21%|██        | 97/466 [3:05:50<11:00:31, 107.40s/it] 21%|██        | 98/466 [3:07:38<10:59:56, 107.60s/it] 21%|██        | 99/466 [3:09:44<11:31:27, 113.05s/it] 21%|██▏       | 100/466 [3:11:52<11:56:46, 117.50s/it]                                                       {'loss': 0.7148, 'grad_norm': 0.1831493228673935, 'learning_rate': 9.624759327900131e-05, 'epoch': 0.43}
 21%|██▏       | 100/466 [3:11:52<11:56:46, 117.50s/it] 22%|██▏       | 101/466 [3:13:40<11:37:05, 114.59s/it] 22%|██▏       | 102/466 [3:15:34<11:35:32, 114.65s/it] 22%|██▏       | 103/466 [3:17:40<11:52:49, 117.82s/it] 22%|██▏       | 104/466 [3:19:43<12:01:21, 119.56s/it] 23%|██▎       | 105/466 [3:21:41<11:56:36, 119.10s/it] 23%|██▎       | 106/466 [3:23:38<11:51:18, 118.55s/it] 23%|██▎       | 107/466 [3:25:17<11:13:51, 112.62s/it] 23%|██▎       | 108/466 [3:27:18<11:26:15, 115.01s/it] 23%|██▎       | 109/466 [3:29:25<11:45:22, 118.55s/it] 24%|██▎       | 110/466 [3:31:17<11:31:56, 116.62s/it]                                                       {'loss': 0.7198, 'grad_norm': 0.1763754040002823, 'learning_rate': 9.469408816854897e-05, 'epoch': 0.47}
 24%|██▎       | 110/466 [3:31:17<11:31:56, 116.62s/it] 24%|██▍       | 111/466 [3:33:39<12:15:09, 124.25s/it] 24%|██▍       | 112/466 [3:35:31<11:50:59, 120.51s/it] 24%|██▍       | 113/466 [3:37:18<11:25:51, 116.58s/it] 24%|██▍       | 114/466 [3:39:16<11:25:32, 116.85s/it] 25%|██▍       | 115/466 [3:40:46<10:37:34, 108.99s/it] 25%|██▍       | 116/466 [3:42:24<10:15:45, 105.56s/it] 25%|██▌       | 117/466 [3:44:27<10:45:25, 110.96s/it] 25%|██▌       | 118/466 [3:46:33<11:09:08, 115.37s/it] 26%|██▌       | 119/466 [3:48:34<11:17:45, 117.19s/it] 26%|██▌       | 120/466 [3:50:37<11:25:54, 118.94s/it]                                                       {'loss': 0.7138, 'grad_norm': 0.18059410154819489, 'learning_rate': 9.288944165670651e-05, 'epoch': 0.52}
 26%|██▌       | 120/466 [3:50:37<11:25:54, 118.94s/it] 26%|██▌       | 121/466 [3:52:31<11:14:36, 117.32s/it] 26%|██▌       | 122/466 [3:54:35<11:24:47, 119.44s/it] 26%|██▋       | 123/466 [3:56:40<11:31:44, 121.01s/it] 27%|██▋       | 124/466 [3:58:31<11:12:40, 118.01s/it] 27%|██▋       | 125/466 [4:00:34<11:19:29, 119.56s/it] 27%|██▋       | 126/466 [4:02:24<11:01:29, 116.73s/it] 27%|██▋       | 127/466 [4:04:13<10:45:51, 114.31s/it] 27%|██▋       | 128/466 [4:05:49<10:12:31, 108.73s/it] 28%|██▊       | 129/466 [4:07:55<10:39:37, 113.88s/it] 28%|██▊       | 130/466 [4:09:47<10:35:25, 113.47s/it]                                                       {'loss': 0.7201, 'grad_norm': 0.1817609816789627, 'learning_rate': 9.084379426703245e-05, 'epoch': 0.56}
 28%|██▊       | 130/466 [4:09:47<10:35:25, 113.47s/it] 28%|██▊       | 131/466 [4:11:41<10:33:49, 113.52s/it] 28%|██▊       | 132/466 [4:13:21<10:10:16, 109.63s/it] 29%|██▊       | 133/466 [4:15:13<10:11:34, 110.19s/it] 29%|██▉       | 134/466 [4:17:12<10:23:55, 112.76s/it] 29%|██▉       | 135/466 [4:18:53<10:02:46, 109.26s/it] 29%|██▉       | 136/466 [4:20:56<10:23:39, 113.39s/it] 29%|██▉       | 137/466 [4:22:56<10:33:17, 115.50s/it] 30%|██▉       | 138/466 [4:24:23<9:43:58, 106.83s/it]  30%|██▉       | 139/466 [4:26:19<9:58:09, 109.75s/it] 30%|███       | 140/466 [4:27:58<9:38:53, 106.54s/it]                                                      {'loss': 0.7194, 'grad_norm': 0.1679532378911972, 'learning_rate': 8.856864073570429e-05, 'epoch': 0.6}
 30%|███       | 140/466 [4:27:58<9:38:53, 106.54s/it] 30%|███       | 141/466 [4:29:58<9:57:37, 110.33s/it] 30%|███       | 142/466 [4:31:59<10:13:47, 113.66s/it] 31%|███       | 143/466 [4:33:50<10:07:00, 112.76s/it] 31%|███       | 144/466 [4:35:47<10:12:31, 114.14s/it] 31%|███       | 145/466 [4:37:43<10:14:13, 114.81s/it] 31%|███▏      | 146/466 [4:39:32<10:02:31, 112.97s/it] 32%|███▏      | 147/466 [4:41:32<10:11:11, 114.96s/it] 32%|███▏      | 148/466 [4:43:30<10:14:37, 115.97s/it] 32%|███▏      | 149/466 [4:45:25<10:10:47, 115.61s/it] 32%|███▏      | 150/466 [4:47:23<10:12:27, 116.29s/it]                                                       {'loss': 0.7131, 'grad_norm': 0.19720910489559174, 'learning_rate': 8.607676542122782e-05, 'epoch': 0.65}
 32%|███▏      | 150/466 [4:47:23<10:12:27, 116.29s/it] 32%|███▏      | 151/466 [4:49:08<9:53:00, 112.95s/it]  33%|███▎      | 152/466 [4:51:12<10:08:40, 116.31s/it] 33%|███▎      | 153/466 [4:53:26<10:34:31, 121.63s/it] 33%|███▎      | 154/466 [4:55:06<9:58:40, 115.13s/it]  33%|███▎      | 155/466 [4:57:11<10:11:59, 118.07s/it] 33%|███▎      | 156/466 [4:59:21<10:28:26, 121.63s/it] 34%|███▎      | 157/466 [5:01:22<10:25:21, 121.43s/it] 34%|███▍      | 158/466 [5:03:19<10:16:25, 120.08s/it] 34%|███▍      | 159/466 [5:05:05<9:53:57, 116.08s/it]  34%|███▍      | 160/466 [5:07:05<9:57:00, 117.06s/it]                                                      {'loss': 0.723, 'grad_norm': 0.17712168395519257, 'learning_rate': 8.33821704676049e-05, 'epoch': 0.69}
 34%|███▍      | 160/466 [5:07:05<9:57:00, 117.06s/it] 35%|███▍      | 161/466 [5:08:53<9:41:02, 114.30s/it] 35%|███▍      | 162/466 [5:10:33<9:18:35, 110.25s/it] 35%|███▍      | 163/466 [5:12:33<9:30:47, 113.03s/it] 35%|███▌      | 164/466 [5:14:30<9:34:35, 114.16s/it] 35%|███▌      | 165/466 [5:16:23<9:31:57, 114.01s/it] 36%|███▌      | 166/466 [5:18:22<9:36:38, 115.33s/it] 36%|███▌      | 167/466 [5:20:12<9:27:31, 113.88s/it] 36%|███▌      | 168/466 [5:21:50<9:02:11, 109.17s/it] 36%|███▋      | 169/466 [5:23:53<9:19:43, 113.08s/it] 36%|███▋      | 170/466 [5:25:43<9:13:10, 112.13s/it]                                                      {'loss': 0.7089, 'grad_norm': 0.21124345064163208, 'learning_rate': 8.049999712461957e-05, 'epoch': 0.73}
 36%|███▋      | 170/466 [5:25:43<9:13:10, 112.13s/it] 37%|███▋      | 171/466 [5:27:40<9:19:04, 113.71s/it] 37%|███▋      | 172/466 [5:29:46<9:35:02, 117.36s/it] 37%|███▋      | 173/466 [5:31:44<9:34:33, 117.66s/it] 37%|███▋      | 174/466 [5:33:38<9:27:28, 116.60s/it] 38%|███▊      | 175/466 [5:35:27<9:13:27, 114.12s/it] 38%|███▊      | 176/466 [5:37:20<9:09:46, 113.75s/it] 38%|███▊      | 177/466 [5:39:13<9:07:38, 113.70s/it] 38%|███▊      | 178/466 [5:41:23<9:28:28, 118.43s/it] 38%|███▊      | 179/466 [5:43:17<9:20:26, 117.17s/it] 39%|███▊      | 180/466 [5:45:06<9:07:34, 114.88s/it]                                                      {'loss': 0.7152, 'grad_norm': 0.2223653793334961, 'learning_rate': 7.744644066735333e-05, 'epoch': 0.77}
 39%|███▊      | 180/466 [5:45:06<9:07:34, 114.88s/it] 39%|███▉      | 181/466 [5:46:58<9:01:35, 114.02s/it] 39%|███▉      | 182/466 [5:48:31<8:28:47, 107.49s/it] 39%|███▉      | 183/466 [5:50:29<8:42:30, 110.78s/it] 39%|███▉      | 184/466 [5:52:23<8:44:40, 111.63s/it] 40%|███▉      | 185/466 [5:54:04<8:28:01, 108.48s/it] 40%|███▉      | 186/466 [5:55:57<8:33:28, 110.03s/it] 40%|████      | 187/466 [5:57:59<8:47:55, 113.53s/it] 40%|████      | 188/466 [6:00:00<8:56:34, 115.81s/it] 41%|████      | 189/466 [6:01:48<8:43:21, 113.36s/it] 41%|████      | 190/466 [6:03:28<8:23:26, 109.44s/it]                                                      {'loss': 0.7323, 'grad_norm': 0.19280384480953217, 'learning_rate': 7.423865939300674e-05, 'epoch': 0.82}
 41%|████      | 190/466 [6:03:28<8:23:26, 109.44s/it] 41%|████      | 191/466 [6:05:34<8:44:08, 114.36s/it] 41%|████      | 192/466 [6:07:24<8:36:46, 113.16s/it] 41%|████▏     | 193/466 [6:09:21<8:39:41, 114.22s/it] 42%|████▏     | 194/466 [6:11:14<8:35:24, 113.69s/it] 42%|████▏     | 195/466 [6:13:04<8:29:41, 112.85s/it] 42%|████▏     | 196/466 [6:15:04<8:37:00, 114.89s/it] 42%|████▏     | 197/466 [6:16:57<8:32:00, 114.20s/it] 42%|████▏     | 198/466 [6:18:49<8:27:41, 113.66s/it] 43%|████▎     | 199/466 [6:20:46<8:29:45, 114.55s/it] 43%|████▎     | 200/466 [6:22:45<8:34:36, 116.08s/it]                                                      {'loss': 0.7123, 'grad_norm': 0.1857731193304062, 'learning_rate': 7.089467820638491e-05, 'epoch': 0.86}
 43%|████▎     | 200/466 [6:22:45<8:34:36, 116.08s/it]
***** Running Evaluation *****

***** Running Evaluation *****

***** Running Evaluation *****

***** Running Evaluation *****

***** Running Evaluation *****
[INFO|trainer.py:4643] 2025-12-09 07:53:15,732 >> 
***** Running Evaluation *****

***** Running Evaluation *****

***** Running Evaluation *****
  Num examples = 3305
  Num examples = 3305
  Num examples = 3305
  Batch size = 1
  Num examples = 3305
  Batch size = 1
  Num examples = 3305
  Batch size = 1
  Batch size = 1
  Batch size = 1
  Num examples = 3305
  Num examples = 3305
  Batch size = 1
  Batch size = 1
[INFO|trainer.py:4645] 2025-12-09 07:53:15,732 >>   Num examples = 3305
[INFO|trainer.py:4648] 2025-12-09 07:53:15,732 >>   Batch size = 1

  0%|          | 0/414 [00:00<?, ?it/s][A
  0%|          | 2/414 [00:04<14:23,  2.10s/it][A
  1%|          | 3/414 [00:08<20:36,  3.01s/it][A
  1%|          | 4/414 [00:14<28:52,  4.23s/it][A
  1%|          | 5/414 [00:21<35:51,  5.26s/it][A
  1%|▏         | 6/414 [00:25<31:45,  4.67s/it][A
  2%|▏         | 7/414 [00:29<29:28,  4.35s/it][A
  2%|▏         | 8/414 [00:33<28:52,  4.27s/it][A
  2%|▏         | 9/414 [00:37<28:55,  4.28s/it][A
  2%|▏         | 10/414 [00:44<33:26,  4.97s/it][A
  3%|▎         | 11/414 [00:48<32:44,  4.87s/it][A
  3%|▎         | 12/414 [00:53<33:22,  4.98s/it][A
  3%|▎         | 13/414 [00:59<34:10,  5.11s/it][A
  3%|▎         | 14/414 [01:04<33:18,  5.00s/it][A
  4%|▎         | 15/414 [01:09<33:35,  5.05s/it][A
  4%|▍         | 16/414 [01:13<32:30,  4.90s/it][A
  4%|▍         | 17/414 [01:20<35:12,  5.32s/it][A
  4%|▍         | 18/414 [01:25<34:40,  5.25s/it][A
  5%|▍         | 19/414 [01:30<34:51,  5.30s/it][A
  5%|▍         | 20/414 [01:36<36:59,  5.63s/it][A
  5%|▌         | 21/414 [01:40<33:28,  5.11s/it][A
  5%|▌         | 22/414 [01:45<32:26,  4.97s/it][A
  6%|▌         | 23/414 [01:51<33:52,  5.20s/it][A
  6%|▌         | 24/414 [01:56<33:47,  5.20s/it][A
  6%|▌         | 25/414 [02:01<34:13,  5.28s/it][A
  6%|▋         | 26/414 [02:07<35:08,  5.44s/it][A
  7%|▋         | 27/414 [02:11<31:43,  4.92s/it][A
  7%|▋         | 28/414 [02:16<31:05,  4.83s/it][A
  7%|▋         | 29/414 [02:20<30:18,  4.72s/it][A
  7%|▋         | 30/414 [02:26<31:59,  5.00s/it][A
  7%|▋         | 31/414 [02:31<31:57,  5.01s/it][A
  8%|▊         | 32/414 [02:34<28:52,  4.53s/it][A
  8%|▊         | 33/414 [02:39<29:47,  4.69s/it][A
  8%|▊         | 34/414 [02:44<30:17,  4.78s/it][A
  8%|▊         | 35/414 [02:49<31:00,  4.91s/it][A
  9%|▊         | 36/414 [02:54<30:02,  4.77s/it][A
  9%|▉         | 37/414 [02:59<30:08,  4.80s/it][A
  9%|▉         | 38/414 [03:03<29:53,  4.77s/it][A
  9%|▉         | 39/414 [03:07<27:30,  4.40s/it][A
 10%|▉         | 40/414 [03:12<27:45,  4.45s/it][A
 10%|▉         | 41/414 [03:17<28:56,  4.66s/it][A
 10%|█         | 42/414 [03:21<28:07,  4.54s/it][A
 10%|█         | 43/414 [03:28<33:05,  5.35s/it][A
 11%|█         | 44/414 [03:38<41:10,  6.68s/it][A
 11%|█         | 45/414 [03:44<40:16,  6.55s/it][A
 11%|█         | 46/414 [03:48<34:22,  5.60s/it][A
 11%|█▏        | 47/414 [03:51<29:47,  4.87s/it][A
 12%|█▏        | 48/414 [03:54<26:53,  4.41s/it][A
 12%|█▏        | 49/414 [03:57<23:30,  3.86s/it][A
 12%|█▏        | 50/414 [03:59<21:31,  3.55s/it][A
 12%|█▏        | 51/414 [04:03<20:42,  3.42s/it][A
 13%|█▎        | 52/414 [04:06<20:11,  3.35s/it][A
 13%|█▎        | 53/414 [04:12<25:33,  4.25s/it][A
 13%|█▎        | 54/414 [04:19<30:13,  5.04s/it][A
 13%|█▎        | 55/414 [04:25<31:04,  5.19s/it][A
 14%|█▎        | 56/414 [04:30<31:34,  5.29s/it][A
 14%|█▍        | 57/414 [04:34<29:17,  4.92s/it][A
 14%|█▍        | 58/414 [04:40<30:00,  5.06s/it][A
 14%|█▍        | 59/414 [04:46<32:50,  5.55s/it][A
 14%|█▍        | 60/414 [04:51<31:05,  5.27s/it][A
 15%|█▍        | 61/414 [04:54<28:09,  4.79s/it][A
 15%|█▍        | 62/414 [04:58<25:58,  4.43s/it][A
 15%|█▌        | 63/414 [05:02<24:50,  4.25s/it][A
 15%|█▌        | 64/414 [05:06<23:50,  4.09s/it][A
 16%|█▌        | 65/414 [05:10<24:41,  4.25s/it][A
 16%|█▌        | 66/414 [05:15<26:04,  4.50s/it][A
 16%|█▌        | 67/414 [05:22<29:33,  5.11s/it][A
 16%|█▋        | 68/414 [05:27<29:29,  5.11s/it][A
 17%|█▋        | 69/414 [05:32<29:40,  5.16s/it][A
 17%|█▋        | 70/414 [05:37<29:30,  5.15s/it][A
 17%|█▋        | 71/414 [05:41<27:36,  4.83s/it][A
 17%|█▋        | 72/414 [05:45<25:45,  4.52s/it][A
 18%|█▊        | 73/414 [05:50<25:26,  4.48s/it][A
 18%|█▊        | 74/414 [05:53<23:50,  4.21s/it][A
 18%|█▊        | 75/414 [05:56<22:04,  3.91s/it][A
 18%|█▊        | 76/414 [06:00<20:56,  3.72s/it][A
 19%|█▊        | 77/414 [06:04<22:26,  3.99s/it][A
 19%|█▉        | 78/414 [06:10<24:45,  4.42s/it][A
 19%|█▉        | 79/414 [06:14<24:34,  4.40s/it][A
 19%|█▉        | 80/414 [06:19<24:54,  4.47s/it][A
 20%|█▉        | 81/414 [06:23<24:21,  4.39s/it][A
 20%|█▉        | 82/414 [06:27<23:20,  4.22s/it][A
 20%|██        | 83/414 [06:33<27:10,  4.92s/it][A
 20%|██        | 84/414 [06:40<29:11,  5.31s/it][A
 21%|██        | 85/414 [06:43<26:49,  4.89s/it][A
 21%|██        | 86/414 [06:47<24:54,  4.56s/it][A
 21%|██        | 87/414 [06:50<22:39,  4.16s/it][A
 21%|██▏       | 88/414 [06:55<22:26,  4.13s/it][A
 21%|██▏       | 89/414 [07:00<23:50,  4.40s/it][A
 22%|██▏       | 90/414 [07:05<25:17,  4.68s/it][A
 22%|██▏       | 91/414 [07:09<24:46,  4.60s/it][A
 22%|██▏       | 92/414 [07:14<24:38,  4.59s/it][A
 22%|██▏       | 93/414 [07:18<24:25,  4.56s/it][A
 23%|██▎       | 94/414 [07:22<23:07,  4.34s/it][A
 23%|██▎       | 95/414 [07:26<22:48,  4.29s/it][A
 23%|██▎       | 96/414 [07:32<24:15,  4.58s/it][A
 23%|██▎       | 97/414 [07:36<23:47,  4.50s/it][A
 24%|██▎       | 98/414 [07:39<21:48,  4.14s/it][A
 24%|██▍       | 99/414 [07:43<20:59,  4.00s/it][A
 24%|██▍       | 100/414 [07:48<22:15,  4.25s/it][A
 24%|██▍       | 101/414 [07:54<24:35,  4.71s/it][A
 25%|██▍       | 102/414 [08:00<26:59,  5.19s/it][A
 25%|██▍       | 103/414 [08:03<24:19,  4.69s/it][A
 25%|██▌       | 104/414 [08:07<22:45,  4.40s/it][A
 25%|██▌       | 105/414 [08:11<22:31,  4.37s/it][A
 26%|██▌       | 106/414 [08:17<24:09,  4.71s/it][A
 26%|██▌       | 107/414 [08:22<25:21,  4.95s/it][A
 26%|██▌       | 108/414 [08:27<24:21,  4.78s/it][A
 26%|██▋       | 109/414 [08:31<23:50,  4.69s/it][A
 27%|██▋       | 110/414 [08:36<23:49,  4.70s/it][A
 27%|██▋       | 111/414 [08:41<23:29,  4.65s/it][A
 27%|██▋       | 112/414 [08:45<23:05,  4.59s/it][A
 27%|██▋       | 113/414 [08:49<22:46,  4.54s/it][A
 28%|██▊       | 114/414 [08:55<24:16,  4.85s/it][A
 28%|██▊       | 115/414 [09:00<23:46,  4.77s/it][A
 28%|██▊       | 116/414 [09:04<23:23,  4.71s/it][A
 28%|██▊       | 117/414 [09:10<24:35,  4.97s/it][A
 29%|██▊       | 118/414 [09:15<25:36,  5.19s/it][A
 29%|██▊       | 119/414 [09:21<26:27,  5.38s/it][A
 29%|██▉       | 120/414 [09:27<26:56,  5.50s/it][A
 29%|██▉       | 121/414 [09:32<26:13,  5.37s/it][A
 29%|██▉       | 122/414 [09:37<25:53,  5.32s/it][A
 30%|██▉       | 123/414 [09:42<24:36,  5.07s/it][A
 30%|██▉       | 124/414 [09:47<24:06,  4.99s/it][A
 30%|███       | 125/414 [09:51<23:24,  4.86s/it][A
 30%|███       | 126/414 [09:56<22:49,  4.76s/it][A
 31%|███       | 127/414 [10:01<24:00,  5.02s/it][A
 31%|███       | 128/414 [10:09<28:24,  5.96s/it][A
 31%|███       | 129/414 [10:15<28:10,  5.93s/it][A
 31%|███▏      | 130/414 [10:22<28:41,  6.06s/it][A
 32%|███▏      | 131/414 [10:28<29:24,  6.23s/it][A
 32%|███▏      | 132/414 [10:35<29:15,  6.23s/it][A
 32%|███▏      | 133/414 [10:38<25:28,  5.44s/it][A
 32%|███▏      | 134/414 [10:44<25:21,  5.43s/it][A
 33%|███▎      | 135/414 [10:48<24:23,  5.24s/it][A
 33%|███▎      | 136/414 [10:55<25:54,  5.59s/it][A
 33%|███▎      | 137/414 [11:01<27:03,  5.86s/it][A
 33%|███▎      | 138/414 [11:07<26:23,  5.74s/it][A
 34%|███▎      | 139/414 [11:10<23:30,  5.13s/it][A
 34%|███▍      | 140/414 [11:14<21:22,  4.68s/it][A
 34%|███▍      | 141/414 [11:18<19:57,  4.39s/it][A
 34%|███▍      | 142/414 [11:22<19:56,  4.40s/it][A
 35%|███▍      | 143/414 [11:27<20:13,  4.48s/it][A
 35%|███▍      | 144/414 [11:31<19:28,  4.33s/it][A
 35%|███▌      | 145/414 [11:35<18:51,  4.21s/it][A
 35%|███▌      | 146/414 [11:38<17:26,  3.90s/it][A
 36%|███▌      | 147/414 [11:42<17:45,  3.99s/it][A
 36%|███▌      | 148/414 [11:46<18:09,  4.10s/it][A
 36%|███▌      | 149/414 [11:51<18:15,  4.13s/it][A
 36%|███▌      | 150/414 [11:54<17:06,  3.89s/it][A
 36%|███▋      | 151/414 [11:57<15:57,  3.64s/it][A
 37%|███▋      | 152/414 [12:01<16:57,  3.88s/it][A
 37%|███▋      | 153/414 [12:06<17:32,  4.03s/it][A
 37%|███▋      | 154/414 [12:09<16:38,  3.84s/it][A
 37%|███▋      | 155/414 [12:13<16:07,  3.74s/it][A
 38%|███▊      | 156/414 [12:17<16:25,  3.82s/it][A
 38%|███▊      | 157/414 [12:21<17:12,  4.02s/it][A
 38%|███▊      | 158/414 [12:27<19:16,  4.52s/it][A
 38%|███▊      | 159/414 [12:32<20:27,  4.81s/it][A
 39%|███▊      | 160/414 [12:37<20:08,  4.76s/it][A
 39%|███▉      | 161/414 [12:42<20:39,  4.90s/it][A
 39%|███▉      | 162/414 [12:47<20:24,  4.86s/it][A
 39%|███▉      | 163/414 [12:52<20:39,  4.94s/it][A
 40%|███▉      | 164/414 [12:58<22:05,  5.30s/it][A
 40%|███▉      | 165/414 [13:05<23:42,  5.71s/it][A
 40%|████      | 166/414 [13:12<25:05,  6.07s/it][A
 40%|████      | 167/414 [13:17<24:00,  5.83s/it][A
 41%|████      | 168/414 [13:21<21:59,  5.36s/it][A
 41%|████      | 169/414 [13:26<20:25,  5.00s/it][A
 41%|████      | 170/414 [13:30<19:08,  4.71s/it][A
 41%|████▏     | 171/414 [13:33<17:59,  4.44s/it][A
 42%|████▏     | 172/414 [13:38<17:37,  4.37s/it][A
 42%|████▏     | 173/414 [13:43<18:52,  4.70s/it][A
 42%|████▏     | 174/414 [13:47<18:00,  4.50s/it][A
 42%|████▏     | 175/414 [13:52<18:32,  4.66s/it][A
 43%|████▎     | 176/414 [13:57<18:03,  4.55s/it][A
 43%|████▎     | 177/414 [14:01<18:26,  4.67s/it][A
 43%|████▎     | 178/414 [14:06<18:16,  4.64s/it][A
 43%|████▎     | 179/414 [14:10<17:13,  4.40s/it][A
 43%|████▎     | 180/414 [14:14<16:51,  4.32s/it][A
 44%|████▎     | 181/414 [14:19<17:10,  4.42s/it][A
 44%|████▍     | 182/414 [14:23<17:00,  4.40s/it][A
 44%|████▍     | 183/414 [14:27<17:02,  4.43s/it][A
 44%|████▍     | 184/414 [14:32<17:17,  4.51s/it][A
 45%|████▍     | 185/414 [14:36<16:29,  4.32s/it][A
 45%|████▍     | 186/414 [14:41<17:20,  4.56s/it][A
 45%|████▌     | 187/414 [14:51<22:57,  6.07s/it][A
 45%|████▌     | 188/414 [14:58<24:02,  6.38s/it][A
 46%|████▌     | 189/414 [15:04<23:38,  6.31s/it][A
 46%|████▌     | 190/414 [15:11<24:42,  6.62s/it][A
 46%|████▌     | 191/414 [15:16<22:11,  5.97s/it][A
 46%|████▋     | 192/414 [15:20<20:27,  5.53s/it][A
 47%|████▋     | 193/414 [15:24<18:42,  5.08s/it][A
 47%|████▋     | 194/414 [15:29<18:19,  5.00s/it][A
 47%|████▋     | 195/414 [15:33<17:27,  4.78s/it][A
 47%|████▋     | 196/414 [15:37<16:03,  4.42s/it][A
 48%|████▊     | 197/414 [15:43<17:13,  4.76s/it][A
 48%|████▊     | 198/414 [15:48<17:50,  4.96s/it][A
 48%|████▊     | 199/414 [15:52<16:41,  4.66s/it][A
 48%|████▊     | 200/414 [15:57<17:12,  4.83s/it][A
 49%|████▊     | 201/414 [16:02<17:30,  4.93s/it][A
 49%|████▉     | 202/414 [16:07<17:26,  4.94s/it][A
 49%|████▉     | 203/414 [16:11<16:34,  4.72s/it][A
 49%|████▉     | 204/414 [16:15<15:38,  4.47s/it][A
 50%|████▉     | 205/414 [16:19<14:39,  4.21s/it][A
 50%|████▉     | 206/414 [16:24<15:33,  4.49s/it][A
 50%|█████     | 207/414 [16:29<15:38,  4.53s/it][A
 50%|█████     | 208/414 [16:34<16:16,  4.74s/it][A
 50%|█████     | 209/414 [16:39<16:37,  4.87s/it][A
 51%|█████     | 210/414 [16:43<15:58,  4.70s/it][A
 51%|█████     | 211/414 [16:47<15:09,  4.48s/it][A
 51%|█████     | 212/414 [16:53<15:41,  4.66s/it][A
 51%|█████▏    | 213/414 [16:59<16:59,  5.07s/it][A
 52%|█████▏    | 214/414 [17:03<16:29,  4.95s/it][A
 52%|█████▏    | 215/414 [17:08<16:14,  4.90s/it][A
 52%|█████▏    | 216/414 [17:15<18:32,  5.62s/it][A
 52%|█████▏    | 217/414 [17:22<19:30,  5.94s/it][A
 53%|█████▎    | 218/414 [17:26<17:44,  5.43s/it][A
 53%|█████▎    | 219/414 [17:30<16:10,  4.97s/it][A
 53%|█████▎    | 220/414 [17:34<15:08,  4.69s/it][A
 53%|█████▎    | 221/414 [17:38<14:42,  4.57s/it][A
 54%|█████▎    | 222/414 [17:44<15:51,  4.96s/it][A
 54%|█████▍    | 223/414 [17:49<15:18,  4.81s/it][A
 54%|█████▍    | 224/414 [17:54<15:37,  4.94s/it][A
 54%|█████▍    | 225/414 [18:00<16:47,  5.33s/it][A
 55%|█████▍    | 226/414 [18:06<17:00,  5.43s/it][A
 55%|█████▍    | 227/414 [18:12<17:39,  5.67s/it][A
 55%|█████▌    | 228/414 [18:18<17:50,  5.76s/it][A
 55%|█████▌    | 229/414 [18:23<16:44,  5.43s/it][A
 56%|█████▌    | 230/414 [18:29<17:47,  5.80s/it][A
 56%|█████▌    | 231/414 [18:35<17:03,  5.59s/it][A
 56%|█████▌    | 232/414 [18:39<16:16,  5.37s/it][A
 56%|█████▋    | 233/414 [18:44<15:43,  5.21s/it][A
 57%|█████▋    | 234/414 [18:49<15:18,  5.11s/it][A
 57%|█████▋    | 235/414 [18:55<15:34,  5.22s/it][A
 57%|█████▋    | 236/414 [18:59<15:12,  5.13s/it][A
 57%|█████▋    | 237/414 [19:04<14:38,  4.97s/it][A
 57%|█████▋    | 238/414 [19:07<12:54,  4.40s/it][A
 58%|█████▊    | 239/414 [19:12<13:11,  4.52s/it][A
 58%|█████▊    | 240/414 [19:19<15:13,  5.25s/it][A
 58%|█████▊    | 241/414 [19:24<14:56,  5.18s/it][A
 58%|█████▊    | 242/414 [19:27<13:24,  4.68s/it][A
 59%|█████▊    | 243/414 [19:31<12:21,  4.33s/it][A
 59%|█████▉    | 244/414 [19:36<12:35,  4.44s/it][A
 59%|█████▉    | 245/414 [19:41<13:06,  4.65s/it][A
 59%|█████▉    | 246/414 [19:45<12:37,  4.51s/it][A
 60%|█████▉    | 247/414 [19:49<12:23,  4.45s/it][A
 60%|█████▉    | 248/414 [19:54<12:15,  4.43s/it][A
 60%|██████    | 249/414 [19:58<11:55,  4.34s/it][A
 60%|██████    | 250/414 [20:01<11:03,  4.05s/it][A
 61%|██████    | 251/414 [20:06<11:37,  4.28s/it][A
 61%|██████    | 252/414 [20:12<12:34,  4.66s/it][A
 61%|██████    | 253/414 [20:18<13:55,  5.19s/it][A
 61%|██████▏   | 254/414 [20:25<15:02,  5.64s/it][A
 62%|██████▏   | 255/414 [20:28<13:03,  4.92s/it][A
 62%|██████▏   | 256/414 [20:32<12:00,  4.56s/it][A
 62%|██████▏   | 257/414 [20:36<11:30,  4.40s/it][A
 62%|██████▏   | 258/414 [20:39<10:39,  4.10s/it][A
 63%|██████▎   | 259/414 [20:43<10:19,  4.00s/it][A
 63%|██████▎   | 260/414 [20:47<10:39,  4.15s/it][A
 63%|██████▎   | 261/414 [20:52<10:44,  4.21s/it][A
 63%|██████▎   | 262/414 [20:57<11:20,  4.48s/it][A
 64%|██████▎   | 263/414 [21:03<12:41,  5.04s/it][A
 64%|██████▍   | 264/414 [21:09<13:25,  5.37s/it][A
 64%|██████▍   | 265/414 [21:14<12:34,  5.06s/it][A
 64%|██████▍   | 266/414 [21:19<12:31,  5.08s/it][A
 64%|██████▍   | 267/414 [21:23<12:13,  4.99s/it][A
 65%|██████▍   | 268/414 [21:29<12:30,  5.14s/it][A
 65%|██████▍   | 269/414 [21:32<11:04,  4.58s/it][A
 65%|██████▌   | 270/414 [21:36<10:33,  4.40s/it][A
 65%|██████▌   | 271/414 [21:40<10:12,  4.29s/it][A
 66%|██████▌   | 272/414 [21:46<10:54,  4.61s/it][A
 66%|██████▌   | 273/414 [21:51<11:33,  4.92s/it][A
 66%|██████▌   | 274/414 [21:56<11:05,  4.75s/it][A
 66%|██████▋   | 275/414 [21:59<10:17,  4.44s/it][A
 67%|██████▋   | 276/414 [22:04<10:03,  4.37s/it][A
 67%|██████▋   | 277/414 [22:07<09:23,  4.11s/it][A
 67%|██████▋   | 278/414 [22:11<08:59,  3.96s/it][A
 67%|██████▋   | 279/414 [22:15<08:59,  4.00s/it][A
 68%|██████▊   | 280/414 [22:19<09:14,  4.14s/it][A
 68%|██████▊   | 281/414 [22:23<09:13,  4.16s/it][A
 68%|██████▊   | 282/414 [22:29<09:58,  4.53s/it][A
 68%|██████▊   | 283/414 [22:35<10:40,  4.89s/it][A
 69%|██████▊   | 284/414 [22:38<09:37,  4.45s/it][A
 69%|██████▉   | 285/414 [22:43<09:50,  4.58s/it][A
 69%|██████▉   | 286/414 [22:49<11:05,  5.20s/it][A
 69%|██████▉   | 287/414 [22:55<11:08,  5.27s/it][A
 70%|██████▉   | 288/414 [23:00<10:51,  5.17s/it][A
 70%|██████▉   | 289/414 [23:03<09:35,  4.60s/it][A
 70%|███████   | 290/414 [23:08<09:45,  4.72s/it][A
 70%|███████   | 291/414 [23:14<10:32,  5.14s/it][A
 71%|███████   | 292/414 [23:18<09:35,  4.72s/it][A
 71%|███████   | 293/414 [23:25<11:09,  5.53s/it][A
 71%|███████   | 294/414 [23:34<12:59,  6.50s/it][A
 71%|███████▏  | 295/414 [23:40<12:27,  6.28s/it][A
 71%|███████▏  | 296/414 [23:46<12:11,  6.20s/it][A
 72%|███████▏  | 297/414 [23:51<11:30,  5.90s/it][A
 72%|███████▏  | 298/414 [23:56<10:55,  5.65s/it][A
 72%|███████▏  | 299/414 [24:02<10:50,  5.66s/it][A
 72%|███████▏  | 300/414 [24:06<10:05,  5.31s/it][A
 73%|███████▎  | 301/414 [24:11<09:46,  5.19s/it][A
 73%|███████▎  | 302/414 [24:15<08:40,  4.65s/it][A
 73%|███████▎  | 303/414 [24:19<08:21,  4.52s/it][A
 73%|███████▎  | 304/414 [24:26<09:25,  5.15s/it][A
 74%|███████▎  | 305/414 [24:32<10:02,  5.53s/it][A
 74%|███████▍  | 306/414 [24:37<09:43,  5.40s/it][A
 74%|███████▍  | 307/414 [24:42<09:16,  5.20s/it][A
 74%|███████▍  | 308/414 [24:48<09:47,  5.55s/it][A
 75%|███████▍  | 309/414 [24:53<09:33,  5.46s/it][A
 75%|███████▍  | 310/414 [24:57<08:20,  4.81s/it][A
 75%|███████▌  | 311/414 [25:00<07:42,  4.49s/it][A
 75%|███████▌  | 312/414 [25:04<07:21,  4.33s/it][A
 76%|███████▌  | 313/414 [25:09<07:12,  4.28s/it][A
 76%|███████▌  | 314/414 [25:12<06:49,  4.09s/it][A
 76%|███████▌  | 315/414 [25:17<06:56,  4.21s/it][A
 76%|███████▋  | 316/414 [25:23<07:43,  4.73s/it][A
 77%|███████▋  | 317/414 [25:29<08:35,  5.32s/it][A
 77%|███████▋  | 318/414 [25:34<08:22,  5.23s/it][A
 77%|███████▋  | 319/414 [25:39<07:59,  5.04s/it][A
 77%|███████▋  | 320/414 [25:43<07:18,  4.66s/it][A
 78%|███████▊  | 321/414 [25:49<07:49,  5.05s/it][A
 78%|███████▊  | 322/414 [25:54<07:58,  5.20s/it][A
 78%|███████▊  | 323/414 [25:58<07:25,  4.90s/it][A
 78%|███████▊  | 324/414 [26:02<06:55,  4.61s/it][A
 79%|███████▊  | 325/414 [26:06<06:27,  4.35s/it][A
 79%|███████▊  | 326/414 [26:10<06:08,  4.19s/it][A
 79%|███████▉  | 327/414 [26:16<06:56,  4.78s/it][A
 79%|███████▉  | 328/414 [26:22<07:11,  5.02s/it][A
 79%|███████▉  | 329/414 [26:28<07:30,  5.29s/it][A
 80%|███████▉  | 330/414 [26:34<07:52,  5.62s/it][A
 80%|███████▉  | 331/414 [26:39<07:33,  5.47s/it][A
 80%|████████  | 332/414 [26:45<07:27,  5.46s/it][A
 80%|████████  | 333/414 [26:50<07:26,  5.51s/it][A
 81%|████████  | 334/414 [26:54<06:41,  5.02s/it][A
 81%|████████  | 335/414 [26:58<06:10,  4.68s/it][A
 81%|████████  | 336/414 [27:03<06:06,  4.70s/it][A
 81%|████████▏ | 337/414 [27:06<05:33,  4.33s/it][A
 82%|████████▏ | 338/414 [27:10<05:25,  4.29s/it][A
 82%|████████▏ | 339/414 [27:17<06:20,  5.07s/it][A
 82%|████████▏ | 340/414 [27:23<06:34,  5.33s/it][A
 82%|████████▏ | 341/414 [27:29<06:36,  5.43s/it][A
 83%|████████▎ | 342/414 [27:32<05:50,  4.87s/it][A
 83%|████████▎ | 343/414 [27:38<06:07,  5.18s/it][A
 83%|████████▎ | 344/414 [27:45<06:38,  5.69s/it][A
 83%|████████▎ | 345/414 [27:50<06:21,  5.52s/it][A
 84%|████████▎ | 346/414 [27:55<05:48,  5.13s/it][A
 84%|████████▍ | 347/414 [27:59<05:29,  4.92s/it][A
 84%|████████▍ | 348/414 [28:03<05:07,  4.65s/it][A
 84%|████████▍ | 349/414 [28:06<04:38,  4.29s/it][A
 85%|████████▍ | 350/414 [28:11<04:46,  4.48s/it][A
 85%|████████▍ | 351/414 [28:18<05:27,  5.19s/it][A
 85%|████████▌ | 352/414 [28:22<04:55,  4.77s/it][A
 85%|████████▌ | 353/414 [28:26<04:28,  4.40s/it][A
 86%|████████▌ | 354/414 [28:29<04:12,  4.21s/it][A
 86%|████████▌ | 355/414 [28:33<03:51,  3.92s/it][A
 86%|████████▌ | 356/414 [28:36<03:46,  3.90s/it][A
 86%|████████▌ | 357/414 [28:41<03:53,  4.09s/it][A
 86%|████████▋ | 358/414 [28:48<04:34,  4.90s/it][A
 87%|████████▋ | 359/414 [28:53<04:42,  5.15s/it][A
 87%|████████▋ | 360/414 [28:58<04:23,  4.87s/it][A
 87%|████████▋ | 361/414 [29:01<04:00,  4.53s/it][A
 87%|████████▋ | 362/414 [29:07<04:10,  4.83s/it][A
 88%|████████▊ | 363/414 [29:14<04:33,  5.36s/it][A
 88%|████████▊ | 364/414 [29:17<04:02,  4.86s/it][A
 88%|████████▊ | 365/414 [29:23<04:09,  5.10s/it][A
 88%|████████▊ | 366/414 [29:29<04:22,  5.46s/it][A
 89%|████████▊ | 367/414 [29:34<04:11,  5.35s/it][A
 89%|████████▉ | 368/414 [29:39<04:02,  5.27s/it][A
 89%|████████▉ | 369/414 [29:44<03:54,  5.21s/it][A
 89%|████████▉ | 370/414 [29:50<03:57,  5.40s/it][A
 90%|████████▉ | 371/414 [29:56<03:56,  5.49s/it][A
 90%|████████▉ | 372/414 [30:02<04:02,  5.78s/it][A
 90%|█████████ | 373/414 [30:08<03:56,  5.76s/it][A
 90%|█████████ | 374/414 [30:13<03:44,  5.62s/it][A
 91%|█████████ | 375/414 [30:18<03:27,  5.33s/it][A
 91%|█████████ | 376/414 [30:22<03:03,  4.83s/it][A
 91%|█████████ | 377/414 [30:27<02:58,  4.83s/it][A
 91%|█████████▏| 378/414 [30:33<03:07,  5.20s/it][A
 92%|█████████▏| 379/414 [30:37<02:54,  5.00s/it][A
 92%|█████████▏| 380/414 [30:43<02:59,  5.27s/it][A
 92%|█████████▏| 381/414 [30:52<03:33,  6.48s/it][A
 92%|█████████▏| 382/414 [30:58<03:22,  6.32s/it][A
 93%|█████████▎| 383/414 [31:03<02:56,  5.69s/it][A
 93%|█████████▎| 384/414 [31:07<02:43,  5.47s/it][A
 93%|█████████▎| 385/414 [31:12<02:31,  5.21s/it][A
 93%|█████████▎| 386/414 [31:16<02:18,  4.96s/it][A
 93%|█████████▎| 387/414 [31:21<02:07,  4.72s/it][A
 94%|█████████▎| 388/414 [31:24<01:53,  4.36s/it][A
 94%|█████████▍| 389/414 [31:28<01:46,  4.25s/it][A
 94%|█████████▍| 390/414 [31:31<01:35,  3.96s/it][A
 94%|█████████▍| 391/414 [31:35<01:30,  3.95s/it][A
 95%|█████████▍| 392/414 [31:40<01:29,  4.08s/it][A
 95%|█████████▍| 393/414 [31:45<01:31,  4.38s/it][A
 95%|█████████▌| 394/414 [31:50<01:33,  4.68s/it][A
 95%|█████████▌| 395/414 [31:54<01:24,  4.45s/it][A
 96%|█████████▌| 396/414 [32:00<01:26,  4.82s/it][A
 96%|█████████▌| 397/414 [32:06<01:26,  5.11s/it][A
 96%|█████████▌| 398/414 [32:10<01:20,  5.01s/it][A
 96%|█████████▋| 399/414 [32:14<01:06,  4.46s/it][A
 97%|█████████▋| 400/414 [32:20<01:12,  5.21s/it][A
 97%|█████████▋| 401/414 [32:28<01:17,  5.98s/it][A
 97%|█████████▋| 402/414 [32:33<01:07,  5.60s/it][A
 97%|█████████▋| 403/414 [32:36<00:54,  4.94s/it][A
 98%|█████████▊| 404/414 [32:40<00:45,  4.56s/it][A
 98%|█████████▊| 405/414 [32:44<00:39,  4.35s/it][A
 98%|█████████▊| 406/414 [32:49<00:36,  4.54s/it][A
 98%|█████████▊| 407/414 [32:55<00:33,  4.86s/it][A
 99%|█████████▊| 408/414 [33:00<00:29,  5.00s/it][A
 99%|█████████▉| 409/414 [33:05<00:24,  4.99s/it][A
 99%|█████████▉| 410/414 [33:09<00:19,  4.88s/it][A
 99%|█████████▉| 411/414 [33:13<00:13,  4.48s/it][A
100%|█████████▉| 412/414 [33:18<00:09,  4.73s/it][A
100%|█████████▉| 413/414 [33:24<00:05,  5.04s/it][A
100%|██████████| 414/414 [33:26<00:00,  4.25s/it][A                                                      
                                                 [A{'eval_loss': 0.7093042731285095, 'eval_runtime': 2013.6895, 'eval_samples_per_second': 1.641, 'eval_steps_per_second': 0.206, 'epoch': 0.86}
 43%|████▎     | 200/466 [6:56:19<8:34:36, 116.08s/it]
100%|██████████| 414/414 [33:27<00:00,  4.25s/it][A
                                                 [A[INFO|trainer.py:4309] 2025-12-09 08:26:57,679 >> Saving model checkpoint to saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200
/opt/conda/lib/python3.11/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in Qwen/Qwen2.5-VL-7B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2421] 2025-12-09 08:26:57,820 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-09 08:26:57,823 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-09 08:26:57,826 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/special_tokens_map.json
[2025-12-09 08:26:58,472] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!
[2025-12-09 08:26:58,499] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/global_step200/mp_rank_00_model_states.pt
[2025-12-09 08:26:58,499] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/global_step200/mp_rank_00_model_states.pt...
[2025-12-09 08:26:58,585] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/global_step200/mp_rank_00_model_states.pt.
[2025-12-09 08:26:58,591] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-12-09 08:26:58,591] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2025-12-09 08:26:58,591] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2025-12-09 08:26:58,591] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2025-12-09 08:26:58,591] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2025-12-09 08:26:58,591] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2025-12-09 08:26:58,591] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2025-12-09 08:26:58,591] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2025-12-09 08:26:58,717] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2025-12-09 08:26:58,717] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2025-12-09 08:26:58,717] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2025-12-09 08:26:58,735] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2025-12-09 08:26:58,735] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt
[2025-12-09 08:26:58,735] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2025-12-09 08:26:58,738] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-12-09 08:26:58,738] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2025-12-09 08:26:58,738] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2025-12-09 08:26:58,738] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2025-12-09 08:26:58,745] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2025-12-09 08:26:58,745] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2025-12-09 08:26:58,745] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2025-12-09 08:26:58,745] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2025-12-09 08:26:58,745] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2025-12-09 08:26:58,745] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2025-12-09 08:26:58,747] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2025-12-09 08:26:58,747] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2025-12-09 08:26:58,747] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2025-12-09 08:26:58,747] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt
[2025-12-09 08:26:58,747] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2025-12-09 08:26:58,747] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2025-12-09 08:26:58,747] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-12-09 08:26:58,748] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[INFO|image_processing_base.py:253] 2025-12-09 08:26:58,764 >> Image processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/preprocessor_config.json
[INFO|tokenization_utils_base.py:2421] 2025-12-09 08:26:58,767 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-09 08:26:58,769 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-09 08:26:58,771 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/special_tokens_map.json
[INFO|video_processing_utils.py:600] 2025-12-09 08:26:58,930 >> Video processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/video_preprocessor_config.json
[INFO|processing_utils.py:814] 2025-12-09 08:26:58,932 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-200/chat_template.jinja
 43%|████▎     | 201/466 [6:58:26<53:15:07, 723.42s/it] 43%|████▎     | 202/466 [7:00:12<39:28:06, 538.21s/it] 44%|████▎     | 203/466 [7:01:53<29:43:51, 406.96s/it] 44%|████▍     | 204/466 [7:03:42<23:07:45, 317.81s/it] 44%|████▍     | 205/466 [7:05:27<18:23:29, 253.67s/it] 44%|████▍     | 206/466 [7:07:36<15:37:29, 216.34s/it] 44%|████▍     | 207/466 [7:09:40<13:35:00, 188.80s/it] 45%|████▍     | 208/466 [7:11:39<12:00:48, 167.63s/it] 45%|████▍     | 209/466 [7:13:29<10:43:59, 150.35s/it] 45%|████▌     | 210/466 [7:15:23<9:54:53, 139.43s/it]                                                       {'loss': 0.7127, 'grad_norm': 0.18156106770038605, 'learning_rate': 6.743328733581212e-05, 'epoch': 0.9}
 45%|████▌     | 210/466 [7:15:23<9:54:53, 139.43s/it] 45%|████▌     | 211/466 [7:17:14<9:16:32, 130.95s/it] 45%|████▌     | 212/466 [7:19:18<9:05:38, 128.89s/it] 46%|████▌     | 213/466 [7:21:18<8:51:59, 126.16s/it] 46%|████▌     | 214/466 [7:23:23<8:49:12, 126.00s/it] 46%|████▌     | 215/466 [7:25:26<8:42:40, 124.94s/it] 46%|████▋     | 216/466 [7:27:24<8:31:54, 122.86s/it] 47%|████▋     | 217/466 [7:29:32<8:36:09, 124.37s/it] 47%|████▋     | 218/466 [7:31:24<8:19:09, 120.76s/it] 47%|████▋     | 219/466 [7:33:19<8:10:06, 119.06s/it] 47%|████▋     | 220/466 [7:35:24<8:15:33, 120.87s/it]                                                      {'loss': 0.7226, 'grad_norm': 0.17221671342849731, 'learning_rate': 6.387393674860204e-05, 'epoch': 0.95}
 47%|████▋     | 220/466 [7:35:24<8:15:33, 120.87s/it] 47%|████▋     | 221/466 [7:37:21<8:08:47, 119.70s/it] 48%|████▊     | 222/466 [7:39:19<8:04:43, 119.20s/it] 48%|████▊     | 223/466 [7:41:20<8:04:28, 119.62s/it] 48%|████▊     | 224/466 [7:43:08<7:48:12, 116.08s/it] 48%|████▊     | 225/466 [7:44:42<7:19:42, 109.47s/it] 48%|████▊     | 226/466 [7:46:27<7:13:21, 108.34s/it] 49%|████▊     | 227/466 [7:48:42<7:43:33, 116.37s/it] 49%|████▉     | 228/466 [7:50:32<7:33:44, 114.39s/it] 49%|████▉     | 229/466 [7:52:20<7:23:46, 112.35s/it] 49%|████▉     | 230/466 [7:54:18<7:28:20, 113.99s/it]                                                      {'loss': 0.7185, 'grad_norm': 0.1718427836894989, 'learning_rate': 6.023662685937643e-05, 'epoch': 0.99}
 49%|████▉     | 230/466 [7:54:18<7:28:20, 113.99s/it] 50%|████▉     | 231/466 [7:56:17<7:33:26, 115.77s/it] 50%|████▉     | 232/466 [7:58:26<7:46:11, 119.54s/it] 50%|█████     | 233/466 [7:58:54<5:57:16, 92.00s/it]  50%|█████     | 234/466 [8:00:39<6:11:31, 96.08s/it] 50%|█████     | 235/466 [8:02:46<6:45:46, 105.39s/it] 51%|█████     | 236/466 [8:04:39<6:52:20, 107.57s/it] 51%|█████     | 237/466 [8:06:39<7:04:21, 111.19s/it] 51%|█████     | 238/466 [8:08:26<6:58:46, 110.21s/it] 51%|█████▏    | 239/466 [8:10:37<7:20:04, 116.32s/it] 52%|█████▏    | 240/466 [8:12:26<7:09:57, 114.15s/it]                                                      {'loss': 0.6878, 'grad_norm': 0.1991375982761383, 'learning_rate': 5.6541796145354564e-05, 'epoch': 1.03}
 52%|█████▏    | 240/466 [8:12:26<7:09:57, 114.15s/it] 52%|█████▏    | 241/466 [8:14:16<7:03:34, 112.96s/it] 52%|█████▏    | 242/466 [8:16:14<7:07:00, 114.38s/it] 52%|█████▏    | 243/466 [8:17:59<6:54:58, 111.65s/it] 52%|█████▏    | 244/466 [8:20:02<7:05:32, 115.01s/it] 53%|█████▎    | 245/466 [8:21:50<6:55:28, 112.80s/it] 53%|█████▎    | 246/466 [8:23:37<6:47:51, 111.23s/it] 53%|█████▎    | 247/466 [8:25:35<6:52:41, 113.06s/it] 53%|█████▎    | 248/466 [8:27:31<6:54:07, 113.98s/it] 53%|█████▎    | 249/466 [8:29:26<6:53:47, 114.41s/it] 54%|█████▎    | 250/466 [8:31:42<7:14:43, 120.76s/it]                                                      {'loss': 0.7101, 'grad_norm': 0.19728471338748932, 'learning_rate': 5.281020630011703e-05, 'epoch': 1.07}
 54%|█████▎    | 250/466 [8:31:42<7:14:43, 120.76s/it] 54%|█████▍    | 251/466 [8:33:25<6:54:15, 115.61s/it] 54%|█████▍    | 252/466 [8:35:22<6:53:14, 115.86s/it] 54%|█████▍    | 253/466 [8:37:17<6:51:05, 115.80s/it] 55%|█████▍    | 254/466 [8:39:15<6:51:28, 116.45s/it] 55%|█████▍    | 255/466 [8:41:34<7:12:48, 123.07s/it] 55%|█████▍    | 256/466 [8:43:15<6:47:48, 116.52s/it] 55%|█████▌    | 257/466 [8:45:27<7:02:16, 121.23s/it] 55%|█████▌    | 258/466 [8:47:39<7:10:52, 124.29s/it] 56%|█████▌    | 259/466 [8:49:44<7:09:28, 124.48s/it] 56%|█████▌    | 260/466 [8:51:47<7:05:50, 124.03s/it]                                                      {'loss': 0.7096, 'grad_norm': 0.18352073431015015, 'learning_rate': 4.9062825571178164e-05, 'epoch': 1.12}
 56%|█████▌    | 260/466 [8:51:47<7:05:50, 124.03s/it] 56%|█████▌    | 261/466 [8:53:38<6:50:57, 120.28s/it] 56%|█████▌    | 262/466 [8:55:23<6:32:36, 115.48s/it] 56%|█████▋    | 263/466 [8:57:31<6:43:42, 119.33s/it] 57%|█████▋    | 264/466 [8:59:32<6:43:57, 119.99s/it] 57%|█████▋    | 265/466 [9:01:17<6:26:17, 115.31s/it] 57%|█████▋    | 266/466 [9:03:07<6:19:15, 113.78s/it] 57%|█████▋    | 267/466 [9:05:12<6:28:52, 117.25s/it] 58%|█████▊    | 268/466 [9:06:56<6:13:01, 113.04s/it] 58%|█████▊    | 269/466 [9:08:55<6:17:30, 114.98s/it] 58%|█████▊    | 270/466 [9:10:46<6:11:13, 113.64s/it]                                                      {'loss': 0.7096, 'grad_norm': 0.177341490983963, 'learning_rate': 4.5320710936907414e-05, 'epoch': 1.16}
 58%|█████▊    | 270/466 [9:10:46<6:11:13, 113.64s/it] 58%|█████▊    | 271/466 [9:12:44<6:13:44, 115.00s/it] 58%|█████▊    | 272/466 [9:14:41<6:13:45, 115.59s/it] 59%|█████▊    | 273/466 [9:16:28<6:03:39, 113.06s/it] 59%|█████▉    | 274/466 [9:18:18<5:59:05, 112.22s/it] 59%|█████▉    | 275/466 [9:20:16<6:03:00, 114.03s/it] 59%|█████▉    | 276/466 [9:22:28<6:17:38, 119.25s/it] 59%|█████▉    | 277/466 [9:24:26<6:14:14, 118.81s/it] 60%|█████▉    | 278/466 [9:26:17<6:05:20, 116.60s/it] 60%|█████▉    | 279/466 [9:28:24<6:13:24, 119.81s/it] 60%|██████    | 280/466 [9:30:27<6:14:17, 120.74s/it]                                                      {'loss': 0.6952, 'grad_norm': 0.16760753095149994, 'learning_rate': 4.1604889784861503e-05, 'epoch': 1.2}
 60%|██████    | 280/466 [9:30:27<6:14:17, 120.74s/it] 60%|██████    | 281/466 [9:32:21<6:05:25, 118.52s/it] 61%|██████    | 282/466 [9:34:43<6:25:53, 125.83s/it] 61%|██████    | 283/466 [9:36:30<6:06:08, 120.05s/it] 61%|██████    | 284/466 [9:38:26<6:00:31, 118.85s/it] 61%|██████    | 285/466 [9:40:25<5:58:18, 118.78s/it] 61%|██████▏   | 286/466 [9:42:21<5:54:07, 118.04s/it] 62%|██████▏   | 287/466 [9:44:10<5:43:41, 115.20s/it] 62%|██████▏   | 288/466 [9:46:01<5:38:27, 114.09s/it] 62%|██████▏   | 289/466 [9:47:53<5:34:22, 113.35s/it] 62%|██████▏   | 290/466 [9:50:01<5:45:13, 117.69s/it]                                                      {'loss': 0.6915, 'grad_norm': 0.18666745722293854, 'learning_rate': 3.793624175639074e-05, 'epoch': 1.25}
 62%|██████▏   | 290/466 [9:50:01<5:45:13, 117.69s/it] 62%|██████▏   | 291/466 [9:51:55<5:40:17, 116.67s/it] 63%|██████▎   | 292/466 [9:54:01<5:46:24, 119.45s/it] 63%|██████▎   | 293/466 [9:56:05<5:48:10, 120.75s/it] 63%|██████▎   | 294/466 [9:57:59<5:40:57, 118.94s/it] 63%|██████▎   | 295/466 [10:00:17<5:55:25, 124.71s/it] 64%|██████▎   | 296/466 [10:02:02<5:35:58, 118.58s/it] 64%|██████▎   | 297/466 [10:04:11<5:43:03, 121.79s/it] 64%|██████▍   | 298/466 [10:05:57<5:27:30, 116.96s/it] 64%|██████▍   | 299/466 [10:07:49<5:21:21, 115.46s/it] 64%|██████▍   | 300/466 [10:09:45<5:20:11, 115.73s/it]                                                       {'loss': 0.6924, 'grad_norm': 0.19171136617660522, 'learning_rate': 3.4335381421449054e-05, 'epoch': 1.29}
 64%|██████▍   | 300/466 [10:09:45<5:20:11, 115.73s/it] 65%|██████▍   | 301/466 [10:11:27<5:07:00, 111.64s/it] 65%|██████▍   | 302/466 [10:13:09<4:57:05, 108.69s/it] 65%|██████▌   | 303/466 [10:15:01<4:58:11, 109.77s/it] 65%|██████▌   | 304/466 [10:17:00<5:04:04, 112.62s/it] 65%|██████▌   | 305/466 [10:18:54<5:03:01, 112.93s/it] 66%|██████▌   | 306/466 [10:20:47<5:01:22, 113.01s/it] 66%|██████▌   | 307/466 [10:22:34<4:54:27, 111.12s/it] 66%|██████▌   | 308/466 [10:24:32<4:58:24, 113.32s/it] 66%|██████▋   | 309/466 [10:26:26<4:56:52, 113.46s/it] 67%|██████▋   | 310/466 [10:28:21<4:55:41, 113.73s/it]                                                       {'loss': 0.7059, 'grad_norm': 0.18621180951595306, 'learning_rate': 3.0822542442871624e-05, 'epoch': 1.33}
 67%|██████▋   | 310/466 [10:28:21<4:55:41, 113.73s/it] 67%|██████▋   | 311/466 [10:30:12<4:51:44, 112.94s/it] 67%|██████▋   | 312/466 [10:31:49<4:38:03, 108.33s/it] 67%|██████▋   | 313/466 [10:33:34<4:33:26, 107.23s/it] 67%|██████▋   | 314/466 [10:35:27<4:35:42, 108.83s/it] 68%|██████▊   | 315/466 [10:37:17<4:34:52, 109.22s/it] 68%|██████▊   | 316/466 [10:38:53<4:23:15, 105.30s/it] 68%|██████▊   | 317/466 [10:40:40<4:22:53, 105.86s/it] 68%|██████▊   | 318/466 [10:42:18<4:15:03, 103.40s/it] 68%|██████▊   | 319/466 [10:44:18<4:26:09, 108.64s/it] 69%|██████▊   | 320/466 [10:46:22<4:35:01, 113.03s/it]                                                       {'loss': 0.6952, 'grad_norm': 0.1774919033050537, 'learning_rate': 2.7417463881014925e-05, 'epoch': 1.37}
 69%|██████▊   | 320/466 [10:46:22<4:35:01, 113.03s/it] 69%|██████▉   | 321/466 [10:48:19<4:36:27, 114.39s/it] 69%|██████▉   | 322/466 [10:50:12<4:33:17, 113.87s/it] 69%|██████▉   | 323/466 [10:52:14<4:37:10, 116.30s/it] 70%|██████▉   | 324/466 [10:54:02<4:29:34, 113.90s/it] 70%|██████▉   | 325/466 [10:55:56<4:27:46, 113.95s/it] 70%|██████▉   | 326/466 [10:58:05<4:36:02, 118.30s/it] 70%|███████   | 327/466 [11:00:08<4:37:11, 119.65s/it] 70%|███████   | 328/466 [11:02:04<4:32:38, 118.54s/it] 71%|███████   | 329/466 [11:04:00<4:29:34, 118.06s/it] 71%|███████   | 330/466 [11:06:08<4:33:49, 120.81s/it]                                                       {'loss': 0.7089, 'grad_norm': 0.19985419511795044, 'learning_rate': 2.4139279277627114e-05, 'epoch': 1.42}
 71%|███████   | 330/466 [11:06:08<4:33:49, 120.81s/it] 71%|███████   | 331/466 [11:08:02<4:27:14, 118.77s/it] 71%|███████   | 332/466 [11:10:11<4:32:13, 121.89s/it] 71%|███████▏  | 333/466 [11:12:08<4:26:49, 120.37s/it] 72%|███████▏  | 334/466 [11:14:15<4:29:12, 122.37s/it] 72%|███████▏  | 335/466 [11:16:06<4:20:08, 119.15s/it] 72%|███████▏  | 336/466 [11:17:58<4:13:04, 116.81s/it] 72%|███████▏  | 337/466 [11:19:48<4:07:11, 114.97s/it] 73%|███████▎  | 338/466 [11:21:39<4:02:33, 113.70s/it] 73%|███████▎  | 339/466 [11:23:29<3:57:58, 112.43s/it] 73%|███████▎  | 340/466 [11:25:15<3:52:00, 110.48s/it]                                                       {'loss': 0.6996, 'grad_norm': 0.1763666868209839, 'learning_rate': 2.100640914219939e-05, 'epoch': 1.46}
 73%|███████▎  | 340/466 [11:25:15<3:52:00, 110.48s/it] 73%|███████▎  | 341/466 [11:27:10<3:53:21, 112.01s/it] 73%|███████▎  | 342/466 [11:28:56<3:47:56, 110.30s/it] 74%|███████▎  | 343/466 [11:30:42<3:42:54, 108.74s/it] 74%|███████▍  | 344/466 [11:32:28<3:39:49, 108.11s/it] 74%|███████▍  | 345/466 [11:34:29<3:45:45, 111.94s/it] 74%|███████▍  | 346/466 [11:36:40<3:55:35, 117.80s/it] 74%|███████▍  | 347/466 [11:38:48<3:59:34, 120.80s/it] 75%|███████▍  | 348/466 [11:40:58<4:02:50, 123.47s/it] 75%|███████▍  | 349/466 [11:42:42<3:49:16, 117.58s/it] 75%|███████▌  | 350/466 [11:44:31<3:42:28, 115.07s/it]                                                       {'loss': 0.686, 'grad_norm': 0.17418374121189117, 'learning_rate': 1.8036457444930644e-05, 'epoch': 1.5}
 75%|███████▌  | 350/466 [11:44:31<3:42:28, 115.07s/it] 75%|███████▌  | 351/466 [11:46:22<3:38:17, 113.89s/it] 76%|███████▌  | 352/466 [11:48:00<3:27:14, 109.08s/it] 76%|███████▌  | 353/466 [11:49:50<3:26:07, 109.45s/it] 76%|███████▌  | 354/466 [11:51:38<3:23:27, 109.00s/it] 76%|███████▌  | 355/466 [11:53:25<3:20:34, 108.42s/it] 76%|███████▋  | 356/466 [11:55:16<3:20:15, 109.23s/it] 77%|███████▋  | 357/466 [11:57:28<3:30:48, 116.04s/it] 77%|███████▋  | 358/466 [11:59:14<3:23:28, 113.04s/it] 77%|███████▋  | 359/466 [12:01:24<3:30:22, 117.97s/it] 77%|███████▋  | 360/466 [12:03:12<3:23:08, 114.99s/it]                                                       {'loss': 0.6947, 'grad_norm': 0.16495759785175323, 'learning_rate': 1.5246112697923387e-05, 'epoch': 1.55}
 77%|███████▋  | 360/466 [12:03:12<3:23:08, 114.99s/it] 77%|███████▋  | 361/466 [12:05:12<3:24:02, 116.59s/it] 78%|███████▊  | 362/466 [12:07:09<3:21:59, 116.53s/it] 78%|███████▊  | 363/466 [12:08:56<3:15:17, 113.76s/it] 78%|███████▊  | 364/466 [12:10:35<3:06:05, 109.47s/it] 78%|███████▊  | 365/466 [12:12:16<2:59:39, 106.73s/it] 79%|███████▊  | 366/466 [12:14:18<3:05:36, 111.36s/it] 79%|███████▉  | 367/466 [12:16:30<3:14:01, 117.59s/it] 79%|███████▉  | 368/466 [12:18:06<3:01:33, 111.16s/it] 79%|███████▉  | 369/466 [12:19:52<2:57:05, 109.54s/it] 79%|███████▉  | 370/466 [12:21:42<2:55:38, 109.78s/it]                                                       {'loss': 0.7083, 'grad_norm': 0.17400696873664856, 'learning_rate': 1.265105418044793e-05, 'epoch': 1.59}
 79%|███████▉  | 370/466 [12:21:42<2:55:38, 109.78s/it] 80%|███████▉  | 371/466 [12:23:36<2:55:36, 110.91s/it] 80%|███████▉  | 372/466 [12:25:42<3:00:42, 115.34s/it] 80%|████████  | 373/466 [12:27:38<2:59:19, 115.70s/it] 80%|████████  | 374/466 [12:29:35<2:57:48, 115.96s/it] 80%|████████  | 375/466 [12:31:30<2:55:38, 115.81s/it] 81%|████████  | 376/466 [12:33:22<2:51:48, 114.54s/it] 81%|████████  | 377/466 [12:35:06<2:45:27, 111.54s/it] 81%|████████  | 378/466 [12:36:54<2:41:44, 110.28s/it] 81%|████████▏ | 379/466 [12:38:49<2:42:19, 111.95s/it] 82%|████████▏ | 380/466 [12:40:46<2:42:21, 113.27s/it]                                                       {'loss': 0.6918, 'grad_norm': 0.18304570019245148, 'learning_rate': 1.0265863835205707e-05, 'epoch': 1.63}
 82%|████████▏ | 380/466 [12:40:46<2:42:21, 113.27s/it] 82%|████████▏ | 381/466 [12:42:26<2:34:59, 109.41s/it] 82%|████████▏ | 382/466 [12:44:21<2:35:24, 111.01s/it] 82%|████████▏ | 383/466 [12:46:00<2:28:49, 107.58s/it] 82%|████████▏ | 384/466 [12:47:51<2:28:21, 108.56s/it] 83%|████████▎ | 385/466 [12:49:32<2:23:11, 106.07s/it] 83%|████████▎ | 386/466 [12:51:22<2:23:09, 107.37s/it] 83%|████████▎ | 387/466 [12:53:07<2:20:33, 106.75s/it] 83%|████████▎ | 388/466 [12:55:02<2:22:01, 109.25s/it] 83%|████████▎ | 389/466 [12:56:57<2:22:16, 110.87s/it] 84%|████████▎ | 390/466 [12:59:09<2:28:33, 117.29s/it]                                                       {'loss': 0.6882, 'grad_norm': 0.17766310274600983, 'learning_rate': 8.103944330657665e-06, 'epoch': 1.68}
 84%|████████▎ | 390/466 [12:59:09<2:28:33, 117.29s/it] 84%|████████▍ | 391/466 [13:01:00<2:24:17, 115.43s/it] 84%|████████▍ | 392/466 [13:03:00<2:24:02, 116.80s/it] 84%|████████▍ | 393/466 [13:05:12<2:27:40, 121.37s/it] 85%|████████▍ | 394/466 [13:07:13<2:25:16, 121.06s/it] 85%|████████▍ | 395/466 [13:09:09<2:21:29, 119.58s/it] 85%|████████▍ | 396/466 [13:11:25<2:25:25, 124.65s/it] 85%|████████▌ | 397/466 [13:13:26<2:21:52, 123.37s/it] 85%|████████▌ | 398/466 [13:15:09<2:12:49, 117.20s/it] 86%|████████▌ | 399/466 [13:17:05<2:10:46, 117.11s/it] 86%|████████▌ | 400/466 [13:18:49<2:04:23, 113.08s/it]                                                       {'loss': 0.6966, 'grad_norm': 0.19120194017887115, 'learning_rate': 6.177443749834744e-06, 'epoch': 1.72}
 86%|████████▌ | 400/466 [13:18:49<2:04:23, 113.08s/it]
***** Running Evaluation *****

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****

***** Running Evaluation *****

***** Running Evaluation *****

***** Running Evaluation *****
[INFO|trainer.py:4643] 2025-12-09 14:49:19,470 >> 
***** Running Evaluation *****
  Num examples = 3305
  Num examples = 3305
  Batch size = 1
  Batch size = 1
  Num examples = 3305
  Num examples = 3305
  Batch size = 1
  Batch size = 1
[INFO|trainer.py:4645] 2025-12-09 14:49:19,471 >>   Num examples = 3305

***** Running Evaluation *****
[INFO|trainer.py:4648] 2025-12-09 14:49:19,471 >>   Batch size = 1
  Num examples = 3305
  Batch size = 1

  0%|          | 0/414 [00:00<?, ?it/s][A
  0%|          | 2/414 [00:04<14:23,  2.10s/it][A
  1%|          | 3/414 [00:08<20:35,  3.01s/it][A
  1%|          | 4/414 [00:14<29:12,  4.27s/it][A
  1%|          | 5/414 [00:21<35:38,  5.23s/it][A
  1%|▏         | 6/414 [00:25<31:38,  4.65s/it][A
  2%|▏         | 7/414 [00:29<29:22,  4.33s/it][A
  2%|▏         | 8/414 [00:33<28:43,  4.24s/it][A
  2%|▏         | 9/414 [00:37<28:56,  4.29s/it][A
  2%|▏         | 10/414 [00:43<33:27,  4.97s/it][A
  3%|▎         | 11/414 [00:48<32:47,  4.88s/it][A
  3%|▎         | 12/414 [00:53<33:32,  5.01s/it][A
  3%|▎         | 13/414 [00:59<34:19,  5.14s/it][A
  3%|▎         | 14/414 [01:04<33:27,  5.02s/it][A
  4%|▎         | 15/414 [01:09<33:40,  5.06s/it][A
  4%|▍         | 16/414 [01:13<32:31,  4.90s/it][A
  4%|▍         | 17/414 [01:20<35:12,  5.32s/it][A
  4%|▍         | 18/414 [01:25<34:44,  5.26s/it][A
  5%|▍         | 19/414 [01:30<34:55,  5.30s/it][A
  5%|▍         | 20/414 [01:37<37:02,  5.64s/it][A
  5%|▌         | 21/414 [01:41<33:33,  5.12s/it][A
  5%|▌         | 22/414 [01:45<32:31,  4.98s/it][A
  6%|▌         | 23/414 [01:51<33:53,  5.20s/it][A
  6%|▌         | 24/414 [01:56<33:41,  5.18s/it][A
  6%|▌         | 25/414 [02:01<34:08,  5.27s/it][A
  6%|▋         | 26/414 [02:07<34:49,  5.38s/it][A
  7%|▋         | 27/414 [02:11<31:32,  4.89s/it][A
  7%|▋         | 28/414 [02:15<30:55,  4.81s/it][A
  7%|▋         | 29/414 [02:20<30:19,  4.73s/it][A
  7%|▋         | 30/414 [02:26<32:00,  5.00s/it][A
  7%|▋         | 31/414 [02:31<32:00,  5.02s/it][A
  8%|▊         | 32/414 [02:34<28:56,  4.55s/it][A
  8%|▊         | 33/414 [02:39<29:49,  4.70s/it][A
  8%|▊         | 34/414 [02:44<30:22,  4.79s/it][A
  8%|▊         | 35/414 [02:49<31:07,  4.93s/it][A
  9%|▊         | 36/414 [02:54<30:08,  4.78s/it][A
  9%|▉         | 37/414 [02:59<30:15,  4.82s/it][A
  9%|▉         | 38/414 [03:04<29:56,  4.78s/it][A
  9%|▉         | 39/414 [03:07<27:35,  4.42s/it][A
 10%|▉         | 40/414 [03:12<27:44,  4.45s/it][A
 10%|▉         | 41/414 [03:17<28:56,  4.66s/it][A
 10%|█         | 42/414 [03:21<28:07,  4.54s/it][A
 10%|█         | 43/414 [03:28<33:04,  5.35s/it][A
 11%|█         | 44/414 [03:38<40:46,  6.61s/it][A
 11%|█         | 45/414 [03:44<40:04,  6.52s/it][A
 11%|█         | 46/414 [03:48<34:16,  5.59s/it][A
 11%|█▏        | 47/414 [03:51<29:42,  4.86s/it][A
 12%|█▏        | 48/414 [03:54<26:55,  4.41s/it][A
 12%|█▏        | 49/414 [03:57<23:36,  3.88s/it][A
 12%|█▏        | 50/414 [03:59<21:31,  3.55s/it][A
 12%|█▏        | 51/414 [04:03<20:42,  3.42s/it][A
 13%|█▎        | 52/414 [04:06<20:04,  3.33s/it][A
 13%|█▎        | 53/414 [04:12<25:28,  4.24s/it][A
 13%|█▎        | 54/414 [04:18<29:16,  4.88s/it][A
 13%|█▎        | 55/414 [04:24<30:26,  5.09s/it][A
 14%|█▎        | 56/414 [04:30<31:08,  5.22s/it][A
 14%|█▍        | 57/414 [04:34<29:02,  4.88s/it][A
 14%|█▍        | 58/414 [04:39<29:51,  5.03s/it][A
 14%|█▍        | 59/414 [04:46<32:49,  5.55s/it][A
 14%|█▍        | 60/414 [04:50<31:06,  5.27s/it][A
 15%|█▍        | 61/414 [04:54<28:07,  4.78s/it][A
 15%|█▍        | 62/414 [04:58<25:57,  4.43s/it][A
 15%|█▌        | 63/414 [05:01<24:54,  4.26s/it][A
 15%|█▌        | 64/414 [05:05<23:56,  4.10s/it][A
 16%|█▌        | 65/414 [05:10<24:43,  4.25s/it][A
 16%|█▌        | 66/414 [05:15<26:08,  4.51s/it][A
 16%|█▌        | 67/414 [05:21<29:37,  5.12s/it][A
 16%|█▋        | 68/414 [05:27<29:35,  5.13s/it][A
 17%|█▋        | 69/414 [05:32<29:46,  5.18s/it][A
 17%|█▋        | 70/414 [05:37<29:35,  5.16s/it][A
 17%|█▋        | 71/414 [05:41<27:46,  4.86s/it][A
 17%|█▋        | 72/414 [05:45<25:50,  4.53s/it][A
 18%|█▊        | 73/414 [05:49<25:33,  4.50s/it][A
 18%|█▊        | 74/414 [05:53<23:53,  4.22s/it][A
 18%|█▊        | 75/414 [05:56<22:07,  3.92s/it][A
 18%|█▊        | 76/414 [05:59<21:01,  3.73s/it][A
 19%|█▊        | 77/414 [06:04<22:35,  4.02s/it][A
 19%|█▉        | 78/414 [06:10<24:59,  4.46s/it][A
 19%|█▉        | 79/414 [06:14<24:44,  4.43s/it][A
 19%|█▉        | 80/414 [06:19<25:01,  4.50s/it][A
 20%|█▉        | 81/414 [06:23<24:31,  4.42s/it][A
 20%|█▉        | 82/414 [06:27<23:26,  4.24s/it][A
 20%|██        | 83/414 [06:33<27:14,  4.94s/it][A
 20%|██        | 84/414 [06:39<29:06,  5.29s/it][A
 21%|██        | 85/414 [06:43<26:50,  4.90s/it][A
 21%|██        | 86/414 [06:47<24:56,  4.56s/it][A
 21%|██        | 87/414 [06:50<22:45,  4.17s/it][A
 21%|██▏       | 88/414 [06:54<22:29,  4.14s/it][A
 21%|██▏       | 89/414 [07:00<23:53,  4.41s/it][A
 22%|██▏       | 90/414 [07:05<25:16,  4.68s/it][A
 22%|██▏       | 91/414 [07:09<24:42,  4.59s/it][A
 22%|██▏       | 92/414 [07:14<24:32,  4.57s/it][A
 22%|██▏       | 93/414 [07:18<24:21,  4.55s/it][A
 23%|██▎       | 94/414 [07:22<23:05,  4.33s/it][A
 23%|██▎       | 95/414 [07:26<22:46,  4.28s/it][A
 23%|██▎       | 96/414 [07:32<24:17,  4.58s/it][A
 23%|██▎       | 97/414 [07:36<23:47,  4.50s/it][A
 24%|██▎       | 98/414 [07:39<21:45,  4.13s/it][A
 24%|██▍       | 99/414 [07:43<20:56,  3.99s/it][A
 24%|██▍       | 100/414 [07:48<22:18,  4.26s/it][A
 24%|██▍       | 101/414 [07:53<24:40,  4.73s/it][A
 25%|██▍       | 102/414 [08:00<26:59,  5.19s/it][A
 25%|██▍       | 103/414 [08:03<24:19,  4.69s/it][A
 25%|██▌       | 104/414 [08:07<22:45,  4.41s/it][A
 25%|██▌       | 105/414 [08:11<22:33,  4.38s/it][A
 26%|██▌       | 106/414 [08:17<24:11,  4.71s/it][A
 26%|██▌       | 107/414 [08:22<25:22,  4.96s/it][A
 26%|██▌       | 108/414 [08:27<24:25,  4.79s/it][A
 26%|██▋       | 109/414 [08:31<23:53,  4.70s/it][A
 27%|██▋       | 110/414 [08:36<23:51,  4.71s/it][A
 27%|██▋       | 111/414 [08:41<23:31,  4.66s/it][A
 27%|██▋       | 112/414 [08:45<23:05,  4.59s/it][A
 27%|██▋       | 113/414 [08:49<22:49,  4.55s/it][A
 28%|██▊       | 114/414 [08:55<24:17,  4.86s/it][A
 28%|██▊       | 115/414 [09:00<23:52,  4.79s/it][A
 28%|██▊       | 116/414 [09:04<23:28,  4.73s/it][A
 28%|██▊       | 117/414 [09:10<24:42,  4.99s/it][A
 29%|██▊       | 118/414 [09:16<25:46,  5.22s/it][A
 29%|██▊       | 119/414 [09:21<26:35,  5.41s/it][A
 29%|██▉       | 120/414 [09:27<27:00,  5.51s/it][A
 29%|██▉       | 121/414 [09:32<26:17,  5.38s/it][A
 29%|██▉       | 122/414 [09:37<25:58,  5.34s/it][A
 30%|██▉       | 123/414 [09:42<24:44,  5.10s/it][A
 30%|██▉       | 124/414 [09:47<24:10,  5.00s/it][A
 30%|███       | 125/414 [09:51<23:30,  4.88s/it][A
 30%|███       | 126/414 [09:56<22:58,  4.79s/it][A
 31%|███       | 127/414 [10:02<24:11,  5.06s/it][A
 31%|███       | 128/414 [10:10<28:23,  5.96s/it][A
 31%|███       | 129/414 [10:16<28:14,  5.95s/it][A
 31%|███▏      | 130/414 [10:22<28:47,  6.08s/it][A
 32%|███▏      | 131/414 [10:29<29:28,  6.25s/it][A
 32%|███▏      | 132/414 [10:35<29:07,  6.20s/it][A
 32%|███▏      | 133/414 [10:38<25:23,  5.42s/it][A
 32%|███▏      | 134/414 [10:44<25:21,  5.43s/it][A
 33%|███▎      | 135/414 [10:49<24:23,  5.25s/it][A
 33%|███▎      | 136/414 [10:55<25:50,  5.58s/it][A
 33%|███▎      | 137/414 [11:01<26:59,  5.85s/it][A
 33%|███▎      | 138/414 [11:07<26:17,  5.72s/it][A
 34%|███▎      | 139/414 [11:11<23:27,  5.12s/it][A
 34%|███▍      | 140/414 [11:14<21:21,  4.68s/it][A
 34%|███▍      | 141/414 [11:18<19:56,  4.38s/it][A
 34%|███▍      | 142/414 [11:22<19:55,  4.40s/it][A
 35%|███▍      | 143/414 [11:27<20:11,  4.47s/it][A
 35%|███▍      | 144/414 [11:31<19:27,  4.33s/it][A
 35%|███▌      | 145/414 [11:35<18:52,  4.21s/it][A
 35%|███▌      | 146/414 [11:38<17:25,  3.90s/it][A
 36%|███▌      | 147/414 [11:42<17:43,  3.98s/it][A
 36%|███▌      | 148/414 [11:47<18:12,  4.11s/it][A
 36%|███▌      | 149/414 [11:51<18:14,  4.13s/it][A
 36%|███▌      | 150/414 [11:54<17:04,  3.88s/it][A
 36%|███▋      | 151/414 [11:57<15:56,  3.64s/it][A
 37%|███▋      | 152/414 [12:02<16:55,  3.87s/it][A
 37%|███▋      | 153/414 [12:06<17:31,  4.03s/it][A
 37%|███▋      | 154/414 [12:09<16:35,  3.83s/it][A
 37%|███▋      | 155/414 [12:13<16:06,  3.73s/it][A
 38%|███▊      | 156/414 [12:17<16:24,  3.82s/it][A
 38%|███▊      | 157/414 [12:21<17:11,  4.01s/it][A
 38%|███▊      | 158/414 [12:27<19:12,  4.50s/it][A
 38%|███▊      | 159/414 [12:33<20:23,  4.80s/it][A
 39%|███▊      | 160/414 [12:37<20:08,  4.76s/it][A
 39%|███▉      | 161/414 [12:42<20:40,  4.90s/it][A
 39%|███▉      | 162/414 [12:47<20:23,  4.85s/it][A
 39%|███▉      | 163/414 [12:52<20:39,  4.94s/it][A
 40%|███▉      | 164/414 [12:58<22:06,  5.30s/it][A
 40%|███▉      | 165/414 [13:05<23:38,  5.70s/it][A
 40%|████      | 166/414 [13:12<25:02,  6.06s/it][A
 40%|████      | 167/414 [13:17<23:59,  5.83s/it][A
 41%|████      | 168/414 [13:22<21:56,  5.35s/it][A
 41%|████      | 169/414 [13:26<20:24,  5.00s/it][A
 41%|████      | 170/414 [13:30<19:07,  4.70s/it][A
 41%|████▏     | 171/414 [13:33<17:57,  4.43s/it][A
 42%|████▏     | 172/414 [13:38<17:36,  4.37s/it][A
 42%|████▏     | 173/414 [13:43<18:48,  4.68s/it][A
 42%|████▏     | 174/414 [13:47<18:01,  4.51s/it][A
 42%|████▏     | 175/414 [13:52<18:36,  4.67s/it][A
 43%|████▎     | 176/414 [13:57<18:06,  4.56s/it][A
 43%|████▎     | 177/414 [14:02<18:31,  4.69s/it][A
 43%|████▎     | 178/414 [14:06<18:20,  4.66s/it][A
 43%|████▎     | 179/414 [14:10<17:16,  4.41s/it][A
 43%|████▎     | 180/414 [14:14<16:55,  4.34s/it][A
 44%|████▎     | 181/414 [14:19<17:12,  4.43s/it][A
 44%|████▍     | 182/414 [14:23<17:03,  4.41s/it][A
 44%|████▍     | 183/414 [14:28<17:04,  4.43s/it][A
 44%|████▍     | 184/414 [14:32<17:19,  4.52s/it][A
 45%|████▍     | 185/414 [14:36<16:29,  4.32s/it][A
 45%|████▍     | 186/414 [14:41<17:20,  4.56s/it][A
 45%|████▌     | 187/414 [14:51<22:58,  6.07s/it][A
 45%|████▌     | 188/414 [14:58<24:03,  6.39s/it][A
 46%|████▌     | 189/414 [15:04<23:40,  6.31s/it][A
 46%|████▌     | 190/414 [15:12<24:40,  6.61s/it][A
 46%|████▌     | 191/414 [15:16<22:11,  5.97s/it][A
 46%|████▋     | 192/414 [15:21<20:27,  5.53s/it][A
 47%|████▋     | 193/414 [15:25<18:40,  5.07s/it][A
 47%|████▋     | 194/414 [15:29<18:18,  4.99s/it][A
 47%|████▋     | 195/414 [15:34<17:28,  4.79s/it][A
 47%|████▋     | 196/414 [15:37<16:03,  4.42s/it][A
 48%|████▊     | 197/414 [15:43<17:12,  4.76s/it][A
 48%|████▊     | 198/414 [15:48<17:49,  4.95s/it][A
 48%|████▊     | 199/414 [15:52<16:33,  4.62s/it][A
 48%|████▊     | 200/414 [15:57<17:05,  4.79s/it][A
 49%|████▊     | 201/414 [16:02<17:19,  4.88s/it][A
 49%|████▉     | 202/414 [16:07<17:19,  4.90s/it][A
 49%|████▉     | 203/414 [16:11<16:27,  4.68s/it][A
 49%|████▉     | 204/414 [16:15<15:30,  4.43s/it][A
 50%|████▉     | 205/414 [16:19<14:34,  4.18s/it][A
 50%|████▉     | 206/414 [16:24<15:27,  4.46s/it][A
 50%|█████     | 207/414 [16:29<15:32,  4.51s/it][A
 50%|█████     | 208/414 [16:34<16:08,  4.70s/it][A
 50%|█████     | 209/414 [16:39<16:30,  4.83s/it][A
 51%|█████     | 210/414 [16:43<15:53,  4.67s/it][A
 51%|█████     | 211/414 [16:47<15:04,  4.45s/it][A
 51%|█████     | 212/414 [16:52<15:39,  4.65s/it][A
 51%|█████▏    | 213/414 [16:58<16:59,  5.07s/it][A
 52%|█████▏    | 214/414 [17:03<16:25,  4.93s/it][A
 52%|█████▏    | 215/414 [17:08<16:13,  4.89s/it][A
 52%|█████▏    | 216/414 [17:15<18:34,  5.63s/it][A
 52%|█████▏    | 217/414 [17:22<19:32,  5.95s/it][A
 53%|█████▎    | 218/414 [17:26<17:39,  5.41s/it][A
 53%|█████▎    | 219/414 [17:30<16:01,  4.93s/it][A
 53%|█████▎    | 220/414 [17:34<15:03,  4.66s/it][A
 53%|█████▎    | 221/414 [17:38<14:38,  4.55s/it][A
 54%|█████▎    | 222/414 [17:44<15:46,  4.93s/it][A
 54%|█████▍    | 223/414 [17:48<15:14,  4.79s/it][A
 54%|█████▍    | 224/414 [17:53<15:33,  4.91s/it][A
 54%|█████▍    | 225/414 [18:00<16:43,  5.31s/it][A
 55%|█████▍    | 226/414 [18:05<16:58,  5.42s/it][A
 55%|█████▍    | 227/414 [18:12<17:36,  5.65s/it][A
 55%|█████▌    | 228/414 [18:18<17:47,  5.74s/it][A
 55%|█████▌    | 229/414 [18:22<16:42,  5.42s/it][A
 56%|█████▌    | 230/414 [18:29<17:47,  5.80s/it][A
 56%|█████▌    | 231/414 [18:34<17:05,  5.61s/it][A
 56%|█████▌    | 232/414 [18:39<16:19,  5.38s/it][A
 56%|█████▋    | 233/414 [18:44<15:46,  5.23s/it][A
 57%|█████▋    | 234/414 [18:49<15:23,  5.13s/it][A
 57%|█████▋    | 235/414 [18:54<15:36,  5.23s/it][A
 57%|█████▋    | 236/414 [18:59<15:14,  5.14s/it][A
 57%|█████▋    | 237/414 [19:04<14:42,  4.98s/it][A
 57%|█████▋    | 238/414 [19:07<12:55,  4.41s/it][A
 58%|█████▊    | 239/414 [19:12<13:13,  4.54s/it][A
 58%|█████▊    | 240/414 [19:18<15:14,  5.26s/it][A
 58%|█████▊    | 241/414 [19:24<14:59,  5.20s/it][A
 58%|█████▊    | 242/414 [19:27<13:24,  4.68s/it][A
 59%|█████▊    | 243/414 [19:31<12:22,  4.34s/it][A
 59%|█████▉    | 244/414 [19:35<12:34,  4.44s/it][A
 59%|█████▉    | 245/414 [19:40<13:06,  4.65s/it][A
 59%|█████▉    | 246/414 [19:45<12:38,  4.52s/it][A
 60%|█████▉    | 247/414 [19:49<12:26,  4.47s/it][A
 60%|█████▉    | 248/414 [19:53<12:15,  4.43s/it][A
 60%|██████    | 249/414 [19:57<11:55,  4.34s/it][A
 60%|██████    | 250/414 [20:01<11:04,  4.05s/it][A
 61%|██████    | 251/414 [20:06<11:37,  4.28s/it][A
 61%|██████    | 252/414 [20:11<12:34,  4.65s/it][A
 61%|██████    | 253/414 [20:18<13:56,  5.20s/it][A
 61%|██████▏   | 254/414 [20:24<15:03,  5.65s/it][A
 62%|██████▏   | 255/414 [20:28<13:04,  4.93s/it][A
 62%|██████▏   | 256/414 [20:31<12:00,  4.56s/it][A
 62%|██████▏   | 257/414 [20:35<11:31,  4.40s/it][A
 62%|██████▏   | 258/414 [20:39<10:38,  4.09s/it][A
 63%|██████▎   | 259/414 [20:42<10:20,  4.00s/it][A
 63%|██████▎   | 260/414 [20:47<10:41,  4.17s/it][A
 63%|██████▎   | 261/414 [20:51<10:48,  4.24s/it][A
 63%|██████▎   | 262/414 [20:57<11:24,  4.50s/it][A
 64%|██████▎   | 263/414 [21:03<12:47,  5.08s/it][A
 64%|██████▍   | 264/414 [21:09<13:28,  5.39s/it][A
 64%|██████▍   | 265/414 [21:13<12:38,  5.09s/it][A
 64%|██████▍   | 266/414 [21:19<12:43,  5.16s/it][A
 64%|██████▍   | 267/414 [21:24<12:20,  5.04s/it][A
 65%|██████▍   | 268/414 [21:29<12:36,  5.18s/it][A
 65%|██████▍   | 269/414 [21:32<11:09,  4.62s/it][A
 65%|██████▌   | 270/414 [21:36<10:37,  4.43s/it][A
 65%|██████▌   | 271/414 [21:40<10:16,  4.31s/it][A
 66%|██████▌   | 272/414 [21:46<10:55,  4.62s/it][A
 66%|██████▌   | 273/414 [21:51<11:32,  4.91s/it][A
 66%|██████▌   | 274/414 [21:56<11:06,  4.76s/it][A
 66%|██████▋   | 275/414 [21:59<10:15,  4.43s/it][A
 67%|██████▋   | 276/414 [22:04<10:02,  4.37s/it][A
 67%|██████▋   | 277/414 [22:07<09:23,  4.11s/it][A
 67%|██████▋   | 278/414 [22:11<08:58,  3.96s/it][A
 67%|██████▋   | 279/414 [22:15<09:00,  4.01s/it][A
 68%|██████▊   | 280/414 [22:19<09:19,  4.17s/it][A
 68%|██████▊   | 281/414 [22:24<09:18,  4.20s/it][A
 68%|██████▊   | 282/414 [22:29<10:04,  4.58s/it][A
 68%|██████▊   | 283/414 [22:35<10:44,  4.92s/it][A
 69%|██████▊   | 284/414 [22:38<09:42,  4.48s/it][A
 69%|██████▉   | 285/414 [22:43<09:51,  4.59s/it][A
 69%|██████▉   | 286/414 [22:50<11:05,  5.20s/it][A
 69%|██████▉   | 287/414 [22:55<11:08,  5.26s/it][A
 70%|██████▉   | 288/414 [23:00<10:51,  5.17s/it][A
 70%|██████▉   | 289/414 [23:03<09:35,  4.60s/it][A
 70%|███████   | 290/414 [23:08<09:45,  4.72s/it][A
 70%|███████   | 291/414 [23:14<10:30,  5.12s/it][A
 71%|███████   | 292/414 [23:18<09:36,  4.72s/it][A
 71%|███████   | 293/414 [23:26<11:10,  5.54s/it][A
 71%|███████   | 294/414 [23:34<13:01,  6.51s/it][A
 71%|███████▏  | 295/414 [23:40<12:26,  6.27s/it][A
 71%|███████▏  | 296/414 [23:46<12:11,  6.20s/it][A
 72%|███████▏  | 297/414 [23:51<11:31,  5.91s/it][A
 72%|███████▏  | 298/414 [23:57<10:56,  5.66s/it][A
 72%|███████▏  | 299/414 [24:02<10:53,  5.68s/it][A
 72%|███████▏  | 300/414 [24:07<10:08,  5.34s/it][A
 73%|███████▎  | 301/414 [24:12<09:49,  5.21s/it][A
 73%|███████▎  | 302/414 [24:15<08:42,  4.67s/it][A
 73%|███████▎  | 303/414 [24:19<08:23,  4.53s/it][A
 73%|███████▎  | 304/414 [24:26<09:27,  5.16s/it][A
 74%|███████▎  | 305/414 [24:32<10:00,  5.51s/it][A
 74%|███████▍  | 306/414 [24:37<09:40,  5.38s/it][A
 74%|███████▍  | 307/414 [24:42<09:14,  5.18s/it][A
 74%|███████▍  | 308/414 [24:48<09:42,  5.50s/it][A
 75%|███████▍  | 309/414 [24:54<09:29,  5.42s/it][A
 75%|███████▍  | 310/414 [24:57<08:17,  4.79s/it][A
 75%|███████▌  | 311/414 [25:01<07:40,  4.47s/it][A
 75%|███████▌  | 312/414 [25:05<07:20,  4.31s/it][A
 76%|███████▌  | 313/414 [25:09<07:09,  4.26s/it][A
 76%|███████▌  | 314/414 [25:12<06:48,  4.09s/it][A
 76%|███████▌  | 315/414 [25:17<06:55,  4.20s/it][A
 76%|███████▋  | 316/414 [25:23<07:43,  4.73s/it][A
 77%|███████▋  | 317/414 [25:29<08:32,  5.29s/it][A
 77%|███████▋  | 318/414 [25:34<08:20,  5.22s/it][A
 77%|███████▋  | 319/414 [25:39<07:58,  5.04s/it][A
 77%|███████▋  | 320/414 [25:43<07:18,  4.67s/it][A
 78%|███████▊  | 321/414 [25:49<07:50,  5.06s/it][A
 78%|███████▊  | 322/414 [25:54<07:59,  5.21s/it][A
 78%|███████▊  | 323/414 [25:59<07:27,  4.91s/it][A
 78%|███████▊  | 324/414 [26:03<06:56,  4.63s/it][A
 79%|███████▊  | 325/414 [26:06<06:28,  4.37s/it][A
 79%|███████▊  | 326/414 [26:10<06:09,  4.19s/it][A
 79%|███████▉  | 327/414 [26:16<06:58,  4.81s/it][A
 79%|███████▉  | 328/414 [26:22<07:12,  5.03s/it][A
 79%|███████▉  | 329/414 [26:28<07:30,  5.31s/it][A
 80%|███████▉  | 330/414 [26:34<07:52,  5.62s/it][A
 80%|███████▉  | 331/414 [26:39<07:34,  5.47s/it][A
 80%|████████  | 332/414 [26:45<07:27,  5.46s/it][A
 80%|████████  | 333/414 [26:50<07:26,  5.51s/it][A
 81%|████████  | 334/414 [26:54<06:42,  5.03s/it][A
 81%|████████  | 335/414 [26:58<06:10,  4.69s/it][A
 81%|████████  | 336/414 [27:03<06:06,  4.70s/it][A
 81%|████████▏ | 337/414 [27:06<05:32,  4.32s/it][A
 82%|████████▏ | 338/414 [27:11<05:24,  4.28s/it][A
 82%|████████▏ | 339/414 [27:17<06:19,  5.07s/it][A
 82%|████████▏ | 340/414 [27:23<06:30,  5.28s/it][A
 82%|████████▏ | 341/414 [27:29<06:34,  5.40s/it][A
 83%|████████▎ | 342/414 [27:32<05:49,  4.85s/it][A
 83%|████████▎ | 343/414 [27:38<06:07,  5.18s/it][A
 83%|████████▎ | 344/414 [27:45<06:38,  5.70s/it][A
 83%|████████▎ | 345/414 [27:50<06:21,  5.53s/it][A
 84%|████████▎ | 346/414 [27:55<05:48,  5.12s/it][A
 84%|████████▍ | 347/414 [27:59<05:30,  4.93s/it][A
 84%|████████▍ | 348/414 [28:03<05:07,  4.66s/it][A
 84%|████████▍ | 349/414 [28:07<04:38,  4.29s/it][A
 85%|████████▍ | 350/414 [28:11<04:46,  4.48s/it][A
 85%|████████▍ | 351/414 [28:18<05:28,  5.21s/it][A
 85%|████████▌ | 352/414 [28:22<04:56,  4.79s/it][A
 85%|████████▌ | 353/414 [28:26<04:29,  4.41s/it][A
 86%|████████▌ | 354/414 [28:30<04:13,  4.22s/it][A
 86%|████████▌ | 355/414 [28:33<03:52,  3.94s/it][A
 86%|████████▌ | 356/414 [28:37<03:46,  3.91s/it][A
 86%|████████▌ | 357/414 [28:41<03:53,  4.10s/it][A
 86%|████████▋ | 358/414 [28:48<04:35,  4.92s/it][A
 87%|████████▋ | 359/414 [28:54<04:44,  5.17s/it][A
 87%|████████▋ | 360/414 [28:58<04:23,  4.88s/it][A
 87%|████████▋ | 361/414 [29:02<04:00,  4.53s/it][A
 87%|████████▋ | 362/414 [29:07<04:10,  4.82s/it][A
 88%|████████▊ | 363/414 [29:14<04:32,  5.35s/it][A
 88%|████████▊ | 364/414 [29:17<04:02,  4.85s/it][A
 88%|████████▊ | 365/414 [29:23<04:09,  5.08s/it][A
 88%|████████▊ | 366/414 [29:29<04:21,  5.45s/it][A
 89%|████████▊ | 367/414 [29:34<04:09,  5.32s/it][A
 89%|████████▉ | 368/414 [29:39<04:01,  5.24s/it][A
 89%|████████▉ | 369/414 [29:45<03:54,  5.20s/it][A
 89%|████████▉ | 370/414 [29:50<03:57,  5.40s/it][A
 90%|████████▉ | 371/414 [29:56<03:55,  5.48s/it][A
 90%|████████▉ | 372/414 [30:03<04:02,  5.77s/it][A
 90%|█████████ | 373/414 [30:08<03:55,  5.74s/it][A
 90%|█████████ | 374/414 [30:14<03:44,  5.61s/it][A
 91%|█████████ | 375/414 [30:18<03:27,  5.32s/it][A
 91%|█████████ | 376/414 [30:22<03:03,  4.83s/it][A
 91%|█████████ | 377/414 [30:27<02:57,  4.80s/it][A
 91%|█████████▏| 378/414 [30:33<03:06,  5.17s/it][A
 92%|█████████▏| 379/414 [30:37<02:53,  4.97s/it][A
 92%|█████████▏| 380/414 [30:43<02:58,  5.25s/it][A
 92%|█████████▏| 381/414 [30:52<03:34,  6.49s/it][A
 92%|█████████▏| 382/414 [30:58<03:21,  6.31s/it][A
 93%|█████████▎| 383/414 [31:02<02:55,  5.67s/it][A
 93%|█████████▎| 384/414 [31:07<02:43,  5.44s/it][A
 93%|█████████▎| 385/414 [31:12<02:30,  5.18s/it][A
 93%|█████████▎| 386/414 [31:16<02:18,  4.93s/it][A
 93%|█████████▎| 387/414 [31:20<02:07,  4.71s/it][A
 94%|█████████▎| 388/414 [31:24<01:53,  4.35s/it][A
 94%|█████████▍| 389/414 [31:28<01:46,  4.26s/it][A
 94%|█████████▍| 390/414 [31:31<01:34,  3.95s/it][A
 94%|█████████▍| 391/414 [31:35<01:30,  3.94s/it][A
 95%|█████████▍| 392/414 [31:40<01:29,  4.07s/it][A
 95%|█████████▍| 393/414 [31:45<01:31,  4.36s/it][A
 95%|█████████▌| 394/414 [31:50<01:32,  4.65s/it][A
 95%|█████████▌| 395/414 [31:54<01:23,  4.41s/it][A
 96%|█████████▌| 396/414 [31:59<01:26,  4.79s/it][A
 96%|█████████▌| 397/414 [32:05<01:26,  5.06s/it][A
 96%|█████████▌| 398/414 [32:10<01:19,  4.97s/it][A
 96%|█████████▋| 399/414 [32:13<01:06,  4.43s/it][A
 97%|█████████▋| 400/414 [32:20<01:12,  5.17s/it][A
 97%|█████████▋| 401/414 [32:28<01:17,  5.92s/it][A
 97%|█████████▋| 402/414 [32:32<01:06,  5.56s/it][A
 97%|█████████▋| 403/414 [32:36<00:53,  4.90s/it][A
 98%|█████████▊| 404/414 [32:39<00:45,  4.53s/it][A
 98%|█████████▊| 405/414 [32:43<00:38,  4.32s/it][A
 98%|█████████▊| 406/414 [32:48<00:36,  4.54s/it][A
 98%|█████████▊| 407/414 [32:54<00:33,  4.85s/it][A
 99%|█████████▊| 408/414 [32:59<00:30,  5.00s/it][A
 99%|█████████▉| 409/414 [33:04<00:24,  4.99s/it][A
 99%|█████████▉| 410/414 [33:09<00:19,  4.87s/it][A
 99%|█████████▉| 411/414 [33:12<00:13,  4.48s/it][A
100%|█████████▉| 412/414 [33:18<00:09,  4.73s/it][A
100%|█████████▉| 413/414 [33:23<00:05,  5.04s/it][A
100%|██████████| 414/414 [33:26<00:00,  4.25s/it][A                                                       
                                                 [A{'eval_loss': 0.6905620098114014, 'eval_runtime': 2013.1122, 'eval_samples_per_second': 1.642, 'eval_steps_per_second': 0.206, 'epoch': 1.72}
 86%|████████▌ | 400/466 [13:52:22<2:04:23, 113.08s/it]
100%|██████████| 414/414 [33:26<00:00,  4.25s/it][A
                                                 [A[INFO|trainer.py:4309] 2025-12-09 15:23:00,751 >> Saving model checkpoint to saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400
/opt/conda/lib/python3.11/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in Qwen/Qwen2.5-VL-7B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2421] 2025-12-09 15:23:00,875 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-09 15:23:00,878 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-09 15:23:00,880 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/special_tokens_map.json
[2025-12-09 15:23:01,537] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step400 is about to be saved!
[2025-12-09 15:23:01,571] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/global_step400/mp_rank_00_model_states.pt
[2025-12-09 15:23:01,571] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/global_step400/mp_rank_00_model_states.pt...
[2025-12-09 15:23:01,633] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/global_step400/mp_rank_00_model_states.pt.
[2025-12-09 15:23:01,638] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2025-12-09 15:23:01,638] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2025-12-09 15:23:01,638] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-12-09 15:23:01,638] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2025-12-09 15:23:01,638] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2025-12-09 15:23:01,638] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2025-12-09 15:23:01,638] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2025-12-09 15:23:01,638] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2025-12-09 15:23:01,737] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2025-12-09 15:23:01,737] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt
[2025-12-09 15:23:01,737] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2025-12-09 15:23:01,768] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2025-12-09 15:23:01,768] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2025-12-09 15:23:01,768] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2025-12-09 15:23:01,769] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2025-12-09 15:23:01,770] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2025-12-09 15:23:01,770] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2025-12-09 15:23:01,774] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2025-12-09 15:23:01,774] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt
[2025-12-09 15:23:01,774] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2025-12-09 15:23:01,784] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2025-12-09 15:23:01,785] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2025-12-09 15:23:01,785] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2025-12-09 15:23:01,786] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-12-09 15:23:01,786] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2025-12-09 15:23:01,786] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2025-12-09 15:23:01,787] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2025-12-09 15:23:01,787] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2025-12-09 15:23:01,787] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2025-12-09 15:23:01,787] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2025-12-09 15:23:01,790] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-12-09 15:23:01,791] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[INFO|image_processing_base.py:253] 2025-12-09 15:23:01,814 >> Image processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/preprocessor_config.json
[INFO|tokenization_utils_base.py:2421] 2025-12-09 15:23:01,817 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-09 15:23:01,821 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-09 15:23:01,823 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/special_tokens_map.json
[INFO|video_processing_utils.py:600] 2025-12-09 15:23:02,015 >> Video processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/video_preprocessor_config.json
[INFO|processing_utils.py:814] 2025-12-09 15:23:02,017 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-400/chat_template.jinja
 86%|████████▌ | 401/466 [13:54:45<13:06:18, 725.83s/it] 86%|████████▋ | 402/466 [13:56:57<9:44:19, 547.81s/it]  86%|████████▋ | 403/466 [13:58:42<7:15:47, 415.04s/it] 87%|████████▋ | 404/466 [14:00:33<5:34:28, 323.68s/it] 87%|████████▋ | 405/466 [14:02:30<4:25:59, 261.64s/it] 87%|████████▋ | 406/466 [14:04:42<3:42:55, 222.93s/it] 87%|████████▋ | 407/466 [14:06:50<3:11:12, 194.44s/it] 88%|████████▊ | 408/466 [14:08:38<2:42:44, 168.36s/it] 88%|████████▊ | 409/466 [14:10:17<2:20:18, 147.69s/it] 88%|████████▊ | 410/466 [14:12:08<2:07:21, 136.46s/it]                                                       {'loss': 0.6901, 'grad_norm': 0.1805630624294281, 'learning_rate': 4.497187328813424e-06, 'epoch': 1.76}
 88%|████████▊ | 410/466 [14:12:08<2:07:21, 136.46s/it] 88%|████████▊ | 411/466 [14:13:59<1:58:17, 129.04s/it] 88%|████████▊ | 412/466 [14:16:04<1:54:54, 127.68s/it] 89%|████████▊ | 413/466 [14:17:43<1:45:12, 119.09s/it] 89%|████████▉ | 414/466 [14:19:58<1:47:20, 123.86s/it] 89%|████████▉ | 415/466 [14:21:49<1:41:59, 119.99s/it] 89%|████████▉ | 416/466 [14:23:45<1:38:58, 118.78s/it] 89%|████████▉ | 417/466 [14:25:49<1:38:26, 120.54s/it] 90%|████████▉ | 418/466 [14:27:43<1:34:42, 118.39s/it] 90%|████████▉ | 419/466 [14:30:03<1:37:45, 124.80s/it] 90%|█████████ | 420/466 [14:32:04<1:34:52, 123.75s/it]                                                       {'loss': 0.6919, 'grad_norm': 0.14010834693908691, 'learning_rate': 3.072616628425601e-06, 'epoch': 1.8}
 90%|█████████ | 420/466 [14:32:04<1:34:52, 123.75s/it] 90%|█████████ | 421/466 [14:33:42<1:27:09, 116.21s/it] 91%|█████████ | 422/466 [14:35:44<1:26:19, 117.71s/it] 91%|█████████ | 423/466 [14:37:41<1:24:12, 117.49s/it] 91%|█████████ | 424/466 [14:39:40<1:22:41, 118.13s/it] 91%|█████████ | 425/466 [14:41:37<1:20:20, 117.57s/it] 91%|█████████▏| 426/466 [14:43:18<1:15:08, 112.70s/it] 92%|█████████▏| 427/466 [14:45:00<1:11:12, 109.55s/it] 92%|█████████▏| 428/466 [14:46:55<1:10:22, 111.11s/it] 92%|█████████▏| 429/466 [14:48:57<1:10:37, 114.54s/it] 92%|█████████▏| 430/466 [14:51:11<1:12:04, 120.14s/it]                                                       {'loss': 0.6829, 'grad_norm': 0.17138159275054932, 'learning_rate': 1.911736481004489e-06, 'epoch': 1.85}
 92%|█████████▏| 430/466 [14:51:11<1:12:04, 120.14s/it] 92%|█████████▏| 431/466 [14:53:03<1:08:39, 117.71s/it] 93%|█████████▎| 432/466 [14:54:55<1:05:44, 116.00s/it] 93%|█████████▎| 433/466 [14:56:58<1:04:59, 118.18s/it] 93%|█████████▎| 434/466 [14:58:50<1:02:04, 116.39s/it] 93%|█████████▎| 435/466 [15:00:31<57:46, 111.83s/it]   94%|█████████▎| 436/466 [15:02:12<54:19, 108.64s/it] 94%|█████████▍| 437/466 [15:03:48<50:36, 104.69s/it] 94%|█████████▍| 438/466 [15:05:55<52:01, 111.49s/it] 94%|█████████▍| 439/466 [15:07:59<51:53, 115.30s/it] 94%|█████████▍| 440/466 [15:09:57<50:13, 115.90s/it]                                                     {'loss': 0.6931, 'grad_norm': 0.19358114898204803, 'learning_rate': 1.0210700102788796e-06, 'epoch': 1.89}
 94%|█████████▍| 440/466 [15:09:57<50:13, 115.90s/it] 95%|█████████▍| 441/466 [15:11:48<47:41, 114.47s/it] 95%|█████████▍| 442/466 [15:13:39<45:25, 113.56s/it] 95%|█████████▌| 443/466 [15:15:44<44:49, 116.92s/it] 95%|█████████▌| 444/466 [15:17:40<42:42, 116.49s/it] 95%|█████████▌| 445/466 [15:19:26<39:43, 113.51s/it] 96%|█████████▌| 446/466 [15:21:19<37:47, 113.38s/it] 96%|█████████▌| 447/466 [15:22:59<34:36, 109.27s/it] 96%|█████████▌| 448/466 [15:25:12<34:53, 116.29s/it] 96%|█████████▋| 449/466 [15:27:01<32:19, 114.09s/it] 97%|█████████▋| 450/466 [15:29:03<31:04, 116.50s/it]                                                     {'loss': 0.6862, 'grad_norm': 0.19313716888427734, 'learning_rate': 4.0562197716448313e-07, 'epoch': 1.93}
 97%|█████████▋| 450/466 [15:29:03<31:04, 116.50s/it] 97%|█████████▋| 451/466 [15:30:48<28:18, 113.22s/it] 97%|█████████▋| 452/466 [15:32:30<25:38, 109.88s/it] 97%|█████████▋| 453/466 [15:34:24<24:02, 110.97s/it] 97%|█████████▋| 454/466 [15:36:07<21:43, 108.63s/it] 98%|█████████▊| 455/466 [15:38:14<20:56, 114.22s/it] 98%|█████████▊| 456/466 [15:40:13<19:14, 115.49s/it] 98%|█████████▊| 457/466 [15:42:12<17:29, 116.58s/it] 98%|█████████▊| 458/466 [15:44:00<15:11, 113.99s/it] 98%|█████████▊| 459/466 [15:45:54<13:17, 113.99s/it] 99%|█████████▊| 460/466 [15:47:43<11:15, 112.60s/it]                                                     {'loss': 0.6881, 'grad_norm': 0.18634502589702606, 'learning_rate': 6.885065741661367e-08, 'epoch': 1.98}
 99%|█████████▊| 460/466 [15:47:43<11:15, 112.60s/it] 99%|█████████▉| 461/466 [15:49:39<09:28, 113.61s/it] 99%|█████████▉| 462/466 [15:51:30<07:30, 112.72s/it] 99%|█████████▉| 463/466 [15:53:12<05:28, 109.66s/it]100%|█████████▉| 464/466 [15:55:05<03:40, 110.46s/it]100%|█████████▉| 465/466 [15:56:41<01:46, 106.29s/it]100%|██████████| 466/466 [15:57:24<00:00, 87.40s/it] [INFO|trainer.py:4309] 2025-12-09 17:28:02,989 >> Saving model checkpoint to saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466
/opt/conda/lib/python3.11/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in Qwen/Qwen2.5-VL-7B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2421] 2025-12-09 17:28:03,082 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-09 17:28:03,085 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-09 17:28:03,088 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/special_tokens_map.json
[2025-12-09 17:28:03,994] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step466 is about to be saved!
[2025-12-09 17:28:04,019] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/global_step466/mp_rank_00_model_states.pt
[2025-12-09 17:28:04,020] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/global_step466/mp_rank_00_model_states.pt...
[2025-12-09 17:28:04,101] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/global_step466/mp_rank_00_model_states.pt.
[2025-12-09 17:28:04,106] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/global_step466/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-12-09 17:28:04,106] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/global_step466/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2025-12-09 17:28:04,106] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/global_step466/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2025-12-09 17:28:04,106] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/global_step466/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2025-12-09 17:28:04,106] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/global_step466/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2025-12-09 17:28:04,106] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/global_step466/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2025-12-09 17:28:04,106] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/global_step466/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2025-12-09 17:28:04,106] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/global_step466/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2025-12-09 17:28:04,203] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/global_step466/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-12-09 17:28:04,215] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/global_step466/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2025-12-09 17:28:04,215] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/global_step466/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2025-12-09 17:28:04,215] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step466 is ready now!
[2025-12-09 17:28:04,230] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/global_step466/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2025-12-09 17:28:04,230] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/global_step466/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2025-12-09 17:28:04,230] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step466 is ready now!
[2025-12-09 17:28:04,231] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/global_step466/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2025-12-09 17:28:04,231] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/global_step466/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt
[2025-12-09 17:28:04,231] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step466 is ready now!
[2025-12-09 17:28:04,240] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/global_step466/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-12-09 17:28:04,240] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step466 is ready now!
[2025-12-09 17:28:04,246] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/global_step466/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2025-12-09 17:28:04,247] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/global_step466/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2025-12-09 17:28:04,247] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step466 is ready now!
[2025-12-09 17:28:04,250] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/global_step466/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2025-12-09 17:28:04,251] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/global_step466/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt
[2025-12-09 17:28:04,251] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step466 is ready now!
[2025-12-09 17:28:04,251] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/global_step466/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2025-12-09 17:28:04,252] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/global_step466/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2025-12-09 17:28:04,252] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step466 is ready now!
[2025-12-09 17:28:04,252] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/global_step466/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2025-12-09 17:28:04,252] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/global_step466/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2025-12-09 17:28:04,252] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step466 is ready now!


Training completed. Do not forget to share your model on huggingface.co/models =)




Training completed. Do not forget to share your model on huggingface.co/models =)




Training completed. Do not forget to share your model on huggingface.co/models =)




Training completed. Do not forget to share your model on huggingface.co/models =)




Training completed. Do not forget to share your model on huggingface.co/models =)




Training completed. Do not forget to share your model on huggingface.co/models =)




Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|image_processing_base.py:253] 2025-12-09 17:28:04,273 >> Image processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/preprocessor_config.json
[INFO|tokenization_utils_base.py:2421] 2025-12-09 17:28:04,275 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-09 17:28:04,278 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-09 17:28:04,280 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/special_tokens_map.json
[INFO|video_processing_utils.py:600] 2025-12-09 17:28:04,440 >> Video processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/video_preprocessor_config.json
[INFO|processing_utils.py:814] 2025-12-09 17:28:04,442 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/checkpoint-466/chat_template.jinja
[INFO|trainer.py:2810] 2025-12-09 17:28:04,737 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                    {'train_runtime': 57457.6865, 'train_samples_per_second': 1.035, 'train_steps_per_second': 0.008, 'train_loss': 0.7066506664128774, 'epoch': 2.0}
100%|██████████| 466/466 [15:57:34<00:00, 87.40s/it]100%|██████████| 466/466 [15:57:34<00:00, 123.29s/it]
[INFO|image_processing_base.py:253] 2025-12-09 17:28:04,746 >> Image processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/preprocessor_config.json
[INFO|tokenization_utils_base.py:2421] 2025-12-09 17:28:04,749 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-09 17:28:04,751 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-09 17:28:04,754 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/special_tokens_map.json
[INFO|video_processing_utils.py:600] 2025-12-09 17:28:04,878 >> Video processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/video_preprocessor_config.json
[INFO|processing_utils.py:814] 2025-12-09 17:28:04,880 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/chat_template.jinja

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1
[INFO|trainer.py:4309] 2025-12-09 17:28:13,299 >> Saving model checkpoint to saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu
/opt/conda/lib/python3.11/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in Qwen/Qwen2.5-VL-7B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2421] 2025-12-09 17:28:13,405 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-09 17:28:13,408 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-09 17:28:13,410 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/special_tokens_map.json

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1
***** train metrics *****
  epoch                    =           2.0
  total_flos               = 24075249152GF
  train_loss               =        0.7067
  train_runtime            =   15:57:37.68
  train_samples_per_second =         1.035
  train_steps_per_second   =         0.008
Figure saved at: saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/training_loss.png
Figure saved at: saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_resumefromcheckpoint_8gpu/training_eval_loss.png
[WARNING|2025-12-09 17:28:14] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.
[INFO|trainer.py:4643] 2025-12-09 17:28:14,238 >> 
***** Running Evaluation *****
[INFO|trainer.py:4645] 2025-12-09 17:28:14,238 >>   Num examples = 3305
[INFO|trainer.py:4648] 2025-12-09 17:28:14,238 >>   Batch size = 1
  0%|          | 0/414 [00:00<?, ?it/s]  0%|          | 2/414 [00:04<14:06,  2.06s/it]  1%|          | 3/414 [00:08<20:46,  3.03s/it]  1%|          | 4/414 [00:14<29:16,  4.28s/it]  1%|          | 5/414 [00:21<35:40,  5.23s/it]  1%|▏         | 6/414 [00:25<31:26,  4.62s/it]  2%|▏         | 7/414 [00:28<29:16,  4.31s/it]  2%|▏         | 8/414 [00:33<28:47,  4.25s/it]  2%|▏         | 9/414 [00:37<28:57,  4.29s/it]  2%|▏         | 10/414 [00:43<33:27,  4.97s/it]  3%|▎         | 11/414 [00:48<32:43,  4.87s/it]  3%|▎         | 12/414 [00:53<33:28,  5.00s/it]  3%|▎         | 13/414 [00:59<34:15,  5.13s/it]  3%|▎         | 14/414 [01:04<33:19,  5.00s/it]  4%|▎         | 15/414 [01:09<33:27,  5.03s/it]  4%|▍         | 16/414 [01:13<32:24,  4.88s/it]  4%|▍         | 17/414 [01:19<35:07,  5.31s/it]  4%|▍         | 18/414 [01:25<34:47,  5.27s/it]  5%|▍         | 19/414 [01:30<34:56,  5.31s/it]  5%|▍         | 20/414 [01:36<37:03,  5.64s/it]  5%|▌         | 21/414 [01:40<33:29,  5.11s/it]  5%|▌         | 22/414 [01:45<32:22,  4.96s/it]  6%|▌         | 23/414 [01:51<33:52,  5.20s/it]  6%|▌         | 24/414 [01:56<33:45,  5.19s/it]  6%|▌         | 25/414 [02:01<34:04,  5.26s/it]  6%|▋         | 26/414 [02:07<34:45,  5.38s/it]  7%|▋         | 27/414 [02:11<31:29,  4.88s/it]  7%|▋         | 28/414 [02:15<30:46,  4.78s/it]  7%|▋         | 29/414 [02:20<30:06,  4.69s/it]  7%|▋         | 30/414 [02:25<31:50,  4.98s/it]  7%|▋         | 31/414 [02:30<31:54,  5.00s/it]  8%|▊         | 32/414 [02:34<28:46,  4.52s/it]  8%|▊         | 33/414 [02:39<29:41,  4.67s/it]  8%|▊         | 34/414 [02:44<30:16,  4.78s/it]  8%|▊         | 35/414 [02:49<30:56,  4.90s/it]  9%|▊         | 36/414 [02:53<30:00,  4.76s/it]  9%|▉         | 37/414 [02:58<30:06,  4.79s/it]  9%|▉         | 38/414 [03:03<29:52,  4.77s/it]  9%|▉         | 39/414 [03:07<27:27,  4.39s/it] 10%|▉         | 40/414 [03:11<27:40,  4.44s/it] 10%|▉         | 41/414 [03:16<28:48,  4.63s/it] 10%|█         | 42/414 [03:20<27:59,  4.51s/it] 10%|█         | 43/414 [03:28<33:00,  5.34s/it] 11%|█         | 44/414 [03:37<40:44,  6.61s/it] 11%|█         | 45/414 [03:44<40:00,  6.50s/it] 11%|█         | 46/414 [03:47<34:10,  5.57s/it] 11%|█▏        | 47/414 [03:50<29:34,  4.84s/it] 12%|█▏        | 48/414 [03:53<26:42,  4.38s/it] 12%|█▏        | 49/414 [03:56<23:23,  3.85s/it] 12%|█▏        | 50/414 [03:59<21:24,  3.53s/it] 12%|█▏        | 51/414 [04:02<20:35,  3.40s/it] 13%|█▎        | 52/414 [04:05<20:05,  3.33s/it] 13%|█▎        | 53/414 [04:11<25:30,  4.24s/it] 13%|█▎        | 54/414 [04:18<29:14,  4.87s/it] 13%|█▎        | 55/414 [04:23<30:24,  5.08s/it] 14%|█▎        | 56/414 [04:29<31:12,  5.23s/it] 14%|█▍        | 57/414 [04:33<29:04,  4.89s/it] 14%|█▍        | 58/414 [04:38<29:51,  5.03s/it] 14%|█▍        | 59/414 [04:45<32:42,  5.53s/it] 14%|█▍        | 60/414 [04:50<30:59,  5.25s/it] 15%|█▍        | 61/414 [04:53<27:59,  4.76s/it] 15%|█▍        | 62/414 [04:57<25:53,  4.41s/it] 15%|█▌        | 63/414 [05:01<24:43,  4.23s/it] 15%|█▌        | 64/414 [05:04<23:44,  4.07s/it] 16%|█▌        | 65/414 [05:09<24:34,  4.22s/it] 16%|█▌        | 66/414 [05:14<25:57,  4.48s/it] 16%|█▌        | 67/414 [05:21<29:35,  5.12s/it] 16%|█▋        | 68/414 [05:26<29:30,  5.12s/it] 17%|█▋        | 69/414 [05:31<29:39,  5.16s/it] 17%|█▋        | 70/414 [05:36<29:29,  5.14s/it] 17%|█▋        | 71/414 [05:40<27:40,  4.84s/it] 17%|█▋        | 72/414 [05:44<25:50,  4.53s/it] 18%|█▊        | 73/414 [05:48<25:32,  4.50s/it] 18%|█▊        | 74/414 [05:52<23:47,  4.20s/it] 18%|█▊        | 75/414 [05:55<22:01,  3.90s/it] 18%|█▊        | 76/414 [05:58<20:53,  3.71s/it] 19%|█▊        | 77/414 [06:03<22:30,  4.01s/it] 19%|█▉        | 78/414 [06:09<24:51,  4.44s/it] 19%|█▉        | 79/414 [06:13<24:36,  4.41s/it] 19%|█▉        | 80/414 [06:17<24:50,  4.46s/it] 20%|█▉        | 81/414 [06:22<24:22,  4.39s/it] 20%|█▉        | 82/414 [06:26<23:22,  4.22s/it] 20%|██        | 83/414 [06:32<27:10,  4.93s/it] 20%|██        | 84/414 [06:38<29:00,  5.27s/it] 21%|██        | 85/414 [06:42<26:43,  4.88s/it] 21%|██        | 86/414 [06:46<24:51,  4.55s/it] 21%|██        | 87/414 [06:49<22:38,  4.15s/it] 21%|██▏       | 88/414 [06:53<22:24,  4.12s/it] 21%|██▏       | 89/414 [06:58<23:51,  4.41s/it] 22%|██▏       | 90/414 [07:04<25:16,  4.68s/it] 22%|██▏       | 91/414 [07:08<24:43,  4.59s/it] 22%|██▏       | 92/414 [07:13<24:36,  4.59s/it] 22%|██▏       | 93/414 [07:17<24:24,  4.56s/it] 23%|██▎       | 94/414 [07:21<23:03,  4.32s/it] 23%|██▎       | 95/414 [07:25<22:43,  4.27s/it] 23%|██▎       | 96/414 [07:30<24:18,  4.59s/it] 23%|██▎       | 97/414 [07:35<23:50,  4.51s/it] 24%|██▎       | 98/414 [07:38<21:50,  4.15s/it] 24%|██▍       | 99/414 [07:42<20:58,  3.99s/it] 24%|██▍       | 100/414 [07:46<22:14,  4.25s/it] 24%|██▍       | 101/414 [07:52<24:40,  4.73s/it] 25%|██▍       | 102/414 [07:59<27:03,  5.20s/it] 25%|██▍       | 103/414 [08:02<24:24,  4.71s/it] 25%|██▌       | 104/414 [08:06<22:50,  4.42s/it] 25%|██▌       | 105/414 [08:10<22:36,  4.39s/it] 26%|██▌       | 106/414 [08:16<24:13,  4.72s/it] 26%|██▌       | 107/414 [08:21<25:25,  4.97s/it] 26%|██▌       | 108/414 [08:26<24:27,  4.80s/it] 26%|██▋       | 109/414 [08:30<23:54,  4.70s/it] 27%|██▋       | 110/414 [08:35<23:50,  4.70s/it] 27%|██▋       | 111/414 [08:39<23:26,  4.64s/it] 27%|██▋       | 112/414 [08:44<23:05,  4.59s/it] 27%|██▋       | 113/414 [08:48<22:48,  4.55s/it] 28%|██▊       | 114/414 [08:54<24:20,  4.87s/it] 28%|██▊       | 115/414 [08:58<23:53,  4.79s/it] 28%|██▊       | 116/414 [09:03<23:31,  4.74s/it] 28%|██▊       | 117/414 [09:09<24:43,  5.00s/it] 29%|██▊       | 118/414 [09:14<25:40,  5.21s/it] 29%|██▊       | 119/414 [09:20<26:28,  5.39s/it] 29%|██▉       | 120/414 [09:26<26:51,  5.48s/it] 29%|██▉       | 121/414 [09:31<26:07,  5.35s/it] 29%|██▉       | 122/414 [09:36<25:50,  5.31s/it] 30%|██▉       | 123/414 [09:41<24:33,  5.07s/it] 30%|██▉       | 124/414 [09:45<24:02,  4.97s/it] 30%|███       | 125/414 [09:50<23:22,  4.85s/it] 30%|███       | 126/414 [09:55<22:51,  4.76s/it] 31%|███       | 127/414 [10:00<24:08,  5.05s/it] 31%|███       | 128/414 [10:08<28:22,  5.95s/it] 31%|███       | 129/414 [10:14<28:13,  5.94s/it] 31%|███▏      | 130/414 [10:21<28:45,  6.08s/it] 32%|███▏      | 131/414 [10:27<29:30,  6.26s/it] 32%|███▏      | 132/414 [10:33<29:09,  6.20s/it] 32%|███▏      | 133/414 [10:37<25:22,  5.42s/it] 32%|███▏      | 134/414 [10:42<25:16,  5.41s/it] 33%|███▎      | 135/414 [10:47<24:19,  5.23s/it] 33%|███▎      | 136/414 [10:53<25:46,  5.56s/it] 33%|███▎      | 137/414 [11:00<26:55,  5.83s/it] 33%|███▎      | 138/414 [11:05<26:14,  5.71s/it] 34%|███▎      | 139/414 [11:09<23:25,  5.11s/it] 34%|███▍      | 140/414 [11:13<21:22,  4.68s/it] 34%|███▍      | 141/414 [11:16<19:57,  4.39s/it] 34%|███▍      | 142/414 [11:21<19:57,  4.40s/it] 35%|███▍      | 143/414 [11:26<20:12,  4.47s/it] 35%|███▍      | 144/414 [11:30<19:30,  4.33s/it] 35%|███▌      | 145/414 [11:33<18:55,  4.22s/it] 35%|███▌      | 146/414 [11:37<17:30,  3.92s/it] 36%|███▌      | 147/414 [11:41<17:46,  3.99s/it] 36%|███▌      | 148/414 [11:45<18:11,  4.10s/it] 36%|███▌      | 149/414 [11:49<18:16,  4.14s/it] 36%|███▌      | 150/414 [11:53<17:03,  3.88s/it] 36%|███▋      | 151/414 [11:56<15:55,  3.63s/it] 37%|███▋      | 152/414 [12:00<16:54,  3.87s/it] 37%|███▋      | 153/414 [12:05<17:29,  4.02s/it] 37%|███▋      | 154/414 [12:08<16:36,  3.83s/it] 37%|███▋      | 155/414 [12:11<16:07,  3.73s/it] 38%|███▊      | 156/414 [12:16<16:27,  3.83s/it] 38%|███▊      | 157/414 [12:20<17:14,  4.03s/it] 38%|███▊      | 158/414 [12:26<19:20,  4.53s/it] 38%|███▊      | 159/414 [12:31<20:31,  4.83s/it] 39%|███▊      | 160/414 [12:36<20:05,  4.75s/it] 39%|███▉      | 161/414 [12:41<20:38,  4.90s/it] 39%|███▉      | 162/414 [12:46<20:23,  4.85s/it] 39%|███▉      | 163/414 [12:51<20:42,  4.95s/it] 40%|███▉      | 164/414 [12:57<22:08,  5.32s/it] 40%|███▉      | 165/414 [13:04<23:42,  5.71s/it] 40%|████      | 166/414 [13:11<25:07,  6.08s/it] 40%|████      | 167/414 [13:16<24:02,  5.84s/it] 41%|████      | 168/414 [13:20<21:59,  5.36s/it] 41%|████      | 169/414 [13:24<20:27,  5.01s/it] 41%|████      | 170/414 [13:28<19:10,  4.72s/it] 41%|████▏     | 171/414 [13:32<18:01,  4.45s/it] 42%|████▏     | 172/414 [13:37<17:38,  4.38s/it] 42%|████▏     | 173/414 [13:42<18:52,  4.70s/it] 42%|████▏     | 174/414 [13:46<18:04,  4.52s/it] 42%|████▏     | 175/414 [13:51<18:36,  4.67s/it] 43%|████▎     | 176/414 [13:55<18:06,  4.57s/it] 43%|████▎     | 177/414 [14:00<18:27,  4.67s/it] 43%|████▎     | 178/414 [14:05<18:15,  4.64s/it] 43%|████▎     | 179/414 [14:09<17:12,  4.39s/it] 43%|████▎     | 180/414 [14:13<16:53,  4.33s/it] 44%|████▎     | 181/414 [14:18<17:11,  4.43s/it] 44%|████▍     | 182/414 [14:22<17:02,  4.41s/it] 44%|████▍     | 183/414 [14:26<17:05,  4.44s/it] 44%|████▍     | 184/414 [14:31<17:19,  4.52s/it] 45%|████▍     | 185/414 [14:35<16:30,  4.32s/it] 45%|████▍     | 186/414 [14:40<17:22,  4.57s/it] 45%|████▌     | 187/414 [14:50<23:02,  6.09s/it] 45%|████▌     | 188/414 [14:57<24:07,  6.41s/it] 46%|████▌     | 189/414 [15:03<23:40,  6.31s/it] 46%|████▌     | 190/414 [15:10<24:41,  6.61s/it] 46%|████▌     | 191/414 [15:15<22:13,  5.98s/it] 46%|████▋     | 192/414 [15:19<20:28,  5.53s/it] 47%|████▋     | 193/414 [15:23<18:40,  5.07s/it] 47%|████▋     | 194/414 [15:28<18:16,  4.98s/it] 47%|████▋     | 195/414 [15:32<17:29,  4.79s/it] 47%|████▋     | 196/414 [15:36<16:02,  4.41s/it] 48%|████▊     | 197/414 [15:42<17:12,  4.76s/it] 48%|████▊     | 198/414 [15:47<17:50,  4.96s/it] 48%|████▊     | 199/414 [15:51<16:34,  4.63s/it] 48%|████▊     | 200/414 [15:56<17:08,  4.81s/it] 49%|████▊     | 201/414 [16:01<17:25,  4.91s/it] 49%|████▉     | 202/414 [16:06<17:23,  4.92s/it] 49%|████▉     | 203/414 [16:10<16:31,  4.70s/it] 49%|████▉     | 204/414 [16:14<15:33,  4.44s/it] 50%|████▉     | 205/414 [16:18<14:35,  4.19s/it] 50%|████▉     | 206/414 [16:23<15:30,  4.47s/it] 50%|█████     | 207/414 [16:28<15:36,  4.53s/it] 50%|█████     | 208/414 [16:33<16:15,  4.74s/it] 50%|█████     | 209/414 [16:38<16:36,  4.86s/it] 51%|█████     | 210/414 [16:42<15:54,  4.68s/it] 51%|█████     | 211/414 [16:46<15:04,  4.46s/it] 51%|█████     | 212/414 [16:51<15:41,  4.66s/it] 51%|█████▏    | 213/414 [16:57<17:00,  5.08s/it] 52%|█████▏    | 214/414 [17:02<16:29,  4.95s/it] 52%|█████▏    | 215/414 [17:07<16:15,  4.90s/it] 52%|█████▏    | 216/414 [17:14<18:35,  5.63s/it] 52%|█████▏    | 217/414 [17:21<19:32,  5.95s/it] 53%|█████▎    | 218/414 [17:25<17:38,  5.40s/it] 53%|█████▎    | 219/414 [17:29<16:01,  4.93s/it] 53%|█████▎    | 220/414 [17:33<15:04,  4.66s/it] 53%|█████▎    | 221/414 [17:37<14:40,  4.56s/it] 54%|█████▎    | 222/414 [17:43<15:51,  4.96s/it] 54%|█████▍    | 223/414 [17:47<15:18,  4.81s/it] 54%|█████▍    | 224/414 [17:53<15:37,  4.93s/it] 54%|█████▍    | 225/414 [17:59<16:46,  5.32s/it] 55%|█████▍    | 226/414 [18:05<16:59,  5.42s/it] 55%|█████▍    | 227/414 [18:11<17:37,  5.65s/it] 55%|█████▌    | 228/414 [18:17<17:47,  5.74s/it] 55%|█████▌    | 229/414 [18:21<16:41,  5.42s/it] 56%|█████▌    | 230/414 [18:28<17:46,  5.80s/it] 56%|█████▌    | 231/414 [18:33<17:03,  5.59s/it] 56%|█████▌    | 232/414 [18:38<16:18,  5.38s/it] 56%|█████▋    | 233/414 [18:43<15:45,  5.23s/it] 57%|█████▋    | 234/414 [18:48<15:22,  5.12s/it] 57%|█████▋    | 235/414 [18:53<15:35,  5.23s/it] 57%|█████▋    | 236/414 [18:58<15:12,  5.12s/it] 57%|█████▋    | 237/414 [19:03<14:38,  4.97s/it] 57%|█████▋    | 238/414 [19:06<12:51,  4.38s/it] 58%|█████▊    | 239/414 [19:11<13:09,  4.51s/it] 58%|█████▊    | 240/414 [19:17<15:09,  5.23s/it] 58%|█████▊    | 241/414 [19:22<14:53,  5.16s/it] 58%|█████▊    | 242/414 [19:26<13:20,  4.66s/it] 59%|█████▊    | 243/414 [19:29<12:19,  4.32s/it] 59%|█████▉    | 244/414 [19:34<12:31,  4.42s/it] 59%|█████▉    | 245/414 [19:39<13:03,  4.64s/it] 59%|█████▉    | 246/414 [19:43<12:34,  4.49s/it] 60%|█████▉    | 247/414 [19:48<12:20,  4.43s/it] 60%|█████▉    | 248/414 [19:52<12:08,  4.39s/it] 60%|██████    | 249/414 [19:56<11:51,  4.31s/it] 60%|██████    | 250/414 [20:00<11:01,  4.03s/it] 61%|██████    | 251/414 [20:04<11:33,  4.26s/it] 61%|██████    | 252/414 [20:10<12:32,  4.64s/it] 61%|██████    | 253/414 [20:16<13:54,  5.18s/it] 61%|██████▏   | 254/414 [20:23<15:01,  5.63s/it] 62%|██████▏   | 255/414 [20:26<13:01,  4.92s/it] 62%|██████▏   | 256/414 [20:30<11:59,  4.55s/it] 62%|██████▏   | 257/414 [20:34<11:28,  4.39s/it] 62%|██████▏   | 258/414 [20:37<10:36,  4.08s/it] 63%|██████▎   | 259/414 [20:41<10:17,  3.99s/it] 63%|██████▎   | 260/414 [20:46<10:37,  4.14s/it] 63%|██████▎   | 261/414 [20:50<10:43,  4.20s/it] 63%|██████▎   | 262/414 [20:55<11:19,  4.47s/it] 64%|██████▎   | 263/414 [21:01<12:42,  5.05s/it] 64%|██████▍   | 264/414 [21:08<13:27,  5.38s/it] 64%|██████▍   | 265/414 [21:12<12:36,  5.08s/it] 64%|██████▍   | 266/414 [21:17<12:34,  5.10s/it] 64%|██████▍   | 267/414 [21:22<12:15,  5.00s/it] 65%|██████▍   | 268/414 [21:27<12:33,  5.16s/it] 65%|██████▍   | 269/414 [21:31<11:06,  4.59s/it] 65%|██████▌   | 270/414 [21:35<10:35,  4.41s/it] 65%|██████▌   | 271/414 [21:39<10:13,  4.29s/it] 66%|██████▌   | 272/414 [21:44<10:54,  4.61s/it] 66%|██████▌   | 273/414 [21:50<11:35,  4.93s/it] 66%|██████▌   | 274/414 [21:54<11:07,  4.77s/it] 66%|██████▋   | 275/414 [21:58<10:17,  4.45s/it] 67%|██████▋   | 276/414 [22:02<10:04,  4.38s/it] 67%|██████▋   | 277/414 [22:05<09:23,  4.11s/it] 67%|██████▋   | 278/414 [22:09<08:59,  3.97s/it] 67%|██████▋   | 279/414 [22:13<09:00,  4.01s/it] 68%|██████▊   | 280/414 [22:18<09:16,  4.15s/it] 68%|██████▊   | 281/414 [22:22<09:16,  4.18s/it] 68%|██████▊   | 282/414 [22:27<10:00,  4.55s/it] 68%|██████▊   | 283/414 [22:33<10:40,  4.89s/it] 69%|██████▊   | 284/414 [22:36<09:39,  4.46s/it] 69%|██████▉   | 285/414 [22:41<09:50,  4.58s/it] 69%|██████▉   | 286/414 [22:48<11:04,  5.19s/it] 69%|██████▉   | 287/414 [22:53<11:07,  5.26s/it] 70%|██████▉   | 288/414 [22:58<10:50,  5.16s/it] 70%|██████▉   | 289/414 [23:02<09:33,  4.59s/it] 70%|███████   | 290/414 [23:07<09:44,  4.72s/it] 70%|███████   | 291/414 [23:13<10:30,  5.13s/it] 71%|███████   | 292/414 [23:16<09:33,  4.70s/it] 71%|███████   | 293/414 [23:24<11:08,  5.53s/it] 71%|███████   | 294/414 [23:33<12:59,  6.50s/it] 71%|███████▏  | 295/414 [23:38<12:28,  6.29s/it] 71%|███████▏  | 296/414 [23:44<12:13,  6.22s/it] 72%|███████▏  | 297/414 [23:50<11:32,  5.92s/it] 72%|███████▏  | 298/414 [23:55<10:56,  5.66s/it] 72%|███████▏  | 299/414 [24:00<10:52,  5.68s/it] 72%|███████▏  | 300/414 [24:05<10:07,  5.33s/it] 73%|███████▎  | 301/414 [24:10<09:47,  5.20s/it] 73%|███████▎  | 302/414 [24:13<08:41,  4.66s/it] 73%|███████▎  | 303/414 [24:17<08:22,  4.53s/it] 73%|███████▎  | 304/414 [24:24<09:28,  5.16s/it] 74%|███████▎  | 305/414 [24:30<10:00,  5.51s/it] 74%|███████▍  | 306/414 [24:36<09:42,  5.39s/it] 74%|███████▍  | 307/414 [24:40<09:16,  5.20s/it] 74%|███████▍  | 308/414 [24:47<09:43,  5.51s/it] 75%|███████▍  | 309/414 [24:52<09:29,  5.42s/it] 75%|███████▍  | 310/414 [24:55<08:18,  4.79s/it] 75%|███████▌  | 311/414 [24:59<07:41,  4.48s/it] 75%|███████▌  | 312/414 [25:03<07:19,  4.30s/it] 76%|███████▌  | 313/414 [25:07<07:09,  4.25s/it] 76%|███████▌  | 314/414 [25:10<06:47,  4.07s/it] 76%|███████▌  | 315/414 [25:15<06:54,  4.19s/it] 76%|███████▋  | 316/414 [25:21<07:43,  4.73s/it] 77%|███████▋  | 317/414 [25:28<08:32,  5.29s/it] 77%|███████▋  | 318/414 [25:33<08:21,  5.22s/it] 77%|███████▋  | 319/414 [25:37<07:59,  5.04s/it] 77%|███████▋  | 320/414 [25:41<07:19,  4.67s/it] 78%|███████▊  | 321/414 [25:47<07:49,  5.05s/it] 78%|███████▊  | 322/414 [25:53<07:57,  5.19s/it] 78%|███████▊  | 323/414 [25:57<07:26,  4.90s/it] 78%|███████▊  | 324/414 [26:01<06:54,  4.61s/it] 79%|███████▊  | 325/414 [26:04<06:26,  4.34s/it] 79%|███████▊  | 326/414 [26:08<06:07,  4.18s/it] 79%|███████▉  | 327/414 [26:14<06:56,  4.79s/it] 79%|███████▉  | 328/414 [26:20<07:10,  5.01s/it] 79%|███████▉  | 329/414 [26:26<07:30,  5.29s/it] 80%|███████▉  | 330/414 [26:32<07:51,  5.62s/it] 80%|███████▉  | 331/414 [26:37<07:33,  5.46s/it] 80%|████████  | 332/414 [26:43<07:27,  5.45s/it] 80%|████████  | 333/414 [26:48<07:26,  5.51s/it] 81%|████████  | 334/414 [26:52<06:42,  5.03s/it] 81%|████████  | 335/414 [26:56<06:11,  4.70s/it] 81%|████████  | 336/414 [27:01<06:06,  4.70s/it] 81%|████████▏ | 337/414 [27:04<05:32,  4.32s/it] 82%|████████▏ | 338/414 [27:09<05:25,  4.28s/it] 82%|████████▏ | 339/414 [27:15<06:20,  5.07s/it] 82%|████████▏ | 340/414 [27:21<06:28,  5.25s/it] 82%|████████▏ | 341/414 [27:27<06:32,  5.37s/it] 83%|████████▎ | 342/414 [27:30<05:47,  4.83s/it] 83%|████████▎ | 343/414 [27:36<06:07,  5.18s/it] 83%|████████▎ | 344/414 [27:43<06:38,  5.69s/it] 83%|████████▎ | 345/414 [27:48<06:21,  5.52s/it] 84%|████████▎ | 346/414 [27:53<05:48,  5.12s/it] 84%|████████▍ | 347/414 [27:57<05:29,  4.92s/it] 84%|████████▍ | 348/414 [28:01<05:06,  4.65s/it] 84%|████████▍ | 349/414 [28:04<04:39,  4.30s/it] 85%|████████▍ | 350/414 [28:09<04:47,  4.49s/it] 85%|████████▍ | 351/414 [28:16<05:28,  5.21s/it] 85%|████████▌ | 352/414 [28:20<04:56,  4.78s/it] 85%|████████▌ | 353/414 [28:24<04:28,  4.40s/it] 86%|████████▌ | 354/414 [28:27<04:12,  4.21s/it] 86%|████████▌ | 355/414 [28:31<03:51,  3.92s/it] 86%|████████▌ | 356/414 [28:35<03:46,  3.91s/it] 86%|████████▌ | 357/414 [28:39<03:53,  4.09s/it] 86%|████████▋ | 358/414 [28:46<04:34,  4.90s/it] 87%|████████▋ | 359/414 [28:52<04:42,  5.14s/it] 87%|████████▋ | 360/414 [28:56<04:22,  4.87s/it] 87%|████████▋ | 361/414 [29:00<04:00,  4.54s/it] 87%|████████▋ | 362/414 [29:05<04:11,  4.83s/it] 88%|████████▊ | 363/414 [29:12<04:32,  5.35s/it] 88%|████████▊ | 364/414 [29:15<04:02,  4.84s/it] 88%|████████▊ | 365/414 [29:21<04:08,  5.08s/it] 88%|████████▊ | 366/414 [29:27<04:21,  5.44s/it] 89%|████████▊ | 367/414 [29:32<04:09,  5.31s/it] 89%|████████▉ | 368/414 [29:37<04:01,  5.24s/it] 89%|████████▉ | 369/414 [29:42<03:53,  5.19s/it] 89%|████████▉ | 370/414 [29:48<03:57,  5.40s/it] 90%|████████▉ | 371/414 [29:54<03:56,  5.49s/it] 90%|████████▉ | 372/414 [30:00<04:02,  5.78s/it] 90%|█████████ | 373/414 [30:06<03:55,  5.74s/it] 90%|█████████ | 374/414 [30:11<03:43,  5.60s/it] 91%|█████████ | 375/414 [30:16<03:26,  5.30s/it] 91%|█████████ | 376/414 [30:20<03:02,  4.81s/it] 91%|█████████ | 377/414 [30:24<02:58,  4.81s/it] 91%|█████████▏| 378/414 [30:30<03:06,  5.17s/it] 92%|█████████▏| 379/414 [30:35<02:54,  4.98s/it] 92%|█████████▏| 380/414 [30:41<02:58,  5.24s/it] 92%|█████████▏| 381/414 [30:50<03:33,  6.48s/it] 92%|█████████▏| 382/414 [30:56<03:22,  6.32s/it] 93%|█████████▎| 383/414 [31:00<02:55,  5.66s/it] 93%|█████████▎| 384/414 [31:05<02:44,  5.49s/it] 93%|█████████▎| 385/414 [31:10<02:31,  5.23s/it] 93%|█████████▎| 386/414 [31:14<02:19,  4.98s/it] 93%|█████████▎| 387/414 [31:18<02:07,  4.74s/it] 94%|█████████▎| 388/414 [31:22<01:53,  4.36s/it] 94%|█████████▍| 389/414 [31:26<01:46,  4.25s/it] 94%|█████████▍| 390/414 [31:29<01:34,  3.95s/it] 94%|█████████▍| 391/414 [31:33<01:30,  3.94s/it] 95%|█████████▍| 392/414 [31:37<01:29,  4.07s/it] 95%|█████████▍| 393/414 [31:43<01:31,  4.38s/it] 95%|█████████▌| 394/414 [31:48<01:33,  4.67s/it] 95%|█████████▌| 395/414 [31:52<01:24,  4.43s/it] 96%|█████████▌| 396/414 [31:58<01:27,  4.83s/it] 96%|█████████▌| 397/414 [32:03<01:26,  5.10s/it] 96%|█████████▌| 398/414 [32:08<01:19,  4.99s/it] 96%|█████████▋| 399/414 [32:11<01:06,  4.45s/it] 97%|█████████▋| 400/414 [32:18<01:12,  5.19s/it] 97%|█████████▋| 401/414 [32:26<01:17,  5.93s/it] 97%|█████████▋| 402/414 [32:31<01:06,  5.56s/it] 97%|█████████▋| 403/414 [32:34<00:54,  4.91s/it] 98%|█████████▊| 404/414 [32:38<00:45,  4.54s/it] 98%|█████████▊| 405/414 [32:41<00:39,  4.33s/it] 98%|█████████▊| 406/414 [32:46<00:36,  4.52s/it] 98%|█████████▊| 407/414 [32:52<00:33,  4.85s/it] 99%|█████████▊| 408/414 [32:57<00:30,  5.00s/it] 99%|█████████▉| 409/414 [33:02<00:24,  4.99s/it] 99%|█████████▉| 410/414 [33:07<00:19,  4.87s/it] 99%|█████████▉| 411/414 [33:10<00:13,  4.47s/it]100%|█████████▉| 412/414 [33:16<00:09,  4.73s/it]100%|█████████▉| 413/414 [33:22<00:05,  5.04s/it]100%|██████████| 414/414 [33:24<00:00,  4.25s/it]100%|██████████| 414/414 [33:24<00:00,  4.84s/it]
***** eval metrics *****
  epoch                   =        2.0
  eval_loss               =     0.6899
  eval_runtime            = 0:33:30.54
  eval_samples_per_second =      1.644
  eval_steps_per_second   =      0.206
[INFO|modelcard.py:456] 2025-12-09 18:01:44,869 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /project/aip-wangcs/indrisch/LLaMA-Factory/wandb/wandb/offline-run-20251209_013027-1usp441a[0m
[1;34mwandb[0m: Find logs at: [1;35m../../../aip-wangcs/indrisch/LLaMA-Factory/wandb/wandb/offline-run-20251209_013027-1usp441a/logs[0m
