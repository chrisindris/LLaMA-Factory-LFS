
==========
== CUDA ==
==========

CUDA Version 12.4.1

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

[INFO|2025-12-15 03:40:53] llamafactory.launcher:143 >> Initializing 4 distributed tasks at: 127.0.0.1:41421
W1215 03:40:54.637000 270887 site-packages/torch/distributed/run.py:792] 
W1215 03:40:54.637000 270887 site-packages/torch/distributed/run.py:792] *****************************************
W1215 03:40:54.637000 270887 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1215 03:40:54.637000 270887 site-packages/torch/distributed/run.py:792] *****************************************
⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode


⚙️  Running in WANDB offline mode
[2025-12-15 03:41:02,138] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-15 03:41:02,138] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-15 03:41:02,184] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-15 03:41:02,356] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2025-12-15 03:41:05,855] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-15 03:41:05,856] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-15 03:41:05,856] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-15 03:41:05,856] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-15 03:41:05,856] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[rank0]: Traceback (most recent call last):
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/accelerate/utils/deepspeed.py", line 148, in __init__
[rank0]:     config = json.loads(config_file_or_dict)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/json/__init__.py", line 346, in loads
[rank0]:     return _default_decoder.decode(s)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/json/decoder.py", line 337, in decode
[rank0]:     obj, end = self.raw_decode(s, idx=_w(s, 0).end())
[rank0]:                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/json/decoder.py", line 355, in raw_decode
[rank0]:     raise JSONDecodeError("Expecting value", s, err.value) from None
[rank0]: json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

[rank0]: During handling of the above exception, another exception occurred:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/accelerate/utils/deepspeed.py", line 151, in __init__
[rank0]:     config_decoded = base64.urlsafe_b64decode(config_file_or_dict).decode("utf-8")
[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa6 in position 2: invalid start byte

[rank0]: During handling of the above exception, another exception occurred:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/app/src/llamafactory/launcher.py", line 180, in <module>
[rank0]:     run_exp()
[rank0]:   File "/app/src/llamafactory/train/tuner.py", line 110, in run_exp
[rank0]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank0]:   File "/app/src/llamafactory/train/tuner.py", line 55, in _training_function
[rank0]:     model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)
[rank0]:                                                                              ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/app/src/llamafactory/hparams/parser.py", line 219, in get_train_args
[rank0]:     model_args, data_args, training_args, finetuning_args, generating_args = _parse_train_args(args)
[rank0]:                                                                              ^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/app/src/llamafactory/hparams/parser.py", line 197, in _parse_train_args
[rank0]:     return _parse_args(parser, args, allow_extra_keys=allow_extra_keys)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/app/src/llamafactory/hparams/parser.py", line 79, in _parse_args
[rank0]:     return parser.parse_dict(args, allow_extra_keys=allow_extra_keys)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/transformers/hf_argparser.py", line 380, in parse_dict
[rank0]:     obj = dtype(**inputs)
[rank0]:           ^^^^^^^^^^^^^^^
[rank0]:   File "<string>", line 147, in __init__
[rank0]:   File "/app/src/llamafactory/hparams/training_args.py", line 85, in __post_init__
[rank0]:     Seq2SeqTrainingArguments.__post_init__(self)
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/transformers/training_args.py", line 2071, in __post_init__
[rank0]:     self.hf_deepspeed_config = HfTrainerDeepSpeedConfig(self.deepspeed)
[rank0]:                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/transformers/integrations/deepspeed.py", line 89, in __init__
[rank0]:     super().__init__(config_file_or_dict)
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/transformers/integrations/deepspeed.py", line 79, in __init__
[rank0]:     super().__init__(config_file_or_dict)
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/accelerate/utils/deepspeed.py", line 154, in __init__
[rank0]:     raise ValueError(
[rank0]: ValueError: Expected a string path to an existing deepspeed config, or a dictionary, or a base64 encoded string. Received: examples/deepspeed/ds_z3_offload_config_lora.json
[rank0]:[W1215 03:41:06.022633506 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W1215 03:41:08.365000 270887 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 270953 closing signal SIGTERM
W1215 03:41:08.366000 270887 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 270954 closing signal SIGTERM
W1215 03:41:08.366000 270887 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 270955 closing signal SIGTERM
E1215 03:41:08.757000 270887 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 270952) of binary: /opt/conda/bin/python3.11
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/opt/conda/lib/python3.11/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/opt/conda/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/app/src/llamafactory/launcher.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-12-15_03:41:08
  host      : trig0039.scinet.local
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 270952)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Traceback (most recent call last):
  File "/opt/conda/bin/llamafactory-cli", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/app/src/llamafactory/cli.py", line 24, in main
    launcher.launch()
  File "/app/src/llamafactory/launcher.py", line 110, in launch
    process = subprocess.run(
              ^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['torchrun', '--nnodes', '1', '--node_rank', '0', '--nproc_per_node', '4', '--master_addr', '127.0.0.1', '--master_port', '41421', '/app/src/llamafactory/launcher.py', '/scratch/indrisch/LLaMA-Factory/examples/train_lora/qwen3vl_lora_sft_SQA3Devery24_traineval.yaml']' returned non-zero exit status 1.
INFO:    Cleanup error: while stopping driver for /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Core/apptainer/1.3.5/var/apptainer/mnt/session/final: fuse-overlayfs exited: fuse: reading device: Software caused connection abort

scontrol show job 146604
JobId=146604 JobName=slurm_qwen3vl_lora_sft_SQA3Devery24_traineval.sh
   UserId=indrisch(3122343) GroupId=indrisch(3122343) MCS_label=N/A
   Priority=241784 Nice=0 Account=def-wangcs QOS=normal
   JobState=COMPLETING Reason=NonZeroExitCode Dependency=(null)
   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=1:0
   RunTime=00:00:34 TimeLimit=18:00:00 TimeMin=N/A
   SubmitTime=2025-12-15T03:39:36 EligibleTime=2025-12-15T03:39:36
   AccrueTime=2025-12-15T03:39:36
   StartTime=2025-12-15T03:40:36 EndTime=2025-12-15T03:41:10 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2025-12-15T03:40:36 Scheduler=Main
   Partition=compute_full_node AllocNode:Sid=trig-login01:1112633
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=trig0039
   BatchHost=trig0039
   NumNodes=1 NumCPUs=96 NumTasks=1 CPUs/Task=96 ReqB:S:C:T=0:0:*:*
   ReqTRES=cpu=96,mem=770000M,node=1,billing=4,gres/gpu=4
   AllocTRES=cpu=96,mem=770000M,node=1,billing=4,gres/gpu=4
   Socks/Node=* NtasksPerN:B:S:C=1:0:*:* CoreSpec=*
   MinCPUsNode=96 MinMemoryNode=770000M MinTmpDiskNode=0
   Features=(null) DelayBoot=00:00:00
   OverSubscribe=NO Contiguous=0 Licenses=(null) Network=(null)
   Command=/scratch/indrisch/LLaMA-Factory/preliminaries/qwen3vl_lora_sft_SQA3D/slurm_qwen3vl_lora_sft_SQA3Devery24_traineval.sh
   WorkDir=/scratch/indrisch/LLaMA-Factory/preliminaries/qwen3vl_lora_sft_SQA3D
   Comment=/opt/slurm/bin/sbatch --export=NONE --get-user-env=L slurm_qwen3vl_lora_sft_SQA3Devery24_traineval.sh 
   StdErr=/scratch/indrisch/LLaMA-Factory/preliminaries/qwen3vl_lora_sft_SQA3D/out/%N-qwen3vl_lora_sft_SQA3Devery24_traineval-146604.out
   StdIn=/dev/null
   StdOut=/scratch/indrisch/LLaMA-Factory/preliminaries/qwen3vl_lora_sft_SQA3D/out/%N-qwen3vl_lora_sft_SQA3Devery24_traineval-146604.out
   TresPerNode=gres/gpu:h100:4
   TresPerTask=cpu=96
   

sacct -j 146604
JobID           JobName    Account    Elapsed  MaxVMSize     MaxRSS  SystemCPU    UserCPU ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- ---------- ---------- -------- 
146604       slurm_qwe+ def-wangcs   00:00:34                         00:00:00   00:00:00      0:0 
146604.batch      batch def-wangcs   00:00:34                         00:00:00   00:00:00      0:0 
146604.exte+     extern def-wangcs   00:00:34                         00:00:00   00:00:00      0:0 

