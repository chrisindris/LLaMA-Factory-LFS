
==========
== CUDA ==
==========

CUDA Version 12.4.1

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

[INFO|2025-12-16 00:57:58] llamafactory.launcher:143 >> Initializing 4 distributed tasks at: 127.0.0.1:43719
W1216 00:58:00.043000 152710 site-packages/torch/distributed/run.py:792] 
W1216 00:58:00.043000 152710 site-packages/torch/distributed/run.py:792] *****************************************
W1216 00:58:00.043000 152710 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1216 00:58:00.043000 152710 site-packages/torch/distributed/run.py:792] *****************************************
⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode


[2025-12-16 00:58:07,827] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-16 00:58:07,827] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-16 00:58:07,843] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-16 00:58:08,039] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2025-12-16 00:58:12,470] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-16 00:58:12,471] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-16 00:58:12,471] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-16 00:58:12,471] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-12-16 00:58:12,472] [INFO] [comm.py:669:init_distributed] cdb=None
[INFO|2025-12-16 00:58:13] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.
[INFO|2025-12-16 00:58:13] llamafactory.hparams.parser:423 >> Process rank: 0, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|hub.py:421] 2025-12-16 00:58:13,718 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2025-12-16 00:58:13,730 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2025-12-16 00:58:13,739 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen3-VL-8B-Instruct/snapshots/0c351dd01ed87e9c1b53cbc748cba10e6187ff3b/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-12-16 00:58:13,739 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen3-VL-8B-Instruct/snapshots/0c351dd01ed87e9c1b53cbc748cba10e6187ff3b/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-12-16 00:58:13,739 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen3-VL-8B-Instruct/snapshots/0c351dd01ed87e9c1b53cbc748cba10e6187ff3b/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-12-16 00:58:13,739 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-16 00:58:13,739 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-16 00:58:13,739 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen3-VL-8B-Instruct/snapshots/0c351dd01ed87e9c1b53cbc748cba10e6187ff3b/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-12-16 00:58:13,739 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-12-16 00:58:13,946 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|hub.py:421] 2025-12-16 00:58:13,948 >> Offline mode: forcing local_files_only=True
[INFO|hub.py:421] 2025-12-16 00:58:13,950 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:316] 2025-12-16 00:58:13,952 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2025-12-16 00:58:13,955 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen3-VL-8B-Instruct/snapshots/0c351dd01ed87e9c1b53cbc748cba10e6187ff3b/preprocessor_config.json
[INFO|image_processing_base.py:316] 2025-12-16 00:58:13,959 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2025-12-16 00:58:13,961 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen3-VL-8B-Instruct/snapshots/0c351dd01ed87e9c1b53cbc748cba10e6187ff3b/preprocessor_config.json
[INFO|image_processing_base.py:428] 2025-12-16 00:58:13,972 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_pixels": null,
  "merge_size": 2,
  "min_pixels": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 16777216,
    "shortest_edge": 65536
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:1949] 2025-12-16 00:58:13,972 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2025-12-16 00:58:13,977 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen3-VL-8B-Instruct/snapshots/0c351dd01ed87e9c1b53cbc748cba10e6187ff3b/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-12-16 00:58:13,977 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen3-VL-8B-Instruct/snapshots/0c351dd01ed87e9c1b53cbc748cba10e6187ff3b/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-12-16 00:58:13,977 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen3-VL-8B-Instruct/snapshots/0c351dd01ed87e9c1b53cbc748cba10e6187ff3b/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-12-16 00:58:13,977 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-16 00:58:13,977 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-16 00:58:13,977 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen3-VL-8B-Instruct/snapshots/0c351dd01ed87e9c1b53cbc748cba10e6187ff3b/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-12-16 00:58:13,977 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-12-16 00:58:14,142 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:660] 2025-12-16 00:58:14,144 >> Offline mode: forcing local_files_only=True
[INFO|video_processing_utils.py:726] 2025-12-16 00:58:14,150 >> loading configuration file video_preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen3-VL-8B-Instruct/snapshots/0c351dd01ed87e9c1b53cbc748cba10e6187ff3b/video_preprocessor_config.json
[INFO|video_processing_utils.py:770] 2025-12-16 00:58:14,152 >> Video processor Qwen3VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": true,
  "fps": 2,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_frames": 768,
  "merge_size": 2,
  "min_frames": 4,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 25165824,
    "shortest_edge": 4096
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen3VLVideoProcessor"
}

[INFO|processing_utils.py:928] 2025-12-16 00:58:14,152 >> Offline mode: forcing local_files_only=True
[INFO|processing_utils.py:1116] 2025-12-16 00:58:14,158 >> loading configuration file processor_config.json from cache at None
[INFO|processing_utils.py:1199] 2025-12-16 00:58:14,466 >> Processor Qwen3VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_pixels": null,
  "merge_size": 2,
  "min_pixels": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 16777216,
    "shortest_edge": 65536
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen3-VL-8B-Instruct', vocab_size=151643, model_max_length=262144, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151665: AddedToken("<tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151666: AddedToken("</tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151667: AddedToken("<think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151668: AddedToken("</think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen3VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": true,
  "fps": 2,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_frames": 768,
  "merge_size": 2,
  "min_frames": 4,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 25165824,
    "shortest_edge": 4096
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen3VLVideoProcessor"
}


{
  "processor_class": "Qwen3VLProcessor"
}

[INFO|2025-12-16 00:58:14] llamafactory.data.loader:143 >> Loading dataset /scratch/indrisch/huggingface/hub/datasets--cvis-tmu--llamafactory-sqa3d-traces-multiimage-vqa/snapshots/ce5c54adc1608d1726730c9ff334e65b6dd70e46/data/...
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
[INFO|2025-12-16 00:58:14] llamafactory.hparams.parser:423 >> Process rank: 2, world size: 4, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-12-16 00:58:14] llamafactory.hparams.parser:423 >> Process rank: 1, world size: 4, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-12-16 00:58:14] llamafactory.hparams.parser:423 >> Process rank: 3, world size: 4, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
[rank0]:[W1216 00:58:15.200808439 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W1216 00:58:15.794595882 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W1216 00:58:15.857361319 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W1216 00:58:15.951461029 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
training example:
input_ids:
[151644, 8948, 271, 262, 1446, 525, 264, 16585, 11129, 4142, 11528, 17847, 369, 27979, 647, 47522, 389, 264, 3175, 29519, 6109, 6839, 3941, 5248, 5335, 382, 262, 26149, 510, 262, 68252, 1283, 419, 1943, 11, 498, 686, 5258, 451, 6109, 5335, 438, 366, 1805, 29, 9492, 13, 18877, 1105, 438, 2155, 6194, 315, 279, 83490, 6109, 13, 28634, 311, 5335, 553, 220, 16, 5980, 1922, 304, 1973, 315, 11094, 25, 508, 16, 1125, 508, 17, 1125, 4593, 11, 508, 45, 29562, 262, 3596, 35304, 1718, 8575, 38439, 3298, 14017, 510, 262, 7288, 7854, 264, 12966, 220, 18, 35, 10502, 1614, 3941, 6194, 25, 3754, 6171, 3941, 5335, 11, 23583, 8674, 6249, 17040, 11, 323, 990, 5452, 14, 509, 8957, 14688, 21060, 369, 7990, 55916, 624, 262, 7288, 72077, 409, 849, 292, 3793, 26087, 983, 847, 1290, 97089, 17996, 4065, 14, 29898, 484, 32511, 56111, 7612, 1825, 35978, 28455, 448, 5091, 311, 279, 64171, 14, 8092, 13, 1416, 279, 58385, 374, 54761, 11, 1638, 311, 279, 1429, 38219, 8622, 1651, 323, 1977, 773, 624, 262, 7288, 5443, 1172, 9434, 5904, 26, 653, 537, 17023, 63133, 3565, 476, 17188, 389, 9250, 6540, 7797, 6770, 1633, 43898, 3793, 624, 262, 7288, 3197, 5248, 11178, 3000, 11, 8830, 553, 279, 1850, 2432, 311, 27979, 17133, 488, 4274, 1835, 3941, 6194, 13, 1416, 2058, 54761, 1283, 13295, 678, 5335, 11, 1584, 429, 9355, 382, 262, 30990, 320, 327, 32739, 1378, 10010, 11, 304, 419, 1973, 982, 257, 151667, 198, 262, 14822, 14319, 29208, 32711, 304, 220, 18, 4142, 16, 15, 2805, 7354, 13, 356, 632, 5904, 448, 2168, 14937, 320, 68, 1302, 2572, 65750, 18, 5669, 3100, 7579, 18010, 1290, 315, 8718, 26, 508, 16, 5669, 1852, 1894, 11, 42396, 64212, 2823, 63594, 714, 4583, 320, 2640, 37294, 17, 20, 15, 11211, 7241, 5871, 568, 7036, 894, 17796, 8957, 14, 2969, 26745, 487, 323, 1246, 498, 19673, 432, 624, 257, 151668, 198, 262, 366, 9217, 397, 262, 362, 3175, 63594, 4226, 1172, 11, 892, 1231, 387, 510, 262, 7288, 1416, 14016, 25, 3776, 37328, 4226, 11, 4504, 320, 68, 1302, 2572, 1036, 17, 32511, 476, 7414, 14, 2753, 320, 68, 1302, 2572, 1036, 9693, 854, 4292, 262, 7288, 1416, 537, 14016, 25, 362, 11652, 476, 1378, 624, 262, 1416, 4226, 537, 9434, 11, 421, 1052, 374, 902, 2797, 5904, 11, 476, 421, 279, 4226, 374, 35197, 54761, 25, 1036, 33260, 8253, 854, 624, 262, 690, 9217, 1339, 262, 62947, 609, 43797, 50, 510, 262, 7288, 3155, 4183, 13153, 279, 3405, 476, 1140, 5335, 26, 653, 4183, 2550, 4718, 476, 4960, 14158, 624, 262, 7288, 84468, 4285, 1894, 5036, 448, 518, 1429, 825, 22739, 26087, 4145, 3446, 838, 4322, 1574, 58689, 7256, 854, 4292, 262, 7288, 2823, 12966, 3941, 6194, 26, 421, 6194, 28295, 11, 62552, 279, 11299, 15432, 5904, 323, 5185, 432, 304, 220, 151667, 624, 257, 151645, 198, 151644, 872, 198, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 30, 220, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151645, 198, 151644, 77091, 198, 151667, 271, 151668, 271, 198, 32462, 432, 11, 1077, 594, 1430, 311, 7071, 700, 1128, 594, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 62410, 13, 5512, 11, 1077, 594, 50087, 279, 14852, 6505, 13, 576, 21921, 374, 4363, 264, 4158, 11, 1378, 1737, 95112, 4982, 13, 2411, 220, 16, 16, 297, 62410, 320, 3481, 7950, 3082, 701, 582, 1184, 311, 1779, 429, 7702, 13, 21131, 518, 279, 5335, 11, 5310, 279, 825, 9027, 279, 21921, 3082, 11, 1052, 594, 264, 17846, 3403, 279, 19309, 11, 714, 3783, 11, 902, 2293, 11489, 11, 279, 3405, 374, 911, 279, 21921, 13, 13824, 11, 7196, 279, 330, 66, 75512, 1, 1588, 374, 279, 5819, 4982, 13, 13824, 11, 902, 11, 279, 19309, 702, 264, 21921, 3685, 30, 13824, 11, 902, 11, 279, 1887, 21921, 374, 279, 4158, 61607, 13, 13824, 11, 304, 279, 2168, 1380, 279, 6249, 4933, 279, 19309, 323, 21921, 11, 279, 1909, 315, 279, 21921, 2293, 11489, 11, 902, 11, 7196, 279, 17846, 30, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 2727, 330, 65, 77832, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 3263, 2055, 220, 16, 16, 297, 62410, 1035, 387, 279, 8416, 2115, 3082, 8674, 311, 279, 19309, 13, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 279, 19309, 3082, 13, 2160, 429, 949, 315, 279, 21921, 6505, 30, 13824, 11, 902, 11, 7196, 279, 21921, 374, 279, 4158, 61607, 1212, 279, 19309, 13, 13824, 11, 902, 11, 279, 1909, 315, 429, 21921, 2293, 11489, 11, 902, 11, 304, 1045, 5335, 11, 1052, 594, 264, 17846, 25911, 476, 21810, 13, 13824, 11, 902, 11, 1077, 594, 1779, 279, 1156, 2421, 5335, 13, 576, 17196, 45766, 374, 33866, 11, 1221, 279, 19309, 3082, 13, 13824, 11, 279, 4396, 4226, 25, 3330, 518, 279, 5335, 11, 279, 1633, 389, 1909, 315, 279, 14852, 21921, 320, 1782, 4158, 26482, 4982, 8, 518, 220, 16, 16, 297, 62410, 374, 264, 17846, 13, 13824, 11, 902, 11, 3783, 21677, 17846, 374, 389, 279, 7002, 3403, 279, 19309, 13, 13824, 11, 7196, 358, 5786, 878, 13, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 594, 3405, 25, 330, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 7521, 2055, 1156, 11, 10542, 279, 21921, 13, 576, 4158, 26482, 4982, 3143, 220, 16, 16, 297, 62410, 320, 3481, 7950, 568, 3555, 594, 389, 1909, 30, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 429, 3082, 13, 13824, 11, 9834, 0, 758, 279, 21844, 476, 21810, 11, 1052, 594, 264, 17846, 13, 2055, 279, 4226, 374, 264, 17846, 4192, 1249, 8253, 1128, 374, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 527, 20666, 11, 23643, 279, 27979, 6789, 323, 5424, 304, 279, 5335, 25, 18611, 16, 13, 95210, 279, 1036, 65, 77832, 21921, 854, 304, 3405, 25, 362, 4158, 11, 1378, 481, 26482, 5819, 4982, 374, 9434, 3143, 279, 1909, 481, 2115, 320, 16, 16, 297, 527, 20666, 8, 3082, 8674, 311, 1008, 37664, 1075, 279, 19309, 323, 17196, 13, 2303, 17, 13, 1374, 19991, 6171, 3403, 419, 21921, 25, 758, 279, 5335, 11, 264, 3070, 73225, 334, 374, 21810, 389, 279, 7002, 3403, 419, 21921, 3082, 13, 79226, 54216, 151645, 198]
inputs:
<|im_start|>system

    You are a careful vision–language assistant for spatial VQA on a single indoor scene shown across multiple images.

    INPUT:
    Immediately after this message, you will receive N scene images as <image> tags. Treat them as different views of the SAME scene. Refer to images by 1-based index in order of appearance: [1], [2], …, [N].

    REASONING PRINCIPLES:
    • Build a consistent 3D mental model across views: track objects across images, infer relative camera pose, and use scale/occlusion/shadows for depth cues.
    • Interpret deictic terms (“to my right/left/in front/behind”) EGOCENTRICALLY with respect to the narrator/agent. If the viewpoint is ambiguous, default to the most informative central view and say so.
    • Use only visible evidence; do not invent unseen details or rely on external knowledge beyond basic object/color terms.
    • When multiple candidates exist, resolve by the best match to spatial phrase + salience across views. If still ambiguous after checking all images, state that clearly.

    OUTPUT (exactly two blocks, in this order):
    <think>
    Step-by-step reasoning in 3–10 short steps. Cite evidence with image indices (e.g., “[3]: light wood desk right of monitor; [1]: same color, confirms”). Be concise but complete (aim ≤250 tokens unless necessary). Note any occlusion/ambiguity and how you resolved it.
    </think>
    <answer>
    A single concise answer only, which may be:
    • If sufficient: One-word answer, Count (e.g., “2”) or Yes/No (e.g., “yes”).
    • If not sufficient: A sentence or two.
    If answer not visible, if there is no clear evidence, or if the answer is genuinely ambiguous: “cannot determine”.
    </answer>

    STYLE & RULES:
    • Do NOT repeat the question or list images; do NOT output JSON or extra sections.
    • Prefer simple color names with at most one modifier (“light/dark/pale/beige”).
    • Be consistent across views; if views disagree, prioritize the clearest evidence and note it in <think>.
    <|im_end|>
<|im_start|>user
What is on top of the bathroom cabinet that is on my 11 o'clock? <|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|im_end|>
<|im_start|>assistant
<think>

</think>


Got it, let's try to figure out what's on top of the bathroom cabinet at 11 o'clock. First, let's visualize the bathroom setup. The cabinet is likely a white, two-drawer unit. At 11 o'clock (top-left area), we need to check that spot. Looking at the images, especially the one showing the cabinet area, there's a mirror above the sink, but wait, no—wait, the question is about the cabinet. Wait, maybe the "cabinet" here is the storage unit. Wait, no, the sink has a cabinet below? Wait, no, the main cabinet is the white drawers. Wait, in the image where the camera shows the sink and cabinet, the top of the cabinet—wait, no, maybe the mirror? Wait, no, let's check again. Wait, the user says "bathroom cabinet that is on my 11 o'clock". So 11 o'clock would be the upper left area relative to the sink. Looking at the images, there's a mirror mounted on the wall above the sink area. Is that part of the cabinet setup? Wait, no, maybe the cabinet is the white drawers under the sink. Wait, no, the top of that cabinet—wait, no, in some images, there's a mirror reflected or mounted. Wait, no, let's check the first few images. The shower curtain is colorful, then the sink area. Wait, the correct answer: looking at the images, the object on top of the bathroom cabinet (the white drawer unit) at 11 o'clock is a mirror. Wait, no, wait—the mirror is on the wall above the sink. Wait, maybe I misread. Wait, no, let's check again. Wait, the user's question: "What is on top of the bathroom cabinet that is on my 11 o'clock?" So first, identify the cabinet. The white drawer unit near 11 o'clock (top-left). What's on top? Looking at the images, there's a mirror mounted on the wall above that area. Wait, yes! In the reflection or mounted, there's a mirror. So the answer is a mirror.


To determine what is on top of the bathroom cabinet at 11 o’clock, analyze the spatial layout and elements in the images:  

1. Locate the “bathroom cabinet” in question: A white, two - drawer storage unit is visible near the top - left (11 o’clock) area relative to other fixtures like the sink and shower.  
2. Examine objects above this cabinet: In the images, a **mirror** is mounted on the wall above this cabinet area.  


Mirror<|im_end|>

label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 151667, 271, 151668, 271, 198, 32462, 432, 11, 1077, 594, 1430, 311, 7071, 700, 1128, 594, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 62410, 13, 5512, 11, 1077, 594, 50087, 279, 14852, 6505, 13, 576, 21921, 374, 4363, 264, 4158, 11, 1378, 1737, 95112, 4982, 13, 2411, 220, 16, 16, 297, 62410, 320, 3481, 7950, 3082, 701, 582, 1184, 311, 1779, 429, 7702, 13, 21131, 518, 279, 5335, 11, 5310, 279, 825, 9027, 279, 21921, 3082, 11, 1052, 594, 264, 17846, 3403, 279, 19309, 11, 714, 3783, 11, 902, 2293, 11489, 11, 279, 3405, 374, 911, 279, 21921, 13, 13824, 11, 7196, 279, 330, 66, 75512, 1, 1588, 374, 279, 5819, 4982, 13, 13824, 11, 902, 11, 279, 19309, 702, 264, 21921, 3685, 30, 13824, 11, 902, 11, 279, 1887, 21921, 374, 279, 4158, 61607, 13, 13824, 11, 304, 279, 2168, 1380, 279, 6249, 4933, 279, 19309, 323, 21921, 11, 279, 1909, 315, 279, 21921, 2293, 11489, 11, 902, 11, 7196, 279, 17846, 30, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 2727, 330, 65, 77832, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 3263, 2055, 220, 16, 16, 297, 62410, 1035, 387, 279, 8416, 2115, 3082, 8674, 311, 279, 19309, 13, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 279, 19309, 3082, 13, 2160, 429, 949, 315, 279, 21921, 6505, 30, 13824, 11, 902, 11, 7196, 279, 21921, 374, 279, 4158, 61607, 1212, 279, 19309, 13, 13824, 11, 902, 11, 279, 1909, 315, 429, 21921, 2293, 11489, 11, 902, 11, 304, 1045, 5335, 11, 1052, 594, 264, 17846, 25911, 476, 21810, 13, 13824, 11, 902, 11, 1077, 594, 1779, 279, 1156, 2421, 5335, 13, 576, 17196, 45766, 374, 33866, 11, 1221, 279, 19309, 3082, 13, 13824, 11, 279, 4396, 4226, 25, 3330, 518, 279, 5335, 11, 279, 1633, 389, 1909, 315, 279, 14852, 21921, 320, 1782, 4158, 26482, 4982, 8, 518, 220, 16, 16, 297, 62410, 374, 264, 17846, 13, 13824, 11, 902, 11, 3783, 21677, 17846, 374, 389, 279, 7002, 3403, 279, 19309, 13, 13824, 11, 7196, 358, 5786, 878, 13, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 594, 3405, 25, 330, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 7521, 2055, 1156, 11, 10542, 279, 21921, 13, 576, 4158, 26482, 4982, 3143, 220, 16, 16, 297, 62410, 320, 3481, 7950, 568, 3555, 594, 389, 1909, 30, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 429, 3082, 13, 13824, 11, 9834, 0, 758, 279, 21844, 476, 21810, 11, 1052, 594, 264, 17846, 13, 2055, 279, 4226, 374, 264, 17846, 4192, 1249, 8253, 1128, 374, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 527, 20666, 11, 23643, 279, 27979, 6789, 323, 5424, 304, 279, 5335, 25, 18611, 16, 13, 95210, 279, 1036, 65, 77832, 21921, 854, 304, 3405, 25, 362, 4158, 11, 1378, 481, 26482, 5819, 4982, 374, 9434, 3143, 279, 1909, 481, 2115, 320, 16, 16, 297, 527, 20666, 8, 3082, 8674, 311, 1008, 37664, 1075, 279, 19309, 323, 17196, 13, 2303, 17, 13, 1374, 19991, 6171, 3403, 419, 21921, 25, 758, 279, 5335, 11, 264, 3070, 73225, 334, 374, 21810, 389, 279, 7002, 3403, 419, 21921, 3082, 13, 79226, 54216, 151645, 198]
labels:
<think>

</think>


Got it, let's try to figure out what's on top of the bathroom cabinet at 11 o'clock. First, let's visualize the bathroom setup. The cabinet is likely a white, two-drawer unit. At 11 o'clock (top-left area), we need to check that spot. Looking at the images, especially the one showing the cabinet area, there's a mirror above the sink, but wait, no—wait, the question is about the cabinet. Wait, maybe the "cabinet" here is the storage unit. Wait, no, the sink has a cabinet below? Wait, no, the main cabinet is the white drawers. Wait, in the image where the camera shows the sink and cabinet, the top of the cabinet—wait, no, maybe the mirror? Wait, no, let's check again. Wait, the user says "bathroom cabinet that is on my 11 o'clock". So 11 o'clock would be the upper left area relative to the sink. Looking at the images, there's a mirror mounted on the wall above the sink area. Is that part of the cabinet setup? Wait, no, maybe the cabinet is the white drawers under the sink. Wait, no, the top of that cabinet—wait, no, in some images, there's a mirror reflected or mounted. Wait, no, let's check the first few images. The shower curtain is colorful, then the sink area. Wait, the correct answer: looking at the images, the object on top of the bathroom cabinet (the white drawer unit) at 11 o'clock is a mirror. Wait, no, wait—the mirror is on the wall above the sink. Wait, maybe I misread. Wait, no, let's check again. Wait, the user's question: "What is on top of the bathroom cabinet that is on my 11 o'clock?" So first, identify the cabinet. The white drawer unit near 11 o'clock (top-left). What's on top? Looking at the images, there's a mirror mounted on the wall above that area. Wait, yes! In the reflection or mounted, there's a mirror. So the answer is a mirror.


To determine what is on top of the bathroom cabinet at 11 o’clock, analyze the spatial layout and elements in the images:  

1. Locate the “bathroom cabinet” in question: A white, two - drawer storage unit is visible near the top - left (11 o’clock) area relative to other fixtures like the sink and shower.  
2. Examine objects above this cabinet: In the images, a **mirror** is mounted on the wall above this cabinet area.  


Mirror<|im_end|>

[INFO|hub.py:421] 2025-12-16 00:58:18,808 >> Offline mode: forcing local_files_only=True
[INFO|configuration_utils.py:765] 2025-12-16 00:58:18,812 >> loading configuration file config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen3-VL-8B-Instruct/snapshots/0c351dd01ed87e9c1b53cbc748cba10e6187ff3b/config.json
[INFO|configuration_utils.py:839] 2025-12-16 00:58:18,814 >> Model config Qwen3VLConfig {
  "architectures": [
    "Qwen3VLForConditionalGeneration"
  ],
  "image_token_id": 151655,
  "model_type": "qwen3_vl",
  "text_config": {
    "attention_bias": false,
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "dtype": "bfloat16",
    "eos_token_id": 151645,
    "head_dim": 128,
    "hidden_act": "silu",
    "hidden_size": 4096,
    "initializer_range": 0.02,
    "intermediate_size": 12288,
    "max_position_embeddings": 262144,
    "model_type": "qwen3_vl_text",
    "num_attention_heads": 32,
    "num_hidden_layers": 36,
    "num_key_value_heads": 8,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_interleaved": true,
      "mrope_section": [
        24,
        20,
        20
      ],
      "rope_type": "default"
    },
    "rope_theta": 5000000,
    "use_cache": true,
    "vocab_size": 151936
  },
  "tie_word_embeddings": false,
  "transformers_version": "4.57.0",
  "video_token_id": 151656,
  "vision_config": {
    "deepstack_visual_indexes": [
      8,
      16,
      24
    ],
    "depth": 27,
    "hidden_act": "gelu_pytorch_tanh",
    "hidden_size": 1152,
    "in_channels": 3,
    "initializer_range": 0.02,
    "intermediate_size": 4304,
    "model_type": "qwen3_vl",
    "num_heads": 16,
    "num_position_embeddings": 2304,
    "out_hidden_size": 4096,
    "patch_size": 16,
    "spatial_merge_size": 2,
    "temporal_patch_size": 2
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652
}

[INFO|2025-12-16 00:58:18] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
[WARNING|2025-12-16 00:58:18] llamafactory.model.model_utils.liger_kernel:148 >> Current model does not support liger kernel.
[INFO|hub.py:421] 2025-12-16 00:58:18,904 >> Offline mode: forcing local_files_only=True
[INFO|hub.py:421] 2025-12-16 00:58:18,911 >> Offline mode: forcing local_files_only=True
[INFO|modeling_utils.py:4840] 2025-12-16 00:58:18,912 >> Offline mode: forcing local_files_only=True
[INFO|modeling_utils.py:1176] 2025-12-16 00:58:18,914 >> loading weights file model.safetensors from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen3-VL-8B-Instruct/snapshots/0c351dd01ed87e9c1b53cbc748cba10e6187ff3b/model.safetensors.index.json
[INFO|modeling_utils.py:4377] 2025-12-16 00:58:18,920 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[2025-12-16 00:58:18,920] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
[WARNING|logging.py:328] 2025-12-16 00:58:18,931 >> Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3VLForConditionalGeneration is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[INFO|configuration_utils.py:986] 2025-12-16 00:58:18,934 >> Generate config GenerationConfig {
  "use_cache": false
}

[WARNING|logging.py:328] 2025-12-16 00:58:18,936 >> Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3VLModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[INFO|modeling_utils.py:2345] 2025-12-16 00:58:18,937 >> Instantiating Qwen3VLVisionModel model under default dtype torch.float32.
[WARNING|logging.py:328] 2025-12-16 00:58:18,939 >> Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3VLVisionModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[2025-12-16 00:58:19,239] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
[2025-12-16 00:58:19,239] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
[2025-12-16 00:58:19,241] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3VLForConditionalGeneration is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3VLForConditionalGeneration is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3VLForConditionalGeneration is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3VLModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3VLVisionModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3VLModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3VLModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3VLVisionModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3VLVisionModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[INFO|modeling_utils.py:2345] 2025-12-16 00:58:19,926 >> Instantiating Qwen3VLTextModel model under default dtype torch.float32.
[WARNING|logging.py:328] 2025-12-16 00:58:19,929 >> Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3VLTextModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3VLTextModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3VLTextModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3VLTextModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[2025-12-16 00:58:23,452] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 750, num_elems = 8.77B
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:15<00:46, 15.56s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:15<00:46, 15.57s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:15<00:46, 15.57s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:15<00:47, 15.73s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:31<00:31, 15.54s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:31<00:31, 15.54s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:31<00:31, 15.54s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:31<00:31, 15.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:48<00:16, 16.55s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:48<00:16, 16.55s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:48<00:16, 16.55s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:48<00:16, 16.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:57<00:00, 13.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:57<00:00, 14.49s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:57<00:00, 13.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:57<00:00, 13.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:57<00:00, 14.48s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:57<00:00, 14.49s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:58<00:00, 13.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:58<00:00, 14.51s/it]
[INFO|configuration_utils.py:941] 2025-12-16 00:59:21,519 >> loading configuration file generation_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen3-VL-8B-Instruct/snapshots/0c351dd01ed87e9c1b53cbc748cba10e6187ff3b/generation_config.json
[INFO|configuration_utils.py:986] 2025-12-16 00:59:21,519 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

[INFO|dynamic_module_utils.py:423] 2025-12-16 00:59:21,521 >> Could not locate the custom_generate/generate.py inside Qwen/Qwen3-VL-8B-Instruct.
[INFO|2025-12-16 00:59:21] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2025-12-16 00:59:21] llamafactory.model.model_utils.attention:143 >> Using FlashAttention-2 for faster training and inference.
[INFO|2025-12-16 00:59:21] llamafactory.model.adapter:143 >> DeepSpeed ZeRO3 detected, remaining trainable params in float32.
[INFO|2025-12-16 00:59:21] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA
[INFO|2025-12-16 00:59:21] llamafactory.model.model_utils.misc:143 >> Found linear modules: linear_fc2,up_proj,linear_fc1,gate_proj,v_proj,o_proj,k_proj,down_proj,q_proj
[INFO|2025-12-16 00:59:21] llamafactory.model.model_utils.visual:143 >> Set vision model not trainable: ['visual.patch_embed', 'visual.blocks'].
[INFO|2025-12-16 00:59:21] llamafactory.model.model_utils.visual:143 >> Set multi model projector not trainable: visual.merger.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.
[INFO|2025-12-16 00:59:21] llamafactory.model.loader:143 >> trainable params: 22,253,568 || all params: 8,789,377,264 || trainable%: 0.2532
name: base_model.model.model.visual.patch_embed.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.patch_embed.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.pos_embed.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.norm1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.norm2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.linear_fc1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.linear_fc1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.linear_fc2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.linear_fc2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.norm1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.norm2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.linear_fc1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.linear_fc1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.linear_fc2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.linear_fc2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.norm1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.norm2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.linear_fc1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.linear_fc1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.linear_fc2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.linear_fc2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.norm1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.norm2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.linear_fc1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.linear_fc1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.linear_fc2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.linear_fc2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.norm1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.norm2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.linear_fc1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.linear_fc1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.linear_fc2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.linear_fc2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.norm1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.norm2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.linear_fc1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.linear_fc1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.linear_fc2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.linear_fc2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.norm1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.norm2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.linear_fc1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.linear_fc1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.linear_fc2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.linear_fc2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.norm1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.norm2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.linear_fc1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.linear_fc1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.linear_fc2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.linear_fc2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.norm1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.norm2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.linear_fc1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.linear_fc1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.linear_fc2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.linear_fc2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.norm1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.norm2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.linear_fc1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.linear_fc1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.linear_fc2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.linear_fc2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.norm1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.norm2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.linear_fc1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.linear_fc1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.linear_fc2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.linear_fc2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.norm1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.norm2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.linear_fc1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.linear_fc1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.linear_fc2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.linear_fc2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.norm1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.norm2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.linear_fc1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.linear_fc1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.linear_fc2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.linear_fc2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.norm1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.norm2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.linear_fc1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.linear_fc1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.linear_fc2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.linear_fc2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.norm1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.norm2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.linear_fc1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.linear_fc1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.linear_fc2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.linear_fc2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.norm1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.norm2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.linear_fc1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.linear_fc1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.linear_fc2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.linear_fc2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.norm1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.norm2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.linear_fc1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.linear_fc1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.linear_fc2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.linear_fc2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.norm1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.norm2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.linear_fc1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.linear_fc1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.linear_fc2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.linear_fc2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.norm1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.norm2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.linear_fc1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.linear_fc1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.linear_fc2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.linear_fc2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.norm1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.norm2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.linear_fc1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.linear_fc1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.linear_fc2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.linear_fc2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.norm1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.norm2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.linear_fc1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.linear_fc1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.linear_fc2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.linear_fc2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.norm1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.norm2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.linear_fc1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.linear_fc1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.linear_fc2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.linear_fc2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.norm1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.norm2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.linear_fc1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.linear_fc1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.linear_fc2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.linear_fc2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.norm1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.norm2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.linear_fc1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.linear_fc1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.linear_fc2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.linear_fc2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.norm1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.norm2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.linear_fc1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.linear_fc1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.linear_fc2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.linear_fc2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.norm1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.norm2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.linear_fc1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.linear_fc1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.linear_fc2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.linear_fc2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.norm1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.norm2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.linear_fc1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.linear_fc1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.linear_fc2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.linear_fc2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.norm.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.linear_fc1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.linear_fc1.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.linear_fc2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.linear_fc2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.deepstack_merger_list.0.norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.deepstack_merger_list.0.norm.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.deepstack_merger_list.0.linear_fc1.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.deepstack_merger_list.0.linear_fc1.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.deepstack_merger_list.0.linear_fc1.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.visual.deepstack_merger_list.0.linear_fc1.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.visual.deepstack_merger_list.0.linear_fc2.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.deepstack_merger_list.0.linear_fc2.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.deepstack_merger_list.0.linear_fc2.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.visual.deepstack_merger_list.0.linear_fc2.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.visual.deepstack_merger_list.1.norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.deepstack_merger_list.1.norm.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.deepstack_merger_list.1.linear_fc1.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.deepstack_merger_list.1.linear_fc1.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.deepstack_merger_list.1.linear_fc1.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.visual.deepstack_merger_list.1.linear_fc1.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.visual.deepstack_merger_list.1.linear_fc2.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.deepstack_merger_list.1.linear_fc2.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.deepstack_merger_list.1.linear_fc2.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.visual.deepstack_merger_list.1.linear_fc2.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.visual.deepstack_merger_list.2.norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.deepstack_merger_list.2.norm.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.deepstack_merger_list.2.linear_fc1.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.deepstack_merger_list.2.linear_fc1.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.deepstack_merger_list.2.linear_fc1.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.visual.deepstack_merger_list.2.linear_fc1.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.visual.deepstack_merger_list.2.linear_fc2.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.deepstack_merger_list.2.linear_fc2.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.deepstack_merger_list.2.linear_fc2.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.visual.deepstack_merger_list.2.linear_fc2.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.embed_tokens.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.28.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.28.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.28.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.28.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.28.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.28.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.28.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.28.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.28.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.28.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.28.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.28.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.28.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.28.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.28.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.28.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.28.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.28.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.28.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.28.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.28.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.28.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.28.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.28.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.28.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.29.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.29.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.29.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.29.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.29.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.29.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.29.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.29.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.29.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.29.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.29.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.29.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.29.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.29.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.29.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.29.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.29.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.29.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.29.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.29.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.29.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.29.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.29.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.29.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.29.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.30.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.30.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.30.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.30.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.30.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.30.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.30.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.30.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.30.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.30.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.30.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.30.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.30.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.30.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.30.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.30.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.30.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.30.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.30.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.30.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.30.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.30.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.30.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.30.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.30.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.31.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.31.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.31.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.31.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.31.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.31.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.31.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.31.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.31.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.31.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.31.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.31.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.31.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.31.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.31.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.31.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.31.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.31.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.31.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.31.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.31.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.31.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.31.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.31.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.31.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.32.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.32.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.32.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.32.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.32.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.32.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.32.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.32.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.32.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.32.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.32.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.32.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.32.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.32.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.32.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.32.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.32.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.32.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.32.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.32.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.32.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.32.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.32.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.32.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.32.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.33.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.33.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.33.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.33.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.33.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.33.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.33.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.33.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.33.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.33.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.33.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.33.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.33.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.33.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.33.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.33.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.33.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.33.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.33.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.33.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.33.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.33.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.33.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.33.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.33.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.34.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.34.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.34.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.34.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.34.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.34.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.34.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.34.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.34.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.34.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.34.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.34.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.34.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.34.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.34.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.34.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.34.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.34.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.34.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.34.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.34.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.34.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.34.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.34.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.34.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.35.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.35.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.35.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.35.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.35.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.35.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.35.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.35.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.35.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.35.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.35.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.35.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.35.self_attn.q_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.35.self_attn.k_norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.35.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.35.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.35.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.35.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.35.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.35.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.35.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.35.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.35.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.35.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.35.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.lm_head.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
[INFO|trainer.py:749] 2025-12-16 00:59:21,907 >> Using auto half precision backend
[WARNING|2025-12-16 00:59:21] llamafactory.train.callbacks:154 >> Previous trainer log in this folder will be deleted.
[WARNING|trainer.py:982] 2025-12-16 00:59:21,910 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.
Currently training with a batch size of: 1
Currently training with a batch size of: 1
Currently training with a batch size of: 1
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
[DEBUG|trainer.py:2373] 2025-12-16 00:59:22,360 >> Currently training with a batch size of: 1
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 16. Using DeepSpeed's value.
[INFO|deepspeed.py:383] 2025-12-16 00:59:22,379 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Using /dev/shm/slurm.indrisch.148825/.cache/torch_extensions as PyTorch extensions root...
Creating extension directory /dev/shm/slurm.indrisch.148825/.cache/torch_extensions/cpu_adam...
Using /dev/shm/slurm.indrisch.148825/.cache/torch_extensions as PyTorch extensions root...
Using /dev/shm/slurm.indrisch.148825/.cache/torch_extensions as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /dev/shm/slurm.indrisch.148825/.cache/torch_extensions/cpu_adam/build.ninja...
/opt/conda/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module cpu_adam...
Using envvar MAX_JOBS (16) as the number of workers...
Using /dev/shm/slurm.indrisch.148825/.cache/torch_extensions as PyTorch extensions root...
[1/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.11/site-packages/torch/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o 
[2/3] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.11/site-packages/torch/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o 
[3/3] c++ cpu_adam.o cpu_adam_impl.o -shared -lcurand -L/usr/local/cuda/lib64 -L/opt/conda/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so
Loading extension module cpu_adam...
Time to load cpu_adam op: 23.18329930305481 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-12-16 00:59:46,521] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
Loading extension module cpu_adam...Loading extension module cpu_adam...

Time to load cpu_adam op: 23.213566541671753 secondsTime to load cpu_adam op: 23.21357250213623 seconds

Adam Optimizer #0 is created with AVX512 arithmetic capability.
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-12-16 00:59:46,552] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
[2025-12-16 00:59:46,552] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
Loading extension module cpu_adam...
Time to load cpu_adam op: 22.997406005859375 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-12-16 00:59:46,564] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.9, git-hash=unknown, git-branch=unknown
[2025-12-16 00:59:46,564] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
[2025-12-16 00:59:46,596] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-12-16 00:59:46,599] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-12-16 00:59:46,599] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-12-16 00:59:46,632] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2025-12-16 00:59:46,632] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2025-12-16 00:59:46,632] [INFO] [logging.py:107:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2025-12-16 00:59:46,632] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2025-12-16 00:59:46,841] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[2025-12-16 00:59:46,842] [INFO] [utils.py:782:see_memory_usage] MA 0.04 GB         Max_MA 3.48 GB         CA 0.08 GB         Max_CA 4 GB 
[2025-12-16 00:59:46,842] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 48.73 GB, percent = 6.4%
[2025-12-16 00:59:46,849] [INFO] [stage3.py:170:__init__] Reduce bucket size 16777216
[2025-12-16 00:59:46,849] [INFO] [stage3.py:171:__init__] Prefetch bucket size 15099494
[2025-12-16 00:59:47,049] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2025-12-16 00:59:47,050] [INFO] [utils.py:782:see_memory_usage] MA 0.04 GB         Max_MA 0.04 GB         CA 0.08 GB         Max_CA 0 GB 
[2025-12-16 00:59:47,050] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 48.73 GB, percent = 6.4%
Parameter Offload: Total persistent parameters: 12407024 in 786 params
[2025-12-16 00:59:47,969] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2025-12-16 00:59:47,969] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.04 GB         CA 0.08 GB         Max_CA 0 GB 
[2025-12-16 00:59:47,970] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 48.89 GB, percent = 6.5%
[2025-12-16 00:59:48,270] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[2025-12-16 00:59:48,271] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.08 GB         Max_CA 0 GB 
[2025-12-16 00:59:48,271] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 48.89 GB, percent = 6.5%
[2025-12-16 00:59:48,594] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 1
[2025-12-16 00:59:48,595] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.08 GB         Max_CA 0 GB 
[2025-12-16 00:59:48,595] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 49.02 GB, percent = 6.5%
[2025-12-16 00:59:48,896] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[2025-12-16 00:59:48,897] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.08 GB         Max_CA 0 GB 
[2025-12-16 00:59:48,897] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 49.02 GB, percent = 6.5%
[2025-12-16 00:59:49,217] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[2025-12-16 00:59:49,217] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.08 GB         Max_CA 0 GB 
[2025-12-16 00:59:49,218] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 49.05 GB, percent = 6.5%
[2025-12-16 00:59:49,560] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-12-16 00:59:49,560] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.08 GB         Max_CA 0 GB 
[2025-12-16 00:59:49,561] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 49.22 GB, percent = 6.5%
[2025-12-16 00:59:49,902] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-12-16 00:59:49,902] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.08 GB         Max_CA 0 GB 
[2025-12-16 00:59:49,902] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 49.3 GB, percent = 6.5%
[2025-12-16 00:59:49,903] [INFO] [stage3.py:534:_setup_for_real_optimizer] optimizer state initialized
***** Running training *****
***** Running training *****
  Num examples = 29,742
  Num Epochs = 1
  Num examples = 29,742
  Instantaneous batch size per device = 1
  Num Epochs = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Instantaneous batch size per device = 1
  Gradient Accumulation steps = 16
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Total optimization steps = 465
  Gradient Accumulation steps = 16
  Total optimization steps = 465
***** Running training *****
  Num examples = 29,742
  Num Epochs = 1
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 16
  Total optimization steps = 465
  Number of trainable parameters = 22,253,568
  Number of trainable parameters = 22,253,568
  Number of trainable parameters = 22,253,568
[2025-12-16 00:59:50,353] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-12-16 00:59:50,354] [INFO] [utils.py:782:see_memory_usage] MA 0.03 GB         Max_MA 0.03 GB         CA 0.12 GB         Max_CA 0 GB 
[2025-12-16 00:59:50,354] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 49.4 GB, percent = 6.5%
[2025-12-16 00:59:50,354] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[2025-12-16 00:59:50,354] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-12-16 00:59:50,354] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-12-16 00:59:50,354] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2025-12-16 00:59:50,361] [INFO] [config.py:1003:print] DeepSpeedEngine configuration:
[2025-12-16 00:59:50,361] [INFO] [config.py:1007:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-12-16 00:59:50,361] [INFO] [config.py:1007:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-12-16 00:59:50,361] [INFO] [config.py:1007:print]   amp_enabled .................. False
[2025-12-16 00:59:50,361] [INFO] [config.py:1007:print]   amp_params ................... False
[2025-12-16 00:59:50,361] [INFO] [config.py:1007:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-12-16 00:59:50,361] [INFO] [config.py:1007:print]   bfloat16_enabled ............. True
[2025-12-16 00:59:50,361] [INFO] [config.py:1007:print]   bfloat16_immediate_grad_update  True
[2025-12-16 00:59:50,361] [INFO] [config.py:1007:print]   checkpoint_parallel_write_pipeline  False
[2025-12-16 00:59:50,361] [INFO] [config.py:1007:print]   checkpoint_tag_validation_enabled  True
[2025-12-16 00:59:50,361] [INFO] [config.py:1007:print]   checkpoint_tag_validation_fail  False
[2025-12-16 00:59:50,361] [INFO] [config.py:1007:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x15327723d250>
[2025-12-16 00:59:50,361] [INFO] [config.py:1007:print]   communication_data_type ...... None
[2025-12-16 00:59:50,361] [INFO] [config.py:1007:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False
[2025-12-16 00:59:50,361] [INFO] [config.py:1007:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-12-16 00:59:50,361] [INFO] [config.py:1007:print]   curriculum_enabled_legacy .... False
[2025-12-16 00:59:50,361] [INFO] [config.py:1007:print]   curriculum_params_legacy ..... False
[2025-12-16 00:59:50,361] [INFO] [config.py:1007:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-12-16 00:59:50,361] [INFO] [config.py:1007:print]   data_efficiency_enabled ...... False
[2025-12-16 00:59:50,361] [INFO] [config.py:1007:print]   dataloader_drop_last ......... False
[2025-12-16 00:59:50,361] [INFO] [config.py:1007:print]   disable_allgather ............ False
[2025-12-16 00:59:50,361] [INFO] [config.py:1007:print]   dump_state ................... False
[2025-12-16 00:59:50,361] [INFO] [config.py:1007:print]   dynamic_loss_scale_args ...... None
[2025-12-16 00:59:50,361] [INFO] [config.py:1007:print]   eigenvalue_enabled ........... False
[2025-12-16 00:59:50,361] [INFO] [config.py:1007:print]   eigenvalue_gas_boundary_resolution  1
[2025-12-16 00:59:50,361] [INFO] [config.py:1007:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   eigenvalue_layer_num ......... 0
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   eigenvalue_max_iter .......... 100
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   eigenvalue_stability ......... 1e-06
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   eigenvalue_tol ............... 0.01
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   eigenvalue_verbose ........... False
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   elasticity_enabled ........... False
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   fp16_auto_cast ............... None
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   fp16_enabled ................. False
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   fp16_master_weights_and_gradients  False
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   global_rank .................. 0
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   grad_accum_dtype ............. None
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   gradient_accumulation_steps .. 16
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   gradient_clipping ............ 1.0
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   gradient_predivide_factor .... 1.0
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   graph_harvesting ............. False
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   initial_dynamic_scale ........ 1
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   load_universal_checkpoint .... False
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   loss_scale ................... 1.0
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   memory_breakdown ............. False
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   mics_hierarchial_params_gather  False
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   mics_shard_size .............. -1
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   optimizer_legacy_fusion ...... False
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   optimizer_name ............... None
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   optimizer_params ............. None
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   pld_enabled .................. False
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   pld_params ................... False
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   prescale_gradients ........... False
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   scheduler_name ............... None
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   scheduler_params ............. None
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   seq_parallel_communication_data_type  torch.float32
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   sparse_attention ............. None
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   sparse_gradients_enabled ..... False
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   steps_per_print .............. inf
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   timers_config ................ enabled=True synchronized=True
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   train_batch_size ............. 64
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   train_micro_batch_size_per_gpu  1
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   use_data_before_expert_parallel_  False
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   use_node_local_storage ....... False
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   wall_clock_breakdown ......... False
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   weight_quantization_config ... None
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   world_size ................... 4
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   zero_allow_untested_optimizer  True
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=16777216 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=15099494 param_persistence_threshold=40960 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   zero_enabled ................. True
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   zero_force_ds_cpu_optimizer .. True
[2025-12-16 00:59:50,362] [INFO] [config.py:1007:print]   zero_optimization_stage ...... 3
[2025-12-16 00:59:50,363] [INFO] [config.py:993:print_user_config]   json = {
    "train_batch_size": 64, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 16, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "offload_param": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "overlap_comm": false, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 1.677722e+07, 
        "stage3_prefetch_bucket_size": 1.509949e+07, 
        "stage3_param_persistence_threshold": 4.096000e+04, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "steps_per_print": inf
}
[INFO|trainer.py:2519] 2025-12-16 00:59:50,364 >> ***** Running training *****
[INFO|trainer.py:2520] 2025-12-16 00:59:50,364 >>   Num examples = 29,742
[INFO|trainer.py:2521] 2025-12-16 00:59:50,364 >>   Num Epochs = 1
[INFO|trainer.py:2522] 2025-12-16 00:59:50,364 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2525] 2025-12-16 00:59:50,364 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2526] 2025-12-16 00:59:50,364 >>   Gradient Accumulation steps = 16
[INFO|trainer.py:2527] 2025-12-16 00:59:50,364 >>   Total optimization steps = 465
[INFO|trainer.py:2528] 2025-12-16 00:59:50,368 >>   Number of trainable parameters = 22,253,568
[INFO|integration_utils.py:867] 2025-12-16 00:59:50,372 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
2025/12/16 00:59:51 ERROR main: failed to get logger path error="error creating log directory: mkdir /home/indrisch/.cache/wandb: permission denied"
2025/12/16 00:59:51 INFO server: accepting connections addr=/tmp/wandb-152776-153431-398346821/socket
2025/12/16 00:59:51 INFO server: will exit if parent process dies ppid=152776
2025/12/16 00:59:51 INFO connection: ManageConnectionData: new connection created id=1(@)
2025/12/16 00:59:51 INFO handleInformInit: received streamId=66gov2lo id=1(@)
2025/12/16 00:59:51 INFO handleInformInit: stream started streamId=66gov2lo id=1(@)
wandb: Tracking run with wandb version 0.23.1
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /scratch/indrisch/LLaMA-Factory/wandb/wandb/offline-run-20251216_005950-66gov2lo
  0%|          | 0/465 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:610: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /dev/shm/slurm.indrisch.148825/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  total_tokens = int(torch.prod(grid_thw, dim=1).sum().item())
/opt/conda/lib/python3.11/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:610: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /dev/shm/slurm.indrisch.148825/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  total_tokens = int(torch.prod(grid_thw, dim=1).sum().item())
/opt/conda/lib/python3.11/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:610: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /dev/shm/slurm.indrisch.148825/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  total_tokens = int(torch.prod(grid_thw, dim=1).sum().item())
/opt/conda/lib/python3.11/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:610: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /dev/shm/slurm.indrisch.148825/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  total_tokens = int(torch.prod(grid_thw, dim=1).sum().item())
  0%|          | 1/465 [02:25<18:43:29, 145.28s/it]  0%|          | 2/465 [04:22<16:35:18, 128.98s/it]  1%|          | 3/465 [06:50<17:37:14, 137.30s/it]  1%|          | 4/465 [09:04<17:26:38, 136.22s/it]  1%|          | 5/465 [11:11<16:59:02, 132.92s/it]  1%|▏         | 6/465 [13:09<16:17:32, 127.78s/it]  2%|▏         | 7/465 [15:05<15:45:14, 123.83s/it]  2%|▏         | 8/465 [17:09<15:45:21, 124.12s/it]  2%|▏         | 9/465 [19:28<16:17:20, 128.60s/it]  2%|▏         | 10/465 [21:31<16:01:37, 126.81s/it]                                                    {'loss': 1.9374, 'grad_norm': 1.5576359033584595, 'learning_rate': 1.9148936170212766e-05, 'epoch': 0.02}
  2%|▏         | 10/465 [21:31<16:01:37, 126.81s/it]  2%|▏         | 11/465 [23:34<15:50:35, 125.63s/it]  3%|▎         | 12/465 [25:35<15:37:51, 124.22s/it]  3%|▎         | 13/465 [27:51<16:02:34, 127.77s/it]  3%|▎         | 14/465 [29:50<15:41:36, 125.27s/it]  3%|▎         | 15/465 [32:05<16:01:33, 128.21s/it]  3%|▎         | 16/465 [33:56<15:19:54, 122.93s/it]  4%|▎         | 17/465 [35:59<15:18:19, 122.99s/it]  4%|▍         | 18/465 [38:01<15:15:24, 122.87s/it]  4%|▍         | 19/465 [40:23<15:55:49, 128.59s/it]  4%|▍         | 20/465 [42:34<15:57:50, 129.15s/it]                                                    {'loss': 1.7785, 'grad_norm': 1.0592563152313232, 'learning_rate': 4.0425531914893614e-05, 'epoch': 0.04}
  4%|▍         | 20/465 [42:34<15:57:50, 129.15s/it]  5%|▍         | 21/465 [44:41<15:51:38, 128.60s/it]  5%|▍         | 22/465 [46:40<15:27:56, 125.68s/it]  5%|▍         | 23/465 [48:44<15:22:31, 125.23s/it]  5%|▌         | 24/465 [50:48<15:17:44, 124.86s/it]  5%|▌         | 25/465 [53:01<15:33:10, 127.25s/it]  6%|▌         | 26/465 [55:15<15:46:40, 129.39s/it]  6%|▌         | 27/465 [57:07<15:05:17, 124.01s/it]  6%|▌         | 28/465 [59:08<14:57:44, 123.26s/it]  6%|▌         | 29/465 [1:01:19<15:11:22, 125.42s/it]  6%|▋         | 30/465 [1:03:36<15:35:21, 129.02s/it]                                                      {'loss': 1.3425, 'grad_norm': 0.5915508270263672, 'learning_rate': 6.170212765957447e-05, 'epoch': 0.06}
  6%|▋         | 30/465 [1:03:36<15:35:21, 129.02s/it]  7%|▋         | 31/465 [1:05:57<15:57:37, 132.39s/it]  7%|▋         | 32/465 [1:08:07<15:51:29, 131.85s/it]  7%|▋         | 33/465 [1:10:11<15:31:39, 129.40s/it]  7%|▋         | 34/465 [1:12:08<15:02:50, 125.68s/it]  8%|▊         | 35/465 [1:14:11<14:54:33, 124.82s/it]  8%|▊         | 36/465 [1:16:14<14:48:54, 124.32s/it]  8%|▊         | 37/465 [1:18:22<14:54:44, 125.43s/it]  8%|▊         | 38/465 [1:20:29<14:56:24, 125.96s/it]  8%|▊         | 39/465 [1:22:52<15:30:40, 131.08s/it]  9%|▊         | 40/465 [1:25:02<15:26:11, 130.76s/it]                                                      {'loss': 1.0573, 'grad_norm': 0.3004857897758484, 'learning_rate': 8.297872340425533e-05, 'epoch': 0.09}
  9%|▊         | 40/465 [1:25:02<15:26:11, 130.76s/it]  9%|▉         | 41/465 [1:27:02<15:00:31, 127.43s/it]  9%|▉         | 42/465 [1:29:05<14:50:36, 126.33s/it]  9%|▉         | 43/465 [1:31:15<14:54:45, 127.22s/it]  9%|▉         | 44/465 [1:33:33<15:15:13, 130.44s/it] 10%|▉         | 45/465 [1:35:46<15:19:01, 131.29s/it] 10%|▉         | 46/465 [1:37:50<15:01:32, 129.10s/it] 10%|█         | 47/465 [1:40:04<15:10:28, 130.69s/it] 10%|█         | 48/465 [1:42:19<15:17:06, 131.96s/it] 11%|█         | 49/465 [1:44:17<14:45:57, 127.78s/it] 11%|█         | 50/465 [1:46:32<14:57:44, 129.79s/it]                                                      {'loss': 0.9106, 'grad_norm': 0.26059916615486145, 'learning_rate': 9.999435142363484e-05, 'epoch': 0.11}
 11%|█         | 50/465 [1:46:32<14:57:44, 129.79s/it] 11%|█         | 51/465 [1:48:54<15:20:25, 133.39s/it] 11%|█         | 52/465 [1:51:06<15:16:24, 133.13s/it] 11%|█▏        | 53/465 [1:53:11<14:56:28, 130.55s/it] 12%|█▏        | 54/465 [1:55:19<14:49:14, 129.82s/it] 12%|█▏        | 55/465 [1:57:13<14:15:36, 125.21s/it] 12%|█▏        | 56/465 [1:59:26<14:28:21, 127.39s/it] 12%|█▏        | 57/465 [2:01:47<14:54:30, 131.55s/it] 12%|█▏        | 58/465 [2:03:59<14:54:04, 131.80s/it] 13%|█▎        | 59/465 [2:06:14<14:58:23, 132.77s/it] 13%|█▎        | 60/465 [2:08:15<14:32:24, 129.25s/it]                                                      {'loss': 0.8281, 'grad_norm': 0.21206134557724, 'learning_rate': 9.979678522550382e-05, 'epoch': 0.13}
 13%|█▎        | 60/465 [2:08:15<14:32:24, 129.25s/it] 13%|█▎        | 61/465 [2:10:20<14:21:49, 127.99s/it] 13%|█▎        | 62/465 [2:12:25<14:12:50, 126.97s/it] 14%|█▎        | 63/465 [2:14:33<14:13:11, 127.34s/it] 14%|█▍        | 64/465 [2:16:33<13:55:25, 125.00s/it] 14%|█▍        | 65/465 [2:18:44<14:05:23, 126.81s/it] 14%|█▍        | 66/465 [2:20:46<13:53:31, 125.34s/it] 14%|█▍        | 67/465 [2:22:56<14:00:56, 126.78s/it] 15%|█▍        | 68/465 [2:25:04<14:02:23, 127.31s/it] 15%|█▍        | 69/465 [2:27:10<13:56:17, 126.71s/it] 15%|█▌        | 70/465 [2:29:06<13:32:38, 123.44s/it]                                                      {'loss': 0.7865, 'grad_norm': 0.18164245784282684, 'learning_rate': 9.931806517013612e-05, 'epoch': 0.15}
 15%|█▌        | 70/465 [2:29:06<13:32:38, 123.44s/it] 15%|█▌        | 71/465 [2:31:20<13:53:09, 126.88s/it] 15%|█▌        | 72/465 [2:33:17<13:31:01, 123.82s/it] 16%|█▌        | 73/465 [2:35:23<13:32:24, 124.35s/it] 16%|█▌        | 74/465 [2:37:22<13:20:33, 122.85s/it] 16%|█▌        | 75/465 [2:39:16<13:01:00, 120.16s/it] 16%|█▋        | 76/465 [2:41:24<13:14:59, 122.62s/it] 17%|█▋        | 77/465 [2:43:38<13:35:24, 126.10s/it] 17%|█▋        | 78/465 [2:45:40<13:25:17, 124.85s/it] 17%|█▋        | 79/465 [2:47:48<13:28:47, 125.72s/it] 17%|█▋        | 80/465 [2:50:11<13:59:59, 130.91s/it]                                                      {'loss': 0.7745, 'grad_norm': 0.21483556926250458, 'learning_rate': 9.856089412257606e-05, 'epoch': 0.17}
 17%|█▋        | 80/465 [2:50:11<13:59:59, 130.91s/it] 17%|█▋        | 81/465 [2:52:15<13:44:04, 128.76s/it] 18%|█▊        | 82/465 [2:54:31<13:55:27, 130.88s/it] 18%|█▊        | 83/465 [2:56:50<14:09:52, 133.49s/it] 18%|█▊        | 84/465 [2:58:51<13:44:08, 129.79s/it] 18%|█▊        | 85/465 [3:00:38<12:58:02, 122.85s/it] 18%|█▊        | 86/465 [3:02:55<13:22:46, 127.09s/it] 19%|█▊        | 87/465 [3:05:08<13:31:50, 128.86s/it] 19%|█▉        | 88/465 [3:07:18<13:32:01, 129.23s/it] 19%|█▉        | 89/465 [3:09:16<13:08:35, 125.84s/it] 19%|█▉        | 90/465 [3:11:10<12:43:39, 122.18s/it]                                                      {'loss': 0.7504, 'grad_norm': 0.19538390636444092, 'learning_rate': 9.752954708892377e-05, 'epoch': 0.19}
 19%|█▉        | 90/465 [3:11:10<12:43:39, 122.18s/it] 20%|█▉        | 91/465 [3:13:18<12:53:42, 124.12s/it] 20%|█▉        | 92/465 [3:15:27<13:00:24, 125.54s/it] 20%|██        | 93/465 [3:17:49<13:28:00, 130.32s/it] 20%|██        | 94/465 [3:19:46<13:01:22, 126.37s/it] 20%|██        | 95/465 [3:21:56<13:06:29, 127.54s/it] 21%|██        | 96/465 [3:24:13<13:21:36, 130.34s/it] 21%|██        | 97/465 [3:26:20<13:13:37, 129.40s/it] 21%|██        | 98/465 [3:28:27<13:07:12, 128.70s/it] 21%|██▏       | 99/465 [3:30:59<13:47:12, 135.61s/it] 22%|██▏       | 100/465 [3:33:02<13:22:32, 131.93s/it]                                                       {'loss': 0.7293, 'grad_norm': 0.21050500869750977, 'learning_rate': 9.622984707954732e-05, 'epoch': 0.22}
 22%|██▏       | 100/465 [3:33:02<13:22:32, 131.93s/it] 22%|██▏       | 101/465 [3:35:36<13:59:59, 138.46s/it] 22%|██▏       | 102/465 [3:37:45<13:39:55, 135.53s/it] 22%|██▏       | 103/465 [3:39:56<13:29:01, 134.09s/it] 22%|██▏       | 104/465 [3:42:01<13:11:05, 131.48s/it] 23%|██▎       | 105/465 [3:44:01<12:48:37, 128.10s/it] 23%|██▎       | 106/465 [3:46:19<13:04:15, 131.07s/it] 23%|██▎       | 107/465 [3:48:21<12:46:16, 128.43s/it] 23%|██▎       | 108/465 [3:50:25<12:34:45, 126.85s/it] 23%|██▎       | 109/465 [3:52:25<12:21:01, 124.89s/it] 24%|██▎       | 110/465 [3:54:25<12:10:10, 123.41s/it]                                                       {'loss': 0.7343, 'grad_norm': 0.24947066605091095, 'learning_rate': 9.466913223222467e-05, 'epoch': 0.24}
 24%|██▎       | 110/465 [3:54:25<12:10:10, 123.41s/it] 24%|██▍       | 111/465 [3:56:45<12:37:30, 128.39s/it] 24%|██▍       | 112/465 [3:58:50<12:30:15, 127.52s/it] 24%|██▍       | 113/465 [4:01:10<12:50:05, 131.27s/it] 25%|██▍       | 114/465 [4:03:15<12:36:07, 129.25s/it] 25%|██▍       | 115/465 [4:05:25<12:35:33, 129.52s/it] 25%|██▍       | 116/465 [4:07:36<12:35:50, 129.94s/it] 25%|██▌       | 117/465 [4:09:47<12:35:21, 130.23s/it] 25%|██▌       | 118/465 [4:12:00<12:38:27, 131.14s/it] 26%|██▌       | 119/465 [4:14:13<12:39:00, 131.62s/it] 26%|██▌       | 120/465 [4:16:11<12:14:08, 127.68s/it]                                                       {'loss': 0.7155, 'grad_norm': 0.22595630586147308, 'learning_rate': 9.285621438083998e-05, 'epoch': 0.26}
 26%|██▌       | 120/465 [4:16:11<12:14:08, 127.68s/it] 26%|██▌       | 121/465 [4:18:38<12:44:42, 133.38s/it] 26%|██▌       | 122/465 [4:21:16<13:23:52, 140.62s/it] 26%|██▋       | 123/465 [4:23:12<12:40:57, 133.50s/it] 27%|██▋       | 124/465 [4:25:24<12:35:42, 132.97s/it] 27%|██▋       | 125/465 [4:27:24<12:11:12, 129.04s/it] 27%|██▋       | 126/465 [4:29:25<11:55:42, 126.68s/it] 27%|██▋       | 127/465 [4:31:32<11:52:56, 126.56s/it] 28%|██▊       | 128/465 [4:33:39<11:53:10, 126.97s/it] 28%|██▊       | 129/465 [4:36:00<12:14:24, 131.14s/it] 28%|██▊       | 130/465 [4:38:25<12:34:10, 135.08s/it]                                                       {'loss': 0.6956, 'grad_norm': 0.24507524073123932, 'learning_rate': 9.080132930355567e-05, 'epoch': 0.28}
 28%|██▊       | 130/465 [4:38:25<12:34:10, 135.08s/it] 28%|██▊       | 131/465 [4:40:25<12:07:23, 130.67s/it] 28%|██▊       | 132/465 [4:42:33<12:00:43, 129.86s/it] 29%|██▊       | 133/465 [4:44:35<11:45:36, 127.52s/it] 29%|██▉       | 134/465 [4:46:39<11:38:14, 126.57s/it] 29%|██▉       | 135/465 [4:48:40<11:26:50, 124.88s/it] 29%|██▉       | 136/465 [4:50:53<11:37:13, 127.15s/it] 29%|██▉       | 137/465 [4:52:54<11:26:04, 125.50s/it] 30%|██▉       | 138/465 [4:55:10<11:40:41, 128.57s/it] 30%|██▉       | 139/465 [4:57:14<11:30:18, 127.05s/it] 30%|███       | 140/465 [4:59:26<11:37:25, 128.75s/it]                                                       {'loss': 0.6871, 'grad_norm': 0.22325362265110016, 'learning_rate': 8.851607893136065e-05, 'epoch': 0.3}
 30%|███       | 140/465 [4:59:26<11:37:25, 128.75s/it] 30%|███       | 141/465 [5:01:29<11:25:29, 126.94s/it] 31%|███       | 142/465 [5:03:40<11:29:28, 128.07s/it] 31%|███       | 143/465 [5:05:37<11:10:36, 124.96s/it] 31%|███       | 144/465 [5:07:38<11:01:26, 123.63s/it] 31%|███       | 145/465 [5:09:48<11:09:19, 125.50s/it] 31%|███▏      | 146/465 [5:11:58<11:15:00, 126.96s/it] 32%|███▏      | 147/465 [5:14:10<11:20:14, 128.35s/it] 32%|███▏      | 148/465 [5:16:10<11:05:01, 125.87s/it] 32%|███▏      | 149/465 [5:18:26<11:19:40, 129.05s/it] 32%|███▏      | 150/465 [5:20:37<11:20:30, 129.62s/it]                                                       {'loss': 0.6906, 'grad_norm': 0.2489396333694458, 'learning_rate': 8.601336584328659e-05, 'epoch': 0.32}
 32%|███▏      | 150/465 [5:20:37<11:20:30, 129.62s/it] 32%|███▏      | 151/465 [5:22:44<11:14:19, 128.85s/it] 33%|███▎      | 152/465 [5:24:41<10:52:27, 125.07s/it] 33%|███▎      | 153/465 [5:26:46<10:50:54, 125.17s/it] 33%|███▎      | 154/465 [5:28:54<10:53:23, 126.06s/it] 33%|███▎      | 155/465 [5:30:53<10:39:43, 123.82s/it] 34%|███▎      | 156/465 [5:33:07<10:53:02, 126.80s/it] 34%|███▍      | 157/465 [5:34:59<10:28:37, 122.46s/it] 34%|███▍      | 158/465 [5:37:13<10:44:01, 125.87s/it] 34%|███▍      | 159/465 [5:39:24<10:50:49, 127.61s/it] 34%|███▍      | 160/465 [5:41:27<10:41:21, 126.17s/it]                                                       {'loss': 0.6919, 'grad_norm': 0.22894810140132904, 'learning_rate': 8.330732041813367e-05, 'epoch': 0.34}
 34%|███▍      | 160/465 [5:41:27<10:41:21, 126.17s/it] 35%|███▍      | 161/465 [5:43:17<10:14:28, 121.28s/it] 35%|███▍      | 162/465 [5:45:11<10:01:24, 119.09s/it] 35%|███▌      | 163/465 [5:47:13<10:03:08, 119.83s/it] 35%|███▌      | 164/465 [5:49:21<10:13:20, 122.26s/it] 35%|███▌      | 165/465 [5:51:31<10:23:58, 124.79s/it] 36%|███▌      | 166/465 [5:53:57<10:52:54, 131.02s/it] 36%|███▌      | 167/465 [5:56:07<10:49:44, 130.82s/it] 36%|███▌      | 168/465 [5:58:01<10:21:58, 125.65s/it] 36%|███▋      | 169/465 [5:59:54<10:01:09, 121.86s/it] 37%|███▋      | 170/465 [6:01:51<9:51:54, 120.39s/it]                                                       {'loss': 0.6776, 'grad_norm': 0.2473636269569397, 'learning_rate': 8.041322105400922e-05, 'epoch': 0.37}
 37%|███▋      | 170/465 [6:01:51<9:51:54, 120.39s/it] 37%|███▋      | 171/465 [6:04:03<10:07:37, 124.00s/it] 37%|███▋      | 172/465 [6:06:16<10:19:12, 126.80s/it] 37%|███▋      | 173/465 [6:08:21<10:14:01, 126.17s/it] 37%|███▋      | 174/465 [6:10:26<10:10:42, 125.92s/it] 38%|███▊      | 175/465 [6:12:29<10:03:18, 124.82s/it] 38%|███▊      | 176/465 [6:14:38<10:07:58, 126.22s/it] 38%|███▊      | 177/465 [6:16:39<9:58:30, 124.69s/it]  38%|███▊      | 178/465 [6:18:34<9:42:11, 121.71s/it] 38%|███▊      | 179/465 [6:20:27<9:27:08, 118.98s/it] 39%|███▊      | 180/465 [6:22:33<9:35:01, 121.06s/it]                                                      {'loss': 0.6785, 'grad_norm': 0.26246196031570435, 'learning_rate': 7.734740790612136e-05, 'epoch': 0.39}
 39%|███▊      | 180/465 [6:22:33<9:35:01, 121.06s/it] 39%|███▉      | 181/465 [6:24:29<9:26:17, 119.64s/it] 39%|███▉      | 182/465 [6:26:37<9:36:26, 122.21s/it] 39%|███▉      | 183/465 [6:28:30<9:21:14, 119.41s/it] 40%|███▉      | 184/465 [6:30:32<9:22:19, 120.07s/it] 40%|███▉      | 185/465 [6:32:33<9:21:31, 120.33s/it] 40%|████      | 186/465 [6:34:36<9:23:32, 121.19s/it] 40%|████      | 187/465 [6:36:49<9:38:16, 124.81s/it] 40%|████      | 188/465 [6:38:48<9:27:40, 122.96s/it] 41%|████      | 189/465 [6:41:08<9:49:23, 128.13s/it] 41%|████      | 190/465 [6:43:15<9:45:43, 127.80s/it]                                                      {'loss': 0.6718, 'grad_norm': 0.28243064880371094, 'learning_rate': 7.412719062986632e-05, 'epoch': 0.41}
 41%|████      | 190/465 [6:43:15<9:45:43, 127.80s/it] 41%|████      | 191/465 [6:45:14<9:31:49, 125.22s/it] 41%|████▏     | 192/465 [6:47:03<9:06:56, 120.21s/it] 42%|████▏     | 193/465 [6:49:13<9:18:36, 123.22s/it] 42%|████▏     | 194/465 [6:51:14<9:14:03, 122.67s/it] 42%|████▏     | 195/465 [6:53:23<9:20:28, 124.55s/it] 42%|████▏     | 196/465 [6:55:28<9:18:55, 124.67s/it] 42%|████▏     | 197/465 [6:57:55<9:47:07, 131.45s/it] 43%|████▎     | 198/465 [7:00:12<9:51:46, 132.98s/it] 43%|████▎     | 199/465 [7:02:35<10:02:42, 135.95s/it] 43%|████▎     | 200/465 [7:04:41<9:47:51, 133.10s/it]                                                       {'loss': 0.6704, 'grad_norm': 0.2485094666481018, 'learning_rate': 7.077075065009433e-05, 'epoch': 0.43}
 43%|████▎     | 200/465 [7:04:41<9:47:51, 133.10s/it]
***** Running Evaluation *****

***** Running Evaluation *****
  Num examples = 3305
  Num examples = 3305
  Batch size = 1
  Batch size = 1
[INFO|trainer.py:4643] 2025-12-16 08:04:33,881 >> 
***** Running Evaluation *****

***** Running Evaluation *****
  Num examples = 3305
[INFO|trainer.py:4645] 2025-12-16 08:04:33,882 >>   Num examples = 3305
  Batch size = 1
[INFO|trainer.py:4648] 2025-12-16 08:04:33,882 >>   Batch size = 1

  0%|          | 0/827 [00:00<?, ?it/s][A
  0%|          | 2/827 [00:04<33:14,  2.42s/it][A
  0%|          | 3/827 [00:08<39:57,  2.91s/it][A
  0%|          | 4/827 [00:12<45:25,  3.31s/it][A
  1%|          | 5/827 [00:17<52:20,  3.82s/it][A
  1%|          | 6/827 [00:21<55:23,  4.05s/it][A
  1%|          | 7/827 [00:26<58:42,  4.30s/it][A
  1%|          | 8/827 [00:32<1:06:41,  4.89s/it][A
  1%|          | 9/827 [00:41<1:23:44,  6.14s/it][A
  1%|          | 10/827 [00:48<1:28:14,  6.48s/it][A
  1%|▏         | 11/827 [00:52<1:17:47,  5.72s/it][A
  1%|▏         | 12/827 [00:56<1:10:45,  5.21s/it][A
  2%|▏         | 13/827 [01:00<1:04:40,  4.77s/it][A
  2%|▏         | 14/827 [01:05<1:03:20,  4.67s/it][A
  2%|▏         | 15/827 [01:09<1:03:27,  4.69s/it][A
  2%|▏         | 16/827 [01:13<1:00:58,  4.51s/it][A
  2%|▏         | 17/827 [01:17<57:37,  4.27s/it]  [A
  2%|▏         | 18/827 [01:21<56:51,  4.22s/it][A
  2%|▏         | 19/827 [01:27<1:01:24,  4.56s/it][A
  2%|▏         | 20/827 [01:32<1:06:14,  4.92s/it][A
  3%|▎         | 21/827 [01:38<1:09:07,  5.15s/it][A
  3%|▎         | 22/827 [01:43<1:09:01,  5.14s/it][A
  3%|▎         | 23/827 [01:47<1:02:28,  4.66s/it][A
  3%|▎         | 24/827 [01:51<1:02:52,  4.70s/it][A
  3%|▎         | 25/827 [01:58<1:11:17,  5.33s/it][A
  3%|▎         | 26/827 [02:06<1:19:12,  5.93s/it][A
  3%|▎         | 27/827 [02:11<1:15:40,  5.68s/it][A
  3%|▎         | 28/827 [02:14<1:07:35,  5.08s/it][A
  4%|▎         | 29/827 [02:20<1:08:27,  5.15s/it][A
  4%|▎         | 30/827 [02:26<1:12:02,  5.42s/it][A
  4%|▎         | 31/827 [02:31<1:10:07,  5.29s/it][A
  4%|▍         | 32/827 [02:36<1:09:52,  5.27s/it][A
  4%|▍         | 33/827 [02:42<1:14:06,  5.60s/it][A
  4%|▍         | 34/827 [02:47<1:09:43,  5.28s/it][A
  4%|▍         | 35/827 [02:51<1:06:39,  5.05s/it][A
  4%|▍         | 36/827 [02:55<1:01:12,  4.64s/it][A
  4%|▍         | 37/827 [03:00<1:04:02,  4.86s/it][A
  5%|▍         | 38/827 [03:07<1:11:50,  5.46s/it][A
  5%|▍         | 39/827 [03:14<1:16:11,  5.80s/it][A
  5%|▍         | 40/827 [03:19<1:15:17,  5.74s/it][A
  5%|▍         | 41/827 [03:24<1:09:41,  5.32s/it][A
  5%|▌         | 42/827 [03:29<1:07:45,  5.18s/it][A
  5%|▌         | 43/827 [03:33<1:03:50,  4.89s/it][A
  5%|▌         | 44/827 [03:37<1:00:59,  4.67s/it][A
  5%|▌         | 45/827 [03:43<1:05:01,  4.99s/it][A
  6%|▌         | 46/827 [03:50<1:12:11,  5.55s/it][A
  6%|▌         | 47/827 [03:55<1:12:03,  5.54s/it][A
  6%|▌         | 48/827 [03:59<1:06:13,  5.10s/it][A
  6%|▌         | 49/827 [04:03<1:01:57,  4.78s/it][A
  6%|▌         | 50/827 [04:07<57:28,  4.44s/it]  [A
  6%|▌         | 51/827 [04:13<1:03:25,  4.90s/it][A
  6%|▋         | 52/827 [04:19<1:09:31,  5.38s/it][A
  6%|▋         | 53/827 [04:23<1:02:27,  4.84s/it][A
  7%|▋         | 54/827 [04:27<58:25,  4.53s/it]  [A
  7%|▋         | 55/827 [04:30<55:06,  4.28s/it][A
  7%|▋         | 56/827 [04:34<53:34,  4.17s/it][A
  7%|▋         | 57/827 [04:39<55:20,  4.31s/it][A
  7%|▋         | 58/827 [04:43<54:50,  4.28s/it][A
  7%|▋         | 59/827 [04:47<53:25,  4.17s/it][A
  7%|▋         | 60/827 [04:52<56:11,  4.40s/it][A
  7%|▋         | 61/827 [04:57<59:45,  4.68s/it][A
  7%|▋         | 62/827 [05:01<55:25,  4.35s/it][A
  8%|▊         | 63/827 [05:05<53:10,  4.18s/it][A
  8%|▊         | 64/827 [05:09<52:03,  4.09s/it][A
  8%|▊         | 65/827 [05:13<53:42,  4.23s/it][A
  8%|▊         | 66/827 [05:19<1:00:06,  4.74s/it][A
  8%|▊         | 67/827 [05:24<1:01:45,  4.88s/it][A
  8%|▊         | 68/827 [05:28<56:11,  4.44s/it]  [A
  8%|▊         | 69/827 [05:32<54:07,  4.28s/it][A
  8%|▊         | 70/827 [05:35<51:57,  4.12s/it][A
  9%|▊         | 71/827 [05:41<56:24,  4.48s/it][A
  9%|▊         | 72/827 [05:46<58:54,  4.68s/it][A
  9%|▉         | 73/827 [05:51<58:54,  4.69s/it][A
  9%|▉         | 74/827 [05:57<1:03:35,  5.07s/it][A
  9%|▉         | 75/827 [06:02<1:04:40,  5.16s/it][A
  9%|▉         | 76/827 [06:06<59:14,  4.73s/it]  [A
  9%|▉         | 77/827 [06:10<58:01,  4.64s/it][A
  9%|▉         | 78/827 [06:14<55:47,  4.47s/it][A
 10%|▉         | 79/827 [06:18<53:13,  4.27s/it][A
 10%|▉         | 80/827 [06:23<55:25,  4.45s/it][A
 10%|▉         | 81/827 [06:29<1:01:12,  4.92s/it][A
 10%|▉         | 82/827 [06:34<1:00:19,  4.86s/it][A
 10%|█         | 83/827 [06:38<59:58,  4.84s/it]  [A
 10%|█         | 84/827 [06:44<1:02:40,  5.06s/it][A
 10%|█         | 85/827 [06:49<1:01:55,  5.01s/it][A
 10%|█         | 86/827 [06:52<56:42,  4.59s/it]  [A
 11%|█         | 87/827 [06:59<1:04:42,  5.25s/it][A
 11%|█         | 88/827 [07:09<1:21:34,  6.62s/it][A
 11%|█         | 89/827 [07:17<1:27:45,  7.13s/it][A
 11%|█         | 90/827 [07:24<1:23:55,  6.83s/it][A
 11%|█         | 91/827 [07:28<1:14:41,  6.09s/it][A
 11%|█         | 92/827 [07:32<1:08:54,  5.63s/it][A
 11%|█         | 93/827 [07:36<1:02:52,  5.14s/it][A
 11%|█▏        | 94/827 [07:40<57:52,  4.74s/it]  [A
 11%|█▏        | 95/827 [07:44<55:02,  4.51s/it][A
 12%|█▏        | 96/827 [07:48<52:28,  4.31s/it][A
 12%|█▏        | 97/827 [07:51<48:12,  3.96s/it][A
 12%|█▏        | 98/827 [07:55<46:04,  3.79s/it][A
 12%|█▏        | 99/827 [07:58<44:39,  3.68s/it][A
 12%|█▏        | 100/827 [08:02<44:09,  3.64s/it][A
 12%|█▏        | 101/827 [08:06<46:13,  3.82s/it][A
 12%|█▏        | 102/827 [08:10<47:16,  3.91s/it][A
 12%|█▏        | 103/827 [08:14<47:27,  3.93s/it][A
 13%|█▎        | 104/827 [08:18<47:28,  3.94s/it][A
 13%|█▎        | 105/827 [08:22<48:56,  4.07s/it][A
 13%|█▎        | 106/827 [08:29<58:34,  4.87s/it][A
 13%|█▎        | 107/827 [08:37<1:08:03,  5.67s/it][A
 13%|█▎        | 108/827 [08:42<1:06:01,  5.51s/it][A
 13%|█▎        | 109/827 [08:47<1:03:46,  5.33s/it][A
 13%|█▎        | 110/827 [08:51<1:00:06,  5.03s/it][A
 13%|█▎        | 111/827 [08:57<1:02:26,  5.23s/it][A
 14%|█▎        | 112/827 [09:03<1:05:27,  5.49s/it][A
 14%|█▎        | 113/827 [09:07<1:01:59,  5.21s/it][A
 14%|█▍        | 114/827 [09:11<57:49,  4.87s/it]  [A
 14%|█▍        | 115/827 [09:16<55:30,  4.68s/it][A
 14%|█▍        | 116/827 [09:20<54:10,  4.57s/it][A
 14%|█▍        | 117/827 [09:25<55:57,  4.73s/it][A
 14%|█▍        | 118/827 [09:32<1:02:44,  5.31s/it][A
 14%|█▍        | 119/827 [09:38<1:07:02,  5.68s/it][A
 15%|█▍        | 120/827 [09:44<1:06:46,  5.67s/it][A
 15%|█▍        | 121/827 [09:48<1:02:12,  5.29s/it][A
 15%|█▍        | 122/827 [09:52<56:59,  4.85s/it]  [A
 15%|█▍        | 123/827 [09:56<52:00,  4.43s/it][A
 15%|█▍        | 124/827 [10:00<51:24,  4.39s/it][A
 15%|█▌        | 125/827 [10:04<51:32,  4.41s/it][A
 15%|█▌        | 126/827 [10:09<51:13,  4.38s/it][A
 15%|█▌        | 127/827 [10:13<52:12,  4.48s/it][A
 15%|█▌        | 128/827 [10:18<52:03,  4.47s/it][A
 16%|█▌        | 129/827 [10:22<51:35,  4.43s/it][A
 16%|█▌        | 130/827 [10:26<51:28,  4.43s/it][A
 16%|█▌        | 131/827 [10:31<51:52,  4.47s/it][A
 16%|█▌        | 132/827 [10:37<55:31,  4.79s/it][A
 16%|█▌        | 133/827 [10:42<56:25,  4.88s/it][A
 16%|█▌        | 134/827 [10:46<54:23,  4.71s/it][A
 16%|█▋        | 135/827 [10:52<58:02,  5.03s/it][A
 16%|█▋        | 136/827 [10:58<1:02:23,  5.42s/it][A
 17%|█▋        | 137/827 [11:03<59:38,  5.19s/it]  [A
 17%|█▋        | 138/827 [11:09<1:03:39,  5.54s/it][A
 17%|█▋        | 139/827 [11:16<1:08:54,  6.01s/it][A
 17%|█▋        | 140/827 [11:21<1:06:05,  5.77s/it][A
 17%|█▋        | 141/827 [11:26<1:01:58,  5.42s/it][A
 17%|█▋        | 142/827 [11:30<58:17,  5.11s/it]  [A
 17%|█▋        | 143/827 [11:34<54:35,  4.79s/it][A
 17%|█▋        | 144/827 [11:38<50:42,  4.45s/it][A
 18%|█▊        | 145/827 [11:42<50:15,  4.42s/it][A
 18%|█▊        | 146/827 [11:47<50:15,  4.43s/it][A
 18%|█▊        | 147/827 [11:51<48:34,  4.29s/it][A
 18%|█▊        | 148/827 [11:55<47:32,  4.20s/it][A
 18%|█▊        | 149/827 [11:59<46:16,  4.10s/it][A
 18%|█▊        | 150/827 [12:02<44:30,  3.94s/it][A
 18%|█▊        | 151/827 [12:06<42:21,  3.76s/it][A
 18%|█▊        | 152/827 [12:10<43:48,  3.89s/it][A
 19%|█▊        | 153/827 [12:14<46:11,  4.11s/it][A
 19%|█▊        | 154/827 [12:19<48:24,  4.32s/it][A
 19%|█▊        | 155/827 [12:25<52:21,  4.67s/it][A
 19%|█▉        | 156/827 [12:30<53:56,  4.82s/it][A
 19%|█▉        | 157/827 [12:34<50:48,  4.55s/it][A
 19%|█▉        | 158/827 [12:38<49:23,  4.43s/it][A
 19%|█▉        | 159/827 [12:43<51:14,  4.60s/it][A
 19%|█▉        | 160/827 [12:48<52:49,  4.75s/it][A
 19%|█▉        | 161/827 [12:51<47:56,  4.32s/it][A
 20%|█▉        | 162/827 [12:56<48:16,  4.36s/it][A
 20%|█▉        | 163/827 [13:01<50:54,  4.60s/it][A
 20%|█▉        | 164/827 [13:05<49:12,  4.45s/it][A
 20%|█▉        | 165/827 [13:09<47:59,  4.35s/it][A
 20%|██        | 166/827 [13:16<55:55,  5.08s/it][A
 20%|██        | 167/827 [13:25<1:09:10,  6.29s/it][A
 20%|██        | 168/827 [13:32<1:09:26,  6.32s/it][A
 20%|██        | 169/827 [13:35<1:01:11,  5.58s/it][A
 21%|██        | 170/827 [13:40<57:59,  5.30s/it]  [A
 21%|██        | 171/827 [13:45<56:29,  5.17s/it][A
 21%|██        | 172/827 [13:49<52:15,  4.79s/it][A
 21%|██        | 173/827 [13:52<48:12,  4.42s/it][A
 21%|██        | 174/827 [13:57<48:12,  4.43s/it][A
 21%|██        | 175/827 [14:01<47:58,  4.42s/it][A
 21%|██▏       | 176/827 [14:05<45:16,  4.17s/it][A
 21%|██▏       | 177/827 [14:08<43:05,  3.98s/it][A
 22%|██▏       | 178/827 [14:13<45:36,  4.22s/it][A
 22%|██▏       | 179/827 [14:19<51:38,  4.78s/it][A
 22%|██▏       | 180/827 [14:24<50:28,  4.68s/it][A
 22%|██▏       | 181/827 [14:29<51:06,  4.75s/it][A
 22%|██▏       | 182/827 [14:34<52:17,  4.86s/it][A
 22%|██▏       | 183/827 [14:38<49:29,  4.61s/it][A
 22%|██▏       | 184/827 [14:43<50:29,  4.71s/it][A
 22%|██▏       | 185/827 [14:48<52:10,  4.88s/it][A
 22%|██▏       | 186/827 [14:52<48:21,  4.53s/it][A
 23%|██▎       | 187/827 [14:56<46:15,  4.34s/it][A
 23%|██▎       | 188/827 [15:01<48:17,  4.53s/it][A
 23%|██▎       | 189/827 [15:06<50:29,  4.75s/it][A
 23%|██▎       | 190/827 [15:10<50:14,  4.73s/it][A
 23%|██▎       | 191/827 [15:15<49:56,  4.71s/it][A
 23%|██▎       | 192/827 [15:20<51:18,  4.85s/it][A
 23%|██▎       | 193/827 [15:26<53:12,  5.04s/it][A
 23%|██▎       | 194/827 [15:30<51:30,  4.88s/it][A
 24%|██▎       | 195/827 [15:34<47:57,  4.55s/it][A
 24%|██▎       | 196/827 [15:38<44:37,  4.24s/it][A
 24%|██▍       | 197/827 [15:41<42:28,  4.05s/it][A
 24%|██▍       | 198/827 [15:45<42:16,  4.03s/it][A
 24%|██▍       | 199/827 [15:50<45:18,  4.33s/it][A
 24%|██▍       | 200/827 [15:56<49:35,  4.75s/it][A
 24%|██▍       | 201/827 [16:02<53:52,  5.16s/it][A
 24%|██▍       | 202/827 [16:08<55:33,  5.33s/it][A
 25%|██▍       | 203/827 [16:15<1:00:32,  5.82s/it][A
 25%|██▍       | 204/827 [16:21<1:00:47,  5.86s/it][A
 25%|██▍       | 205/827 [16:25<54:59,  5.30s/it]  [A
 25%|██▍       | 206/827 [16:29<52:49,  5.10s/it][A
 25%|██▌       | 207/827 [16:34<50:15,  4.86s/it][A
 25%|██▌       | 208/827 [16:38<48:38,  4.72s/it][A
 25%|██▌       | 209/827 [16:42<46:44,  4.54s/it][A
 25%|██▌       | 210/827 [16:46<45:10,  4.39s/it][A
 26%|██▌       | 211/827 [16:52<48:01,  4.68s/it][A
 26%|██▌       | 212/827 [16:58<54:15,  5.29s/it][A
 26%|██▌       | 213/827 [17:04<55:47,  5.45s/it][A
 26%|██▌       | 214/827 [17:08<51:25,  5.03s/it][A
 26%|██▌       | 215/827 [17:12<47:34,  4.66s/it][A
 26%|██▌       | 216/827 [17:17<47:18,  4.65s/it][A
 26%|██▌       | 217/827 [17:22<49:49,  4.90s/it][A
 26%|██▋       | 218/827 [17:27<49:14,  4.85s/it][A
 26%|██▋       | 219/827 [17:31<46:22,  4.58s/it][A
 27%|██▋       | 220/827 [17:36<48:00,  4.75s/it][A
 27%|██▋       | 221/827 [17:42<51:05,  5.06s/it][A
 27%|██▋       | 222/827 [17:46<49:40,  4.93s/it][A
 27%|██▋       | 223/827 [17:51<47:43,  4.74s/it][A
 27%|██▋       | 224/827 [17:55<46:21,  4.61s/it][A
 27%|██▋       | 225/827 [17:59<44:26,  4.43s/it][A
 27%|██▋       | 226/827 [18:02<40:53,  4.08s/it][A
 27%|██▋       | 227/827 [18:07<43:42,  4.37s/it][A
 28%|██▊       | 228/827 [18:13<46:59,  4.71s/it][A
 28%|██▊       | 229/827 [18:17<47:09,  4.73s/it][A
 28%|██▊       | 230/827 [18:23<49:14,  4.95s/it][A
 28%|██▊       | 231/827 [18:27<46:54,  4.72s/it][A
 28%|██▊       | 232/827 [18:31<44:15,  4.46s/it][A
 28%|██▊       | 233/827 [18:36<46:41,  4.72s/it][A
 28%|██▊       | 234/827 [18:44<54:21,  5.50s/it][A
 28%|██▊       | 235/827 [18:50<56:34,  5.73s/it][A
 29%|██▊       | 236/827 [18:54<53:01,  5.38s/it][A
 29%|██▊       | 237/827 [19:01<56:02,  5.70s/it][A
 29%|██▉       | 238/827 [19:07<58:08,  5.92s/it][A
 29%|██▉       | 239/827 [19:13<58:09,  5.93s/it][A
 29%|██▉       | 240/827 [19:21<1:02:58,  6.44s/it][A
 29%|██▉       | 241/827 [19:27<1:01:36,  6.31s/it][A
 29%|██▉       | 242/827 [19:32<58:30,  6.00s/it]  [A
 29%|██▉       | 243/827 [19:38<58:57,  6.06s/it][A
 30%|██▉       | 244/827 [19:44<58:32,  6.02s/it][A
 30%|██▉       | 245/827 [19:49<55:07,  5.68s/it][A
 30%|██▉       | 246/827 [19:54<51:04,  5.27s/it][A
 30%|██▉       | 247/827 [19:59<50:04,  5.18s/it][A
 30%|██▉       | 248/827 [20:05<55:00,  5.70s/it][A
 30%|███       | 249/827 [20:11<53:30,  5.55s/it][A
 30%|███       | 250/827 [20:14<46:10,  4.80s/it][A
 30%|███       | 251/827 [20:18<45:59,  4.79s/it][A
 30%|███       | 252/827 [20:24<47:56,  5.00s/it][A
 31%|███       | 253/827 [20:27<43:29,  4.55s/it][A
 31%|███       | 254/827 [20:33<47:35,  4.98s/it][A
 31%|███       | 255/827 [20:42<56:38,  5.94s/it][A
 31%|███       | 256/827 [20:48<57:27,  6.04s/it][A
 31%|███       | 257/827 [20:53<54:34,  5.75s/it][A
 31%|███       | 258/827 [20:58<52:29,  5.54s/it][A
 31%|███▏      | 259/827 [21:04<54:40,  5.77s/it][A
 31%|███▏      | 260/827 [21:10<55:43,  5.90s/it][A
 32%|███▏      | 261/827 [21:16<53:28,  5.67s/it][A
 32%|███▏      | 262/827 [21:22<55:39,  5.91s/it][A
 32%|███▏      | 263/827 [21:29<58:07,  6.18s/it][A
 32%|███▏      | 264/827 [21:32<50:22,  5.37s/it][A
 32%|███▏      | 265/827 [21:36<45:41,  4.88s/it][A
 32%|███▏      | 266/827 [21:40<42:39,  4.56s/it][A
 32%|███▏      | 267/827 [21:44<40:17,  4.32s/it][A
 32%|███▏      | 268/827 [21:48<40:06,  4.30s/it][A
 33%|███▎      | 269/827 [21:54<43:50,  4.71s/it][A
 33%|███▎      | 270/827 [21:59<45:55,  4.95s/it][A
 33%|███▎      | 271/827 [22:03<44:00,  4.75s/it][A
 33%|███▎      | 272/827 [22:08<43:05,  4.66s/it][A
 33%|███▎      | 273/827 [22:14<47:56,  5.19s/it][A
 33%|███▎      | 274/827 [22:23<57:21,  6.22s/it][A
 33%|███▎      | 275/827 [22:29<57:27,  6.25s/it][A
 33%|███▎      | 276/827 [22:34<53:50,  5.86s/it][A
 33%|███▎      | 277/827 [22:39<51:39,  5.64s/it][A
 34%|███▎      | 278/827 [22:44<48:40,  5.32s/it][A
 34%|███▎      | 279/827 [22:48<44:49,  4.91s/it][A
 34%|███▍      | 280/827 [22:52<42:00,  4.61s/it][A
 34%|███▍      | 281/827 [22:56<41:36,  4.57s/it][A
 34%|███▍      | 282/827 [23:01<42:03,  4.63s/it][A
 34%|███▍      | 283/827 [23:06<42:25,  4.68s/it][A
 34%|███▍      | 284/827 [23:11<43:25,  4.80s/it][A
 34%|███▍      | 285/827 [23:17<46:08,  5.11s/it][A
 35%|███▍      | 286/827 [23:22<47:22,  5.25s/it][A
 35%|███▍      | 287/827 [23:26<43:10,  4.80s/it][A
 35%|███▍      | 288/827 [23:30<41:09,  4.58s/it][A
 35%|███▍      | 289/827 [23:34<40:20,  4.50s/it][A
 35%|███▌      | 290/827 [23:39<39:42,  4.44s/it][A
 35%|███▌      | 291/827 [23:43<38:06,  4.27s/it][A
 35%|███▌      | 292/827 [23:47<37:18,  4.18s/it][A
 35%|███▌      | 293/827 [23:51<37:24,  4.20s/it][A
 36%|███▌      | 294/827 [23:55<38:27,  4.33s/it][A
 36%|███▌      | 295/827 [24:00<40:16,  4.54s/it][A
 36%|███▌      | 296/827 [24:06<41:43,  4.71s/it][A
 36%|███▌      | 297/827 [24:10<41:35,  4.71s/it][A
 36%|███▌      | 298/827 [24:14<39:53,  4.52s/it][A
 36%|███▌      | 299/827 [24:18<38:33,  4.38s/it][A
 36%|███▋      | 300/827 [24:22<36:35,  4.17s/it][A
 36%|███▋      | 301/827 [24:26<35:16,  4.02s/it][A
 37%|███▋      | 302/827 [24:30<35:15,  4.03s/it][A
 37%|███▋      | 303/827 [24:33<33:22,  3.82s/it][A
 37%|███▋      | 304/827 [24:37<33:40,  3.86s/it][A
 37%|███▋      | 305/827 [24:42<37:09,  4.27s/it][A
 37%|███▋      | 306/827 [24:48<39:28,  4.55s/it][A
 37%|███▋      | 307/827 [24:51<37:40,  4.35s/it][A
 37%|███▋      | 308/827 [24:55<36:53,  4.26s/it][A
 37%|███▋      | 309/827 [25:00<36:14,  4.20s/it][A
 37%|███▋      | 310/827 [25:03<34:27,  4.00s/it][A
 38%|███▊      | 311/827 [25:07<33:29,  3.89s/it][A
 38%|███▊      | 312/827 [25:12<36:20,  4.23s/it][A
 38%|███▊      | 313/827 [25:17<38:17,  4.47s/it][A
 38%|███▊      | 314/827 [25:21<37:13,  4.35s/it][A
 38%|███▊      | 315/827 [25:26<38:40,  4.53s/it][A
 38%|███▊      | 316/827 [25:31<41:34,  4.88s/it][A
 38%|███▊      | 317/827 [25:38<46:16,  5.44s/it][A
 38%|███▊      | 318/827 [25:44<47:36,  5.61s/it][A
 39%|███▊      | 319/827 [25:49<44:44,  5.29s/it][A
 39%|███▊      | 320/827 [25:53<42:36,  5.04s/it][A
 39%|███▉      | 321/827 [25:59<43:13,  5.13s/it][A
 39%|███▉      | 322/827 [26:03<42:26,  5.04s/it][A
 39%|███▉      | 323/827 [26:09<43:06,  5.13s/it][A
 39%|███▉      | 324/827 [26:15<44:41,  5.33s/it][A
 39%|███▉      | 325/827 [26:19<41:46,  4.99s/it][A
 39%|███▉      | 326/827 [26:24<41:34,  4.98s/it][A
 40%|███▉      | 327/827 [26:29<41:40,  5.00s/it][A
 40%|███▉      | 328/827 [26:34<42:13,  5.08s/it][A
 40%|███▉      | 329/827 [26:41<45:44,  5.51s/it][A
 40%|███▉      | 330/827 [26:47<48:27,  5.85s/it][A
 40%|████      | 331/827 [26:52<46:57,  5.68s/it][A
 40%|████      | 332/827 [26:57<45:18,  5.49s/it][A
 40%|████      | 333/827 [27:03<45:36,  5.54s/it][A
 40%|████      | 334/827 [27:08<43:15,  5.27s/it][A
 41%|████      | 335/827 [27:12<40:35,  4.95s/it][A
 41%|████      | 336/827 [27:16<38:33,  4.71s/it][A
 41%|████      | 337/827 [27:21<38:05,  4.67s/it][A
 41%|████      | 338/827 [27:25<36:03,  4.43s/it][A
 41%|████      | 339/827 [27:28<34:47,  4.28s/it][A
 41%|████      | 340/827 [27:33<34:26,  4.24s/it][A
 41%|████      | 341/827 [27:38<36:03,  4.45s/it][A
 41%|████▏     | 342/827 [27:42<35:43,  4.42s/it][A
 41%|████▏     | 343/827 [27:47<36:09,  4.48s/it][A
 42%|████▏     | 344/827 [27:52<39:10,  4.87s/it][A
 42%|████▏     | 345/827 [27:58<40:43,  5.07s/it][A
 42%|████▏     | 346/827 [28:02<39:32,  4.93s/it][A
 42%|████▏     | 347/827 [28:07<38:57,  4.87s/it][A
 42%|████▏     | 348/827 [28:11<37:11,  4.66s/it][A
 42%|████▏     | 349/827 [28:17<38:13,  4.80s/it][A
 42%|████▏     | 350/827 [28:22<40:39,  5.11s/it][A
 42%|████▏     | 351/827 [28:27<39:04,  4.93s/it][A
 43%|████▎     | 352/827 [28:32<38:44,  4.89s/it][A
 43%|████▎     | 353/827 [28:36<36:50,  4.66s/it][A
 43%|████▎     | 354/827 [28:40<36:08,  4.58s/it][A
 43%|████▎     | 355/827 [28:46<38:40,  4.92s/it][A
 43%|████▎     | 356/827 [28:52<41:30,  5.29s/it][A
 43%|████▎     | 357/827 [28:57<39:55,  5.10s/it][A
 43%|████▎     | 358/827 [29:00<36:38,  4.69s/it][A
 43%|████▎     | 359/827 [29:05<35:28,  4.55s/it][A
 44%|████▎     | 360/827 [29:10<37:23,  4.80s/it][A
 44%|████▎     | 361/827 [29:15<37:05,  4.78s/it][A
 44%|████▍     | 362/827 [29:18<34:01,  4.39s/it][A
 44%|████▍     | 363/827 [29:23<34:57,  4.52s/it][A
 44%|████▍     | 364/827 [29:28<35:16,  4.57s/it][A
 44%|████▍     | 365/827 [29:32<35:10,  4.57s/it][A
 44%|████▍     | 366/827 [29:38<38:29,  5.01s/it][A
 44%|████▍     | 367/827 [29:44<39:04,  5.10s/it][A
 44%|████▍     | 368/827 [29:48<36:20,  4.75s/it][A
 45%|████▍     | 369/827 [29:52<34:53,  4.57s/it][A
 45%|████▍     | 370/827 [29:56<35:07,  4.61s/it][A
 45%|████▍     | 371/827 [30:01<35:38,  4.69s/it][A
 45%|████▍     | 372/827 [30:06<34:49,  4.59s/it][A
 45%|████▌     | 373/827 [30:11<37:20,  4.94s/it][A
 45%|████▌     | 374/827 [30:19<42:13,  5.59s/it][A
 45%|████▌     | 375/827 [30:27<49:18,  6.55s/it][A
 45%|████▌     | 376/827 [30:36<54:37,  7.27s/it][A
 46%|████▌     | 377/827 [30:41<49:46,  6.64s/it][A
 46%|████▌     | 378/827 [30:48<49:32,  6.62s/it][A
 46%|████▌     | 379/827 [30:57<55:40,  7.46s/it][A
 46%|████▌     | 380/827 [31:04<53:12,  7.14s/it][A
 46%|████▌     | 381/827 [31:09<47:49,  6.43s/it][A
 46%|████▌     | 382/827 [31:14<45:13,  6.10s/it][A
 46%|████▋     | 383/827 [31:19<43:50,  5.92s/it][A
 46%|████▋     | 384/827 [31:24<40:12,  5.45s/it][A
 47%|████▋     | 385/827 [31:28<37:34,  5.10s/it][A
 47%|████▋     | 386/827 [31:32<35:47,  4.87s/it][A
 47%|████▋     | 387/827 [31:37<34:58,  4.77s/it][A
 47%|████▋     | 388/827 [31:42<35:19,  4.83s/it][A
 47%|████▋     | 389/827 [31:47<35:57,  4.93s/it][A
 47%|████▋     | 390/827 [31:51<34:33,  4.74s/it][A
 47%|████▋     | 391/827 [31:55<32:38,  4.49s/it][A
 47%|████▋     | 392/827 [32:00<33:17,  4.59s/it][A
 48%|████▊     | 393/827 [32:06<35:33,  4.92s/it][A
 48%|████▊     | 394/827 [32:11<37:02,  5.13s/it][A
 48%|████▊     | 395/827 [32:18<39:50,  5.53s/it][A
 48%|████▊     | 396/827 [32:23<38:59,  5.43s/it][A
 48%|████▊     | 397/827 [32:27<35:43,  4.98s/it][A
 48%|████▊     | 398/827 [32:31<34:31,  4.83s/it][A
 48%|████▊     | 399/827 [32:36<33:48,  4.74s/it][A
 48%|████▊     | 400/827 [32:41<34:30,  4.85s/it][A
 48%|████▊     | 401/827 [32:46<35:15,  4.97s/it][A
 49%|████▊     | 402/827 [32:52<35:55,  5.07s/it][A
 49%|████▊     | 403/827 [32:57<36:23,  5.15s/it][A
 49%|████▉     | 404/827 [33:01<33:05,  4.69s/it][A
 49%|████▉     | 405/827 [33:05<32:29,  4.62s/it][A
 49%|████▉     | 406/827 [33:10<32:12,  4.59s/it][A
 49%|████▉     | 407/827 [33:13<30:32,  4.36s/it][A
 49%|████▉     | 408/827 [33:18<30:06,  4.31s/it][A
 49%|████▉     | 409/827 [33:22<29:45,  4.27s/it][A
 50%|████▉     | 410/827 [33:25<28:24,  4.09s/it][A
 50%|████▉     | 411/827 [33:30<28:16,  4.08s/it][A
 50%|████▉     | 412/827 [33:34<29:14,  4.23s/it][A
 50%|████▉     | 413/827 [33:40<32:50,  4.76s/it][A
 50%|█████     | 414/827 [33:46<34:14,  4.97s/it][A
 50%|█████     | 415/827 [33:49<31:32,  4.59s/it][A
 50%|█████     | 416/827 [33:54<32:04,  4.68s/it][A
 50%|█████     | 417/827 [34:01<37:23,  5.47s/it][A
 51%|█████     | 418/827 [34:07<37:45,  5.54s/it][A
 51%|█████     | 419/827 [34:12<36:48,  5.41s/it][A
 51%|█████     | 420/827 [34:18<38:07,  5.62s/it][A
 51%|█████     | 421/827 [34:23<36:32,  5.40s/it][A
 51%|█████     | 422/827 [34:27<32:59,  4.89s/it][A
 51%|█████     | 423/827 [34:31<30:45,  4.57s/it][A
 51%|█████▏    | 424/827 [34:35<30:00,  4.47s/it][A
 51%|█████▏    | 425/827 [34:41<32:14,  4.81s/it][A
 52%|█████▏    | 426/827 [34:46<34:02,  5.09s/it][A
 52%|█████▏    | 427/827 [34:52<34:40,  5.20s/it][A
 52%|█████▏    | 428/827 [34:58<37:09,  5.59s/it][A
 52%|█████▏    | 429/827 [35:03<35:25,  5.34s/it][A
 52%|█████▏    | 430/827 [35:08<35:00,  5.29s/it][A
 52%|█████▏    | 431/827 [35:15<37:16,  5.65s/it][A
 52%|█████▏    | 432/827 [35:23<42:10,  6.41s/it][A
 52%|█████▏    | 433/827 [35:32<46:31,  7.08s/it][A
 52%|█████▏    | 434/827 [35:37<43:23,  6.62s/it][A
 53%|█████▎    | 435/827 [35:41<37:54,  5.80s/it][A
 53%|█████▎    | 436/827 [35:46<36:08,  5.55s/it][A
 53%|█████▎    | 437/827 [35:51<34:54,  5.37s/it][A
 53%|█████▎    | 438/827 [35:55<32:23,  5.00s/it][A
 53%|█████▎    | 439/827 [36:00<31:55,  4.94s/it][A
 53%|█████▎    | 440/827 [36:05<32:26,  5.03s/it][A
 53%|█████▎    | 441/827 [36:09<29:48,  4.63s/it][A
 53%|█████▎    | 442/827 [36:13<28:06,  4.38s/it][A
 54%|█████▎    | 443/827 [36:17<27:57,  4.37s/it][A
 54%|█████▎    | 444/827 [36:22<29:38,  4.64s/it][A
 54%|█████▍    | 445/827 [36:28<31:42,  4.98s/it][A
 54%|█████▍    | 446/827 [36:33<31:10,  4.91s/it][A
 54%|█████▍    | 447/827 [36:38<31:13,  4.93s/it][A
 54%|█████▍    | 448/827 [36:44<34:08,  5.41s/it][A
 54%|█████▍    | 449/827 [36:52<38:39,  6.14s/it][A
 54%|█████▍    | 450/827 [36:58<37:44,  6.01s/it][A
 55%|█████▍    | 451/827 [37:02<34:38,  5.53s/it][A
 55%|█████▍    | 452/827 [37:08<35:19,  5.65s/it][A
 55%|█████▍    | 453/827 [37:15<36:41,  5.89s/it][A
 55%|█████▍    | 454/827 [37:19<34:00,  5.47s/it][A
 55%|█████▌    | 455/827 [37:25<33:57,  5.48s/it][A
 55%|█████▌    | 456/827 [37:31<35:26,  5.73s/it][A
 55%|█████▌    | 457/827 [37:36<34:04,  5.53s/it][A
 55%|█████▌    | 458/827 [37:41<32:53,  5.35s/it][A
 56%|█████▌    | 459/827 [37:46<32:37,  5.32s/it][A
 56%|█████▌    | 460/827 [37:53<34:37,  5.66s/it][A
 56%|█████▌    | 461/827 [38:00<38:06,  6.25s/it][A
 56%|█████▌    | 462/827 [38:06<36:47,  6.05s/it][A
 56%|█████▌    | 463/827 [38:10<32:47,  5.41s/it][A
 56%|█████▌    | 464/827 [38:15<32:25,  5.36s/it][A
 56%|█████▌    | 465/827 [38:21<32:40,  5.42s/it][A
 56%|█████▋    | 466/827 [38:25<31:41,  5.27s/it][A
 56%|█████▋    | 467/827 [38:31<31:57,  5.33s/it][A
 57%|█████▋    | 468/827 [38:37<33:05,  5.53s/it][A
 57%|█████▋    | 469/827 [38:42<32:33,  5.46s/it][A
 57%|█████▋    | 470/827 [38:46<30:08,  5.07s/it][A
 57%|█████▋    | 471/827 [38:53<32:03,  5.40s/it][A
 57%|█████▋    | 472/827 [38:59<34:23,  5.81s/it][A
 57%|█████▋    | 473/827 [39:05<34:28,  5.84s/it][A
 57%|█████▋    | 474/827 [39:10<32:51,  5.59s/it][A
 57%|█████▋    | 475/827 [39:14<29:37,  5.05s/it][A
 58%|█████▊    | 476/827 [39:17<26:04,  4.46s/it][A
 58%|█████▊    | 477/827 [39:20<23:58,  4.11s/it][A
 58%|█████▊    | 478/827 [39:25<25:30,  4.39s/it][A
 58%|█████▊    | 479/827 [39:31<27:02,  4.66s/it][A
 58%|█████▊    | 480/827 [39:36<27:34,  4.77s/it][A
 58%|█████▊    | 481/827 [39:41<28:37,  4.96s/it][A
 58%|█████▊    | 482/827 [39:45<26:52,  4.67s/it][A
 58%|█████▊    | 483/827 [39:49<24:52,  4.34s/it][A
 59%|█████▊    | 484/827 [39:53<24:00,  4.20s/it][A
 59%|█████▊    | 485/827 [39:57<23:46,  4.17s/it][A
 59%|█████▉    | 486/827 [40:01<24:01,  4.23s/it][A
 59%|█████▉    | 487/827 [40:06<24:56,  4.40s/it][A
 59%|█████▉    | 488/827 [40:11<26:24,  4.67s/it][A
 59%|█████▉    | 489/827 [40:17<28:30,  5.06s/it][A
 59%|█████▉    | 490/827 [40:22<28:38,  5.10s/it][A
 59%|█████▉    | 491/827 [40:26<26:48,  4.79s/it][A
 59%|█████▉    | 492/827 [40:30<25:36,  4.59s/it][A
 60%|█████▉    | 493/827 [40:34<24:34,  4.41s/it][A
 60%|█████▉    | 494/827 [40:39<25:17,  4.56s/it][A
 60%|█████▉    | 495/827 [40:45<26:28,  4.78s/it][A
 60%|█████▉    | 496/827 [40:49<26:01,  4.72s/it][A
 60%|██████    | 497/827 [40:53<24:55,  4.53s/it][A
 60%|██████    | 498/827 [40:58<24:56,  4.55s/it][A
 60%|██████    | 499/827 [41:03<25:26,  4.65s/it][A
 60%|██████    | 500/827 [41:07<24:38,  4.52s/it][A
 61%|██████    | 501/827 [41:10<22:49,  4.20s/it][A
 61%|██████    | 502/827 [41:16<24:06,  4.45s/it][A
 61%|██████    | 503/827 [41:21<25:45,  4.77s/it][A
 61%|██████    | 504/827 [41:25<24:39,  4.58s/it][A
 61%|██████    | 505/827 [41:29<24:07,  4.50s/it][A
 61%|██████    | 506/827 [41:34<24:25,  4.57s/it][A
 61%|██████▏   | 507/827 [41:41<28:32,  5.35s/it][A
 61%|██████▏   | 508/827 [41:48<30:35,  5.75s/it][A
 62%|██████▏   | 509/827 [41:52<27:46,  5.24s/it][A
 62%|██████▏   | 510/827 [41:56<26:13,  4.96s/it][A
 62%|██████▏   | 511/827 [42:00<24:33,  4.66s/it][A
 62%|██████▏   | 512/827 [42:04<22:59,  4.38s/it][A
 62%|██████▏   | 513/827 [42:09<23:00,  4.40s/it][A
 62%|██████▏   | 514/827 [42:13<22:46,  4.37s/it][A
 62%|██████▏   | 515/827 [42:17<22:07,  4.25s/it][A
 62%|██████▏   | 516/827 [42:22<22:42,  4.38s/it][A
 63%|██████▎   | 517/827 [42:26<22:05,  4.28s/it][A
 63%|██████▎   | 518/827 [42:30<21:36,  4.20s/it][A
 63%|██████▎   | 519/827 [42:33<20:45,  4.05s/it][A
 63%|██████▎   | 520/827 [42:37<20:53,  4.08s/it][A
 63%|██████▎   | 521/827 [42:43<23:50,  4.68s/it][A
 63%|██████▎   | 522/827 [42:49<25:12,  4.96s/it][A
 63%|██████▎   | 523/827 [42:54<24:19,  4.80s/it][A
 63%|██████▎   | 524/827 [42:58<24:15,  4.80s/it][A
 63%|██████▎   | 525/827 [43:05<26:30,  5.27s/it][A
 64%|██████▎   | 526/827 [43:11<28:14,  5.63s/it][A
 64%|██████▎   | 527/827 [43:16<26:46,  5.35s/it][A
 64%|██████▍   | 528/827 [43:21<26:36,  5.34s/it][A
 64%|██████▍   | 529/827 [43:27<26:44,  5.38s/it][A
 64%|██████▍   | 530/827 [43:32<26:11,  5.29s/it][A
 64%|██████▍   | 531/827 [43:36<24:25,  4.95s/it][A
 64%|██████▍   | 532/827 [43:40<23:31,  4.79s/it][A
 64%|██████▍   | 533/827 [43:46<24:39,  5.03s/it][A
 65%|██████▍   | 534/827 [43:53<27:31,  5.64s/it][A
 65%|██████▍   | 535/827 [43:59<28:11,  5.79s/it][A
 65%|██████▍   | 536/827 [44:04<26:05,  5.38s/it][A
 65%|██████▍   | 537/827 [44:07<23:39,  4.90s/it][A
 65%|██████▌   | 538/827 [44:11<22:02,  4.58s/it][A
 65%|██████▌   | 539/827 [44:15<21:29,  4.48s/it][A
 65%|██████▌   | 540/827 [44:20<21:35,  4.51s/it][A
 65%|██████▌   | 541/827 [44:25<21:46,  4.57s/it][A
 66%|██████▌   | 542/827 [44:30<22:19,  4.70s/it][A
 66%|██████▌   | 543/827 [44:35<22:45,  4.81s/it][A
 66%|██████▌   | 544/827 [44:40<23:27,  4.97s/it][A
 66%|██████▌   | 545/827 [44:46<24:03,  5.12s/it][A
 66%|██████▌   | 546/827 [44:51<24:26,  5.22s/it][A
 66%|██████▌   | 547/827 [44:56<24:04,  5.16s/it][A
 66%|██████▋   | 548/827 [45:00<21:39,  4.66s/it][A
 66%|██████▋   | 549/827 [45:04<20:39,  4.46s/it][A
 67%|██████▋   | 550/827 [45:09<21:34,  4.67s/it][A
 67%|██████▋   | 551/827 [45:14<21:55,  4.77s/it][A
 67%|██████▋   | 552/827 [45:19<21:56,  4.79s/it][A
 67%|██████▋   | 553/827 [45:23<21:00,  4.60s/it][A
 67%|██████▋   | 554/827 [45:26<19:26,  4.27s/it][A
 67%|██████▋   | 555/827 [45:30<19:17,  4.26s/it][A
 67%|██████▋   | 556/827 [45:35<19:03,  4.22s/it][A
 67%|██████▋   | 557/827 [45:38<18:11,  4.04s/it][A
 67%|██████▋   | 558/827 [45:43<18:46,  4.19s/it][A
 68%|██████▊   | 559/827 [45:48<20:02,  4.49s/it][A
 68%|██████▊   | 560/827 [45:52<19:52,  4.46s/it][A
 68%|██████▊   | 561/827 [45:57<20:08,  4.54s/it][A
 68%|██████▊   | 562/827 [46:02<20:02,  4.54s/it][A
 68%|██████▊   | 563/827 [46:06<19:35,  4.45s/it][A
 68%|██████▊   | 564/827 [46:10<19:05,  4.36s/it][A
 68%|██████▊   | 565/827 [46:15<20:12,  4.63s/it][A
 68%|██████▊   | 566/827 [46:20<20:41,  4.76s/it][A
 69%|██████▊   | 567/827 [46:24<19:55,  4.60s/it][A
 69%|██████▊   | 568/827 [46:29<19:53,  4.61s/it][A
 69%|██████▉   | 569/827 [46:33<19:29,  4.53s/it][A
 69%|██████▉   | 570/827 [46:39<20:37,  4.81s/it][A
 69%|██████▉   | 571/827 [46:45<22:45,  5.33s/it][A
 69%|██████▉   | 572/827 [46:52<24:31,  5.77s/it][A
 69%|██████▉   | 573/827 [46:58<24:38,  5.82s/it][A
 69%|██████▉   | 574/827 [47:04<23:59,  5.69s/it][A
 70%|██████▉   | 575/827 [47:09<23:54,  5.69s/it][A
 70%|██████▉   | 576/827 [47:14<22:16,  5.32s/it][A
 70%|██████▉   | 577/827 [47:18<21:13,  5.09s/it][A
 70%|██████▉   | 578/827 [47:23<20:23,  4.91s/it][A
 70%|███████   | 579/827 [47:27<19:00,  4.60s/it][A
 70%|███████   | 580/827 [47:33<20:27,  4.97s/it][A
 70%|███████   | 581/827 [47:40<23:08,  5.64s/it][A
 70%|███████   | 582/827 [47:45<22:08,  5.42s/it][A
 70%|███████   | 583/827 [47:49<20:38,  5.08s/it][A
 71%|███████   | 584/827 [47:53<19:25,  4.80s/it][A
 71%|███████   | 585/827 [47:57<18:17,  4.54s/it][A
 71%|███████   | 586/827 [48:02<18:17,  4.55s/it][A
 71%|███████   | 587/827 [48:09<22:07,  5.53s/it][A
 71%|███████   | 588/827 [48:18<25:39,  6.44s/it][A
 71%|███████   | 589/827 [48:23<24:27,  6.16s/it][A
 71%|███████▏  | 590/827 [48:31<25:32,  6.47s/it][A
 71%|███████▏  | 591/827 [48:38<26:12,  6.66s/it][A
 72%|███████▏  | 592/827 [48:42<23:10,  5.92s/it][A
 72%|███████▏  | 593/827 [48:46<20:57,  5.38s/it][A
 72%|███████▏  | 594/827 [48:50<19:20,  4.98s/it][A
 72%|███████▏  | 595/827 [48:55<18:53,  4.89s/it][A
 72%|███████▏  | 596/827 [49:02<21:24,  5.56s/it][A
 72%|███████▏  | 597/827 [49:08<22:23,  5.84s/it][A
 72%|███████▏  | 598/827 [49:12<20:15,  5.31s/it][A
 72%|███████▏  | 599/827 [49:17<18:52,  4.97s/it][A
 73%|███████▎  | 600/827 [49:22<18:45,  4.96s/it][A
 73%|███████▎  | 601/827 [49:27<19:26,  5.16s/it][A
 73%|███████▎  | 602/827 [49:32<18:41,  4.99s/it][A
 73%|███████▎  | 603/827 [49:36<17:41,  4.74s/it][A
 73%|███████▎  | 604/827 [49:40<16:58,  4.57s/it][A
 73%|███████▎  | 605/827 [49:44<16:25,  4.44s/it][A
 73%|███████▎  | 606/827 [49:49<16:49,  4.57s/it][A
 73%|███████▎  | 607/827 [49:54<16:55,  4.61s/it][A
 74%|███████▎  | 608/827 [49:58<16:26,  4.51s/it][A
 74%|███████▎  | 609/827 [50:05<19:00,  5.23s/it][A
 74%|███████▍  | 610/827 [50:13<21:28,  5.94s/it][A
 74%|███████▍  | 611/827 [50:17<20:08,  5.60s/it][A
 74%|███████▍  | 612/827 [50:22<19:26,  5.42s/it][A
 74%|███████▍  | 613/827 [50:28<19:16,  5.40s/it][A
 74%|███████▍  | 614/827 [50:33<18:53,  5.32s/it][A
 74%|███████▍  | 615/827 [50:37<17:54,  5.07s/it][A
 74%|███████▍  | 616/827 [50:41<16:29,  4.69s/it][A
 75%|███████▍  | 617/827 [50:47<17:10,  4.91s/it][A
 75%|███████▍  | 618/827 [50:54<19:13,  5.52s/it][A
 75%|███████▍  | 619/827 [50:58<18:04,  5.21s/it][A
 75%|███████▍  | 620/827 [51:02<17:01,  4.93s/it][A
 75%|███████▌  | 621/827 [51:07<16:17,  4.75s/it][A
 75%|███████▌  | 622/827 [51:11<15:40,  4.59s/it][A
 75%|███████▌  | 623/827 [51:15<15:36,  4.59s/it][A
 75%|███████▌  | 624/827 [51:21<16:04,  4.75s/it][A
 76%|███████▌  | 625/827 [51:26<16:12,  4.81s/it][A
 76%|███████▌  | 626/827 [51:30<16:15,  4.85s/it][A
 76%|███████▌  | 627/827 [51:35<15:39,  4.70s/it][A
 76%|███████▌  | 628/827 [51:38<14:31,  4.38s/it][A
 76%|███████▌  | 629/827 [51:42<13:54,  4.21s/it][A
 76%|███████▌  | 630/827 [51:47<14:19,  4.36s/it][A
 76%|███████▋  | 631/827 [51:53<15:48,  4.84s/it][A
 76%|███████▋  | 632/827 [51:59<16:35,  5.11s/it][A
 77%|███████▋  | 633/827 [52:06<18:23,  5.69s/it][A
 77%|███████▋  | 634/827 [52:13<19:50,  6.17s/it][A
 77%|███████▋  | 635/827 [52:17<17:45,  5.55s/it][A
 77%|███████▋  | 636/827 [52:22<16:45,  5.27s/it][A
 77%|███████▋  | 637/827 [52:28<17:13,  5.44s/it][A
 77%|███████▋  | 638/827 [52:33<17:20,  5.50s/it][A
 77%|███████▋  | 639/827 [52:37<15:35,  4.97s/it][A
 77%|███████▋  | 640/827 [52:41<14:41,  4.71s/it][A
 78%|███████▊  | 641/827 [52:46<14:48,  4.77s/it][A
 78%|███████▊  | 642/827 [52:50<14:13,  4.61s/it][A
 78%|███████▊  | 643/827 [52:55<14:34,  4.75s/it][A
 78%|███████▊  | 644/827 [53:01<15:40,  5.14s/it][A
 78%|███████▊  | 645/827 [53:06<15:11,  5.01s/it][A
 78%|███████▊  | 646/827 [53:10<14:23,  4.77s/it][A
 78%|███████▊  | 647/827 [53:14<13:38,  4.55s/it][A
 78%|███████▊  | 648/827 [53:19<13:29,  4.52s/it][A
 78%|███████▊  | 649/827 [53:23<13:24,  4.52s/it][A
 79%|███████▊  | 650/827 [53:27<13:04,  4.43s/it][A
 79%|███████▊  | 651/827 [53:32<12:49,  4.37s/it][A
 79%|███████▉  | 652/827 [53:36<12:32,  4.30s/it][A
 79%|███████▉  | 653/827 [53:40<12:41,  4.37s/it][A
 79%|███████▉  | 654/827 [53:47<14:23,  4.99s/it][A
 79%|███████▉  | 655/827 [53:53<15:06,  5.27s/it][A
 79%|███████▉  | 656/827 [53:56<13:32,  4.75s/it][A
 79%|███████▉  | 657/827 [54:00<12:44,  4.50s/it][A
 80%|███████▉  | 658/827 [54:06<14:01,  4.98s/it][A
 80%|███████▉  | 659/827 [54:14<16:27,  5.88s/it][A
 80%|███████▉  | 660/827 [54:22<17:30,  6.29s/it][A
 80%|███████▉  | 661/827 [54:27<17:02,  6.16s/it][A
 80%|████████  | 662/827 [54:32<15:34,  5.66s/it][A
 80%|████████  | 663/827 [54:37<15:02,  5.50s/it][A
 80%|████████  | 664/827 [54:43<14:57,  5.51s/it][A
 80%|████████  | 665/827 [54:48<14:33,  5.39s/it][A
 81%|████████  | 666/827 [54:53<14:15,  5.31s/it][A
 81%|████████  | 667/827 [54:57<13:25,  5.04s/it][A
 81%|████████  | 668/827 [55:02<12:48,  4.84s/it][A
 81%|████████  | 669/827 [55:05<11:38,  4.42s/it][A
 81%|████████  | 670/827 [55:09<10:56,  4.18s/it][A
 81%|████████  | 671/827 [55:13<11:11,  4.31s/it][A
 81%|████████▏ | 672/827 [55:17<11:01,  4.27s/it][A
 81%|████████▏ | 673/827 [55:21<10:50,  4.22s/it][A
 81%|████████▏ | 674/827 [55:26<10:48,  4.24s/it][A
 82%|████████▏ | 675/827 [55:30<10:45,  4.25s/it][A
 82%|████████▏ | 676/827 [55:36<11:40,  4.64s/it][A
 82%|████████▏ | 677/827 [55:41<12:21,  4.94s/it][A
 82%|████████▏ | 678/827 [55:47<12:31,  5.04s/it][A
 82%|████████▏ | 679/827 [55:52<12:32,  5.09s/it][A
 82%|████████▏ | 680/827 [55:56<11:35,  4.73s/it][A
 82%|████████▏ | 681/827 [56:02<12:22,  5.09s/it][A
 82%|████████▏ | 682/827 [56:08<13:14,  5.48s/it][A
 83%|████████▎ | 683/827 [56:12<12:09,  5.06s/it][A
 83%|████████▎ | 684/827 [56:16<11:01,  4.62s/it][A
 83%|████████▎ | 685/827 [56:19<10:05,  4.26s/it][A
 83%|████████▎ | 686/827 [56:26<11:35,  4.93s/it][A
 83%|████████▎ | 687/827 [56:33<13:09,  5.64s/it][A
 83%|████████▎ | 688/827 [56:39<13:11,  5.70s/it][A
 83%|████████▎ | 689/827 [56:44<13:12,  5.74s/it][A
 83%|████████▎ | 690/827 [56:49<12:13,  5.35s/it][A
 84%|████████▎ | 691/827 [56:53<11:28,  5.06s/it][A
 84%|████████▎ | 692/827 [56:58<11:03,  4.92s/it][A
 84%|████████▍ | 693/827 [57:02<10:23,  4.65s/it][A
 84%|████████▍ | 694/827 [57:06<10:01,  4.52s/it][A
 84%|████████▍ | 695/827 [57:11<10:27,  4.75s/it][A
 84%|████████▍ | 696/827 [57:17<10:45,  4.93s/it][A
 84%|████████▍ | 697/827 [57:21<10:08,  4.68s/it][A
 84%|████████▍ | 698/827 [57:25<09:40,  4.50s/it][A
 85%|████████▍ | 699/827 [57:30<09:42,  4.55s/it][A
 85%|████████▍ | 700/827 [57:36<11:00,  5.20s/it][A
 85%|████████▍ | 701/827 [57:43<11:54,  5.67s/it][A
 85%|████████▍ | 702/827 [57:48<11:24,  5.48s/it][A
 85%|████████▌ | 703/827 [57:53<10:59,  5.32s/it][A
 85%|████████▌ | 704/827 [57:57<09:58,  4.86s/it][A
 85%|████████▌ | 705/827 [58:00<08:58,  4.42s/it][A
 85%|████████▌ | 706/827 [58:04<08:38,  4.28s/it][A
 85%|████████▌ | 707/827 [58:09<08:39,  4.33s/it][A
 86%|████████▌ | 708/827 [58:14<09:04,  4.57s/it][A
 86%|████████▌ | 709/827 [58:18<08:57,  4.56s/it][A
 86%|████████▌ | 710/827 [58:22<08:39,  4.44s/it][A
 86%|████████▌ | 711/827 [58:27<08:32,  4.42s/it][A
 86%|████████▌ | 712/827 [58:31<08:17,  4.33s/it][A
 86%|████████▌ | 713/827 [58:35<08:14,  4.34s/it][A
 86%|████████▋ | 714/827 [58:41<08:54,  4.73s/it][A
 86%|████████▋ | 715/827 [58:46<09:12,  4.93s/it][A
 87%|████████▋ | 716/827 [58:51<09:10,  4.96s/it][A
 87%|████████▋ | 717/827 [58:58<10:08,  5.53s/it][A
 87%|████████▋ | 718/827 [59:04<09:59,  5.50s/it][A
 87%|████████▋ | 719/827 [59:08<09:08,  5.08s/it][A
 87%|████████▋ | 720/827 [59:12<08:44,  4.90s/it][A
 87%|████████▋ | 721/827 [59:17<08:20,  4.72s/it][A
 87%|████████▋ | 722/827 [59:21<08:00,  4.57s/it][A
 87%|████████▋ | 723/827 [59:25<07:40,  4.42s/it][A
 88%|████████▊ | 724/827 [59:28<07:04,  4.12s/it][A
 88%|████████▊ | 725/827 [59:34<07:42,  4.54s/it][A
 88%|████████▊ | 726/827 [59:41<09:00,  5.35s/it][A
 88%|████████▊ | 727/827 [59:46<08:49,  5.29s/it][A
 88%|████████▊ | 728/827 [59:50<07:58,  4.83s/it][A
 88%|████████▊ | 729/827 [59:54<07:18,  4.48s/it][A
 88%|████████▊ | 730/827 [59:59<07:38,  4.73s/it][A
 88%|████████▊ | 731/827 [1:00:06<08:26,  5.27s/it][A
 89%|████████▊ | 732/827 [1:00:12<09:00,  5.69s/it][A
 89%|████████▊ | 733/827 [1:00:17<08:35,  5.49s/it][A
 89%|████████▉ | 734/827 [1:00:21<07:36,  4.91s/it][A
 89%|████████▉ | 735/827 [1:00:25<07:20,  4.79s/it][A
 89%|████████▉ | 736/827 [1:00:31<07:44,  5.10s/it][A
 89%|████████▉ | 737/827 [1:00:36<07:44,  5.17s/it][A
 89%|████████▉ | 738/827 [1:00:40<07:05,  4.78s/it][A
 89%|████████▉ | 739/827 [1:00:46<07:20,  5.00s/it][A
 89%|████████▉ | 740/827 [1:00:52<07:40,  5.29s/it][A
 90%|████████▉ | 741/827 [1:00:56<07:20,  5.12s/it][A
 90%|████████▉ | 742/827 [1:01:03<07:38,  5.39s/it][A
 90%|████████▉ | 743/827 [1:01:09<07:58,  5.69s/it][A
 90%|████████▉ | 744/827 [1:01:14<07:49,  5.66s/it][A
 90%|█████████ | 745/827 [1:01:20<07:28,  5.47s/it][A
 90%|█████████ | 746/827 [1:01:26<07:36,  5.63s/it][A
 90%|█████████ | 747/827 [1:01:31<07:33,  5.67s/it][A
 90%|█████████ | 748/827 [1:01:36<06:58,  5.30s/it][A
 91%|█████████ | 749/827 [1:01:42<07:07,  5.49s/it][A
 91%|█████████ | 750/827 [1:01:47<07:08,  5.56s/it][A
 91%|█████████ | 751/827 [1:01:52<06:30,  5.13s/it][A
 91%|█████████ | 752/827 [1:01:55<05:56,  4.75s/it][A
 91%|█████████ | 753/827 [1:01:59<05:26,  4.41s/it][A
 91%|█████████ | 754/827 [1:02:03<05:05,  4.18s/it][A
 91%|█████████▏| 755/827 [1:02:08<05:32,  4.61s/it][A
 91%|█████████▏| 756/827 [1:02:14<05:44,  4.86s/it][A
 92%|█████████▏| 757/827 [1:02:18<05:36,  4.81s/it][A
 92%|█████████▏| 758/827 [1:02:24<05:41,  4.95s/it][A
 92%|█████████▏| 759/827 [1:02:27<05:10,  4.57s/it][A
 92%|█████████▏| 760/827 [1:02:34<05:39,  5.07s/it][A
 92%|█████████▏| 761/827 [1:02:41<06:14,  5.68s/it][A
 92%|█████████▏| 762/827 [1:02:47<06:23,  5.91s/it][A
 92%|█████████▏| 763/827 [1:02:54<06:36,  6.20s/it][A
 92%|█████████▏| 764/827 [1:02:58<05:48,  5.54s/it][A
 93%|█████████▎| 765/827 [1:03:03<05:24,  5.23s/it][A
 93%|█████████▎| 766/827 [1:03:07<05:05,  5.00s/it][A
 93%|█████████▎| 767/827 [1:03:11<04:48,  4.81s/it][A
 93%|█████████▎| 768/827 [1:03:16<04:48,  4.89s/it][A
 93%|█████████▎| 769/827 [1:03:22<05:01,  5.20s/it][A
 93%|█████████▎| 770/827 [1:03:27<04:51,  5.12s/it][A
 93%|█████████▎| 771/827 [1:03:32<04:33,  4.88s/it][A
 93%|█████████▎| 772/827 [1:03:36<04:18,  4.70s/it][A
 93%|█████████▎| 773/827 [1:03:40<04:11,  4.65s/it][A
 94%|█████████▎| 774/827 [1:03:45<04:05,  4.64s/it][A
 94%|█████████▎| 775/827 [1:03:49<03:57,  4.58s/it][A
 94%|█████████▍| 776/827 [1:03:54<04:00,  4.72s/it][A
 94%|█████████▍| 777/827 [1:03:59<03:53,  4.68s/it][A
 94%|█████████▍| 778/827 [1:04:03<03:43,  4.56s/it][A
 94%|█████████▍| 779/827 [1:04:07<03:28,  4.34s/it][A
 94%|█████████▍| 780/827 [1:04:11<03:20,  4.26s/it][A
 94%|█████████▍| 781/827 [1:04:15<03:14,  4.22s/it][A
 95%|█████████▍| 782/827 [1:04:20<03:14,  4.32s/it][A
 95%|█████████▍| 783/827 [1:04:24<03:05,  4.22s/it][A
 95%|█████████▍| 784/827 [1:04:28<02:56,  4.11s/it][A
 95%|█████████▍| 785/827 [1:04:32<02:54,  4.16s/it][A
 95%|█████████▌| 786/827 [1:04:38<03:12,  4.70s/it][A
 95%|█████████▌| 787/827 [1:04:44<03:26,  5.17s/it][A
 95%|█████████▌| 788/827 [1:04:49<03:12,  4.94s/it][A
 95%|█████████▌| 789/827 [1:04:54<03:07,  4.94s/it][A
 96%|█████████▌| 790/827 [1:04:59<03:02,  4.93s/it][A
 96%|█████████▌| 791/827 [1:05:02<02:46,  4.63s/it][A
 96%|█████████▌| 792/827 [1:05:07<02:36,  4.46s/it][A
 96%|█████████▌| 793/827 [1:05:13<02:50,  5.02s/it][A
 96%|█████████▌| 794/827 [1:05:20<03:08,  5.72s/it][A
 96%|█████████▌| 795/827 [1:05:27<03:09,  5.91s/it][A
 96%|█████████▋| 796/827 [1:05:32<02:59,  5.79s/it][A
 96%|█████████▋| 797/827 [1:05:36<02:38,  5.27s/it][A
 96%|█████████▋| 798/827 [1:05:40<02:19,  4.79s/it][A
 97%|█████████▋| 799/827 [1:05:43<02:01,  4.32s/it][A
 97%|█████████▋| 800/827 [1:05:50<02:18,  5.12s/it][A
 97%|█████████▋| 801/827 [1:05:59<02:44,  6.32s/it][A
 97%|█████████▋| 802/827 [1:06:04<02:30,  6.01s/it][A
 97%|█████████▋| 803/827 [1:06:10<02:22,  5.95s/it][A
 97%|█████████▋| 804/827 [1:06:16<02:16,  5.93s/it][A
 97%|█████████▋| 805/827 [1:06:20<01:56,  5.30s/it][A
 97%|█████████▋| 806/827 [1:06:24<01:41,  4.81s/it][A
 98%|█████████▊| 807/827 [1:06:28<01:32,  4.63s/it][A
 98%|█████████▊| 808/827 [1:06:32<01:25,  4.53s/it][A
 98%|█████████▊| 809/827 [1:06:36<01:19,  4.40s/it][A
 98%|█████████▊| 810/827 [1:06:41<01:14,  4.37s/it][A
 98%|█████████▊| 811/827 [1:06:45<01:08,  4.31s/it][A
 98%|█████████▊| 812/827 [1:06:50<01:08,  4.55s/it][A
 98%|█████████▊| 813/827 [1:06:57<01:15,  5.41s/it][A
 98%|█████████▊| 814/827 [1:07:03<01:12,  5.57s/it][A
 99%|█████████▊| 815/827 [1:07:08<01:04,  5.35s/it][A
 99%|█████████▊| 816/827 [1:07:13<00:58,  5.31s/it][A
 99%|█████████▉| 817/827 [1:07:18<00:50,  5.02s/it][A
 99%|█████████▉| 818/827 [1:07:23<00:47,  5.25s/it][A
 99%|█████████▉| 819/827 [1:07:30<00:46,  5.79s/it][A
 99%|█████████▉| 820/827 [1:07:36<00:40,  5.72s/it][A
 99%|█████████▉| 821/827 [1:07:40<00:31,  5.31s/it][A
 99%|█████████▉| 822/827 [1:07:44<00:24,  4.98s/it][A
100%|█████████▉| 823/827 [1:07:48<00:18,  4.51s/it][A
100%|█████████▉| 824/827 [1:07:52<00:12,  4.26s/it][A
100%|█████████▉| 825/827 [1:07:58<00:09,  4.90s/it][A
100%|█████████▉| 826/827 [1:08:05<00:05,  5.41s/it][A
100%|██████████| 827/827 [1:08:08<00:00,  4.69s/it][A                                                      
                                                   [A{'eval_loss': 0.6698227524757385, 'eval_runtime': 4094.1959, 'eval_samples_per_second': 0.807, 'eval_steps_per_second': 0.202, 'epoch': 0.43}
 43%|████▎     | 200/465 [8:12:55<9:47:51, 133.10s/it]
100%|██████████| 827/827 [1:08:08<00:00,  4.69s/it][A
                                                   [A[INFO|trainer.py:4309] 2025-12-16 09:12:51,245 >> Saving model checkpoint to saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200
/opt/conda/lib/python3.11/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in Qwen/Qwen3-VL-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2421] 2025-12-16 09:12:51,336 >> chat template saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-16 09:12:51,339 >> tokenizer config file saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-16 09:12:51,342 >> Special tokens file saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/special_tokens_map.json
[2025-12-16 09:12:51,590] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!
[2025-12-16 09:12:51,611] [INFO] [logging.py:107:log_dist] [Rank 1] Saving model checkpoint: saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/zero_pp_rank_1_mp_rank_00_model_states.pt
[2025-12-16 09:12:51,611] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/zero_pp_rank_2_mp_rank_00_model_states.pt...
[2025-12-16 09:12:51,612] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/zero_pp_rank_1_mp_rank_00_model_states.pt...
[2025-12-16 09:12:51,612] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/zero_pp_rank_3_mp_rank_00_model_states.pt...
[2025-12-16 09:12:51,613] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-12-16 09:12:51,613] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-12-16 09:12:51,633] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/zero_pp_rank_2_mp_rank_00_model_states.pt.
[2025-12-16 09:12:51,634] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/zero_pp_rank_1_mp_rank_00_model_states.pt.
[2025-12-16 09:12:51,635] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/zero_pp_rank_3_mp_rank_00_model_states.pt.
[2025-12-16 09:12:51,637] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-12-16 09:12:51,639] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-12-16 09:12:51,639] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2025-12-16 09:12:51,639] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2025-12-16 09:12:51,639] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2025-12-16 09:12:51,742] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-12-16 09:12:51,743] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2025-12-16 09:12:51,744] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2025-12-16 09:12:51,745] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-12-16 09:12:51,747] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2025-12-16 09:12:51,747] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2025-12-16 09:12:51,747] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2025-12-16 09:12:51,747] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2025-12-16 09:12:51,776] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2025-12-16 09:12:51,777] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2025-12-16 09:12:51,778] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2025-12-16 09:12:51,779] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[INFO|image_processing_base.py:253] 2025-12-16 09:12:51,793 >> Image processor saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/preprocessor_config.json
[INFO|tokenization_utils_base.py:2421] 2025-12-16 09:12:51,796 >> chat template saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-16 09:12:51,799 >> tokenizer config file saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-16 09:12:51,802 >> Special tokens file saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/special_tokens_map.json
[INFO|video_processing_utils.py:600] 2025-12-16 09:12:51,932 >> Video processor saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/video_preprocessor_config.json
[INFO|processing_utils.py:814] 2025-12-16 09:12:51,935 >> chat template saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-200/chat_template.jinja
 43%|████▎     | 201/465 [8:14:51<99:27:13, 1356.19s/it] 43%|████▎     | 202/465 [8:16:52<71:59:58, 985.55s/it]  44%|████▎     | 203/465 [8:18:53<52:50:44, 726.12s/it] 44%|████▍     | 204/465 [8:21:05<39:43:13, 547.87s/it] 44%|████▍     | 205/465 [8:23:20<30:37:53, 424.13s/it] 44%|████▍     | 206/465 [8:25:26<24:03:50, 334.48s/it] 45%|████▍     | 207/465 [8:27:43<19:44:33, 275.48s/it] 45%|████▍     | 208/465 [8:29:55<16:35:06, 232.32s/it] 45%|████▍     | 209/465 [8:32:03<14:17:56, 201.08s/it] 45%|████▌     | 210/465 [8:34:09<12:39:03, 178.60s/it]                                                       {'loss': 0.6715, 'grad_norm': 0.2669609487056732, 'learning_rate': 6.729703850834381e-05, 'epoch': 0.45}
 45%|████▌     | 210/465 [8:34:09<12:39:03, 178.60s/it] 45%|████▌     | 211/465 [8:36:15<11:29:16, 162.82s/it] 46%|████▌     | 212/465 [8:38:26<10:46:31, 153.33s/it] 46%|████▌     | 213/465 [8:40:23<9:57:37, 142.29s/it]  46%|████▌     | 214/465 [8:42:17<9:19:43, 133.80s/it] 46%|████▌     | 215/465 [8:44:22<9:06:30, 131.16s/it] 46%|████▋     | 216/465 [8:46:40<9:12:59, 133.25s/it] 47%|████▋     | 217/465 [8:48:55<9:12:43, 133.72s/it] 47%|████▋     | 218/465 [8:50:59<8:58:27, 130.80s/it] 47%|████▋     | 219/465 [8:52:56<8:39:30, 126.71s/it] 47%|████▋     | 220/465 [8:55:04<8:38:24, 126.96s/it]                                                      {'loss': 0.6738, 'grad_norm': 0.27870669960975647, 'learning_rate': 6.372566686762426e-05, 'epoch': 0.47}
 47%|████▋     | 220/465 [8:55:04<8:38:24, 126.96s/it] 48%|████▊     | 221/465 [8:57:20<8:47:59, 129.83s/it] 48%|████▊     | 222/465 [8:59:51<9:11:45, 136.24s/it] 48%|████▊     | 223/465 [9:01:51<8:49:33, 131.30s/it] 48%|████▊     | 224/465 [9:03:56<8:39:57, 129.45s/it] 48%|████▊     | 225/465 [9:05:58<8:28:00, 127.00s/it] 49%|████▊     | 226/465 [9:08:01<8:21:59, 126.02s/it] 49%|████▉     | 227/465 [9:10:16<8:29:50, 128.53s/it] 49%|████▉     | 228/465 [9:12:22<8:25:11, 127.90s/it] 49%|████▉     | 229/465 [9:14:24<8:15:26, 125.96s/it] 49%|████▉     | 230/465 [9:16:13<7:53:46, 120.97s/it]                                                      {'loss': 0.6595, 'grad_norm': 0.29564985632896423, 'learning_rate': 6.0076799778845105e-05, 'epoch': 0.49}
 49%|████▉     | 230/465 [9:16:13<7:53:46, 120.97s/it] 50%|████▉     | 231/465 [9:18:02<7:38:03, 117.45s/it] 50%|████▉     | 232/465 [9:20:06<7:43:38, 119.39s/it] 50%|█████     | 233/465 [9:22:24<8:03:07, 124.95s/it] 50%|█████     | 234/465 [9:24:32<8:04:42, 125.90s/it] 51%|█████     | 235/465 [9:26:42<8:06:43, 126.97s/it] 51%|█████     | 236/465 [9:28:48<8:04:17, 126.89s/it] 51%|█████     | 237/465 [9:30:57<8:04:08, 127.40s/it] 51%|█████     | 238/465 [9:33:04<8:01:38, 127.30s/it] 51%|█████▏    | 239/465 [9:35:05<7:52:55, 125.56s/it] 52%|█████▏    | 240/465 [9:37:29<8:10:57, 130.92s/it]                                                      {'loss': 0.6658, 'grad_norm': 0.3091762959957123, 'learning_rate': 5.637103883409525e-05, 'epoch': 0.52}
 52%|█████▏    | 240/465 [9:37:29<8:10:57, 130.92s/it] 52%|█████▏    | 241/465 [9:39:34<8:02:01, 129.11s/it] 52%|█████▏    | 242/465 [9:41:35<7:50:37, 126.62s/it] 52%|█████▏    | 243/465 [9:43:53<8:01:40, 130.18s/it] 52%|█████▏    | 244/465 [9:45:59<7:55:06, 128.99s/it] 53%|█████▎    | 245/465 [9:48:14<7:58:49, 130.59s/it] 53%|█████▎    | 246/465 [9:50:22<7:54:48, 130.09s/it] 53%|█████▎    | 247/465 [9:52:23<7:42:23, 127.26s/it] 53%|█████▎    | 248/465 [9:54:31<7:40:56, 127.45s/it] 54%|█████▎    | 249/465 [9:56:48<7:49:11, 130.33s/it] 54%|█████▍    | 250/465 [9:58:56<7:44:37, 129.66s/it]                                                      {'loss': 0.6651, 'grad_norm': 0.2419588267803192, 'learning_rate': 5.2629306849554386e-05, 'epoch': 0.54}
 54%|█████▍    | 250/465 [9:58:56<7:44:37, 129.66s/it] 54%|█████▍    | 251/465 [10:01:01<7:37:39, 128.31s/it] 54%|█████▍    | 252/465 [10:03:02<7:27:08, 125.96s/it] 54%|█████▍    | 253/465 [10:04:55<7:11:42, 122.18s/it] 55%|█████▍    | 254/465 [10:07:02<7:14:58, 123.69s/it] 55%|█████▍    | 255/465 [10:09:00<7:06:26, 121.84s/it] 55%|█████▌    | 256/465 [10:10:54<6:56:49, 119.67s/it] 55%|█████▌    | 257/465 [10:13:12<7:13:31, 125.06s/it] 55%|█████▌    | 258/465 [10:15:39<7:33:56, 131.58s/it] 56%|█████▌    | 259/465 [10:17:45<7:25:50, 129.86s/it] 56%|█████▌    | 260/465 [10:20:02<7:30:45, 131.93s/it]                                                       {'loss': 0.6631, 'grad_norm': 0.28728488087654114, 'learning_rate': 4.88727297347654e-05, 'epoch': 0.56}
 56%|█████▌    | 260/465 [10:20:02<7:30:45, 131.93s/it] 56%|█████▌    | 261/465 [10:22:17<7:32:23, 133.06s/it] 56%|█████▋    | 262/465 [10:24:18<7:17:47, 129.40s/it] 57%|█████▋    | 263/465 [10:26:26<7:13:54, 128.88s/it] 57%|█████▋    | 264/465 [10:28:11<6:48:26, 121.92s/it] 57%|█████▋    | 265/465 [10:30:15<6:48:34, 122.57s/it] 57%|█████▋    | 266/465 [10:32:30<6:58:04, 126.05s/it] 57%|█████▋    | 267/465 [10:34:35<6:54:48, 125.70s/it] 58%|█████▊    | 268/465 [10:36:42<6:54:18, 126.18s/it] 58%|█████▊    | 269/465 [10:38:50<6:54:19, 126.84s/it] 58%|█████▊    | 270/465 [10:40:48<6:42:59, 124.00s/it]                                                       {'loss': 0.658, 'grad_norm': 0.2929334342479706, 'learning_rate': 4.5122517215236595e-05, 'epoch': 0.58}
 58%|█████▊    | 270/465 [10:40:48<6:42:59, 124.00s/it] 58%|█████▊    | 271/465 [10:43:01<6:50:13, 126.87s/it] 58%|█████▊    | 272/465 [10:45:22<7:01:57, 131.18s/it] 59%|█████▊    | 273/465 [10:47:21<6:47:24, 127.31s/it] 59%|█████▉    | 274/465 [10:49:33<6:49:57, 128.79s/it] 59%|█████▉    | 275/465 [10:51:26<6:33:08, 124.15s/it] 59%|█████▉    | 276/465 [10:53:17<6:18:02, 120.01s/it] 60%|█████▉    | 277/465 [10:55:17<6:16:22, 120.12s/it] 60%|█████▉    | 278/465 [10:57:35<6:31:24, 125.59s/it] 60%|██████    | 279/465 [10:59:39<6:27:10, 124.89s/it] 60%|██████    | 280/465 [11:01:33<6:15:27, 121.77s/it]                                                       {'loss': 0.6601, 'grad_norm': 0.3152064085006714, 'learning_rate': 4.139984308181709e-05, 'epoch': 0.6}
 60%|██████    | 280/465 [11:01:33<6:15:27, 121.77s/it] 60%|██████    | 281/465 [11:03:49<6:26:12, 125.94s/it] 61%|██████    | 282/465 [11:06:06<6:34:50, 129.46s/it] 61%|██████    | 283/465 [11:08:14<6:31:06, 128.94s/it] 61%|██████    | 284/465 [11:10:32<6:37:12, 131.67s/it] 61%|██████▏   | 285/465 [11:12:49<6:39:38, 133.21s/it] 62%|██████▏   | 286/465 [11:14:45<6:21:42, 127.95s/it] 62%|██████▏   | 287/465 [11:16:51<6:18:10, 127.47s/it] 62%|██████▏   | 288/465 [11:19:02<6:19:19, 128.58s/it] 62%|██████▏   | 289/465 [11:21:00<6:07:31, 125.30s/it] 62%|██████▏   | 290/465 [11:23:10<6:09:54, 126.82s/it]                                                       {'loss': 0.6536, 'grad_norm': 0.2782992124557495, 'learning_rate': 3.772572564296005e-05, 'epoch': 0.62}
 62%|██████▏   | 290/465 [11:23:10<6:09:54, 126.82s/it] 63%|██████▎   | 291/465 [11:25:02<5:54:39, 122.30s/it] 63%|██████▎   | 292/465 [11:27:18<6:04:17, 126.34s/it] 63%|██████▎   | 293/465 [11:29:30<6:07:31, 128.21s/it] 63%|██████▎   | 294/465 [11:31:42<6:08:26, 129.28s/it] 63%|██████▎   | 295/465 [11:33:49<6:04:09, 128.52s/it] 64%|██████▎   | 296/465 [11:36:05<6:08:17, 130.75s/it] 64%|██████▍   | 297/465 [11:38:21<6:10:42, 132.40s/it] 64%|██████▍   | 298/465 [11:40:18<5:55:16, 127.64s/it] 64%|██████▍   | 299/465 [11:42:23<5:51:05, 126.90s/it] 65%|██████▍   | 300/465 [11:44:40<5:57:24, 129.97s/it]                                                       {'loss': 0.6505, 'grad_norm': 0.28511500358581543, 'learning_rate': 3.4120909054843376e-05, 'epoch': 0.65}
 65%|██████▍   | 300/465 [11:44:40<5:57:24, 129.97s/it] 65%|██████▍   | 301/465 [11:46:39<5:46:46, 126.87s/it] 65%|██████▍   | 302/465 [11:48:48<5:46:05, 127.40s/it] 65%|██████▌   | 303/465 [11:51:01<5:48:46, 129.18s/it] 65%|██████▌   | 304/465 [11:53:07<5:43:58, 128.19s/it] 66%|██████▌   | 305/465 [11:55:31<5:54:06, 132.79s/it] 66%|██████▌   | 306/465 [11:57:40<5:48:54, 131.66s/it] 66%|██████▌   | 307/465 [11:59:31<5:30:08, 125.37s/it] 66%|██████▌   | 308/465 [12:01:38<5:29:17, 125.84s/it] 66%|██████▋   | 309/465 [12:03:50<5:32:43, 127.97s/it] 67%|██████▋   | 310/465 [12:06:14<5:42:31, 132.59s/it]                                                       {'loss': 0.6584, 'grad_norm': 0.29692497849464417, 'learning_rate': 3.0605746199360754e-05, 'epoch': 0.67}
 67%|██████▋   | 310/465 [12:06:14<5:42:31, 132.59s/it] 67%|██████▋   | 311/465 [12:08:28<5:41:33, 133.07s/it] 67%|██████▋   | 312/465 [12:10:45<5:41:56, 134.09s/it] 67%|██████▋   | 313/465 [12:13:00<5:40:58, 134.60s/it] 68%|██████▊   | 314/465 [12:15:16<5:39:48, 135.03s/it] 68%|██████▊   | 315/465 [12:17:20<5:29:13, 131.69s/it] 68%|██████▊   | 316/465 [12:19:25<5:21:59, 129.66s/it] 68%|██████▊   | 317/465 [12:21:28<5:14:35, 127.54s/it] 68%|██████▊   | 318/465 [12:23:35<5:12:03, 127.37s/it] 69%|██████▊   | 319/465 [12:25:47<5:13:44, 128.93s/it] 69%|██████▉   | 320/465 [12:27:48<5:05:37, 126.46s/it]                                                       {'loss': 0.6548, 'grad_norm': 0.32776346802711487, 'learning_rate': 2.7200083771256818e-05, 'epoch': 0.69}
 69%|██████▉   | 320/465 [12:27:48<5:05:37, 126.46s/it] 69%|██████▉   | 321/465 [12:29:47<4:58:04, 124.20s/it] 69%|██████▉   | 322/465 [12:31:51<4:56:01, 124.20s/it] 69%|██████▉   | 323/465 [12:33:57<4:55:27, 124.84s/it] 70%|██████▉   | 324/465 [12:35:59<4:50:50, 123.76s/it] 70%|██████▉   | 325/465 [12:38:08<4:52:34, 125.39s/it] 70%|███████   | 326/465 [12:40:14<4:50:49, 125.54s/it] 70%|███████   | 327/465 [12:42:21<4:49:55, 126.05s/it] 71%|███████   | 328/465 [12:44:25<4:46:35, 125.52s/it] 71%|███████   | 329/465 [12:46:38<4:49:33, 127.75s/it] 71%|███████   | 330/465 [12:48:43<4:45:20, 126.82s/it]                                                       {'loss': 0.633, 'grad_norm': 0.30576786398887634, 'learning_rate': 2.3923150223207176e-05, 'epoch': 0.71}
 71%|███████   | 330/465 [12:48:43<4:45:20, 126.82s/it] 71%|███████   | 331/465 [12:50:52<4:44:31, 127.40s/it] 71%|███████▏  | 332/465 [12:53:05<4:46:40, 129.32s/it] 72%|███████▏  | 333/465 [12:55:03<4:37:03, 125.94s/it] 72%|███████▏  | 334/465 [12:57:07<4:33:34, 125.30s/it] 72%|███████▏  | 335/465 [12:59:02<4:24:46, 122.21s/it] 72%|███████▏  | 336/465 [13:00:57<4:18:07, 120.06s/it] 72%|███████▏  | 337/465 [13:03:16<4:27:44, 125.50s/it] 73%|███████▎  | 338/465 [13:05:15<4:22:03, 123.81s/it] 73%|███████▎  | 339/465 [13:07:22<4:21:50, 124.69s/it] 73%|███████▎  | 340/465 [13:09:22<4:16:59, 123.36s/it]                                                       {'loss': 0.6426, 'grad_norm': 0.3151782751083374, 'learning_rate': 2.0793447201508286e-05, 'epoch': 0.73}
 73%|███████▎  | 340/465 [13:09:22<4:16:59, 123.36s/it] 73%|███████▎  | 341/465 [13:11:30<4:17:33, 124.63s/it] 74%|███████▎  | 342/465 [13:13:41<4:19:10, 126.42s/it] 74%|███████▍  | 343/465 [13:15:46<4:16:26, 126.11s/it] 74%|███████▍  | 344/465 [13:18:03<4:20:41, 129.27s/it] 74%|███████▍  | 345/465 [13:20:09<4:16:33, 128.28s/it] 74%|███████▍  | 346/465 [13:22:15<4:13:21, 127.74s/it] 75%|███████▍  | 347/465 [13:24:22<4:10:59, 127.62s/it] 75%|███████▍  | 348/465 [13:26:31<4:09:35, 127.99s/it] 75%|███████▌  | 349/465 [13:28:33<4:03:49, 126.12s/it] 75%|███████▌  | 350/465 [13:30:40<4:02:12, 126.37s/it]                                                       {'loss': 0.6406, 'grad_norm': 0.2900513708591461, 'learning_rate': 1.7828645085333644e-05, 'epoch': 0.75}
 75%|███████▌  | 350/465 [13:30:40<4:02:12, 126.37s/it] 75%|███████▌  | 351/465 [13:32:58<4:06:31, 129.75s/it] 76%|███████▌  | 352/465 [13:34:55<3:57:39, 126.19s/it] 76%|███████▌  | 353/465 [13:37:05<3:57:26, 127.20s/it] 76%|███████▌  | 354/465 [13:39:07<3:52:17, 125.56s/it] 76%|███████▋  | 355/465 [13:41:13<3:50:48, 125.89s/it] 77%|███████▋  | 356/465 [13:43:26<3:52:07, 127.78s/it] 77%|███████▋  | 357/465 [13:45:38<3:52:29, 129.16s/it] 77%|███████▋  | 358/465 [13:47:34<3:43:26, 125.29s/it] 77%|███████▋  | 359/465 [13:49:29<3:35:44, 122.11s/it] 77%|███████▋  | 360/465 [13:51:34<3:35:12, 122.97s/it]                                                       {'loss': 0.6482, 'grad_norm': 0.31933972239494324, 'learning_rate': 1.5045483219344386e-05, 'epoch': 0.77}
 77%|███████▋  | 360/465 [13:51:34<3:35:12, 122.97s/it] 78%|███████▊  | 361/465 [13:53:44<3:36:58, 125.18s/it] 78%|███████▊  | 362/465 [13:55:44<3:32:00, 123.50s/it] 78%|███████▊  | 363/465 [13:57:33<3:22:39, 119.21s/it] 78%|███████▊  | 364/465 [13:59:27<3:18:13, 117.75s/it] 78%|███████▊  | 365/465 [14:01:29<3:18:17, 118.98s/it] 79%|███████▊  | 366/465 [14:03:38<3:21:06, 121.88s/it] 79%|███████▉  | 367/465 [14:05:50<3:23:55, 124.85s/it] 79%|███████▉  | 368/465 [14:07:53<3:21:01, 124.35s/it] 79%|███████▉  | 369/465 [14:09:50<3:15:34, 122.24s/it] 80%|███████▉  | 370/465 [14:11:56<3:15:01, 123.17s/it]                                                       {'loss': 0.6474, 'grad_norm': 0.2979889214038849, 'learning_rate': 1.2459675402943289e-05, 'epoch': 0.8}
 80%|███████▉  | 370/465 [14:11:56<3:15:01, 123.17s/it] 80%|███████▉  | 371/465 [14:14:01<3:13:56, 123.79s/it] 80%|████████  | 372/465 [14:15:58<3:08:39, 121.71s/it] 80%|████████  | 373/465 [14:17:55<3:04:30, 120.33s/it] 80%|████████  | 374/465 [14:20:09<3:08:55, 124.57s/it] 81%|████████  | 375/465 [14:22:25<3:12:06, 128.07s/it] 81%|████████  | 376/465 [14:24:32<3:09:19, 127.63s/it] 81%|████████  | 377/465 [14:26:43<3:08:32, 128.55s/it] 81%|████████▏ | 378/465 [14:28:49<3:05:36, 128.00s/it] 82%|████████▏ | 379/465 [14:30:50<3:00:14, 125.75s/it] 82%|████████▏ | 380/465 [14:32:46<2:53:49, 122.70s/it]                                                       {'loss': 0.6521, 'grad_norm': 0.27724573016166687, 'learning_rate': 1.00858211697822e-05, 'epoch': 0.82}
 82%|████████▏ | 380/465 [14:32:46<2:53:49, 122.70s/it] 82%|████████▏ | 381/465 [14:34:48<2:51:30, 122.51s/it] 82%|████████▏ | 382/465 [14:37:04<2:55:16, 126.70s/it] 82%|████████▏ | 383/465 [14:39:09<2:52:28, 126.20s/it] 83%|████████▎ | 384/465 [14:41:11<2:48:43, 124.98s/it] 83%|████████▎ | 385/465 [14:43:20<2:48:00, 126.00s/it] 83%|████████▎ | 386/465 [14:45:29<2:47:11, 126.98s/it] 83%|████████▎ | 387/465 [14:47:27<2:41:27, 124.20s/it] 83%|████████▎ | 388/465 [14:49:43<2:44:03, 127.84s/it] 84%|████████▎ | 389/465 [14:51:48<2:40:47, 126.94s/it] 84%|████████▍ | 390/465 [14:53:54<2:38:19, 126.66s/it]                                                       {'loss': 0.6337, 'grad_norm': 0.2897404134273529, 'learning_rate': 7.937323358440935e-06, 'epoch': 0.84}
 84%|████████▍ | 390/465 [14:53:54<2:38:19, 126.66s/it] 84%|████████▍ | 391/465 [14:56:15<2:41:28, 130.93s/it] 84%|████████▍ | 392/465 [14:58:22<2:38:03, 129.91s/it] 85%|████████▍ | 393/465 [15:00:32<2:35:42, 129.76s/it] 85%|████████▍ | 394/465 [15:02:41<2:33:24, 129.63s/it] 85%|████████▍ | 395/465 [15:04:52<2:31:45, 130.08s/it] 85%|████████▌ | 396/465 [15:06:59<2:28:25, 129.07s/it] 85%|████████▌ | 397/465 [15:09:13<2:28:07, 130.69s/it] 86%|████████▌ | 398/465 [15:11:15<2:23:05, 128.14s/it] 86%|████████▌ | 399/465 [15:13:28<2:22:18, 129.37s/it] 86%|████████▌ | 400/465 [15:15:31<2:18:09, 127.53s/it]                                                       {'loss': 0.6258, 'grad_norm': 0.3120563328266144, 'learning_rate': 6.026312439675552e-06, 'epoch': 0.86}
 86%|████████▌ | 400/465 [15:15:31<2:18:09, 127.53s/it]
***** Running Evaluation *****

***** Running Evaluation *****

***** Running Evaluation *****
  Num examples = 3305
[INFO|trainer.py:4643] 2025-12-16 16:15:23,556 >> 
***** Running Evaluation *****
  Batch size = 1
  Num examples = 3305
  Batch size = 1
  Num examples = 3305
  Batch size = 1
[INFO|trainer.py:4645] 2025-12-16 16:15:23,557 >>   Num examples = 3305
[INFO|trainer.py:4648] 2025-12-16 16:15:23,557 >>   Batch size = 1

  0%|          | 0/827 [00:00<?, ?it/s][A
  0%|          | 2/827 [00:04<33:13,  2.42s/it][A
  0%|          | 3/827 [00:08<39:44,  2.89s/it][A
  0%|          | 4/827 [00:12<45:19,  3.30s/it][A
  1%|          | 5/827 [00:17<52:20,  3.82s/it][A
  1%|          | 6/827 [00:21<55:28,  4.05s/it][A
  1%|          | 7/827 [00:26<58:44,  4.30s/it][A
  1%|          | 8/827 [00:32<1:06:46,  4.89s/it][A
  1%|          | 9/827 [00:41<1:23:48,  6.15s/it][A
  1%|          | 10/827 [00:48<1:28:33,  6.50s/it][A
  1%|▏         | 11/827 [00:52<1:18:02,  5.74s/it][A
  1%|▏         | 12/827 [00:56<1:10:56,  5.22s/it][A
  2%|▏         | 13/827 [01:00<1:04:41,  4.77s/it][A
  2%|▏         | 14/827 [01:05<1:03:30,  4.69s/it][A
  2%|▏         | 15/827 [01:09<1:03:51,  4.72s/it][A
  2%|▏         | 16/827 [01:14<1:01:18,  4.54s/it][A
  2%|▏         | 17/827 [01:17<57:47,  4.28s/it]  [A
  2%|▏         | 18/827 [01:21<57:03,  4.23s/it][A
  2%|▏         | 19/827 [01:27<1:01:27,  4.56s/it][A
  2%|▏         | 20/827 [01:33<1:06:29,  4.94s/it][A
  3%|▎         | 21/827 [01:38<1:09:41,  5.19s/it][A
  3%|▎         | 22/827 [01:43<1:09:21,  5.17s/it][A
  3%|▎         | 23/827 [01:47<1:02:41,  4.68s/it][A
  3%|▎         | 24/827 [01:52<1:03:12,  4.72s/it][A
  3%|▎         | 25/827 [01:59<1:11:42,  5.37s/it][A
  3%|▎         | 26/827 [02:06<1:19:32,  5.96s/it][A
  3%|▎         | 27/827 [02:11<1:15:50,  5.69s/it][A
  3%|▎         | 28/827 [02:15<1:07:52,  5.10s/it][A
  4%|▎         | 29/827 [02:20<1:08:53,  5.18s/it][A
  4%|▎         | 30/827 [02:26<1:12:18,  5.44s/it][A
  4%|▎         | 31/827 [02:31<1:10:23,  5.31s/it][A
  4%|▍         | 32/827 [02:36<1:10:06,  5.29s/it][A
  4%|▍         | 33/827 [02:43<1:14:23,  5.62s/it][A
  4%|▍         | 34/827 [02:47<1:09:48,  5.28s/it][A
  4%|▍         | 35/827 [02:52<1:06:44,  5.06s/it][A
  4%|▍         | 36/827 [02:56<1:01:09,  4.64s/it][A
  4%|▍         | 37/827 [03:01<1:04:03,  4.86s/it][A
  5%|▍         | 38/827 [03:08<1:11:50,  5.46s/it][A
  5%|▍         | 39/827 [03:14<1:16:21,  5.81s/it][A
  5%|▍         | 40/827 [03:20<1:15:23,  5.75s/it][A
  5%|▍         | 41/827 [03:24<1:09:43,  5.32s/it][A
  5%|▌         | 42/827 [03:29<1:07:45,  5.18s/it][A
  5%|▌         | 43/827 [03:33<1:03:49,  4.88s/it][A
  5%|▌         | 44/827 [03:38<1:00:57,  4.67s/it][A
  5%|▌         | 45/827 [03:43<1:05:20,  5.01s/it][A
  6%|▌         | 46/827 [03:50<1:12:22,  5.56s/it][A
  6%|▌         | 47/827 [03:56<1:12:06,  5.55s/it][A
  6%|▌         | 48/827 [04:00<1:06:13,  5.10s/it][A
  6%|▌         | 49/827 [04:04<1:01:55,  4.78s/it][A
  6%|▌         | 50/827 [04:07<57:35,  4.45s/it]  [A
  6%|▌         | 51/827 [04:13<1:03:32,  4.91s/it][A
  6%|▋         | 52/827 [04:20<1:09:36,  5.39s/it][A
  6%|▋         | 53/827 [04:24<1:02:29,  4.84s/it][A
  7%|▋         | 54/827 [04:27<58:25,  4.54s/it]  [A
  7%|▋         | 55/827 [04:31<55:07,  4.28s/it][A
  7%|▋         | 56/827 [04:35<53:36,  4.17s/it][A
  7%|▋         | 57/827 [04:40<55:22,  4.32s/it][A
  7%|▋         | 58/827 [04:44<54:47,  4.27s/it][A
  7%|▋         | 59/827 [04:48<53:15,  4.16s/it][A
  7%|▋         | 60/827 [04:53<56:03,  4.39s/it][A
  7%|▋         | 61/827 [04:58<59:37,  4.67s/it][A
  7%|▋         | 62/827 [05:02<55:23,  4.34s/it][A
  8%|▊         | 63/827 [05:05<53:08,  4.17s/it][A
  8%|▊         | 64/827 [05:09<52:01,  4.09s/it][A
  8%|▊         | 65/827 [05:14<53:43,  4.23s/it][A
  8%|▊         | 66/827 [05:20<1:00:09,  4.74s/it][A
  8%|▊         | 67/827 [05:25<1:01:43,  4.87s/it][A
  8%|▊         | 68/827 [05:28<56:12,  4.44s/it]  [A
  8%|▊         | 69/827 [05:32<54:12,  4.29s/it][A
  8%|▊         | 70/827 [05:36<52:06,  4.13s/it][A
  9%|▊         | 71/827 [05:41<56:37,  4.49s/it][A
  9%|▊         | 72/827 [05:47<59:00,  4.69s/it][A
  9%|▉         | 73/827 [05:51<58:54,  4.69s/it][A
  9%|▉         | 74/827 [05:57<1:03:40,  5.07s/it][A
  9%|▉         | 75/827 [06:03<1:04:50,  5.17s/it][A
  9%|▉         | 76/827 [06:06<59:20,  4.74s/it]  [A
  9%|▉         | 77/827 [06:11<57:57,  4.64s/it][A
  9%|▉         | 78/827 [06:15<55:46,  4.47s/it][A
 10%|▉         | 79/827 [06:19<53:09,  4.26s/it][A
 10%|▉         | 80/827 [06:23<55:30,  4.46s/it][A
 10%|▉         | 81/827 [06:29<1:01:16,  4.93s/it][A
 10%|▉         | 82/827 [06:34<1:00:22,  4.86s/it][A
 10%|█         | 83/827 [06:39<1:00:07,  4.85s/it][A
 10%|█         | 84/827 [06:45<1:02:54,  5.08s/it][A
 10%|█         | 85/827 [06:50<1:02:23,  5.05s/it][A
 10%|█         | 86/827 [06:53<56:54,  4.61s/it]  [A
 11%|█         | 87/827 [07:00<1:04:55,  5.26s/it][A
 11%|█         | 88/827 [07:10<1:21:58,  6.66s/it][A
 11%|█         | 89/827 [07:18<1:28:08,  7.17s/it][A
 11%|█         | 90/827 [07:24<1:24:08,  6.85s/it][A
 11%|█         | 91/827 [07:29<1:14:50,  6.10s/it][A
 11%|█         | 92/827 [07:33<1:09:01,  5.63s/it][A
 11%|█         | 93/827 [07:37<1:02:55,  5.14s/it][A
 11%|█▏        | 94/827 [07:41<57:48,  4.73s/it]  [A
 11%|█▏        | 95/827 [07:45<55:07,  4.52s/it][A
 12%|█▏        | 96/827 [07:49<52:29,  4.31s/it][A
 12%|█▏        | 97/827 [07:52<48:13,  3.96s/it][A
 12%|█▏        | 98/827 [07:55<46:11,  3.80s/it][A
 12%|█▏        | 99/827 [07:59<44:50,  3.70s/it][A
 12%|█▏        | 100/827 [08:02<44:15,  3.65s/it][A
 12%|█▏        | 101/827 [08:07<46:20,  3.83s/it][A
 12%|█▏        | 102/827 [08:11<47:33,  3.94s/it][A
 12%|█▏        | 103/827 [08:15<47:37,  3.95s/it][A
 13%|█▎        | 104/827 [08:19<47:37,  3.95s/it][A
 13%|█▎        | 105/827 [08:23<49:20,  4.10s/it][A
 13%|█▎        | 106/827 [08:30<58:53,  4.90s/it][A
 13%|█▎        | 107/827 [08:38<1:08:16,  5.69s/it][A
 13%|█▎        | 108/827 [08:43<1:06:06,  5.52s/it][A
 13%|█▎        | 109/827 [08:48<1:03:49,  5.33s/it][A
 13%|█▎        | 110/827 [08:52<1:00:02,  5.02s/it][A
 13%|█▎        | 111/827 [08:58<1:02:27,  5.23s/it][A
 14%|█▎        | 112/827 [09:04<1:05:26,  5.49s/it][A
 14%|█▎        | 113/827 [09:08<1:02:07,  5.22s/it][A
 14%|█▍        | 114/827 [09:12<57:59,  4.88s/it]  [A
 14%|█▍        | 115/827 [09:17<55:39,  4.69s/it][A
 14%|█▍        | 116/827 [09:21<54:20,  4.59s/it][A
 14%|█▍        | 117/827 [09:26<56:09,  4.75s/it][A
 14%|█▍        | 118/827 [09:33<1:02:54,  5.32s/it][A
 14%|█▍        | 119/827 [09:39<1:07:41,  5.74s/it][A
 15%|█▍        | 120/827 [09:45<1:07:15,  5.71s/it][A
 15%|█▍        | 121/827 [09:50<1:02:38,  5.32s/it][A
 15%|█▍        | 122/827 [09:53<57:15,  4.87s/it]  [A
 15%|█▍        | 123/827 [09:57<52:15,  4.45s/it][A
 15%|█▍        | 124/827 [10:01<51:35,  4.40s/it][A
 15%|█▌        | 125/827 [10:06<51:43,  4.42s/it][A
 15%|█▌        | 126/827 [10:10<51:21,  4.40s/it][A
 15%|█▌        | 127/827 [10:15<52:30,  4.50s/it][A
 15%|█▌        | 128/827 [10:19<52:15,  4.49s/it][A
 16%|█▌        | 129/827 [10:23<51:32,  4.43s/it][A
 16%|█▌        | 130/827 [10:28<51:23,  4.42s/it][A
 16%|█▌        | 131/827 [10:32<51:41,  4.46s/it][A
 16%|█▌        | 132/827 [10:38<55:29,  4.79s/it][A
 16%|█▌        | 133/827 [10:43<56:21,  4.87s/it][A
 16%|█▌        | 134/827 [10:47<54:15,  4.70s/it][A
 16%|█▋        | 135/827 [10:53<58:02,  5.03s/it][A
 16%|█▋        | 136/827 [10:59<1:02:28,  5.43s/it][A
 17%|█▋        | 137/827 [11:04<59:51,  5.20s/it]  [A
 17%|█▋        | 138/827 [11:10<1:03:49,  5.56s/it][A
 17%|█▋        | 139/827 [11:18<1:08:59,  6.02s/it][A
 17%|█▋        | 140/827 [11:23<1:06:07,  5.78s/it][A
 17%|█▋        | 141/827 [11:27<1:01:59,  5.42s/it][A
 17%|█▋        | 142/827 [11:32<58:15,  5.10s/it]  [A
 17%|█▋        | 143/827 [11:36<54:30,  4.78s/it][A
 17%|█▋        | 144/827 [11:39<50:36,  4.45s/it][A
 18%|█▊        | 145/827 [11:44<50:13,  4.42s/it][A
 18%|█▊        | 146/827 [11:48<50:18,  4.43s/it][A
 18%|█▊        | 147/827 [11:52<48:31,  4.28s/it][A
 18%|█▊        | 148/827 [11:56<47:30,  4.20s/it][A
 18%|█▊        | 149/827 [12:00<46:19,  4.10s/it][A
 18%|█▊        | 150/827 [12:04<44:31,  3.95s/it][A
 18%|█▊        | 151/827 [12:07<42:22,  3.76s/it][A
 18%|█▊        | 152/827 [12:11<43:55,  3.90s/it][A
 19%|█▊        | 153/827 [12:16<46:26,  4.13s/it][A
 19%|█▊        | 154/827 [12:21<48:28,  4.32s/it][A
 19%|█▊        | 155/827 [12:26<52:22,  4.68s/it][A
 19%|█▉        | 156/827 [12:31<53:51,  4.82s/it][A
 19%|█▉        | 157/827 [12:35<50:43,  4.54s/it][A
 19%|█▉        | 158/827 [12:39<49:23,  4.43s/it][A
 19%|█▉        | 159/827 [12:44<51:15,  4.60s/it][A
 19%|█▉        | 160/827 [12:49<52:49,  4.75s/it][A
 19%|█▉        | 161/827 [12:53<47:48,  4.31s/it][A
 20%|█▉        | 162/827 [12:57<48:11,  4.35s/it][A
 20%|█▉        | 163/827 [13:02<50:49,  4.59s/it][A
 20%|█▉        | 164/827 [13:06<49:11,  4.45s/it][A
 20%|█▉        | 165/827 [13:11<48:03,  4.36s/it][A
 20%|██        | 166/827 [13:17<56:06,  5.09s/it][A
 20%|██        | 167/827 [13:27<1:09:21,  6.31s/it][A
 20%|██        | 168/827 [13:33<1:09:39,  6.34s/it][A
 20%|██        | 169/827 [13:37<1:01:27,  5.60s/it][A
 21%|██        | 170/827 [13:42<58:25,  5.34s/it]  [A
 21%|██        | 171/827 [13:46<56:46,  5.19s/it][A
 21%|██        | 172/827 [13:50<52:25,  4.80s/it][A
 21%|██        | 173/827 [13:54<48:18,  4.43s/it][A
 21%|██        | 174/827 [13:58<48:18,  4.44s/it][A
 21%|██        | 175/827 [14:03<48:05,  4.43s/it][A
 21%|██▏       | 176/827 [14:06<45:20,  4.18s/it][A
 21%|██▏       | 177/827 [14:10<43:05,  3.98s/it][A
 22%|██▏       | 178/827 [14:15<45:36,  4.22s/it][A
 22%|██▏       | 179/827 [14:21<51:37,  4.78s/it][A
 22%|██▏       | 180/827 [14:25<50:27,  4.68s/it][A
 22%|██▏       | 181/827 [14:30<51:06,  4.75s/it][A
 22%|██▏       | 182/827 [14:35<52:13,  4.86s/it][A
 22%|██▏       | 183/827 [14:39<49:35,  4.62s/it][A
 22%|██▏       | 184/827 [14:44<50:33,  4.72s/it][A
 22%|██▏       | 185/827 [14:49<52:17,  4.89s/it][A
 22%|██▏       | 186/827 [14:53<48:26,  4.53s/it][A
 23%|██▎       | 187/827 [14:57<46:27,  4.35s/it][A
 23%|██▎       | 188/827 [15:02<48:18,  4.54s/it][A
 23%|██▎       | 189/827 [15:07<50:27,  4.75s/it][A
 23%|██▎       | 190/827 [15:12<50:12,  4.73s/it][A
 23%|██▎       | 191/827 [15:17<50:04,  4.72s/it][A
 23%|██▎       | 192/827 [15:22<51:22,  4.85s/it][A
 23%|██▎       | 193/827 [15:27<53:13,  5.04s/it][A
 23%|██▎       | 194/827 [15:32<51:27,  4.88s/it][A
 24%|██▎       | 195/827 [15:36<47:50,  4.54s/it][A
 24%|██▎       | 196/827 [15:39<44:34,  4.24s/it][A
 24%|██▍       | 197/827 [15:43<42:25,  4.04s/it][A
 24%|██▍       | 198/827 [15:47<42:16,  4.03s/it][A
 24%|██▍       | 199/827 [15:52<45:18,  4.33s/it][A
 24%|██▍       | 200/827 [15:57<49:29,  4.74s/it][A
 24%|██▍       | 201/827 [16:04<53:50,  5.16s/it][A
 24%|██▍       | 202/827 [16:09<55:33,  5.33s/it][A
 25%|██▍       | 203/827 [16:16<1:00:29,  5.82s/it][A
 25%|██▍       | 204/827 [16:22<1:00:44,  5.85s/it][A
 25%|██▍       | 205/827 [16:26<55:00,  5.31s/it]  [A
 25%|██▍       | 206/827 [16:31<52:52,  5.11s/it][A
 25%|██▌       | 207/827 [16:35<50:15,  4.86s/it][A
 25%|██▌       | 208/827 [16:40<48:38,  4.72s/it][A
 25%|██▌       | 209/827 [16:44<46:37,  4.53s/it][A
 25%|██▌       | 210/827 [16:48<45:01,  4.38s/it][A
 26%|██▌       | 211/827 [16:53<47:51,  4.66s/it][A
 26%|██▌       | 212/827 [17:00<54:18,  5.30s/it][A
 26%|██▌       | 213/827 [17:06<55:49,  5.46s/it][A
 26%|██▌       | 214/827 [17:10<51:22,  5.03s/it][A
 26%|██▌       | 215/827 [17:13<47:26,  4.65s/it][A
 26%|██▌       | 216/827 [17:18<47:12,  4.64s/it][A
 26%|██▌       | 217/827 [17:24<49:53,  4.91s/it][A
 26%|██▋       | 218/827 [17:28<49:25,  4.87s/it][A
 26%|██▋       | 219/827 [17:32<46:29,  4.59s/it][A
 27%|██▋       | 220/827 [17:37<48:00,  4.75s/it][A
 27%|██▋       | 221/827 [17:43<51:03,  5.05s/it][A
 27%|██▋       | 222/827 [17:48<49:39,  4.93s/it][A
 27%|██▋       | 223/827 [17:52<47:48,  4.75s/it][A
 27%|██▋       | 224/827 [17:56<46:20,  4.61s/it][A
 27%|██▋       | 225/827 [18:00<44:26,  4.43s/it][A
 27%|██▋       | 226/827 [18:04<40:49,  4.08s/it][A
 27%|██▋       | 227/827 [18:09<43:39,  4.37s/it][A
 28%|██▊       | 228/827 [18:14<46:55,  4.70s/it][A
 28%|██▊       | 229/827 [18:19<47:02,  4.72s/it][A
 28%|██▊       | 230/827 [18:24<49:12,  4.95s/it][A
 28%|██▊       | 231/827 [18:29<46:54,  4.72s/it][A
 28%|██▊       | 232/827 [18:32<44:17,  4.47s/it][A
 28%|██▊       | 233/827 [18:38<46:45,  4.72s/it][A
 28%|██▊       | 234/827 [18:45<54:18,  5.49s/it][A
 28%|██▊       | 235/827 [18:51<56:33,  5.73s/it][A
 29%|██▊       | 236/827 [18:56<53:00,  5.38s/it][A
 29%|██▊       | 237/827 [19:02<56:04,  5.70s/it][A
 29%|██▉       | 238/827 [19:09<58:08,  5.92s/it][A
 29%|██▉       | 239/827 [19:15<58:07,  5.93s/it][A
 29%|██▉       | 240/827 [19:22<1:02:55,  6.43s/it][A
 29%|██▉       | 241/827 [19:28<1:01:29,  6.30s/it][A
 29%|██▉       | 242/827 [19:34<58:19,  5.98s/it]  [A
 29%|██▉       | 243/827 [19:40<58:47,  6.04s/it][A
 30%|██▉       | 244/827 [19:46<58:26,  6.01s/it][A
 30%|██▉       | 245/827 [19:51<55:02,  5.68s/it][A
 30%|██▉       | 246/827 [19:55<51:00,  5.27s/it][A
 30%|██▉       | 247/827 [20:00<49:58,  5.17s/it][A
 30%|██▉       | 248/827 [20:07<55:22,  5.74s/it][A
 30%|███       | 249/827 [20:12<53:48,  5.59s/it][A
 30%|███       | 250/827 [20:15<46:22,  4.82s/it][A
 30%|███       | 251/827 [20:20<46:09,  4.81s/it][A
 30%|███       | 252/827 [20:25<48:04,  5.02s/it][A
 31%|███       | 253/827 [20:29<43:28,  4.55s/it][A
 31%|███       | 254/827 [20:35<47:35,  4.98s/it][A
 31%|███       | 255/827 [20:43<56:41,  5.95s/it][A
 31%|███       | 256/827 [20:49<57:31,  6.04s/it][A
 31%|███       | 257/827 [20:54<54:40,  5.75s/it][A
 31%|███       | 258/827 [21:00<52:34,  5.54s/it][A
 31%|███▏      | 259/827 [21:06<54:41,  5.78s/it][A
 31%|███▏      | 260/827 [21:12<55:38,  5.89s/it][A
 32%|███▏      | 261/827 [21:17<53:24,  5.66s/it][A
 32%|███▏      | 262/827 [21:24<55:43,  5.92s/it][A
 32%|███▏      | 263/827 [21:30<58:06,  6.18s/it][A
 32%|███▏      | 264/827 [21:34<50:18,  5.36s/it][A
 32%|███▏      | 265/827 [21:38<45:44,  4.88s/it][A
 32%|███▏      | 266/827 [21:42<42:42,  4.57s/it][A
 32%|███▏      | 267/827 [21:45<40:19,  4.32s/it][A
 32%|███▏      | 268/827 [21:50<40:11,  4.31s/it][A
 33%|███▎      | 269/827 [21:55<43:53,  4.72s/it][A
 33%|███▎      | 270/827 [22:01<46:05,  4.97s/it][A
 33%|███▎      | 271/827 [22:05<44:04,  4.76s/it][A
 33%|███▎      | 272/827 [22:09<43:03,  4.66s/it][A
 33%|███▎      | 273/827 [22:16<47:57,  5.19s/it][A
 33%|███▎      | 274/827 [22:25<57:18,  6.22s/it][A
 33%|███▎      | 275/827 [22:31<57:23,  6.24s/it][A
 33%|███▎      | 276/827 [22:36<53:47,  5.86s/it][A
 33%|███▎      | 277/827 [22:41<51:37,  5.63s/it][A
 34%|███▎      | 278/827 [22:45<48:39,  5.32s/it][A
 34%|███▎      | 279/827 [22:49<44:47,  4.90s/it][A
 34%|███▍      | 280/827 [22:53<41:57,  4.60s/it][A
 34%|███▍      | 281/827 [22:58<41:31,  4.56s/it][A
 34%|███▍      | 282/827 [23:03<41:57,  4.62s/it][A
 34%|███▍      | 283/827 [23:07<42:19,  4.67s/it][A
 34%|███▍      | 284/827 [23:12<43:19,  4.79s/it][A
 34%|███▍      | 285/827 [23:18<46:06,  5.11s/it][A
 35%|███▍      | 286/827 [23:24<47:17,  5.24s/it][A
 35%|███▍      | 287/827 [23:27<43:05,  4.79s/it][A
 35%|███▍      | 288/827 [23:32<41:04,  4.57s/it][A
 35%|███▍      | 289/827 [23:36<40:13,  4.49s/it][A
 35%|███▌      | 290/827 [23:40<39:27,  4.41s/it][A
 35%|███▌      | 291/827 [23:44<37:54,  4.24s/it][A
 35%|███▌      | 292/827 [23:48<37:09,  4.17s/it][A
 35%|███▌      | 293/827 [23:52<37:14,  4.19s/it][A
 36%|███▌      | 294/827 [23:57<38:22,  4.32s/it][A
 36%|███▌      | 295/827 [24:02<40:21,  4.55s/it][A
 36%|███▌      | 296/827 [24:07<41:46,  4.72s/it][A
 36%|███▌      | 297/827 [24:12<41:36,  4.71s/it][A
 36%|███▌      | 298/827 [24:16<39:53,  4.52s/it][A
 36%|███▌      | 299/827 [24:20<38:34,  4.38s/it][A
 36%|███▋      | 300/827 [24:23<36:34,  4.16s/it][A
 36%|███▋      | 301/827 [24:27<35:17,  4.03s/it][A
 37%|███▋      | 302/827 [24:31<35:18,  4.04s/it][A
 37%|███▋      | 303/827 [24:35<33:23,  3.82s/it][A
 37%|███▋      | 304/827 [24:39<33:45,  3.87s/it][A
 37%|███▋      | 305/827 [24:44<37:14,  4.28s/it][A
 37%|███▋      | 306/827 [24:49<39:28,  4.55s/it][A
 37%|███▋      | 307/827 [24:53<37:39,  4.34s/it][A
 37%|███▋      | 308/827 [24:57<36:55,  4.27s/it][A
 37%|███▋      | 309/827 [25:01<36:17,  4.20s/it][A
 37%|███▋      | 310/827 [25:04<34:27,  4.00s/it][A
 38%|███▊      | 311/827 [25:08<33:28,  3.89s/it][A
 38%|███▊      | 312/827 [25:13<36:19,  4.23s/it][A
 38%|███▊      | 313/827 [25:18<38:15,  4.47s/it][A
 38%|███▊      | 314/827 [25:22<37:11,  4.35s/it][A
 38%|███▊      | 315/827 [25:27<38:38,  4.53s/it][A
 38%|███▊      | 316/827 [25:33<41:34,  4.88s/it][A
 38%|███▊      | 317/827 [25:40<46:11,  5.43s/it][A
 38%|███▊      | 318/827 [25:46<47:33,  5.61s/it][A
 39%|███▊      | 319/827 [25:50<44:43,  5.28s/it][A
 39%|███▊      | 320/827 [25:55<42:32,  5.03s/it][A
 39%|███▉      | 321/827 [26:00<43:10,  5.12s/it][A
 39%|███▉      | 322/827 [26:05<42:22,  5.03s/it][A
 39%|███▉      | 323/827 [26:10<43:06,  5.13s/it][A
 39%|███▉      | 324/827 [26:16<44:40,  5.33s/it][A
 39%|███▉      | 325/827 [26:20<41:44,  4.99s/it][A
 39%|███▉      | 326/827 [26:25<41:31,  4.97s/it][A
 40%|███▉      | 327/827 [26:30<41:41,  5.00s/it][A
 40%|███▉      | 328/827 [26:35<42:18,  5.09s/it][A
 40%|███▉      | 329/827 [26:42<45:47,  5.52s/it][A
 40%|███▉      | 330/827 [26:49<48:26,  5.85s/it][A
 40%|████      | 331/827 [26:54<47:00,  5.69s/it][A
 40%|████      | 332/827 [26:59<45:19,  5.49s/it][A
 40%|████      | 333/827 [27:05<45:53,  5.57s/it][A
 40%|████      | 334/827 [27:09<43:25,  5.29s/it][A
 41%|████      | 335/827 [27:13<40:41,  4.96s/it][A
 41%|████      | 336/827 [27:18<38:34,  4.71s/it][A
 41%|████      | 337/827 [27:22<38:12,  4.68s/it][A
 41%|████      | 338/827 [27:26<36:07,  4.43s/it][A
 41%|████      | 339/827 [27:30<34:47,  4.28s/it][A
 41%|████      | 340/827 [27:34<34:26,  4.24s/it][A
 41%|████      | 341/827 [27:39<36:05,  4.46s/it][A
 41%|████▏     | 342/827 [27:43<35:47,  4.43s/it][A
 41%|████▏     | 343/827 [27:48<36:09,  4.48s/it][A
 42%|████▏     | 344/827 [27:54<39:05,  4.86s/it][A
 42%|████▏     | 345/827 [27:59<40:34,  5.05s/it][A
 42%|████▏     | 346/827 [28:04<39:28,  4.92s/it][A
 42%|████▏     | 347/827 [28:09<38:49,  4.85s/it][A
 42%|████▏     | 348/827 [28:13<37:04,  4.64s/it][A
 42%|████▏     | 349/827 [28:18<38:07,  4.79s/it][A
 42%|████▏     | 350/827 [28:24<40:37,  5.11s/it][A
 42%|████▏     | 351/827 [28:28<39:03,  4.92s/it][A
 43%|████▎     | 352/827 [28:33<38:38,  4.88s/it][A
 43%|████▎     | 353/827 [28:37<36:43,  4.65s/it][A
 43%|████▎     | 354/827 [28:42<36:04,  4.58s/it][A
 43%|████▎     | 355/827 [28:47<38:39,  4.91s/it][A
 43%|████▎     | 356/827 [28:53<41:29,  5.29s/it][A
 43%|████▎     | 357/827 [28:58<39:55,  5.10s/it][A
 43%|████▎     | 358/827 [29:02<36:40,  4.69s/it][A
 43%|████▎     | 359/827 [29:06<35:29,  4.55s/it][A
 44%|████▎     | 360/827 [29:11<37:22,  4.80s/it][A
 44%|████▎     | 361/827 [29:16<37:01,  4.77s/it][A
 44%|████▍     | 362/827 [29:20<33:58,  4.38s/it][A
 44%|████▍     | 363/827 [29:24<34:53,  4.51s/it][A
 44%|████▍     | 364/827 [29:29<35:11,  4.56s/it][A
 44%|████▍     | 365/827 [29:34<35:05,  4.56s/it][A
 44%|████▍     | 366/827 [29:40<38:25,  5.00s/it][A
 44%|████▍     | 367/827 [29:45<39:01,  5.09s/it][A
 44%|████▍     | 368/827 [29:49<36:16,  4.74s/it][A
 45%|████▍     | 369/827 [29:53<34:47,  4.56s/it][A
 45%|████▍     | 370/827 [29:58<35:04,  4.61s/it][A
 45%|████▍     | 371/827 [30:03<35:37,  4.69s/it][A
 45%|████▍     | 372/827 [30:07<34:48,  4.59s/it][A
 45%|████▌     | 373/827 [30:13<37:02,  4.89s/it][A
 45%|████▌     | 374/827 [30:20<41:56,  5.55s/it][A
 45%|████▌     | 375/827 [30:28<49:05,  6.52s/it][A
 45%|████▌     | 376/827 [30:37<54:23,  7.24s/it][A
 46%|████▌     | 377/827 [30:42<49:32,  6.61s/it][A
 46%|████▌     | 378/827 [30:49<49:20,  6.59s/it][A
 46%|████▌     | 379/827 [30:58<55:25,  7.42s/it][A
 46%|████▌     | 380/827 [31:05<53:03,  7.12s/it][A
 46%|████▌     | 381/827 [31:10<47:36,  6.40s/it][A
 46%|████▌     | 382/827 [31:15<45:00,  6.07s/it][A
 46%|████▋     | 383/827 [31:20<43:41,  5.90s/it][A
 46%|████▋     | 384/827 [31:25<40:06,  5.43s/it][A
 47%|████▋     | 385/827 [31:29<37:27,  5.08s/it][A
 47%|████▋     | 386/827 [31:33<35:39,  4.85s/it][A
 47%|████▋     | 387/827 [31:38<34:47,  4.74s/it][A
 47%|████▋     | 388/827 [31:43<35:10,  4.81s/it][A
 47%|████▋     | 389/827 [31:48<35:50,  4.91s/it][A
 47%|████▋     | 390/827 [31:52<34:24,  4.72s/it][A
 47%|████▋     | 391/827 [31:56<32:33,  4.48s/it][A
 47%|████▋     | 392/827 [32:01<33:20,  4.60s/it][A
 48%|████▊     | 393/827 [32:07<35:36,  4.92s/it][A
 48%|████▊     | 394/827 [32:12<37:00,  5.13s/it][A
 48%|████▊     | 395/827 [32:19<39:46,  5.53s/it][A
 48%|████▊     | 396/827 [32:24<39:01,  5.43s/it][A
 48%|████▊     | 397/827 [32:28<35:44,  4.99s/it][A
 48%|████▊     | 398/827 [32:32<34:40,  4.85s/it][A
 48%|████▊     | 399/827 [32:37<33:53,  4.75s/it][A
 48%|████▊     | 400/827 [32:42<34:34,  4.86s/it][A
 48%|████▊     | 401/827 [32:47<35:17,  4.97s/it][A
 49%|████▊     | 402/827 [32:53<35:59,  5.08s/it][A
 49%|████▊     | 403/827 [32:58<36:25,  5.15s/it][A
 49%|████▉     | 404/827 [33:02<33:06,  4.70s/it][A
 49%|████▉     | 405/827 [33:06<32:22,  4.60s/it][A
 49%|████▉     | 406/827 [33:10<32:12,  4.59s/it][A
 49%|████▉     | 407/827 [33:14<30:31,  4.36s/it][A
 49%|████▉     | 408/827 [33:19<30:13,  4.33s/it][A
 49%|████▉     | 409/827 [33:23<29:43,  4.27s/it][A
 50%|████▉     | 410/827 [33:26<28:18,  4.07s/it][A
 50%|████▉     | 411/827 [33:30<28:16,  4.08s/it][A
 50%|████▉     | 412/827 [33:35<29:10,  4.22s/it][A
 50%|████▉     | 413/827 [33:41<32:40,  4.74s/it][A
 50%|█████     | 414/827 [33:46<34:05,  4.95s/it][A
 50%|█████     | 415/827 [33:50<31:29,  4.59s/it][A
 50%|█████     | 416/827 [33:55<32:07,  4.69s/it][A
 50%|█████     | 417/827 [34:02<37:28,  5.48s/it][A
 51%|█████     | 418/827 [34:08<37:51,  5.55s/it][A
 51%|█████     | 419/827 [34:13<36:52,  5.42s/it][A
 51%|█████     | 420/827 [34:19<38:19,  5.65s/it][A
 51%|█████     | 421/827 [34:24<36:38,  5.42s/it][A
 51%|█████     | 422/827 [34:28<33:02,  4.90s/it][A
 51%|█████     | 423/827 [34:32<30:43,  4.56s/it][A
 51%|█████▏    | 424/827 [34:36<29:57,  4.46s/it][A
 51%|█████▏    | 425/827 [34:42<32:14,  4.81s/it][A
 52%|█████▏    | 426/827 [34:47<34:05,  5.10s/it][A
 52%|█████▏    | 427/827 [34:53<34:43,  5.21s/it][A
 52%|█████▏    | 428/827 [34:59<37:10,  5.59s/it][A
 52%|█████▏    | 429/827 [35:04<35:25,  5.34s/it][A
 52%|█████▏    | 430/827 [35:09<34:51,  5.27s/it][A
 52%|█████▏    | 431/827 [35:16<37:11,  5.64s/it][A
 52%|█████▏    | 432/827 [35:24<42:03,  6.39s/it][A
 52%|█████▏    | 433/827 [35:32<46:25,  7.07s/it][A
 52%|█████▏    | 434/827 [35:38<43:17,  6.61s/it][A
 53%|█████▎    | 435/827 [35:42<37:55,  5.81s/it][A
 53%|█████▎    | 436/827 [35:47<36:08,  5.55s/it][A
 53%|█████▎    | 437/827 [35:52<34:54,  5.37s/it][A
 53%|█████▎    | 438/827 [35:56<32:25,  5.00s/it][A
 53%|█████▎    | 439/827 [36:01<31:55,  4.94s/it][A
 53%|█████▎    | 440/827 [36:06<32:20,  5.01s/it][A
 53%|█████▎    | 441/827 [36:10<29:40,  4.61s/it][A
 53%|█████▎    | 442/827 [36:13<28:02,  4.37s/it][A
 54%|█████▎    | 443/827 [36:18<27:52,  4.36s/it][A
 54%|█████▎    | 444/827 [36:23<29:35,  4.64s/it][A
 54%|█████▍    | 445/827 [36:29<31:44,  4.99s/it][A
 54%|█████▍    | 446/827 [36:34<31:14,  4.92s/it][A
 54%|█████▍    | 447/827 [36:39<31:11,  4.92s/it][A
 54%|█████▍    | 448/827 [36:45<34:00,  5.38s/it][A
 54%|█████▍    | 449/827 [36:53<38:33,  6.12s/it][A
 54%|█████▍    | 450/827 [36:58<37:38,  5.99s/it][A
 55%|█████▍    | 451/827 [37:03<34:33,  5.51s/it][A
 55%|█████▍    | 452/827 [37:09<35:16,  5.64s/it][A
 55%|█████▍    | 453/827 [37:15<36:38,  5.88s/it][A
 55%|█████▍    | 454/827 [37:20<33:57,  5.46s/it][A
 55%|█████▌    | 455/827 [37:25<33:56,  5.47s/it][A
 55%|█████▌    | 456/827 [37:32<35:22,  5.72s/it][A
 55%|█████▌    | 457/827 [37:37<34:02,  5.52s/it][A
 55%|█████▌    | 458/827 [37:41<32:50,  5.34s/it][A
 56%|█████▌    | 459/827 [37:47<32:32,  5.31s/it][A
 56%|█████▌    | 460/827 [37:53<34:34,  5.65s/it][A
 56%|█████▌    | 461/827 [38:01<38:13,  6.27s/it][A
 56%|█████▌    | 462/827 [38:07<36:57,  6.08s/it][A
 56%|█████▌    | 463/827 [38:10<32:51,  5.42s/it][A
 56%|█████▌    | 464/827 [38:16<32:26,  5.36s/it][A
 56%|█████▌    | 465/827 [38:21<32:39,  5.41s/it][A
 56%|█████▋    | 466/827 [38:26<31:41,  5.27s/it][A
 56%|█████▋    | 467/827 [38:32<31:59,  5.33s/it][A
 57%|█████▋    | 468/827 [38:38<33:08,  5.54s/it][A
 57%|█████▋    | 469/827 [38:43<32:33,  5.46s/it][A
 57%|█████▋    | 470/827 [38:47<30:14,  5.08s/it][A
 57%|█████▋    | 471/827 [38:53<32:07,  5.42s/it][A
 57%|█████▋    | 472/827 [39:00<34:25,  5.82s/it][A
 57%|█████▋    | 473/827 [39:06<34:31,  5.85s/it][A
 57%|█████▋    | 474/827 [39:11<32:56,  5.60s/it][A
 57%|█████▋    | 475/827 [39:15<29:39,  5.06s/it][A
 58%|█████▊    | 476/827 [39:18<26:06,  4.46s/it][A
 58%|█████▊    | 477/827 [39:21<23:57,  4.11s/it][A
 58%|█████▊    | 478/827 [39:26<25:32,  4.39s/it][A
 58%|█████▊    | 479/827 [39:32<27:08,  4.68s/it][A
 58%|█████▊    | 480/827 [39:37<27:39,  4.78s/it][A
 58%|█████▊    | 481/827 [39:42<28:40,  4.97s/it][A
 58%|█████▊    | 482/827 [39:46<26:49,  4.67s/it][A
 58%|█████▊    | 483/827 [39:49<24:51,  4.33s/it][A
 59%|█████▊    | 484/827 [39:53<23:56,  4.19s/it][A
 59%|█████▊    | 485/827 [39:57<23:45,  4.17s/it][A
 59%|█████▉    | 486/827 [40:02<23:58,  4.22s/it][A
 59%|█████▉    | 487/827 [40:07<24:55,  4.40s/it][A
 59%|█████▉    | 488/827 [40:12<26:23,  4.67s/it][A
 59%|█████▉    | 489/827 [40:18<28:33,  5.07s/it][A
 59%|█████▉    | 490/827 [40:23<28:39,  5.10s/it][A
 59%|█████▉    | 491/827 [40:27<26:50,  4.79s/it][A
 59%|█████▉    | 492/827 [40:31<25:38,  4.59s/it][A
 60%|█████▉    | 493/827 [40:35<24:41,  4.44s/it][A
 60%|█████▉    | 494/827 [40:40<25:23,  4.57s/it][A
 60%|█████▉    | 495/827 [40:46<26:31,  4.79s/it][A
 60%|█████▉    | 496/827 [40:50<26:06,  4.73s/it][A
 60%|██████    | 497/827 [40:54<24:56,  4.54s/it][A
 60%|██████    | 498/827 [40:59<24:57,  4.55s/it][A
 60%|██████    | 499/827 [41:04<25:31,  4.67s/it][A
 60%|██████    | 500/827 [41:08<24:43,  4.54s/it][A
 61%|██████    | 501/827 [41:11<22:51,  4.21s/it][A
 61%|██████    | 502/827 [41:16<24:05,  4.45s/it][A
 61%|██████    | 503/827 [41:22<25:43,  4.76s/it][A
 61%|██████    | 504/827 [41:26<24:38,  4.58s/it][A
 61%|██████    | 505/827 [41:30<24:05,  4.49s/it][A
 61%|██████    | 506/827 [41:35<24:25,  4.57s/it][A
 61%|██████▏   | 507/827 [41:42<28:33,  5.36s/it][A
 61%|██████▏   | 508/827 [41:49<30:37,  5.76s/it][A
 62%|██████▏   | 509/827 [41:53<27:54,  5.27s/it][A
 62%|██████▏   | 510/827 [41:57<26:18,  4.98s/it][A
 62%|██████▏   | 511/827 [42:01<24:36,  4.67s/it][A
 62%|██████▏   | 512/827 [42:05<23:03,  4.39s/it][A
 62%|██████▏   | 513/827 [42:10<23:03,  4.41s/it][A
 62%|██████▏   | 514/827 [42:14<22:48,  4.37s/it][A
 62%|██████▏   | 515/827 [42:18<22:07,  4.26s/it][A
 62%|██████▏   | 516/827 [42:23<22:45,  4.39s/it][A
 63%|██████▎   | 517/827 [42:27<22:06,  4.28s/it][A
 63%|██████▎   | 518/827 [42:31<21:40,  4.21s/it][A
 63%|██████▎   | 519/827 [42:34<20:48,  4.05s/it][A
 63%|██████▎   | 520/827 [42:38<20:54,  4.09s/it][A
 63%|██████▎   | 521/827 [42:45<23:51,  4.68s/it][A
 63%|██████▎   | 522/827 [42:50<25:19,  4.98s/it][A
 63%|██████▎   | 523/827 [42:55<24:26,  4.82s/it][A
 63%|██████▎   | 524/827 [42:59<24:23,  4.83s/it][A
 63%|██████▎   | 525/827 [43:06<26:39,  5.30s/it][A
 64%|██████▎   | 526/827 [43:12<28:19,  5.65s/it][A
 64%|██████▎   | 527/827 [43:17<26:51,  5.37s/it][A
 64%|██████▍   | 528/827 [43:22<26:42,  5.36s/it][A
 64%|██████▍   | 529/827 [43:28<26:52,  5.41s/it][A
 64%|██████▍   | 530/827 [43:33<26:17,  5.31s/it][A
 64%|██████▍   | 531/827 [43:37<24:34,  4.98s/it][A
 64%|██████▍   | 532/827 [43:42<23:36,  4.80s/it][A
 64%|██████▍   | 533/827 [43:47<24:43,  5.05s/it][A
 65%|██████▍   | 534/827 [43:54<27:38,  5.66s/it][A
 65%|██████▍   | 535/827 [44:01<28:18,  5.82s/it][A
 65%|██████▍   | 536/827 [44:05<26:11,  5.40s/it][A
 65%|██████▍   | 537/827 [44:09<23:44,  4.91s/it][A
 65%|██████▌   | 538/827 [44:13<22:05,  4.59s/it][A
 65%|██████▌   | 539/827 [44:17<21:34,  4.49s/it][A
 65%|██████▌   | 540/827 [44:21<21:41,  4.54s/it][A
 65%|██████▌   | 541/827 [44:26<21:49,  4.58s/it][A
 66%|██████▌   | 542/827 [44:31<22:22,  4.71s/it][A
 66%|██████▌   | 543/827 [44:36<22:48,  4.82s/it][A
 66%|██████▌   | 544/827 [44:42<23:34,  5.00s/it][A
 66%|██████▌   | 545/827 [44:47<24:07,  5.13s/it][A
 66%|██████▌   | 546/827 [44:53<24:32,  5.24s/it][A
 66%|██████▌   | 547/827 [44:58<24:07,  5.17s/it][A
 66%|██████▋   | 548/827 [45:01<21:40,  4.66s/it][A
 66%|██████▋   | 549/827 [45:05<20:40,  4.46s/it][A
 67%|██████▋   | 550/827 [45:10<21:36,  4.68s/it][A
 67%|██████▋   | 551/827 [45:15<21:56,  4.77s/it][A
 67%|██████▋   | 552/827 [45:20<21:57,  4.79s/it][A
 67%|██████▋   | 553/827 [45:24<21:00,  4.60s/it][A
 67%|██████▋   | 554/827 [45:28<19:28,  4.28s/it][A
 67%|██████▋   | 555/827 [45:32<19:20,  4.27s/it][A
 67%|██████▋   | 556/827 [45:36<19:03,  4.22s/it][A
 67%|██████▋   | 557/827 [45:40<18:10,  4.04s/it][A
 67%|██████▋   | 558/827 [45:44<18:46,  4.19s/it][A
 68%|██████▊   | 559/827 [45:49<20:04,  4.50s/it][A
 68%|██████▊   | 560/827 [45:54<19:51,  4.46s/it][A
 68%|██████▊   | 561/827 [45:59<20:09,  4.55s/it][A
 68%|██████▊   | 562/827 [46:03<20:02,  4.54s/it][A
 68%|██████▊   | 563/827 [46:07<19:33,  4.44s/it][A
 68%|██████▊   | 564/827 [46:11<19:03,  4.35s/it][A
 68%|██████▊   | 565/827 [46:17<20:13,  4.63s/it][A
 68%|██████▊   | 566/827 [46:22<20:41,  4.76s/it][A
 69%|██████▊   | 567/827 [46:26<19:55,  4.60s/it][A
 69%|██████▊   | 568/827 [46:31<19:51,  4.60s/it][A
 69%|██████▉   | 569/827 [46:35<19:28,  4.53s/it][A
 69%|██████▉   | 570/827 [46:40<20:37,  4.81s/it][A
 69%|██████▉   | 571/827 [46:47<22:51,  5.36s/it][A
 69%|██████▉   | 572/827 [46:54<24:35,  5.79s/it][A
 69%|██████▉   | 573/827 [47:00<24:42,  5.84s/it][A
 69%|██████▉   | 574/827 [47:05<24:01,  5.70s/it][A
 70%|██████▉   | 575/827 [47:11<23:55,  5.70s/it][A
 70%|██████▉   | 576/827 [47:15<22:17,  5.33s/it][A
 70%|██████▉   | 577/827 [47:20<21:13,  5.10s/it][A
 70%|██████▉   | 578/827 [47:24<20:22,  4.91s/it][A
 70%|███████   | 579/827 [47:28<18:59,  4.59s/it][A
 70%|███████   | 580/827 [47:34<20:27,  4.97s/it][A
 70%|███████   | 581/827 [47:41<23:09,  5.65s/it][A
 70%|███████   | 582/827 [47:46<22:07,  5.42s/it][A
 70%|███████   | 583/827 [47:50<20:38,  5.07s/it][A
 71%|███████   | 584/827 [47:55<19:25,  4.79s/it][A
 71%|███████   | 585/827 [47:59<18:15,  4.53s/it][A
 71%|███████   | 586/827 [48:03<18:15,  4.55s/it][A
 71%|███████   | 587/827 [48:11<22:07,  5.53s/it][A
 71%|███████   | 588/827 [48:20<25:39,  6.44s/it][A
 71%|███████   | 589/827 [48:25<24:26,  6.16s/it][A
 71%|███████▏  | 590/827 [48:32<25:35,  6.48s/it][A
 71%|███████▏  | 591/827 [48:39<26:16,  6.68s/it][A
 72%|███████▏  | 592/827 [48:44<23:13,  5.93s/it][A
 72%|███████▏  | 593/827 [48:48<21:02,  5.39s/it][A
 72%|███████▏  | 594/827 [48:52<19:22,  4.99s/it][A
 72%|███████▏  | 595/827 [48:56<18:55,  4.89s/it][A
 72%|███████▏  | 596/827 [49:04<21:24,  5.56s/it][A
 72%|███████▏  | 597/827 [49:10<22:25,  5.85s/it][A
 72%|███████▏  | 598/827 [49:14<20:14,  5.30s/it][A
 72%|███████▏  | 599/827 [49:18<18:52,  4.97s/it][A
 73%|███████▎  | 600/827 [49:23<18:45,  4.96s/it][A
 73%|███████▎  | 601/827 [49:29<19:30,  5.18s/it][A
 73%|███████▎  | 602/827 [49:33<18:44,  5.00s/it][A
 73%|███████▎  | 603/827 [49:38<17:40,  4.74s/it][A
 73%|███████▎  | 604/827 [49:42<16:58,  4.57s/it][A
 73%|███████▎  | 605/827 [49:46<16:26,  4.44s/it][A
 73%|███████▎  | 606/827 [49:51<16:54,  4.59s/it][A
 73%|███████▎  | 607/827 [49:56<16:59,  4.63s/it][A
 74%|███████▎  | 608/827 [50:00<16:30,  4.52s/it][A
 74%|███████▎  | 609/827 [50:07<19:04,  5.25s/it][A
 74%|███████▍  | 610/827 [50:14<21:30,  5.94s/it][A
 74%|███████▍  | 611/827 [50:19<20:13,  5.62s/it][A
 74%|███████▍  | 612/827 [50:24<19:28,  5.43s/it][A
 74%|███████▍  | 613/827 [50:30<19:17,  5.41s/it][A
 74%|███████▍  | 614/827 [50:35<18:54,  5.33s/it][A
 74%|███████▍  | 615/827 [50:39<17:55,  5.07s/it][A
 74%|███████▍  | 616/827 [50:43<16:29,  4.69s/it][A
 75%|███████▍  | 617/827 [50:48<17:08,  4.90s/it][A
 75%|███████▍  | 618/827 [50:55<19:11,  5.51s/it][A
 75%|███████▍  | 619/827 [51:00<18:06,  5.22s/it][A
 75%|███████▍  | 620/827 [51:04<17:01,  4.93s/it][A
 75%|███████▌  | 621/827 [51:08<16:16,  4.74s/it][A
 75%|███████▌  | 622/827 [51:13<15:41,  4.59s/it][A
 75%|███████▌  | 623/827 [51:17<15:36,  4.59s/it][A
 75%|███████▌  | 624/827 [51:22<16:04,  4.75s/it][A
 76%|███████▌  | 625/827 [51:27<16:13,  4.82s/it][A
 76%|███████▌  | 626/827 [51:32<16:16,  4.86s/it][A
 76%|███████▌  | 627/827 [51:37<15:42,  4.71s/it][A
 76%|███████▌  | 628/827 [51:40<14:33,  4.39s/it][A
 76%|███████▌  | 629/827 [51:44<13:53,  4.21s/it][A
 76%|███████▌  | 630/827 [51:49<14:17,  4.35s/it][A
 76%|███████▋  | 631/827 [51:55<15:47,  4.84s/it][A
 76%|███████▋  | 632/827 [52:00<16:35,  5.11s/it][A
 77%|███████▋  | 633/827 [52:07<18:21,  5.68s/it][A
 77%|███████▋  | 634/827 [52:15<19:49,  6.16s/it][A
 77%|███████▋  | 635/827 [52:19<17:49,  5.57s/it][A
 77%|███████▋  | 636/827 [52:24<16:48,  5.28s/it][A
 77%|███████▋  | 637/827 [52:29<17:16,  5.45s/it][A
 77%|███████▋  | 638/827 [52:35<17:26,  5.54s/it][A
 77%|███████▋  | 639/827 [52:39<15:39,  5.00s/it][A
 77%|███████▋  | 640/827 [52:43<14:44,  4.73s/it][A
 78%|███████▊  | 641/827 [52:48<14:50,  4.79s/it][A
 78%|███████▊  | 642/827 [52:52<14:14,  4.62s/it][A
 78%|███████▊  | 643/827 [52:57<14:34,  4.75s/it][A
 78%|███████▊  | 644/827 [53:03<15:39,  5.13s/it][A
 78%|███████▊  | 645/827 [53:08<15:10,  5.00s/it][A
 78%|███████▊  | 646/827 [53:12<14:21,  4.76s/it][A
 78%|███████▊  | 647/827 [53:16<13:36,  4.54s/it][A
 78%|███████▊  | 648/827 [53:21<13:29,  4.52s/it][A
 78%|███████▊  | 649/827 [53:25<13:25,  4.53s/it][A
 79%|███████▊  | 650/827 [53:29<13:05,  4.44s/it][A
 79%|███████▊  | 651/827 [53:34<12:50,  4.38s/it][A
 79%|███████▉  | 652/827 [53:38<12:32,  4.30s/it][A
 79%|███████▉  | 653/827 [53:42<12:39,  4.37s/it][A
 79%|███████▉  | 654/827 [53:49<14:21,  4.98s/it][A
 79%|███████▉  | 655/827 [53:55<15:03,  5.25s/it][A
 79%|███████▉  | 656/827 [53:58<13:30,  4.74s/it][A
 79%|███████▉  | 657/827 [54:02<12:40,  4.48s/it][A
 80%|███████▉  | 658/827 [54:08<13:59,  4.97s/it][A
 80%|███████▉  | 659/827 [54:16<16:22,  5.85s/it][A
 80%|███████▉  | 660/827 [54:23<17:26,  6.26s/it][A
 80%|███████▉  | 661/827 [54:29<16:58,  6.14s/it][A
 80%|████████  | 662/827 [54:34<15:29,  5.64s/it][A
 80%|████████  | 663/827 [54:39<14:57,  5.47s/it][A
 80%|████████  | 664/827 [54:44<14:52,  5.48s/it][A
 80%|████████  | 665/827 [54:49<14:28,  5.36s/it][A
 81%|████████  | 666/827 [54:54<14:11,  5.29s/it][A
 81%|████████  | 667/827 [54:59<13:22,  5.02s/it][A
 81%|████████  | 668/827 [55:03<12:44,  4.81s/it][A
 81%|████████  | 669/827 [55:06<11:33,  4.39s/it][A
 81%|████████  | 670/827 [55:10<10:55,  4.17s/it][A
 81%|████████  | 671/827 [55:15<11:10,  4.30s/it][A
 81%|████████▏ | 672/827 [55:19<10:59,  4.26s/it][A
 81%|████████▏ | 673/827 [55:23<10:48,  4.21s/it][A
 81%|████████▏ | 674/827 [55:27<10:48,  4.24s/it][A
 82%|████████▏ | 675/827 [55:32<10:44,  4.24s/it][A
 82%|████████▏ | 676/827 [55:37<11:39,  4.64s/it][A
 82%|████████▏ | 677/827 [55:43<12:20,  4.94s/it][A
 82%|████████▏ | 678/827 [55:48<12:31,  5.04s/it][A
 82%|████████▏ | 679/827 [55:53<12:31,  5.08s/it][A
 82%|████████▏ | 680/827 [55:57<11:33,  4.72s/it][A
 82%|████████▏ | 681/827 [56:03<12:21,  5.08s/it][A
 82%|████████▏ | 682/827 [56:09<13:14,  5.48s/it][A
 83%|████████▎ | 683/827 [56:13<12:09,  5.07s/it][A
 83%|████████▎ | 684/827 [56:17<11:00,  4.62s/it][A
 83%|████████▎ | 685/827 [56:20<10:05,  4.26s/it][A
 83%|████████▎ | 686/827 [56:27<11:36,  4.94s/it][A
 83%|████████▎ | 687/827 [56:34<13:10,  5.65s/it][A
 83%|████████▎ | 688/827 [56:40<13:11,  5.70s/it][A
 83%|████████▎ | 689/827 [56:46<13:15,  5.77s/it][A
 83%|████████▎ | 690/827 [56:50<12:14,  5.36s/it][A
 84%|████████▎ | 691/827 [56:55<11:28,  5.06s/it][A
 84%|████████▎ | 692/827 [56:59<11:03,  4.92s/it][A
 84%|████████▍ | 693/827 [57:03<10:22,  4.65s/it][A
 84%|████████▍ | 694/827 [57:08<10:00,  4.52s/it][A
 84%|████████▍ | 695/827 [57:13<10:26,  4.74s/it][A
 84%|████████▍ | 696/827 [57:18<10:44,  4.92s/it][A
 84%|████████▍ | 697/827 [57:22<10:07,  4.67s/it][A
 84%|████████▍ | 698/827 [57:26<09:40,  4.50s/it][A
 85%|████████▍ | 699/827 [57:31<09:43,  4.56s/it][A
 85%|████████▍ | 700/827 [57:38<11:00,  5.20s/it][A
 85%|████████▍ | 701/827 [57:45<11:55,  5.68s/it][A
 85%|████████▍ | 702/827 [57:50<11:25,  5.48s/it][A
 85%|████████▌ | 703/827 [57:55<10:58,  5.31s/it][A
 85%|████████▌ | 704/827 [57:58<09:56,  4.85s/it][A
 85%|████████▌ | 705/827 [58:02<08:57,  4.41s/it][A
 85%|████████▌ | 706/827 [58:06<08:37,  4.28s/it][A
 85%|████████▌ | 707/827 [58:10<08:38,  4.32s/it][A
 86%|████████▌ | 708/827 [58:15<09:04,  4.57s/it][A
 86%|████████▌ | 709/827 [58:20<08:58,  4.56s/it][A
 86%|████████▌ | 710/827 [58:24<08:39,  4.44s/it][A
 86%|████████▌ | 711/827 [58:28<08:34,  4.43s/it][A
 86%|████████▌ | 712/827 [58:32<08:18,  4.34s/it][A
 86%|████████▌ | 713/827 [58:37<08:15,  4.35s/it][A
 86%|████████▋ | 714/827 [58:43<08:55,  4.74s/it][A
 86%|████████▋ | 715/827 [58:48<09:12,  4.93s/it][A
 87%|████████▋ | 716/827 [58:53<09:10,  4.96s/it][A
 87%|████████▋ | 717/827 [59:00<10:08,  5.53s/it][A
 87%|████████▋ | 718/827 [59:05<09:59,  5.50s/it][A
 87%|████████▋ | 719/827 [59:09<09:08,  5.08s/it][A
 87%|████████▋ | 720/827 [59:14<08:44,  4.90s/it][A
 87%|████████▋ | 721/827 [59:18<08:20,  4.72s/it][A
 87%|████████▋ | 722/827 [59:22<07:59,  4.57s/it][A
 87%|████████▋ | 723/827 [59:26<07:39,  4.42s/it][A
 88%|████████▊ | 724/827 [59:30<07:04,  4.12s/it][A
 88%|████████▊ | 725/827 [59:35<07:42,  4.53s/it][A
 88%|████████▊ | 726/827 [59:43<08:59,  5.35s/it][A
 88%|████████▊ | 727/827 [59:48<08:49,  5.29s/it][A
 88%|████████▊ | 728/827 [59:51<07:58,  4.84s/it][A
 88%|████████▊ | 729/827 [59:55<07:19,  4.48s/it][A
 88%|████████▊ | 730/827 [1:00:00<07:39,  4.73s/it][A
 88%|████████▊ | 731/827 [1:00:07<08:25,  5.27s/it][A
 89%|████████▊ | 732/827 [1:00:14<09:00,  5.69s/it][A
 89%|████████▊ | 733/827 [1:00:19<08:35,  5.48s/it][A
 89%|████████▉ | 734/827 [1:00:22<07:35,  4.90s/it][A
 89%|████████▉ | 735/827 [1:00:27<07:18,  4.77s/it][A
 89%|████████▉ | 736/827 [1:00:32<07:41,  5.07s/it][A
 89%|████████▉ | 737/827 [1:00:38<07:44,  5.16s/it][A
 89%|████████▉ | 738/827 [1:00:42<07:04,  4.77s/it][A
 89%|████████▉ | 739/827 [1:00:47<07:19,  5.00s/it][A
 89%|████████▉ | 740/827 [1:00:53<07:40,  5.29s/it][A
 90%|████████▉ | 741/827 [1:00:58<07:20,  5.12s/it][A
 90%|████████▉ | 742/827 [1:01:04<07:37,  5.39s/it][A
 90%|████████▉ | 743/827 [1:01:10<07:57,  5.68s/it][A
 90%|████████▉ | 744/827 [1:01:16<07:48,  5.64s/it][A
 90%|█████████ | 745/827 [1:01:21<07:27,  5.46s/it][A
 90%|█████████ | 746/827 [1:01:27<07:36,  5.63s/it][A
 90%|█████████ | 747/827 [1:01:33<07:33,  5.66s/it][A
 90%|█████████ | 748/827 [1:01:37<06:59,  5.31s/it][A
 91%|█████████ | 749/827 [1:01:43<07:08,  5.50s/it][A
 91%|█████████ | 750/827 [1:01:49<07:09,  5.58s/it][A
 91%|█████████ | 751/827 [1:01:53<06:30,  5.13s/it][A
 91%|█████████ | 752/827 [1:01:57<05:57,  4.76s/it][A
 91%|█████████ | 753/827 [1:02:00<05:27,  4.43s/it][A
 91%|█████████ | 754/827 [1:02:04<05:07,  4.21s/it][A
 91%|█████████▏| 755/827 [1:02:10<05:34,  4.64s/it][A
 91%|█████████▏| 756/827 [1:02:15<05:46,  4.87s/it][A
 92%|█████████▏| 757/827 [1:02:20<05:37,  4.83s/it][A
 92%|█████████▏| 758/827 [1:02:25<05:41,  4.96s/it][A
 92%|█████████▏| 759/827 [1:02:29<05:11,  4.58s/it][A
 92%|█████████▏| 760/827 [1:02:35<05:39,  5.07s/it][A
 92%|█████████▏| 761/827 [1:02:42<06:14,  5.68s/it][A
 92%|█████████▏| 762/827 [1:02:49<06:24,  5.91s/it][A
 92%|█████████▏| 763/827 [1:02:56<06:37,  6.21s/it][A
 92%|█████████▏| 764/827 [1:03:00<05:49,  5.55s/it][A
 93%|█████████▎| 765/827 [1:03:04<05:24,  5.23s/it][A
 93%|█████████▎| 766/827 [1:03:08<05:03,  4.98s/it][A
 93%|█████████▎| 767/827 [1:03:13<04:47,  4.79s/it][A
 93%|█████████▎| 768/827 [1:03:18<04:48,  4.90s/it][A
 93%|█████████▎| 769/827 [1:03:24<05:02,  5.21s/it][A
 93%|█████████▎| 770/827 [1:03:29<04:51,  5.11s/it][A
 93%|█████████▎| 771/827 [1:03:33<04:32,  4.86s/it][A
 93%|█████████▎| 772/827 [1:03:37<04:17,  4.68s/it][A
 93%|█████████▎| 773/827 [1:03:42<04:11,  4.65s/it][A
 94%|█████████▎| 774/827 [1:03:46<04:05,  4.63s/it][A
 94%|█████████▎| 775/827 [1:03:51<03:57,  4.57s/it][A
 94%|█████████▍| 776/827 [1:03:56<04:00,  4.71s/it][A
 94%|█████████▍| 777/827 [1:04:00<03:53,  4.67s/it][A
 94%|█████████▍| 778/827 [1:04:05<03:43,  4.55s/it][A
 94%|█████████▍| 779/827 [1:04:09<03:28,  4.35s/it][A
 94%|█████████▍| 780/827 [1:04:13<03:20,  4.28s/it][A
 94%|█████████▍| 781/827 [1:04:17<03:15,  4.24s/it][A
 95%|█████████▍| 782/827 [1:04:21<03:15,  4.34s/it][A
 95%|█████████▍| 783/827 [1:04:25<03:05,  4.23s/it][A
 95%|█████████▍| 784/827 [1:04:29<02:56,  4.11s/it][A
 95%|█████████▍| 785/827 [1:04:34<02:54,  4.16s/it][A
 95%|█████████▌| 786/827 [1:04:40<03:13,  4.72s/it][A
 95%|█████████▌| 787/827 [1:04:46<03:27,  5.18s/it][A
 95%|█████████▌| 788/827 [1:04:50<03:13,  4.96s/it][A
 95%|█████████▌| 789/827 [1:04:55<03:08,  4.96s/it][A
 96%|█████████▌| 790/827 [1:05:00<03:02,  4.94s/it][A
 96%|█████████▌| 791/827 [1:05:04<02:48,  4.68s/it][A
 96%|█████████▌| 792/827 [1:05:08<02:37,  4.49s/it][A
 96%|█████████▌| 793/827 [1:05:15<02:51,  5.04s/it][A
 96%|█████████▌| 794/827 [1:05:22<03:09,  5.75s/it][A
 96%|█████████▌| 795/827 [1:05:28<03:09,  5.93s/it][A
 96%|█████████▋| 796/827 [1:05:34<02:59,  5.80s/it][A
 96%|█████████▋| 797/827 [1:05:38<02:38,  5.29s/it][A
 96%|█████████▋| 798/827 [1:05:42<02:19,  4.80s/it][A
 97%|█████████▋| 799/827 [1:05:45<02:01,  4.33s/it][A
 97%|█████████▋| 800/827 [1:05:52<02:18,  5.12s/it][A
 97%|█████████▋| 801/827 [1:06:01<02:44,  6.31s/it][A
 97%|█████████▋| 802/827 [1:06:06<02:30,  6.00s/it][A
 97%|█████████▋| 803/827 [1:06:12<02:22,  5.95s/it][A
 97%|█████████▋| 804/827 [1:06:18<02:16,  5.94s/it][A
 97%|█████████▋| 805/827 [1:06:22<01:56,  5.31s/it][A
 97%|█████████▋| 806/827 [1:06:25<01:40,  4.81s/it][A
 98%|█████████▊| 807/827 [1:06:30<01:32,  4.62s/it][A
 98%|█████████▊| 808/827 [1:06:34<01:25,  4.52s/it][A
 98%|█████████▊| 809/827 [1:06:38<01:19,  4.41s/it][A
 98%|█████████▊| 810/827 [1:06:42<01:14,  4.38s/it][A
 98%|█████████▊| 811/827 [1:06:46<01:09,  4.32s/it][A
 98%|█████████▊| 812/827 [1:06:52<01:08,  4.55s/it][A
 98%|█████████▊| 813/827 [1:06:59<01:15,  5.43s/it][A
 98%|█████████▊| 814/827 [1:07:05<01:12,  5.57s/it][A
 99%|█████████▊| 815/827 [1:07:10<01:04,  5.34s/it][A
 99%|█████████▊| 816/827 [1:07:15<00:58,  5.30s/it][A
 99%|█████████▉| 817/827 [1:07:19<00:50,  5.01s/it][A
 99%|█████████▉| 818/827 [1:07:25<00:47,  5.24s/it][A
 99%|█████████▉| 819/827 [1:07:32<00:46,  5.78s/it][A
 99%|█████████▉| 820/827 [1:07:38<00:39,  5.71s/it][A
 99%|█████████▉| 821/827 [1:07:42<00:31,  5.30s/it][A
 99%|█████████▉| 822/827 [1:07:46<00:24,  4.97s/it][A
100%|█████████▉| 823/827 [1:07:50<00:18,  4.51s/it][A
100%|█████████▉| 824/827 [1:07:53<00:12,  4.26s/it][A
100%|█████████▉| 825/827 [1:08:00<00:09,  4.90s/it][A
100%|█████████▉| 826/827 [1:08:06<00:05,  5.41s/it][A
100%|██████████| 827/827 [1:08:09<00:00,  4.70s/it][A                                                       
                                                   [A{'eval_loss': 0.6436275243759155, 'eval_runtime': 4095.9131, 'eval_samples_per_second': 0.807, 'eval_steps_per_second': 0.202, 'epoch': 0.86}
 86%|████████▌ | 400/465 [16:23:47<2:18:09, 127.53s/it]
100%|██████████| 827/827 [1:08:09<00:00,  4.70s/it][A
                                                   [A[INFO|trainer.py:4309] 2025-12-16 17:23:42,636 >> Saving model checkpoint to saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400
/opt/conda/lib/python3.11/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in Qwen/Qwen3-VL-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2421] 2025-12-16 17:23:42,733 >> chat template saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-16 17:23:42,736 >> tokenizer config file saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-16 17:23:42,738 >> Special tokens file saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/special_tokens_map.json
[2025-12-16 17:23:42,913] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step400 is about to be saved!
[2025-12-16 17:23:42,934] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/zero_pp_rank_3_mp_rank_00_model_states.pt...
[2025-12-16 17:23:42,934] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/zero_pp_rank_2_mp_rank_00_model_states.pt...
[2025-12-16 17:23:42,935] [INFO] [logging.py:107:log_dist] [Rank 1] Saving model checkpoint: saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/zero_pp_rank_1_mp_rank_00_model_states.pt
[2025-12-16 17:23:42,935] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/zero_pp_rank_1_mp_rank_00_model_states.pt...
[2025-12-16 17:23:42,954] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/zero_pp_rank_3_mp_rank_00_model_states.pt.
[2025-12-16 17:23:42,958] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/zero_pp_rank_1_mp_rank_00_model_states.pt.
[2025-12-16 17:23:42,959] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/zero_pp_rank_2_mp_rank_00_model_states.pt.
[2025-12-16 17:23:42,996] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-12-16 17:23:42,997] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-12-16 17:23:43,020] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-12-16 17:23:43,023] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-12-16 17:23:43,023] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2025-12-16 17:23:43,023] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2025-12-16 17:23:43,023] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2025-12-16 17:23:43,119] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-12-16 17:23:43,127] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2025-12-16 17:23:43,127] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2025-12-16 17:23:43,127] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-12-16 17:23:43,138] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2025-12-16 17:23:43,138] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2025-12-16 17:23:43,144] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2025-12-16 17:23:43,144] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2025-12-16 17:23:43,172] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2025-12-16 17:23:43,172] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2025-12-16 17:23:43,172] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2025-12-16 17:23:43,175] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[INFO|image_processing_base.py:253] 2025-12-16 17:23:43,189 >> Image processor saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/preprocessor_config.json
[INFO|tokenization_utils_base.py:2421] 2025-12-16 17:23:43,192 >> chat template saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-16 17:23:43,195 >> tokenizer config file saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-16 17:23:43,198 >> Special tokens file saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/special_tokens_map.json
[INFO|video_processing_utils.py:600] 2025-12-16 17:23:43,321 >> Video processor saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/video_preprocessor_config.json
[INFO|processing_utils.py:814] 2025-12-16 17:23:43,324 >> chat template saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-400/chat_template.jinja
 86%|████████▌ | 401/465 [16:26:02<24:09:15, 1358.68s/it] 86%|████████▋ | 402/465 [16:28:17<17:20:57, 991.38s/it]  87%|████████▋ | 403/465 [16:30:20<12:35:11, 730.83s/it] 87%|████████▋ | 404/465 [16:32:24<9:18:09, 549.00s/it]  87%|████████▋ | 405/465 [16:34:23<7:00:00, 420.01s/it] 87%|████████▋ | 406/465 [16:36:16<5:22:25, 327.88s/it] 88%|████████▊ | 407/465 [16:38:32<4:21:10, 270.18s/it] 88%|████████▊ | 408/465 [16:40:27<3:32:30, 223.69s/it] 88%|████████▊ | 409/465 [16:42:27<2:59:51, 192.71s/it] 88%|████████▊ | 410/465 [16:44:32<2:37:51, 172.21s/it]                                                       {'loss': 0.6386, 'grad_norm': 0.2847443222999573, 'learning_rate': 4.363578027486187e-06, 'epoch': 0.88}
 88%|████████▊ | 410/465 [16:44:32<2:37:51, 172.21s/it] 88%|████████▊ | 411/465 [16:46:51<2:25:57, 162.18s/it] 89%|████████▊ | 412/465 [16:48:52<2:12:22, 149.86s/it] 89%|████████▉ | 413/465 [16:51:19<2:09:19, 149.22s/it] 89%|████████▉ | 414/465 [16:53:36<2:03:39, 145.48s/it] 89%|████████▉ | 415/465 [16:55:50<1:58:18, 141.98s/it] 89%|████████▉ | 416/465 [16:57:59<1:52:49, 138.14s/it] 90%|████████▉ | 417/465 [17:00:19<1:50:54, 138.64s/it] 90%|████████▉ | 418/465 [17:02:16<1:43:37, 132.28s/it] 90%|█████████ | 419/465 [17:04:30<1:41:37, 132.55s/it] 90%|█████████ | 420/465 [17:06:38<1:38:27, 131.27s/it]                                                       {'loss': 0.645, 'grad_norm': 0.26339301466941833, 'learning_rate': 2.958507960694784e-06, 'epoch': 0.9}
 90%|█████████ | 420/465 [17:06:38<1:38:27, 131.27s/it] 91%|█████████ | 421/465 [17:08:31<1:32:10, 125.70s/it] 91%|█████████ | 422/465 [17:10:43<1:31:35, 127.80s/it] 91%|█████████ | 423/465 [17:12:44<1:27:52, 125.54s/it] 91%|█████████ | 424/465 [17:15:04<1:28:54, 130.10s/it] 91%|█████████▏| 425/465 [17:17:15<1:26:54, 130.35s/it] 92%|█████████▏| 426/465 [17:19:20<1:23:34, 128.57s/it] 92%|█████████▏| 427/465 [17:21:22<1:20:11, 126.62s/it] 92%|█████████▏| 428/465 [17:23:44<1:20:59, 131.35s/it] 92%|█████████▏| 429/465 [17:26:07<1:20:56, 134.91s/it] 92%|█████████▏| 430/465 [17:28:11<1:16:40, 131.45s/it]                                                       {'loss': 0.641, 'grad_norm': 0.2806122899055481, 'learning_rate': 1.8190352989793325e-06, 'epoch': 0.93}
 92%|█████████▏| 430/465 [17:28:11<1:16:40, 131.45s/it] 93%|█████████▎| 431/465 [17:30:24<1:14:48, 132.02s/it] 93%|█████████▎| 432/465 [17:32:31<1:11:47, 130.52s/it] 93%|█████████▎| 433/465 [17:34:31<1:07:50, 127.21s/it] 93%|█████████▎| 434/465 [17:36:46<1:07:00, 129.70s/it] 94%|█████████▎| 435/465 [17:39:05<1:06:17, 132.60s/it] 94%|█████████▍| 436/465 [17:40:58<1:01:08, 126.49s/it] 94%|█████████▍| 437/465 [17:43:09<59:46, 128.09s/it]   94%|█████████▍| 438/465 [17:45:17<57:29, 127.77s/it] 94%|█████████▍| 439/465 [17:47:17<54:22, 125.49s/it] 95%|█████████▍| 440/465 [17:49:34<53:45, 129.03s/it]                                                     {'loss': 0.6498, 'grad_norm': 0.2959441840648651, 'learning_rate': 9.51593532626538e-07, 'epoch': 0.95}
 95%|█████████▍| 440/465 [17:49:34<53:45, 129.03s/it] 95%|█████████▍| 441/465 [17:51:48<52:09, 130.41s/it] 95%|█████████▌| 442/465 [17:53:53<49:23, 128.83s/it] 95%|█████████▌| 443/465 [17:55:58<46:48, 127.66s/it] 95%|█████████▌| 444/465 [17:58:04<44:32, 127.27s/it] 96%|█████████▌| 445/465 [18:00:11<42:24, 127.24s/it] 96%|█████████▌| 446/465 [18:02:25<40:53, 129.12s/it] 96%|█████████▌| 447/465 [18:04:28<38:14, 127.45s/it] 96%|█████████▋| 448/465 [18:06:39<36:25, 128.55s/it] 97%|█████████▋| 449/465 [18:08:37<33:24, 125.25s/it] 97%|█████████▋| 450/465 [18:10:30<30:25, 121.68s/it]                                                     {'loss': 0.6412, 'grad_norm': 0.2824432849884033, 'learning_rate': 3.6108025888958453e-07, 'epoch': 0.97}
 97%|█████████▋| 450/465 [18:10:30<30:25, 121.68s/it] 97%|█████████▋| 451/465 [18:12:38<28:47, 123.36s/it] 97%|█████████▋| 452/465 [18:14:39<26:36, 122.83s/it] 97%|█████████▋| 453/465 [18:16:53<25:12, 126.05s/it] 98%|█████████▊| 454/465 [18:19:16<24:04, 131.31s/it] 98%|█████████▊| 455/465 [18:21:15<21:15, 127.53s/it] 98%|█████████▊| 456/465 [18:23:22<19:07, 127.51s/it] 98%|█████████▊| 457/465 [18:25:23<16:44, 125.54s/it] 98%|█████████▊| 458/465 [18:27:31<14:43, 126.26s/it] 99%|█████████▊| 459/465 [18:29:27<12:17, 123.00s/it] 99%|█████████▉| 460/465 [18:31:45<10:38, 127.72s/it]                                                     {'loss': 0.6379, 'grad_norm': 0.30955085158348083, 'learning_rate': 5.082953003528457e-08, 'epoch': 0.99}
 99%|█████████▉| 460/465 [18:31:46<10:38, 127.72s/it] 99%|█████████▉| 461/465 [18:33:48<08:24, 126.24s/it] 99%|█████████▉| 462/465 [18:36:05<06:28, 129.37s/it]100%|█████████▉| 463/465 [18:38:12<04:17, 128.75s/it]100%|█████████▉| 464/465 [18:40:23<02:09, 129.40s/it]100%|██████████| 465/465 [18:41:43<00:00, 114.60s/it][INFO|trainer.py:4309] 2025-12-16 19:41:38,845 >> Saving model checkpoint to saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465
/opt/conda/lib/python3.11/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in Qwen/Qwen3-VL-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2421] 2025-12-16 19:41:38,934 >> chat template saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-16 19:41:38,937 >> tokenizer config file saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-16 19:41:38,939 >> Special tokens file saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/special_tokens_map.json
[2025-12-16 19:41:39,119] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step465 is about to be saved!
[2025-12-16 19:41:39,140] [INFO] [logging.py:107:log_dist] [Rank 1] Saving model checkpoint: saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/zero_pp_rank_1_mp_rank_00_model_states.pt
[2025-12-16 19:41:39,141] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/zero_pp_rank_1_mp_rank_00_model_states.pt...
[2025-12-16 19:41:39,141] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/zero_pp_rank_3_mp_rank_00_model_states.pt...
[2025-12-16 19:41:39,141] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/zero_pp_rank_2_mp_rank_00_model_states.pt...
[2025-12-16 19:41:39,161] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/zero_pp_rank_1_mp_rank_00_model_states.pt.
[2025-12-16 19:41:39,163] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/zero_pp_rank_2_mp_rank_00_model_states.pt.
[2025-12-16 19:41:39,165] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/zero_pp_rank_3_mp_rank_00_model_states.pt.
[2025-12-16 19:41:39,203] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-12-16 19:41:39,203] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-12-16 19:41:39,223] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-12-16 19:41:39,226] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-12-16 19:41:39,226] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2025-12-16 19:41:39,226] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2025-12-16 19:41:39,226] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2025-12-16 19:41:39,304] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-12-16 19:41:39,308] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2025-12-16 19:41:39,308] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2025-12-16 19:41:39,310] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-12-16 19:41:39,314] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2025-12-16 19:41:39,314] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2025-12-16 19:41:39,332] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2025-12-16 19:41:39,333] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2025-12-16 19:41:39,360] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step465 is ready now!
[2025-12-16 19:41:39,361] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step465 is ready now!
[2025-12-16 19:41:39,361] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step465 is ready now!
[2025-12-16 19:41:39,363] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step465 is ready now!


Training completed. Do not forget to share your model on huggingface.co/models =)




Training completed. Do not forget to share your model on huggingface.co/models =)




Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|image_processing_base.py:253] 2025-12-16 19:41:39,383 >> Image processor saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/preprocessor_config.json
[INFO|tokenization_utils_base.py:2421] 2025-12-16 19:41:39,398 >> chat template saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-16 19:41:39,402 >> tokenizer config file saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-16 19:41:39,409 >> Special tokens file saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/special_tokens_map.json
[INFO|video_processing_utils.py:600] 2025-12-16 19:41:39,537 >> Video processor saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/video_preprocessor_config.json
[INFO|processing_utils.py:814] 2025-12-16 19:41:39,540 >> chat template saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/checkpoint-465/chat_template.jinja
[INFO|trainer.py:2810] 2025-12-16 19:41:39,800 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                     {'train_runtime': 67309.4317, 'train_samples_per_second': 0.442, 'train_steps_per_second': 0.007, 'train_loss': 0.7536053360149425, 'epoch': 1.0}
100%|██████████| 465/465 [18:41:47<00:00, 114.60s/it]100%|██████████| 465/465 [18:41:47<00:00, 144.75s/it]
[INFO|image_processing_base.py:253] 2025-12-16 19:41:39,808 >> Image processor saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/preprocessor_config.json
[INFO|tokenization_utils_base.py:2421] 2025-12-16 19:41:39,810 >> chat template saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-16 19:41:39,815 >> tokenizer config file saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-16 19:41:39,818 >> Special tokens file saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/special_tokens_map.json
[INFO|video_processing_utils.py:600] 2025-12-16 19:41:39,941 >> Video processor saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/video_preprocessor_config.json
[INFO|processing_utils.py:814] 2025-12-16 19:41:39,945 >> chat template saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/chat_template.jinja
[INFO|trainer.py:4309] 2025-12-16 19:41:43,164 >> Saving model checkpoint to saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1
/opt/conda/lib/python3.11/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in Qwen/Qwen3-VL-8B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2421] 2025-12-16 19:41:43,244 >> chat template saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-16 19:41:43,247 >> tokenizer config file saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-16 19:41:43,250 >> Special tokens file saved in saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/special_tokens_map.json
***** train metrics *****
  epoch                    =         1.0
  total_flos               =  12558522GF
  train_loss               =      0.7536
  train_runtime            = 18:41:49.43
  train_samples_per_second =       0.442
  train_steps_per_second   =       0.007
Figure saved at: saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/training_loss.png
Figure saved at: saves/qwen3vl-8b/lora/sft/SQA3Devery24_traineval/training_eval_loss.png
[WARNING|2025-12-16 19:41:43] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.
[INFO|trainer.py:4643] 2025-12-16 19:41:43,545 >> 
***** Running Evaluation *****
[INFO|trainer.py:4645] 2025-12-16 19:41:43,545 >>   Num examples = 3305
[INFO|trainer.py:4648] 2025-12-16 19:41:43,545 >>   Batch size = 1
  0%|          | 0/827 [00:00<?, ?it/s]  0%|          | 2/827 [00:04<32:39,  2.38s/it]  0%|          | 3/827 [00:08<39:25,  2.87s/it]  0%|          | 4/827 [00:12<44:39,  3.26s/it]  1%|          | 5/827 [00:16<51:47,  3.78s/it]  1%|          | 6/827 [00:21<54:32,  3.99s/it]  1%|          | 7/827 [00:26<57:52,  4.23s/it]  1%|          | 8/827 [00:32<1:05:57,  4.83s/it]  1%|          | 9/827 [00:41<1:22:51,  6.08s/it]  1%|          | 10/827 [00:48<1:27:24,  6.42s/it]  1%|▏         | 11/827 [00:52<1:16:52,  5.65s/it]  1%|▏         | 12/827 [00:56<1:09:59,  5.15s/it]  2%|▏         | 13/827 [00:59<1:03:48,  4.70s/it]  2%|▏         | 14/827 [01:04<1:02:37,  4.62s/it]  2%|▏         | 15/827 [01:08<1:02:47,  4.64s/it]  2%|▏         | 16/827 [01:13<1:00:13,  4.46s/it]  2%|▏         | 17/827 [01:16<56:50,  4.21s/it]    2%|▏         | 18/827 [01:20<56:03,  4.16s/it]  2%|▏         | 19/827 [01:25<1:00:21,  4.48s/it]  2%|▏         | 20/827 [01:31<1:05:07,  4.84s/it]  3%|▎         | 21/827 [01:37<1:08:15,  5.08s/it]  3%|▎         | 22/827 [01:42<1:08:05,  5.08s/it]  3%|▎         | 23/827 [01:45<1:01:34,  4.60s/it]  3%|▎         | 24/827 [01:50<1:01:51,  4.62s/it]  3%|▎         | 25/827 [01:57<1:10:31,  5.28s/it]  3%|▎         | 26/827 [02:04<1:18:34,  5.89s/it]  3%|▎         | 27/827 [02:09<1:14:57,  5.62s/it]  3%|▎         | 28/827 [02:13<1:06:51,  5.02s/it]  4%|▎         | 29/827 [02:18<1:07:39,  5.09s/it]  4%|▎         | 30/827 [02:24<1:11:06,  5.35s/it]  4%|▎         | 31/827 [02:29<1:09:09,  5.21s/it]  4%|▍         | 32/827 [02:34<1:09:06,  5.22s/it]  4%|▍         | 33/827 [02:40<1:13:19,  5.54s/it]  4%|▍         | 34/827 [02:45<1:08:53,  5.21s/it]  4%|▍         | 35/827 [02:49<1:05:44,  4.98s/it]  4%|▍         | 36/827 [02:53<1:00:20,  4.58s/it]  4%|▍         | 37/827 [02:58<1:03:19,  4.81s/it]  5%|▍         | 38/827 [03:05<1:10:58,  5.40s/it]  5%|▍         | 39/827 [03:12<1:15:22,  5.74s/it]  5%|▍         | 40/827 [03:17<1:14:26,  5.68s/it]  5%|▍         | 41/827 [03:21<1:08:41,  5.24s/it]  5%|▌         | 42/827 [03:26<1:06:52,  5.11s/it]  5%|▌         | 43/827 [03:30<1:03:14,  4.84s/it]  5%|▌         | 44/827 [03:34<1:00:10,  4.61s/it]  5%|▌         | 45/827 [03:40<1:04:23,  4.94s/it]  6%|▌         | 46/827 [03:47<1:11:19,  5.48s/it]  6%|▌         | 47/827 [03:52<1:11:14,  5.48s/it]  6%|▌         | 48/827 [03:56<1:05:32,  5.05s/it]  6%|▌         | 49/827 [04:00<1:01:20,  4.73s/it]  6%|▌         | 50/827 [04:04<56:49,  4.39s/it]    6%|▌         | 51/827 [04:10<1:02:43,  4.85s/it]  6%|▋         | 52/827 [04:16<1:08:54,  5.33s/it]  6%|▋         | 53/827 [04:20<1:01:43,  4.78s/it]  7%|▋         | 54/827 [04:24<57:39,  4.47s/it]    7%|▋         | 55/827 [04:27<54:16,  4.22s/it]  7%|▋         | 56/827 [04:31<52:46,  4.11s/it]  7%|▋         | 57/827 [04:36<54:28,  4.25s/it]  7%|▋         | 58/827 [04:40<54:03,  4.22s/it]  7%|▋         | 59/827 [04:44<52:38,  4.11s/it]  7%|▋         | 60/827 [04:49<55:33,  4.35s/it]  7%|▋         | 61/827 [04:54<59:30,  4.66s/it]  7%|▋         | 62/827 [04:57<55:04,  4.32s/it]  8%|▊         | 63/827 [05:01<52:37,  4.13s/it]  8%|▊         | 64/827 [05:05<51:23,  4.04s/it]  8%|▊         | 65/827 [05:09<52:51,  4.16s/it]  8%|▊         | 66/827 [05:15<59:22,  4.68s/it]  8%|▊         | 67/827 [05:20<1:00:57,  4.81s/it]  8%|▊         | 68/827 [05:24<55:17,  4.37s/it]    8%|▊         | 69/827 [05:28<53:07,  4.20s/it]  8%|▊         | 70/827 [05:31<51:02,  4.05s/it]  9%|▊         | 71/827 [05:36<55:30,  4.41s/it]  9%|▊         | 72/827 [05:42<58:07,  4.62s/it]  9%|▉         | 73/827 [05:46<58:08,  4.63s/it]  9%|▉         | 74/827 [05:52<1:03:01,  5.02s/it]  9%|▉         | 75/827 [05:57<1:03:58,  5.10s/it]  9%|▉         | 76/827 [06:01<58:26,  4.67s/it]    9%|▉         | 77/827 [06:06<57:16,  4.58s/it]  9%|▉         | 78/827 [06:10<55:00,  4.41s/it] 10%|▉         | 79/827 [06:13<52:32,  4.21s/it] 10%|▉         | 80/827 [06:18<54:49,  4.40s/it] 10%|▉         | 81/827 [06:24<1:00:28,  4.86s/it] 10%|▉         | 82/827 [06:29<59:19,  4.78s/it]   10%|█         | 83/827 [06:33<59:04,  4.76s/it] 10%|█         | 84/827 [06:39<1:01:57,  5.00s/it] 10%|█         | 85/827 [06:44<1:01:19,  4.96s/it] 10%|█         | 86/827 [06:47<56:02,  4.54s/it]   11%|█         | 87/827 [06:54<1:04:03,  5.19s/it] 11%|█         | 88/827 [07:04<1:20:56,  6.57s/it] 11%|█         | 89/827 [07:12<1:27:17,  7.10s/it] 11%|█         | 90/827 [07:18<1:23:20,  6.78s/it] 11%|█         | 91/827 [07:23<1:13:59,  6.03s/it] 11%|█         | 92/827 [07:27<1:08:10,  5.57s/it] 11%|█         | 93/827 [07:31<1:02:06,  5.08s/it] 11%|█▏        | 94/827 [07:35<57:03,  4.67s/it]   11%|█▏        | 95/827 [07:39<54:14,  4.45s/it] 12%|█▏        | 96/827 [07:42<51:43,  4.24s/it] 12%|█▏        | 97/827 [07:45<47:25,  3.90s/it] 12%|█▏        | 98/827 [07:49<45:15,  3.72s/it] 12%|█▏        | 99/827 [07:52<43:57,  3.62s/it] 12%|█▏        | 100/827 [07:56<43:23,  3.58s/it] 12%|█▏        | 101/827 [08:00<45:30,  3.76s/it] 12%|█▏        | 102/827 [08:04<46:37,  3.86s/it] 12%|█▏        | 103/827 [08:08<46:45,  3.88s/it] 13%|█▎        | 104/827 [08:12<46:49,  3.89s/it] 13%|█▎        | 105/827 [08:16<48:21,  4.02s/it] 13%|█▎        | 106/827 [08:23<58:04,  4.83s/it] 13%|█▎        | 107/827 [08:30<1:07:17,  5.61s/it] 13%|█▎        | 108/827 [08:35<1:05:04,  5.43s/it] 13%|█▎        | 109/827 [08:40<1:03:00,  5.26s/it] 13%|█▎        | 110/827 [08:44<59:22,  4.97s/it]   13%|█▎        | 111/827 [08:50<1:02:03,  5.20s/it] 14%|█▎        | 112/827 [08:56<1:04:59,  5.45s/it] 14%|█▎        | 113/827 [09:01<1:01:31,  5.17s/it] 14%|█▍        | 114/827 [09:05<57:23,  4.83s/it]   14%|█▍        | 115/827 [09:09<54:59,  4.63s/it] 14%|█▍        | 116/827 [09:13<53:51,  4.54s/it] 14%|█▍        | 117/827 [09:18<55:41,  4.71s/it] 14%|█▍        | 118/827 [09:25<1:02:14,  5.27s/it] 14%|█▍        | 119/827 [09:31<1:06:39,  5.65s/it] 15%|█▍        | 120/827 [09:37<1:06:15,  5.62s/it] 15%|█▍        | 121/827 [09:41<1:01:40,  5.24s/it] 15%|█▍        | 122/827 [09:45<56:22,  4.80s/it]   15%|█▍        | 123/827 [09:48<51:16,  4.37s/it] 15%|█▍        | 124/827 [09:53<50:47,  4.34s/it] 15%|█▌        | 125/827 [09:57<51:02,  4.36s/it] 15%|█▌        | 126/827 [10:01<50:37,  4.33s/it] 15%|█▌        | 127/827 [10:06<51:25,  4.41s/it] 15%|█▌        | 128/827 [10:10<51:19,  4.41s/it] 16%|█▌        | 129/827 [10:15<50:39,  4.35s/it] 16%|█▌        | 130/827 [10:19<50:33,  4.35s/it] 16%|█▌        | 131/827 [10:23<51:00,  4.40s/it] 16%|█▌        | 132/827 [10:29<54:41,  4.72s/it] 16%|█▌        | 133/827 [10:34<55:30,  4.80s/it] 16%|█▌        | 134/827 [10:38<53:35,  4.64s/it] 16%|█▋        | 135/827 [10:44<57:14,  4.96s/it] 16%|█▋        | 136/827 [10:50<1:01:36,  5.35s/it] 17%|█▋        | 137/827 [10:55<58:56,  5.13s/it]   17%|█▋        | 138/827 [11:01<1:02:58,  5.48s/it] 17%|█▋        | 139/827 [11:08<1:08:15,  5.95s/it] 17%|█▋        | 140/827 [11:13<1:05:26,  5.72s/it] 17%|█▋        | 141/827 [11:18<1:01:21,  5.37s/it] 17%|█▋        | 142/827 [11:22<57:39,  5.05s/it]   17%|█▋        | 143/827 [11:26<53:52,  4.73s/it] 17%|█▋        | 144/827 [11:30<49:59,  4.39s/it] 18%|█▊        | 145/827 [11:34<49:37,  4.37s/it] 18%|█▊        | 146/827 [11:38<49:39,  4.38s/it] 18%|█▊        | 147/827 [11:42<47:59,  4.23s/it] 18%|█▊        | 148/827 [11:46<46:46,  4.13s/it] 18%|█▊        | 149/827 [11:50<45:29,  4.03s/it] 18%|█▊        | 150/827 [11:54<43:47,  3.88s/it] 18%|█▊        | 151/827 [11:57<41:40,  3.70s/it] 18%|█▊        | 152/827 [12:01<43:17,  3.85s/it] 19%|█▊        | 153/827 [12:06<45:39,  4.06s/it] 19%|█▊        | 154/827 [12:10<47:49,  4.26s/it] 19%|█▊        | 155/827 [12:16<51:44,  4.62s/it] 19%|█▉        | 156/827 [12:21<53:10,  4.75s/it] 19%|█▉        | 157/827 [12:25<50:07,  4.49s/it] 19%|█▉        | 158/827 [12:29<48:41,  4.37s/it] 19%|█▉        | 159/827 [12:34<50:30,  4.54s/it] 19%|█▉        | 160/827 [12:39<52:08,  4.69s/it] 19%|█▉        | 161/827 [12:42<47:18,  4.26s/it] 20%|█▉        | 162/827 [12:46<47:38,  4.30s/it] 20%|█▉        | 163/827 [12:52<50:16,  4.54s/it] 20%|█▉        | 164/827 [12:56<48:33,  4.39s/it] 20%|█▉        | 165/827 [13:00<47:11,  4.28s/it] 20%|██        | 166/827 [13:06<55:07,  5.00s/it] 20%|██        | 167/827 [13:15<1:08:21,  6.22s/it] 20%|██        | 168/827 [13:22<1:08:45,  6.26s/it] 20%|██        | 169/827 [13:25<1:00:30,  5.52s/it] 21%|██        | 170/827 [13:30<57:22,  5.24s/it]   21%|██        | 171/827 [13:35<55:51,  5.11s/it] 21%|██        | 172/827 [13:39<51:41,  4.73s/it] 21%|██        | 173/827 [13:42<47:35,  4.37s/it] 21%|██        | 174/827 [13:47<47:40,  4.38s/it] 21%|██        | 175/827 [13:51<47:24,  4.36s/it] 21%|██▏       | 176/827 [13:55<44:47,  4.13s/it] 21%|██▏       | 177/827 [13:58<42:26,  3.92s/it] 22%|██▏       | 178/827 [14:03<45:01,  4.16s/it] 22%|██▏       | 179/827 [14:09<51:10,  4.74s/it] 22%|██▏       | 180/827 [14:13<49:55,  4.63s/it] 22%|██▏       | 181/827 [14:18<50:33,  4.70s/it] 22%|██▏       | 182/827 [14:23<51:35,  4.80s/it] 22%|██▏       | 183/827 [14:27<48:49,  4.55s/it] 22%|██▏       | 184/827 [14:32<49:43,  4.64s/it] 22%|██▏       | 185/827 [14:37<51:22,  4.80s/it] 22%|██▏       | 186/827 [14:41<47:33,  4.45s/it] 23%|██▎       | 187/827 [14:45<45:34,  4.27s/it] 23%|██▎       | 188/827 [14:49<47:35,  4.47s/it] 23%|██▎       | 189/827 [14:55<49:49,  4.69s/it] 23%|██▎       | 190/827 [14:59<49:38,  4.68s/it] 23%|██▎       | 191/827 [15:04<49:22,  4.66s/it] 23%|██▎       | 192/827 [15:09<50:40,  4.79s/it] 23%|██▎       | 193/827 [15:14<52:34,  4.98s/it] 23%|██▎       | 194/827 [15:19<50:52,  4.82s/it] 24%|██▎       | 195/827 [15:23<47:23,  4.50s/it] 24%|██▎       | 196/827 [15:26<44:02,  4.19s/it] 24%|██▍       | 197/827 [15:30<41:48,  3.98s/it] 24%|██▍       | 198/827 [15:34<41:31,  3.96s/it] 24%|██▍       | 199/827 [15:38<44:31,  4.25s/it] 24%|██▍       | 200/827 [15:44<48:51,  4.68s/it] 24%|██▍       | 201/827 [15:50<53:05,  5.09s/it] 24%|██▍       | 202/827 [15:56<54:51,  5.27s/it] 25%|██▍       | 203/827 [16:03<59:54,  5.76s/it] 25%|██▍       | 204/827 [16:09<1:00:12,  5.80s/it] 25%|██▍       | 205/827 [16:13<54:29,  5.26s/it]   25%|██▍       | 206/827 [16:17<52:11,  5.04s/it] 25%|██▌       | 207/827 [16:21<49:35,  4.80s/it] 25%|██▌       | 208/827 [16:26<47:55,  4.65s/it] 25%|██▌       | 209/827 [16:30<45:56,  4.46s/it] 25%|██▌       | 210/827 [16:34<44:24,  4.32s/it] 26%|██▌       | 211/827 [16:39<47:15,  4.60s/it] 26%|██▌       | 212/827 [16:46<53:34,  5.23s/it] 26%|██▌       | 213/827 [16:51<55:09,  5.39s/it] 26%|██▌       | 214/827 [16:55<50:46,  4.97s/it] 26%|██▌       | 215/827 [16:59<46:52,  4.60s/it] 26%|██▌       | 216/827 [17:04<46:28,  4.56s/it] 26%|██▌       | 217/827 [17:09<49:03,  4.82s/it] 26%|██▋       | 218/827 [17:14<48:35,  4.79s/it] 26%|██▋       | 219/827 [17:18<45:51,  4.53s/it] 27%|██▋       | 220/827 [17:23<47:26,  4.69s/it] 27%|██▋       | 221/827 [17:29<50:31,  5.00s/it] 27%|██▋       | 222/827 [17:33<49:09,  4.88s/it] 27%|██▋       | 223/827 [17:37<47:15,  4.69s/it] 27%|██▋       | 224/827 [17:42<45:49,  4.56s/it] 27%|██▋       | 225/827 [17:46<43:47,  4.37s/it] 27%|██▋       | 226/827 [17:49<40:17,  4.02s/it] 27%|██▋       | 227/827 [17:54<43:14,  4.32s/it] 28%|██▊       | 228/827 [17:59<46:31,  4.66s/it] 28%|██▊       | 229/827 [18:04<46:36,  4.68s/it] 28%|██▊       | 230/827 [18:09<48:36,  4.88s/it] 28%|██▊       | 231/827 [18:13<46:14,  4.65s/it] 28%|██▊       | 232/827 [18:17<43:48,  4.42s/it] 28%|██▊       | 233/827 [18:23<46:21,  4.68s/it] 28%|██▊       | 234/827 [18:30<53:44,  5.44s/it] 28%|██▊       | 235/827 [18:36<55:50,  5.66s/it] 29%|██▊       | 236/827 [18:40<52:18,  5.31s/it] 29%|██▊       | 237/827 [18:47<55:23,  5.63s/it] 29%|██▉       | 238/827 [18:53<57:25,  5.85s/it] 29%|██▉       | 239/827 [18:59<57:21,  5.85s/it] 29%|██▉       | 240/827 [19:07<1:02:21,  6.37s/it] 29%|██▉       | 241/827 [19:13<1:00:58,  6.24s/it] 29%|██▉       | 242/827 [19:18<57:44,  5.92s/it]   29%|██▉       | 243/827 [19:24<58:15,  5.98s/it] 30%|██▉       | 244/827 [19:30<57:50,  5.95s/it] 30%|██▉       | 245/827 [19:35<54:38,  5.63s/it] 30%|██▉       | 246/827 [19:39<50:31,  5.22s/it] 30%|██▉       | 247/827 [19:44<49:33,  5.13s/it] 30%|██▉       | 248/827 [19:51<54:31,  5.65s/it] 30%|███       | 249/827 [19:56<52:57,  5.50s/it] 30%|███       | 250/827 [19:59<45:36,  4.74s/it] 30%|███       | 251/827 [20:04<45:27,  4.74s/it] 30%|███       | 252/827 [20:09<47:29,  4.96s/it] 31%|███       | 253/827 [20:12<43:00,  4.50s/it] 31%|███       | 254/827 [20:18<47:02,  4.93s/it] 31%|███       | 255/827 [20:27<56:14,  5.90s/it] 31%|███       | 256/827 [20:33<57:05,  6.00s/it] 31%|███       | 257/827 [20:38<54:10,  5.70s/it] 31%|███       | 258/827 [20:43<52:14,  5.51s/it] 31%|███▏      | 259/827 [20:49<54:23,  5.74s/it] 31%|███▏      | 260/827 [20:55<55:22,  5.86s/it] 32%|███▏      | 261/827 [21:00<53:03,  5.62s/it] 32%|███▏      | 262/827 [21:07<55:09,  5.86s/it] 32%|███▏      | 263/827 [21:13<57:30,  6.12s/it] 32%|███▏      | 264/827 [21:17<49:44,  5.30s/it] 32%|███▏      | 265/827 [21:20<44:59,  4.80s/it] 32%|███▏      | 266/827 [21:24<41:58,  4.49s/it] 32%|███▏      | 267/827 [21:28<39:39,  4.25s/it] 32%|███▏      | 268/827 [21:32<39:37,  4.25s/it] 33%|███▎      | 269/827 [21:38<43:21,  4.66s/it] 33%|███▎      | 270/827 [21:43<45:28,  4.90s/it] 33%|███▎      | 271/827 [21:48<43:36,  4.71s/it] 33%|███▎      | 272/827 [21:52<42:31,  4.60s/it] 33%|███▎      | 273/827 [21:58<47:16,  5.12s/it] 33%|███▎      | 274/827 [22:07<56:47,  6.16s/it] 33%|███▎      | 275/827 [22:13<56:58,  6.19s/it] 33%|███▎      | 276/827 [22:18<53:24,  5.82s/it] 33%|███▎      | 277/827 [22:23<51:05,  5.57s/it] 34%|███▎      | 278/827 [22:28<48:05,  5.26s/it] 34%|███▎      | 279/827 [22:31<44:18,  4.85s/it] 34%|███▍      | 280/827 [22:35<41:33,  4.56s/it] 34%|███▍      | 281/827 [22:40<40:56,  4.50s/it] 34%|███▍      | 282/827 [22:44<41:18,  4.55s/it] 34%|███▍      | 283/827 [22:49<41:42,  4.60s/it] 34%|███▍      | 284/827 [22:54<42:48,  4.73s/it] 34%|███▍      | 285/827 [23:00<45:30,  5.04s/it] 35%|███▍      | 286/827 [23:05<46:49,  5.19s/it] 35%|███▍      | 287/827 [23:09<42:35,  4.73s/it] 35%|███▍      | 288/827 [23:13<40:33,  4.51s/it] 35%|███▍      | 289/827 [23:17<39:49,  4.44s/it] 35%|███▌      | 290/827 [23:22<39:10,  4.38s/it] 35%|███▌      | 291/827 [23:25<37:32,  4.20s/it] 35%|███▌      | 292/827 [23:29<36:44,  4.12s/it] 35%|███▌      | 293/827 [23:33<36:51,  4.14s/it] 36%|███▌      | 294/827 [23:38<37:51,  4.26s/it] 36%|███▌      | 295/827 [23:43<39:45,  4.48s/it] 36%|███▌      | 296/827 [23:48<41:14,  4.66s/it] 36%|███▌      | 297/827 [23:53<41:01,  4.64s/it] 36%|███▌      | 298/827 [23:57<39:27,  4.48s/it] 36%|███▌      | 299/827 [24:01<37:58,  4.32s/it] 36%|███▋      | 300/827 [24:04<35:59,  4.10s/it] 36%|███▋      | 301/827 [24:08<34:37,  3.95s/it] 37%|███▋      | 302/827 [24:12<34:40,  3.96s/it] 37%|███▋      | 303/827 [24:15<32:47,  3.75s/it] 37%|███▋      | 304/827 [24:19<33:10,  3.81s/it] 37%|███▋      | 305/827 [24:24<36:28,  4.19s/it] 37%|███▋      | 306/827 [24:29<38:48,  4.47s/it] 37%|███▋      | 307/827 [24:33<37:04,  4.28s/it] 37%|███▋      | 308/827 [24:37<36:16,  4.19s/it] 37%|███▋      | 309/827 [24:41<35:36,  4.12s/it] 37%|███▋      | 310/827 [24:45<33:56,  3.94s/it] 38%|███▊      | 311/827 [24:48<32:52,  3.82s/it] 38%|███▊      | 312/827 [24:53<35:46,  4.17s/it] 38%|███▊      | 313/827 [24:58<37:43,  4.40s/it] 38%|███▊      | 314/827 [25:02<36:42,  4.29s/it] 38%|███▊      | 315/827 [25:07<38:06,  4.47s/it] 38%|███▊      | 316/827 [25:13<40:57,  4.81s/it] 38%|███▊      | 317/827 [25:19<45:32,  5.36s/it] 38%|███▊      | 318/827 [25:25<47:01,  5.54s/it] 39%|███▊      | 319/827 [25:30<44:08,  5.21s/it] 39%|███▊      | 320/827 [25:34<42:05,  4.98s/it] 39%|███▉      | 321/827 [25:39<42:41,  5.06s/it] 39%|███▉      | 322/827 [25:44<41:51,  4.97s/it] 39%|███▉      | 323/827 [25:49<42:40,  5.08s/it] 39%|███▉      | 324/827 [25:55<44:13,  5.28s/it] 39%|███▉      | 325/827 [25:59<41:13,  4.93s/it] 39%|███▉      | 326/827 [26:04<41:02,  4.92s/it] 40%|███▉      | 327/827 [26:09<41:10,  4.94s/it] 40%|███▉      | 328/827 [26:14<41:49,  5.03s/it] 40%|███▉      | 329/827 [26:21<45:21,  5.46s/it] 40%|███▉      | 330/827 [26:27<48:01,  5.80s/it] 40%|████      | 331/827 [26:33<46:27,  5.62s/it] 40%|████      | 332/827 [26:38<44:55,  5.44s/it] 40%|████      | 333/827 [26:43<45:10,  5.49s/it] 40%|████      | 334/827 [26:48<42:51,  5.22s/it] 41%|████      | 335/827 [26:52<40:10,  4.90s/it] 41%|████      | 336/827 [26:56<38:06,  4.66s/it] 41%|████      | 337/827 [27:01<37:42,  4.62s/it] 41%|████      | 338/827 [27:04<35:39,  4.37s/it] 41%|████      | 339/827 [27:08<34:19,  4.22s/it] 41%|████      | 340/827 [27:12<33:59,  4.19s/it] 41%|████      | 341/827 [27:17<35:40,  4.40s/it] 41%|████▏     | 342/827 [27:22<35:15,  4.36s/it] 41%|████▏     | 343/827 [27:26<35:40,  4.42s/it] 42%|████▏     | 344/827 [27:32<38:33,  4.79s/it] 42%|████▏     | 345/827 [27:37<40:07,  5.00s/it] 42%|████▏     | 346/827 [27:42<38:52,  4.85s/it] 42%|████▏     | 347/827 [27:46<38:19,  4.79s/it] 42%|████▏     | 348/827 [27:51<36:34,  4.58s/it] 42%|████▏     | 349/827 [27:56<37:35,  4.72s/it] 42%|████▏     | 350/827 [28:01<39:59,  5.03s/it] 42%|████▏     | 351/827 [28:06<38:27,  4.85s/it] 43%|████▎     | 352/827 [28:11<38:07,  4.82s/it] 43%|████▎     | 353/827 [28:15<36:12,  4.58s/it] 43%|████▎     | 354/827 [28:19<35:30,  4.50s/it] 43%|████▎     | 355/827 [28:24<38:03,  4.84s/it] 43%|████▎     | 356/827 [28:31<40:58,  5.22s/it] 43%|████▎     | 357/827 [28:35<39:26,  5.04s/it] 43%|████▎     | 358/827 [28:39<36:09,  4.63s/it] 43%|████▎     | 359/827 [28:43<34:53,  4.47s/it] 44%|████▎     | 360/827 [28:48<36:54,  4.74s/it] 44%|████▎     | 361/827 [28:53<36:33,  4.71s/it] 44%|████▍     | 362/827 [28:56<33:24,  4.31s/it] 44%|████▍     | 363/827 [29:01<34:13,  4.43s/it] 44%|████▍     | 364/827 [29:06<34:41,  4.49s/it] 44%|████▍     | 365/827 [29:10<34:39,  4.50s/it] 44%|████▍     | 366/827 [29:16<38:01,  4.95s/it] 44%|████▍     | 367/827 [29:21<38:34,  5.03s/it] 44%|████▍     | 368/827 [29:25<35:54,  4.69s/it] 45%|████▍     | 369/827 [29:29<34:29,  4.52s/it] 45%|████▍     | 370/827 [29:34<34:39,  4.55s/it] 45%|████▍     | 371/827 [29:39<35:06,  4.62s/it] 45%|████▍     | 372/827 [29:43<34:15,  4.52s/it] 45%|████▌     | 373/827 [29:49<36:27,  4.82s/it] 45%|████▌     | 374/827 [29:56<41:27,  5.49s/it] 45%|████▌     | 375/827 [30:04<48:36,  6.45s/it] 45%|████▌     | 376/827 [30:13<53:58,  7.18s/it] 46%|████▌     | 377/827 [30:18<49:02,  6.54s/it] 46%|████▌     | 378/827 [30:25<48:50,  6.53s/it] 46%|████▌     | 379/827 [30:34<54:53,  7.35s/it] 46%|████▌     | 380/827 [30:41<52:36,  7.06s/it] 46%|████▌     | 381/827 [30:45<47:14,  6.35s/it] 46%|████▌     | 382/827 [30:50<44:42,  6.03s/it] 46%|████▋     | 383/827 [30:56<43:20,  5.86s/it] 46%|████▋     | 384/827 [31:00<39:42,  5.38s/it] 47%|████▋     | 385/827 [31:04<37:04,  5.03s/it] 47%|████▋     | 386/827 [31:09<35:16,  4.80s/it] 47%|████▋     | 387/827 [31:13<34:22,  4.69s/it] 47%|████▋     | 388/827 [31:18<34:45,  4.75s/it] 47%|████▋     | 389/827 [31:23<35:26,  4.86s/it] 47%|████▋     | 390/827 [31:27<34:04,  4.68s/it] 47%|████▋     | 391/827 [31:31<32:13,  4.43s/it] 47%|████▋     | 392/827 [31:36<32:52,  4.53s/it] 48%|████▊     | 393/827 [31:42<35:07,  4.86s/it] 48%|████▊     | 394/827 [31:47<36:35,  5.07s/it] 48%|████▊     | 395/827 [31:54<39:26,  5.48s/it] 48%|████▊     | 396/827 [31:59<38:37,  5.38s/it] 48%|████▊     | 397/827 [32:03<35:20,  4.93s/it] 48%|████▊     | 398/827 [32:07<34:12,  4.78s/it] 48%|████▊     | 399/827 [32:12<33:30,  4.70s/it] 48%|████▊     | 400/827 [32:17<34:08,  4.80s/it] 48%|████▊     | 401/827 [32:22<34:51,  4.91s/it] 49%|████▊     | 402/827 [32:27<35:27,  5.01s/it] 49%|████▊     | 403/827 [32:32<35:53,  5.08s/it] 49%|████▉     | 404/827 [32:36<32:35,  4.62s/it] 49%|████▉     | 405/827 [32:40<31:50,  4.53s/it] 49%|████▉     | 406/827 [32:45<31:40,  4.51s/it] 49%|████▉     | 407/827 [32:48<30:02,  4.29s/it] 49%|████▉     | 408/827 [32:53<29:38,  4.24s/it] 49%|████▉     | 409/827 [32:57<29:15,  4.20s/it] 50%|████▉     | 410/827 [33:00<27:52,  4.01s/it] 50%|████▉     | 411/827 [33:04<27:42,  4.00s/it] 50%|████▉     | 412/827 [33:09<28:41,  4.15s/it] 50%|████▉     | 413/827 [33:15<32:16,  4.68s/it] 50%|█████     | 414/827 [33:20<33:38,  4.89s/it] 50%|█████     | 415/827 [33:24<31:00,  4.52s/it] 50%|█████     | 416/827 [33:28<31:36,  4.61s/it] 50%|█████     | 417/827 [33:36<36:59,  5.41s/it] 51%|█████     | 418/827 [33:41<37:23,  5.48s/it] 51%|█████     | 419/827 [33:46<36:18,  5.34s/it] 51%|█████     | 420/827 [33:52<37:47,  5.57s/it] 51%|█████     | 421/827 [33:57<36:11,  5.35s/it] 51%|█████     | 422/827 [34:01<32:39,  4.84s/it] 51%|█████     | 423/827 [34:05<30:27,  4.52s/it] 51%|█████▏    | 424/827 [34:09<29:47,  4.43s/it] 51%|█████▏    | 425/827 [34:15<32:01,  4.78s/it] 52%|█████▏    | 426/827 [34:20<33:43,  5.05s/it] 52%|█████▏    | 427/827 [34:26<34:26,  5.17s/it] 52%|█████▏    | 428/827 [34:32<36:44,  5.53s/it] 52%|█████▏    | 429/827 [34:37<35:03,  5.29s/it] 52%|█████▏    | 430/827 [34:42<34:31,  5.22s/it] 52%|█████▏    | 431/827 [34:48<36:54,  5.59s/it] 52%|█████▏    | 432/827 [34:56<41:45,  6.34s/it] 52%|█████▏    | 433/827 [35:05<46:14,  7.04s/it] 52%|█████▏    | 434/827 [35:11<43:08,  6.59s/it] 53%|█████▎    | 435/827 [35:14<37:36,  5.76s/it] 53%|█████▎    | 436/827 [35:19<35:54,  5.51s/it] 53%|█████▎    | 437/827 [35:24<34:33,  5.32s/it] 53%|█████▎    | 438/827 [35:29<32:42,  5.04s/it] 53%|█████▎    | 439/827 [35:33<32:03,  4.96s/it] 53%|█████▎    | 440/827 [35:39<32:26,  5.03s/it] 53%|█████▎    | 441/827 [35:42<29:42,  4.62s/it] 53%|█████▎    | 442/827 [35:46<27:52,  4.34s/it] 54%|█████▎    | 443/827 [35:50<27:39,  4.32s/it] 54%|█████▎    | 444/827 [35:55<29:22,  4.60s/it] 54%|█████▍    | 445/827 [36:01<31:23,  4.93s/it] 54%|█████▍    | 446/827 [36:06<30:51,  4.86s/it] 54%|█████▍    | 447/827 [36:11<30:50,  4.87s/it] 54%|█████▍    | 448/827 [36:17<33:49,  5.35s/it] 54%|█████▍    | 449/827 [36:25<38:20,  6.09s/it] 54%|█████▍    | 450/827 [36:31<37:25,  5.96s/it] 55%|█████▍    | 451/827 [36:35<34:16,  5.47s/it] 55%|█████▍    | 452/827 [36:41<34:59,  5.60s/it] 55%|█████▍    | 453/827 [36:47<36:14,  5.81s/it] 55%|█████▍    | 454/827 [36:52<33:35,  5.40s/it] 55%|█████▌    | 455/827 [36:57<33:38,  5.43s/it] 55%|█████▌    | 456/827 [37:03<35:08,  5.68s/it] 55%|█████▌    | 457/827 [37:08<33:39,  5.46s/it] 55%|█████▌    | 458/827 [37:13<32:27,  5.28s/it] 56%|█████▌    | 459/827 [37:18<32:10,  5.25s/it] 56%|█████▌    | 460/827 [37:25<34:11,  5.59s/it] 56%|█████▌    | 461/827 [37:32<37:40,  6.18s/it] 56%|█████▌    | 462/827 [37:38<36:25,  5.99s/it] 56%|█████▌    | 463/827 [37:42<32:22,  5.34s/it] 56%|█████▌    | 464/827 [37:47<32:02,  5.30s/it] 56%|█████▌    | 465/827 [37:52<32:22,  5.37s/it] 56%|█████▋    | 466/827 [37:57<31:18,  5.20s/it] 56%|█████▋    | 467/827 [38:03<31:38,  5.27s/it] 57%|█████▋    | 468/827 [38:09<32:51,  5.49s/it] 57%|█████▋    | 469/827 [38:14<32:19,  5.42s/it] 57%|█████▋    | 470/827 [38:18<29:49,  5.01s/it] 57%|█████▋    | 471/827 [38:24<31:40,  5.34s/it] 57%|█████▋    | 472/827 [38:31<33:54,  5.73s/it] 57%|█████▋    | 473/827 [38:37<34:05,  5.78s/it] 57%|█████▋    | 474/827 [38:42<32:28,  5.52s/it] 57%|█████▋    | 475/827 [38:45<29:15,  4.99s/it] 58%|█████▊    | 476/827 [38:48<25:42,  4.40s/it] 58%|█████▊    | 477/827 [38:52<23:39,  4.06s/it] 58%|█████▊    | 478/827 [38:57<25:11,  4.33s/it] 58%|█████▊    | 479/827 [39:02<26:43,  4.61s/it] 58%|█████▊    | 480/827 [39:07<27:17,  4.72s/it] 58%|█████▊    | 481/827 [39:12<28:21,  4.92s/it] 58%|█████▊    | 482/827 [39:16<26:34,  4.62s/it] 58%|█████▊    | 483/827 [39:20<24:37,  4.29s/it] 59%|█████▊    | 484/827 [39:23<23:42,  4.15s/it] 59%|█████▊    | 485/827 [39:27<23:31,  4.13s/it] 59%|█████▉    | 486/827 [39:32<23:42,  4.17s/it] 59%|█████▉    | 487/827 [39:36<24:35,  4.34s/it] 59%|█████▉    | 488/827 [39:42<25:59,  4.60s/it] 59%|█████▉    | 489/827 [39:48<28:06,  4.99s/it] 59%|█████▉    | 490/827 [39:53<28:17,  5.04s/it] 59%|█████▉    | 491/827 [39:57<26:26,  4.72s/it] 59%|█████▉    | 492/827 [40:01<25:12,  4.52s/it] 60%|█████▉    | 493/827 [40:05<24:12,  4.35s/it] 60%|█████▉    | 494/827 [40:10<24:52,  4.48s/it] 60%|█████▉    | 495/827 [40:15<26:05,  4.71s/it] 60%|█████▉    | 496/827 [40:19<25:42,  4.66s/it] 60%|██████    | 497/827 [40:23<24:39,  4.48s/it] 60%|██████    | 498/827 [40:28<24:33,  4.48s/it] 60%|██████    | 499/827 [40:33<25:04,  4.59s/it] 60%|██████    | 500/827 [40:37<24:20,  4.47s/it] 61%|██████    | 501/827 [40:40<22:33,  4.15s/it] 61%|██████    | 502/827 [40:45<23:47,  4.39s/it] 61%|██████    | 503/827 [40:51<25:24,  4.71s/it] 61%|██████    | 504/827 [40:55<24:19,  4.52s/it] 61%|██████    | 505/827 [40:59<23:48,  4.44s/it] 61%|██████    | 506/827 [41:04<24:08,  4.51s/it] 61%|██████▏   | 507/827 [41:11<28:14,  5.30s/it] 61%|██████▏   | 508/827 [41:17<30:18,  5.70s/it] 62%|██████▏   | 509/827 [41:21<27:32,  5.20s/it] 62%|██████▏   | 510/827 [41:26<25:54,  4.90s/it] 62%|██████▏   | 511/827 [41:30<24:14,  4.60s/it] 62%|██████▏   | 512/827 [41:33<22:39,  4.31s/it] 62%|██████▏   | 513/827 [41:38<22:44,  4.34s/it] 62%|██████▏   | 514/827 [41:42<22:30,  4.32s/it] 62%|██████▏   | 515/827 [41:46<21:51,  4.20s/it] 62%|██████▏   | 516/827 [41:50<22:23,  4.32s/it] 63%|██████▎   | 517/827 [41:54<21:45,  4.21s/it] 63%|██████▎   | 518/827 [41:58<21:16,  4.13s/it] 63%|██████▎   | 519/827 [42:02<20:27,  3.99s/it] 63%|██████▎   | 520/827 [42:06<20:33,  4.02s/it] 63%|██████▎   | 521/827 [42:12<23:34,  4.62s/it] 63%|██████▎   | 522/827 [42:18<24:55,  4.90s/it] 63%|██████▎   | 523/827 [42:22<24:03,  4.75s/it] 63%|██████▎   | 524/827 [42:27<23:58,  4.75s/it] 63%|██████▎   | 525/827 [42:33<26:13,  5.21s/it] 64%|██████▎   | 526/827 [42:40<28:00,  5.58s/it] 64%|██████▎   | 527/827 [42:44<26:29,  5.30s/it] 64%|██████▍   | 528/827 [42:49<26:16,  5.27s/it] 64%|██████▍   | 529/827 [42:55<26:28,  5.33s/it] 64%|██████▍   | 530/827 [43:00<25:56,  5.24s/it] 64%|██████▍   | 531/827 [43:04<24:09,  4.90s/it] 64%|██████▍   | 532/827 [43:08<23:12,  4.72s/it] 64%|██████▍   | 533/827 [43:14<24:22,  4.97s/it] 65%|██████▍   | 534/827 [43:21<27:10,  5.57s/it] 65%|██████▍   | 535/827 [43:27<27:53,  5.73s/it] 65%|██████▍   | 536/827 [43:31<25:45,  5.31s/it] 65%|██████▍   | 537/827 [43:35<23:19,  4.83s/it] 65%|██████▌   | 538/827 [43:39<21:45,  4.52s/it] 65%|██████▌   | 539/827 [43:43<21:11,  4.41s/it] 65%|██████▌   | 540/827 [43:47<21:20,  4.46s/it] 65%|██████▌   | 541/827 [43:52<21:31,  4.52s/it] 66%|██████▌   | 542/827 [43:57<22:01,  4.64s/it] 66%|██████▌   | 543/827 [44:02<22:27,  4.75s/it] 66%|██████▌   | 544/827 [44:07<23:08,  4.91s/it] 66%|██████▌   | 545/827 [44:13<23:46,  5.06s/it] 66%|██████▌   | 546/827 [44:18<24:09,  5.16s/it] 66%|██████▌   | 547/827 [44:23<23:45,  5.09s/it] 66%|██████▋   | 548/827 [44:26<21:20,  4.59s/it] 66%|██████▋   | 549/827 [44:30<20:19,  4.39s/it] 67%|██████▋   | 550/827 [44:36<21:16,  4.61s/it] 67%|██████▋   | 551/827 [44:40<21:39,  4.71s/it] 67%|██████▋   | 552/827 [44:45<21:36,  4.71s/it] 67%|██████▋   | 553/827 [44:49<20:41,  4.53s/it] 67%|██████▋   | 554/827 [44:53<19:08,  4.21s/it] 67%|██████▋   | 555/827 [44:57<19:02,  4.20s/it] 67%|██████▋   | 556/827 [45:01<18:45,  4.15s/it] 67%|██████▋   | 557/827 [45:05<17:54,  3.98s/it] 67%|██████▋   | 558/827 [45:09<18:32,  4.13s/it] 68%|██████▊   | 559/827 [45:14<19:42,  4.41s/it] 68%|██████▊   | 560/827 [45:18<19:29,  4.38s/it] 68%|██████▊   | 561/827 [45:23<19:46,  4.46s/it] 68%|██████▊   | 562/827 [45:28<19:41,  4.46s/it] 68%|██████▊   | 563/827 [45:32<19:16,  4.38s/it] 68%|██████▊   | 564/827 [45:36<18:43,  4.27s/it] 68%|██████▊   | 565/827 [45:41<19:53,  4.55s/it] 68%|██████▊   | 566/827 [45:46<20:23,  4.69s/it] 69%|██████▊   | 567/827 [45:50<19:37,  4.53s/it] 69%|██████▊   | 568/827 [45:55<19:36,  4.54s/it] 69%|██████▉   | 569/827 [45:59<19:13,  4.47s/it] 69%|██████▉   | 570/827 [46:04<20:22,  4.76s/it] 69%|██████▉   | 571/827 [46:11<22:30,  5.28s/it] 69%|██████▉   | 572/827 [46:18<24:19,  5.73s/it] 69%|██████▉   | 573/827 [46:23<24:21,  5.76s/it] 69%|██████▉   | 574/827 [46:29<23:45,  5.63s/it] 70%|██████▉   | 575/827 [46:34<23:40,  5.64s/it] 70%|██████▉   | 576/827 [46:39<22:01,  5.27s/it] 70%|██████▉   | 577/827 [46:43<20:54,  5.02s/it] 70%|██████▉   | 578/827 [46:48<20:05,  4.84s/it] 70%|███████   | 579/827 [46:52<18:43,  4.53s/it] 70%|███████   | 580/827 [46:57<20:11,  4.90s/it] 70%|███████   | 581/827 [47:05<22:53,  5.58s/it] 70%|███████   | 582/827 [47:09<21:53,  5.36s/it] 70%|███████   | 583/827 [47:14<20:34,  5.06s/it] 71%|███████   | 584/827 [47:18<19:14,  4.75s/it] 71%|███████   | 585/827 [47:22<18:05,  4.48s/it] 71%|███████   | 586/827 [47:26<18:05,  4.50s/it] 71%|███████   | 587/827 [47:34<21:55,  5.48s/it] 71%|███████   | 588/827 [47:42<25:28,  6.40s/it] 71%|███████   | 589/827 [47:48<24:14,  6.11s/it] 71%|███████▏  | 590/827 [47:55<25:20,  6.42s/it] 71%|███████▏  | 591/827 [48:02<25:59,  6.61s/it] 72%|███████▏  | 592/827 [48:06<22:55,  5.85s/it] 72%|███████▏  | 593/827 [48:10<20:43,  5.31s/it] 72%|███████▏  | 594/827 [48:14<19:06,  4.92s/it] 72%|███████▏  | 595/827 [48:19<18:38,  4.82s/it] 72%|███████▏  | 596/827 [48:26<21:06,  5.48s/it] 72%|███████▏  | 597/827 [48:32<22:07,  5.77s/it] 72%|███████▏  | 598/827 [48:36<19:57,  5.23s/it] 72%|███████▏  | 599/827 [48:40<18:38,  4.91s/it] 73%|███████▎  | 600/827 [48:45<18:32,  4.90s/it] 73%|███████▎  | 601/827 [48:51<19:13,  5.10s/it] 73%|███████▎  | 602/827 [48:55<18:30,  4.94s/it] 73%|███████▎  | 603/827 [48:59<17:25,  4.67s/it] 73%|███████▎  | 604/827 [49:04<16:41,  4.49s/it] 73%|███████▎  | 605/827 [49:08<16:12,  4.38s/it] 73%|███████▎  | 606/827 [49:12<16:36,  4.51s/it] 73%|███████▎  | 607/827 [49:17<16:41,  4.55s/it] 74%|███████▎  | 608/827 [49:21<16:12,  4.44s/it] 74%|███████▎  | 609/827 [49:28<18:50,  5.19s/it] 74%|███████▍  | 610/827 [49:36<21:18,  5.89s/it] 74%|███████▍  | 611/827 [49:41<19:59,  5.55s/it] 74%|███████▍  | 612/827 [49:45<19:15,  5.37s/it] 74%|███████▍  | 613/827 [49:51<19:03,  5.34s/it] 74%|███████▍  | 614/827 [49:56<18:42,  5.27s/it] 74%|███████▍  | 615/827 [50:00<17:44,  5.02s/it] 74%|███████▍  | 616/827 [50:04<16:19,  4.64s/it] 75%|███████▍  | 617/827 [50:09<16:55,  4.84s/it] 75%|███████▍  | 618/827 [50:16<18:56,  5.44s/it] 75%|███████▍  | 619/827 [50:21<17:52,  5.15s/it] 75%|███████▍  | 620/827 [50:25<16:49,  4.88s/it] 75%|███████▌  | 621/827 [50:29<16:03,  4.68s/it] 75%|███████▌  | 622/827 [50:33<15:28,  4.53s/it] 75%|███████▌  | 623/827 [50:38<15:23,  4.53s/it] 75%|███████▌  | 624/827 [50:43<15:52,  4.69s/it] 76%|███████▌  | 625/827 [50:48<15:58,  4.74s/it] 76%|███████▌  | 626/827 [50:53<16:01,  4.78s/it] 76%|███████▌  | 627/827 [50:57<15:26,  4.63s/it] 76%|███████▌  | 628/827 [51:01<14:19,  4.32s/it] 76%|███████▌  | 629/827 [51:04<13:41,  4.15s/it] 76%|███████▌  | 630/827 [51:09<14:08,  4.30s/it] 76%|███████▋  | 631/827 [51:15<15:38,  4.79s/it] 76%|███████▋  | 632/827 [51:21<16:25,  5.06s/it] 77%|███████▋  | 633/827 [51:27<18:11,  5.63s/it] 77%|███████▋  | 634/827 [51:35<19:40,  6.11s/it] 77%|███████▋  | 635/827 [51:39<17:34,  5.49s/it] 77%|███████▋  | 636/827 [51:43<16:34,  5.21s/it] 77%|███████▋  | 637/827 [51:49<17:01,  5.37s/it] 77%|███████▋  | 638/827 [51:55<17:08,  5.44s/it] 77%|███████▋  | 639/827 [51:58<15:23,  4.91s/it] 77%|███████▋  | 640/827 [52:02<14:29,  4.65s/it] 78%|███████▊  | 641/827 [52:07<14:37,  4.72s/it] 78%|███████▊  | 642/827 [52:11<14:02,  4.55s/it] 78%|███████▊  | 643/827 [52:16<14:24,  4.70s/it] 78%|███████▊  | 644/827 [52:22<15:31,  5.09s/it] 78%|███████▊  | 645/827 [52:27<15:04,  4.97s/it] 78%|███████▊  | 646/827 [52:31<14:14,  4.72s/it] 78%|███████▊  | 647/827 [52:35<13:27,  4.49s/it] 78%|███████▊  | 648/827 [52:40<13:17,  4.45s/it] 78%|███████▊  | 649/827 [52:44<13:14,  4.46s/it] 79%|███████▊  | 650/827 [52:48<12:53,  4.37s/it] 79%|███████▊  | 651/827 [52:52<12:38,  4.31s/it] 79%|███████▉  | 652/827 [52:57<12:21,  4.24s/it] 79%|███████▉  | 653/827 [53:01<12:28,  4.30s/it] 79%|███████▉  | 654/827 [53:07<14:11,  4.92s/it] 79%|███████▉  | 655/827 [53:13<14:54,  5.20s/it] 79%|███████▉  | 656/827 [53:17<13:20,  4.68s/it] 79%|███████▉  | 657/827 [53:20<12:32,  4.42s/it] 80%|███████▉  | 658/827 [53:27<13:51,  4.92s/it] 80%|███████▉  | 659/827 [53:34<16:14,  5.80s/it] 80%|███████▉  | 660/827 [53:42<17:18,  6.22s/it] 80%|███████▉  | 661/827 [53:47<16:52,  6.10s/it] 80%|████████  | 662/827 [53:52<15:24,  5.60s/it] 80%|████████  | 663/827 [53:57<14:51,  5.44s/it] 80%|████████  | 664/827 [54:02<14:46,  5.44s/it] 80%|████████  | 665/827 [54:07<14:22,  5.32s/it] 81%|████████  | 666/827 [54:13<14:05,  5.25s/it] 81%|████████  | 667/827 [54:17<13:16,  4.98s/it] 81%|████████  | 668/827 [54:21<12:38,  4.77s/it] 81%|████████  | 669/827 [54:24<11:26,  4.35s/it] 81%|████████  | 670/827 [54:28<10:46,  4.12s/it] 81%|████████  | 671/827 [54:33<11:03,  4.25s/it] 81%|████████▏ | 672/827 [54:37<10:52,  4.21s/it] 81%|████████▏ | 673/827 [54:41<10:41,  4.17s/it] 81%|████████▏ | 674/827 [54:45<10:38,  4.17s/it] 82%|████████▏ | 675/827 [54:49<10:36,  4.18s/it] 82%|████████▏ | 676/827 [54:55<11:32,  4.59s/it] 82%|████████▏ | 677/827 [55:00<12:12,  4.88s/it] 82%|████████▏ | 678/827 [55:06<12:22,  4.99s/it] 82%|████████▏ | 679/827 [55:11<12:24,  5.03s/it] 82%|████████▏ | 680/827 [55:14<11:25,  4.66s/it] 82%|████████▏ | 681/827 [55:20<12:11,  5.01s/it] 82%|████████▏ | 682/827 [55:27<13:03,  5.40s/it] 83%|████████▎ | 683/827 [55:31<12:00,  5.00s/it] 83%|████████▎ | 684/827 [55:34<10:53,  4.57s/it] 83%|████████▎ | 685/827 [55:38<09:56,  4.20s/it] 83%|████████▎ | 686/827 [55:44<11:27,  4.87s/it] 83%|████████▎ | 687/827 [55:51<12:59,  5.57s/it] 83%|████████▎ | 688/827 [55:57<13:03,  5.64s/it] 83%|████████▎ | 689/827 [56:03<13:04,  5.68s/it] 83%|████████▎ | 690/827 [56:07<12:05,  5.29s/it] 84%|████████▎ | 691/827 [56:11<11:19,  5.00s/it] 84%|████████▎ | 692/827 [56:16<10:54,  4.85s/it] 84%|████████▍ | 693/827 [56:20<10:14,  4.58s/it] 84%|████████▍ | 694/827 [56:24<09:53,  4.46s/it] 84%|████████▍ | 695/827 [56:29<10:19,  4.69s/it] 84%|████████▍ | 696/827 [56:35<10:36,  4.86s/it] 84%|████████▍ | 697/827 [56:39<09:58,  4.61s/it] 84%|████████▍ | 698/827 [56:43<09:31,  4.43s/it] 85%|████████▍ | 699/827 [56:47<09:34,  4.49s/it] 85%|████████▍ | 700/827 [56:54<10:53,  5.14s/it] 85%|████████▍ | 701/827 [57:01<11:46,  5.61s/it] 85%|████████▍ | 702/827 [57:06<11:18,  5.43s/it] 85%|████████▌ | 703/827 [57:10<10:50,  5.25s/it] 85%|████████▌ | 704/827 [57:14<09:49,  4.80s/it] 85%|████████▌ | 705/827 [57:18<08:50,  4.35s/it] 85%|████████▌ | 706/827 [57:21<08:29,  4.21s/it] 85%|████████▌ | 707/827 [57:26<08:31,  4.26s/it] 86%|████████▌ | 708/827 [57:31<08:57,  4.52s/it] 86%|████████▌ | 709/827 [57:35<08:50,  4.49s/it] 86%|████████▌ | 710/827 [57:39<08:32,  4.38s/it] 86%|████████▌ | 711/827 [57:44<08:26,  4.36s/it] 86%|████████▌ | 712/827 [57:48<08:12,  4.28s/it] 86%|████████▌ | 713/827 [57:52<08:09,  4.29s/it] 86%|████████▋ | 714/827 [57:58<08:49,  4.69s/it] 86%|████████▋ | 715/827 [58:03<09:07,  4.88s/it] 87%|████████▋ | 716/827 [58:08<09:03,  4.90s/it] 87%|████████▋ | 717/827 [58:15<10:00,  5.46s/it] 87%|████████▋ | 718/827 [58:20<09:52,  5.44s/it] 87%|████████▋ | 719/827 [58:24<09:02,  5.02s/it] 87%|████████▋ | 720/827 [58:29<08:38,  4.85s/it] 87%|████████▋ | 721/827 [58:33<08:15,  4.67s/it] 87%|████████▋ | 722/827 [58:37<07:54,  4.52s/it] 87%|████████▋ | 723/827 [58:41<07:34,  4.37s/it] 88%|████████▊ | 724/827 [58:45<06:59,  4.07s/it] 88%|████████▊ | 725/827 [58:50<07:36,  4.47s/it] 88%|████████▊ | 726/827 [58:57<08:52,  5.28s/it] 88%|████████▊ | 727/827 [59:02<08:42,  5.23s/it] 88%|████████▊ | 728/827 [59:06<07:52,  4.77s/it] 88%|████████▊ | 729/827 [59:10<07:13,  4.42s/it] 88%|████████▊ | 730/827 [59:15<07:31,  4.65s/it] 88%|████████▊ | 731/827 [59:21<08:19,  5.21s/it] 89%|████████▊ | 732/827 [59:28<08:53,  5.61s/it] 89%|████████▊ | 733/827 [59:33<08:29,  5.42s/it] 89%|████████▉ | 734/827 [59:36<07:29,  4.84s/it] 89%|████████▉ | 735/827 [59:41<07:14,  4.72s/it] 89%|████████▉ | 736/827 [59:46<07:37,  5.03s/it] 89%|████████▉ | 737/827 [59:52<07:39,  5.11s/it] 89%|████████▉ | 738/827 [59:56<06:59,  4.72s/it] 89%|████████▉ | 739/827 [1:00:01<07:15,  4.95s/it] 89%|████████▉ | 740/827 [1:00:07<07:36,  5.24s/it] 90%|████████▉ | 741/827 [1:00:12<07:16,  5.07s/it] 90%|████████▉ | 742/827 [1:00:18<07:33,  5.34s/it] 90%|████████▉ | 743/827 [1:00:24<07:53,  5.64s/it] 90%|████████▉ | 744/827 [1:00:29<07:44,  5.60s/it] 90%|█████████ | 745/827 [1:00:34<07:23,  5.41s/it] 90%|█████████ | 746/827 [1:00:40<07:29,  5.55s/it] 90%|█████████ | 747/827 [1:00:46<07:28,  5.60s/it] 90%|█████████ | 748/827 [1:00:50<06:53,  5.23s/it] 91%|█████████ | 749/827 [1:00:56<07:03,  5.43s/it] 91%|█████████ | 750/827 [1:01:02<07:03,  5.50s/it] 91%|█████████ | 751/827 [1:01:06<06:25,  5.07s/it] 91%|█████████ | 752/827 [1:01:10<05:51,  4.69s/it] 91%|█████████ | 753/827 [1:01:13<05:22,  4.35s/it] 91%|█████████ | 754/827 [1:01:17<05:01,  4.13s/it] 91%|█████████▏| 755/827 [1:01:23<05:28,  4.56s/it] 91%|█████████▏| 756/827 [1:01:28<05:40,  4.80s/it] 92%|█████████▏| 757/827 [1:01:33<05:33,  4.76s/it] 92%|█████████▏| 758/827 [1:01:38<05:37,  4.89s/it] 92%|█████████▏| 759/827 [1:01:41<05:06,  4.51s/it] 92%|█████████▏| 760/827 [1:01:48<05:35,  5.01s/it] 92%|█████████▏| 761/827 [1:01:55<06:10,  5.61s/it] 92%|█████████▏| 762/827 [1:02:01<06:21,  5.87s/it] 92%|█████████▏| 763/827 [1:02:08<06:33,  6.15s/it] 92%|█████████▏| 764/827 [1:02:12<05:44,  5.47s/it] 93%|█████████▎| 765/827 [1:02:16<05:20,  5.17s/it] 93%|█████████▎| 766/827 [1:02:20<04:59,  4.90s/it] 93%|█████████▎| 767/827 [1:02:25<04:43,  4.73s/it] 93%|█████████▎| 768/827 [1:02:30<04:44,  4.82s/it] 93%|█████████▎| 769/827 [1:02:36<04:58,  5.14s/it] 93%|█████████▎| 770/827 [1:02:41<04:48,  5.06s/it] 93%|█████████▎| 771/827 [1:02:45<04:29,  4.81s/it] 93%|█████████▎| 772/827 [1:02:49<04:13,  4.62s/it] 93%|█████████▎| 773/827 [1:02:53<04:07,  4.58s/it] 94%|█████████▎| 774/827 [1:02:58<04:01,  4.57s/it] 94%|█████████▎| 775/827 [1:03:02<03:54,  4.51s/it] 94%|█████████▍| 776/827 [1:03:07<03:59,  4.69s/it] 94%|█████████▍| 777/827 [1:03:12<03:52,  4.65s/it] 94%|█████████▍| 778/827 [1:03:16<03:41,  4.53s/it] 94%|█████████▍| 779/827 [1:03:20<03:27,  4.33s/it] 94%|█████████▍| 780/827 [1:03:24<03:18,  4.23s/it] 94%|█████████▍| 781/827 [1:03:28<03:12,  4.18s/it] 95%|█████████▍| 782/827 [1:03:33<03:12,  4.27s/it] 95%|█████████▍| 783/827 [1:03:37<03:02,  4.16s/it] 95%|█████████▍| 784/827 [1:03:40<02:54,  4.05s/it] 95%|█████████▍| 785/827 [1:03:45<02:52,  4.11s/it] 95%|█████████▌| 786/827 [1:03:51<03:11,  4.67s/it] 95%|█████████▌| 787/827 [1:03:57<03:25,  5.13s/it] 95%|█████████▌| 788/827 [1:04:01<03:11,  4.90s/it] 95%|█████████▌| 789/827 [1:04:06<03:05,  4.89s/it] 96%|█████████▌| 790/827 [1:04:11<03:00,  4.88s/it] 96%|█████████▌| 791/827 [1:04:15<02:44,  4.57s/it] 96%|█████████▌| 792/827 [1:04:19<02:34,  4.41s/it] 96%|█████████▌| 793/827 [1:04:25<02:48,  4.96s/it] 96%|█████████▌| 794/827 [1:04:32<03:07,  5.67s/it] 96%|█████████▌| 795/827 [1:04:39<03:06,  5.84s/it] 96%|█████████▋| 796/827 [1:04:44<02:57,  5.72s/it] 96%|█████████▋| 797/827 [1:04:48<02:36,  5.21s/it] 96%|█████████▋| 798/827 [1:04:52<02:17,  4.74s/it] 97%|█████████▋| 799/827 [1:04:55<01:59,  4.26s/it] 97%|█████████▋| 800/827 [1:05:02<02:16,  5.06s/it] 97%|█████████▋| 801/827 [1:05:11<02:42,  6.26s/it] 97%|█████████▋| 802/827 [1:05:16<02:28,  5.95s/it] 97%|█████████▋| 803/827 [1:05:22<02:21,  5.90s/it] 97%|█████████▋| 804/827 [1:05:28<02:15,  5.88s/it] 97%|█████████▋| 805/827 [1:05:31<01:55,  5.25s/it] 97%|█████████▋| 806/827 [1:05:35<01:39,  4.75s/it] 98%|█████████▊| 807/827 [1:05:39<01:31,  4.56s/it] 98%|█████████▊| 808/827 [1:05:43<01:24,  4.46s/it] 98%|█████████▊| 809/827 [1:05:47<01:18,  4.35s/it] 98%|█████████▊| 810/827 [1:05:52<01:13,  4.33s/it] 98%|█████████▊| 811/827 [1:05:56<01:08,  4.25s/it] 98%|█████████▊| 812/827 [1:06:01<01:07,  4.50s/it] 98%|█████████▊| 813/827 [1:06:08<01:14,  5.36s/it] 98%|█████████▊| 814/827 [1:06:14<01:11,  5.52s/it] 99%|█████████▊| 815/827 [1:06:19<01:03,  5.30s/it] 99%|█████████▊| 816/827 [1:06:24<00:57,  5.26s/it] 99%|█████████▉| 817/827 [1:06:28<00:49,  4.96s/it] 99%|█████████▉| 818/827 [1:06:34<00:46,  5.19s/it] 99%|█████████▉| 819/827 [1:06:41<00:45,  5.73s/it] 99%|█████████▉| 820/827 [1:06:47<00:39,  5.66s/it] 99%|█████████▉| 821/827 [1:06:51<00:31,  5.25s/it] 99%|█████████▉| 822/827 [1:06:55<00:24,  4.92s/it]100%|█████████▉| 823/827 [1:06:58<00:17,  4.46s/it]100%|█████████▉| 824/827 [1:07:02<00:12,  4.21s/it]100%|█████████▉| 825/827 [1:07:08<00:09,  4.85s/it]100%|█████████▉| 826/827 [1:07:15<00:05,  5.36s/it]100%|██████████| 827/827 [1:07:18<00:00,  4.64s/it]100%|██████████| 827/827 [1:07:18<00:00,  4.88s/it]
***** eval metrics *****
  epoch                   =        1.0
  eval_loss               =     0.6427
  eval_runtime            = 1:07:24.04
  eval_samples_per_second =      0.817
  eval_steps_per_second   =      0.204
[INFO|modelcard.py:456] 2025-12-16 20:49:07,596 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
2025/12/16 20:49:08 INFO handleInformTeardown: server teardown initiated id=1(@)
2025/12/16 20:49:08 INFO server is shutting down
2025/12/16 20:49:08 INFO connection: closing id=1(@)
2025/12/16 20:49:08 INFO connection: closed successfully id=1(@)
2025/12/16 20:49:08 INFO server: listener closed addr=/tmp/wandb-152776-153431-398346821/socket
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /scratch/indrisch/LLaMA-Factory/wandb/wandb/offline-run-20251216_005950-66gov2lo[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/wandb/offline-run-20251216_005950-66gov2lo/logs[0m
2025/12/16 20:49:08 INFO handleInformTeardown: server shutdown complete id=1(@)
2025/12/16 20:49:08 INFO connection: ManageConnectionData: connection closed id=1(@)
2025/12/16 20:49:08 INFO server is closed

scontrol show job 148825
JobId=148825 JobName=slurm_qwen3vl_lora_sft_SQA3Devery24_traineval.sh
   UserId=indrisch(3122343) GroupId=indrisch(3122343) MCS_label=N/A
   Priority=237946 Nice=0 Account=def-wangcs QOS=normal
   JobState=COMPLETING Reason=None Dependency=(null)
   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   RunTime=19:51:40 TimeLimit=22:00:00 TimeMin=N/A
   SubmitTime=2025-12-15T22:21:49 EligibleTime=2025-12-15T22:21:49
   AccrueTime=2025-12-15T22:21:49
   StartTime=2025-12-16T00:57:37 EndTime=2025-12-16T20:49:17 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2025-12-16T00:57:37 Scheduler=Backfill
   Partition=compute_full_node AllocNode:Sid=trig-login01:3729329
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=trig0047
   BatchHost=trig0047
   NumNodes=1 NumCPUs=96 NumTasks=1 CPUs/Task=96 ReqB:S:C:T=0:0:*:*
   ReqTRES=cpu=96,mem=770000M,node=1,billing=4,gres/gpu=4
   AllocTRES=cpu=96,mem=770000M,node=1,billing=4,gres/gpu=4
   Socks/Node=* NtasksPerN:B:S:C=1:0:*:* CoreSpec=*
   MinCPUsNode=96 MinMemoryNode=770000M MinTmpDiskNode=0
   Features=(null) DelayBoot=00:00:00
   OverSubscribe=NO Contiguous=0 Licenses=(null) Network=(null)
   Command=/scratch/indrisch/LLaMA-Factory/preliminaries/qwen3vl_lora_sft_SQA3D/slurm_qwen3vl_lora_sft_SQA3Devery24_traineval.sh
   WorkDir=/scratch/indrisch/LLaMA-Factory/preliminaries/qwen3vl_lora_sft_SQA3D
   Comment=/opt/slurm/bin/sbatch --export=NONE --get-user-env=L slurm_qwen3vl_lora_sft_SQA3Devery24_traineval.sh 
   StdErr=/scratch/indrisch/LLaMA-Factory/preliminaries/qwen3vl_lora_sft_SQA3D/out/%N-qwen3vl_lora_sft_SQA3Devery24_traineval-148825.out
   StdIn=/dev/null
   StdOut=/scratch/indrisch/LLaMA-Factory/preliminaries/qwen3vl_lora_sft_SQA3D/out/%N-qwen3vl_lora_sft_SQA3Devery24_traineval-148825.out
   TresPerNode=gres/gpu:h100:4
   TresPerTask=cpu=96
   

sacct -j 148825
JobID           JobName    Account    Elapsed  MaxVMSize     MaxRSS  SystemCPU    UserCPU ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- ---------- ---------- -------- 
148825       slurm_qwe+ def-wangcs   19:51:41                         00:00:00   00:00:00      0:0 
148825.batch      batch def-wangcs   19:51:41                         00:00:00   00:00:00      0:0 
148825.exte+     extern def-wangcs   19:51:41                         00:00:00   00:00:00      0:0 

