/project/aip-wangcs/indrisch/LLaMA-Factory/scripts /project/aip-wangcs/indrisch/LLaMA-Factory/debug/sft
[WARNING|2026-01-09 03:24:53] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
[INFO|training_args.py:2222] 2026-01-09 03:25:01,528 >> PyTorch: setting up devices
[INFO|training_args.py:1881] 2026-01-09 03:25:01,754 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
[INFO|hub.py:421] 2026-01-09 03:25:03,840 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2026-01-09 03:25:03,873 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2026-01-09 03:25:03,882 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 03:25:03,883 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2026-01-09 03:25:03,883 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 03:25:03,883 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-09 03:25:03,883 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-09 03:25:03,883 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 03:25:03,883 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2026-01-09 03:25:04,212 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|hub.py:421] 2026-01-09 03:25:04,212 >> Offline mode: forcing local_files_only=True
[INFO|hub.py:421] 2026-01-09 03:25:04,214 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:316] 2026-01-09 03:25:04,215 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2026-01-09 03:25:04,219 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:316] 2026-01-09 03:25:04,226 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2026-01-09 03:25:04,228 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:428] 2026-01-09 03:25:04,246 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:1949] 2026-01-09 03:25:04,246 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2026-01-09 03:25:04,251 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 03:25:04,251 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2026-01-09 03:25:04,251 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 03:25:04,251 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-09 03:25:04,251 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-09 03:25:04,251 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 03:25:04,252 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2026-01-09 03:25:04,511 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:660] 2026-01-09 03:25:04,511 >> Offline mode: forcing local_files_only=True
[INFO|video_processing_utils.py:726] 2026-01-09 03:25:04,514 >> loading configuration file video_preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|video_processing_utils.py:770] 2026-01-09 03:25:04,524 >> Video processor Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:928] 2026-01-09 03:25:04,524 >> Offline mode: forcing local_files_only=True
[INFO|processing_utils.py:1116] 2026-01-09 03:25:04,530 >> loading configuration file processor_config.json from cache at None
[INFO|processing_utils.py:1199] 2026-01-09 03:25:04,967 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2_5_VLProcessor"
}

âš™ï¸  Running in WANDB offline mode
INFO 01-09 03:25:05 [arg_utils.py:589] HF_HUB_OFFLINE is True, replace model_id [Qwen/Qwen2.5-VL-7B-Instruct] to model_path [/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5]
INFO 01-09 03:25:05 [utils.py:253] non-default args: {'trust_remote_code': True, 'seed': None, 'max_model_len': 3072, 'tensor_parallel_size': 4, 'disable_log_stats': True, 'limit_mm_per_prompt': {'image': 320}, 'enable_lora': True, 'model': '/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5'}
[WARNING|configuration_utils.py:697] 2026-01-09 03:25:05,022 >> The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[INFO|configuration_utils.py:763] 2026-01-09 03:25:05,024 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
WARNING 01-09 03:25:05 [arg_utils.py:1175] `seed=None` is equivalent to `seed=0` in V1 Engine. You will no longer be allowed to pass `None` in v0.13.
[INFO|configuration_utils.py:763] 2026-01-09 03:25:05,025 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
[INFO|configuration_utils.py:763] 2026-01-09 03:25:05,025 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
[INFO|configuration_utils.py:839] 2026-01-09 03:25:05,031 >> Model config Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 128000,
  "max_window_layers": 28,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "_name_or_path": "/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5",
    "architectures": [
      "Qwen2_5_VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "dtype": "bfloat16",
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "layer_types": [
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention"
    ],
    "max_position_embeddings": 128000,
    "max_window_layers": 28,
    "model_type": "qwen2_5_vl_text",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "use_cache": true,
    "use_sliding_window": false,
    "vision_token_id": 151654,
    "vocab_size": 152064
  },
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 3584,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

[INFO|hub.py:421] 2026-01-09 03:25:05,042 >> Offline mode: forcing local_files_only=True
INFO 01-09 03:25:05 [model.py:637] Resolved architecture: Qwen2_5_VLForConditionalGeneration
INFO 01-09 03:25:05 [model.py:1750] Using max model len 3072
INFO 01-09 03:25:06 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=8192.
[INFO|hub.py:421] 2026-01-09 03:25:06,117 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2026-01-09 03:25:06,118 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:25:06,119 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:25:06,119 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:25:06,119 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:25:06,119 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:25:06,119 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:25:06,119 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:25:06,119 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2364] 2026-01-09 03:25:06,390 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:939] 2026-01-09 03:25:06,513 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/generation_config.json
[INFO|configuration_utils.py:986] 2026-01-09 03:25:06,514 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 1e-06
}

[INFO|hub.py:421] 2026-01-09 03:25:07,369 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2026-01-09 03:25:07,370 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:25:07,372 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:25:07,372 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:25:07,372 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:25:07,372 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:25:07,372 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:25:07,372 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:25:07,372 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2364] 2026-01-09 03:25:07,648 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
WARNING 01-09 03:25:07 [system_utils.py:136] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
[WARNING|2026-01-09 03:25:15] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
[0;36m(EngineCore_DP0 pid=4024137)[0;0m INFO 01-09 03:25:19 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5', speculative_config=None, tokenizer='/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=3072, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=4024137)[0;0m WARNING 01-09 03:25:19 [multiproc_executor.py:880] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[WARNING|2026-01-09 03:25:29] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
[WARNING|2026-01-09 03:25:29] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
[WARNING|2026-01-09 03:25:29] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
[WARNING|2026-01-09 03:25:29] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
INFO 01-09 03:25:36 [parallel_state.py:1200] world_size=4 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:57337 backend=nccl
INFO 01-09 03:25:36 [parallel_state.py:1200] world_size=4 rank=3 local_rank=3 distributed_init_method=tcp://127.0.0.1:57337 backend=nccl
INFO 01-09 03:25:36 [parallel_state.py:1200] world_size=4 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:57337 backend=nccl
INFO 01-09 03:25:36 [parallel_state.py:1200] world_size=4 rank=2 local_rank=2 distributed_init_method=tcp://127.0.0.1:57337 backend=nccl
INFO 01-09 03:25:37 [pynccl.py:111] vLLM is using nccl==2.27.7
WARNING 01-09 03:25:37 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 01-09 03:25:37 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 01-09 03:25:37 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 01-09 03:25:37 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 01-09 03:25:37 [custom_all_reduce.py:154] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 01-09 03:25:37 [custom_all_reduce.py:154] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 01-09 03:25:37 [custom_all_reduce.py:154] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 01-09 03:25:37 [custom_all_reduce.py:154] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 01-09 03:25:37 [parallel_state.py:1408] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
INFO 01-09 03:25:37 [parallel_state.py:1408] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 2, EP rank 2
INFO 01-09 03:25:37 [parallel_state.py:1408] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
INFO 01-09 03:25:37 [parallel_state.py:1408] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 3, EP rank 3
The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.
The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.
The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.
The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.
[0;36m(Worker_TP0 pid=4024150)[0;0m INFO 01-09 03:25:40 [gpu_model_runner.py:3467] Starting to load model /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5...
[0;36m(Worker_TP1 pid=4024151)[0;0m INFO 01-09 03:26:28 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(Worker_TP2 pid=4024152)[0;0m INFO 01-09 03:26:28 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(Worker_TP0 pid=4024150)[0;0m INFO 01-09 03:26:28 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(Worker_TP3 pid=4024153)[0;0m INFO 01-09 03:26:28 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(Worker_TP0 pid=4024150)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]
[0;36m(Worker_TP0 pid=4024150)[0;0m Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:06<00:26,  6.68s/it]
[0;36m(Worker_TP0 pid=4024150)[0;0m Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:13<00:20,  6.96s/it]
[0;36m(Worker_TP0 pid=4024150)[0;0m Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:21<00:14,  7.11s/it]
[0;36m(Worker_TP0 pid=4024150)[0;0m Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:29<00:07,  7.68s/it]
[0;36m(Worker_TP0 pid=4024150)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:30<00:00,  5.23s/it]
[0;36m(Worker_TP0 pid=4024150)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:30<00:00,  6.11s/it]
[0;36m(Worker_TP0 pid=4024150)[0;0m 
[0;36m(Worker_TP0 pid=4024150)[0;0m INFO 01-09 03:26:58 [default_loader.py:308] Loading weights took 30.58 seconds
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [lora_model_runner_mixin.py:37] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.
[0;36m(Worker_TP3 pid=4024153)[0;0m INFO 01-09 03:26:58 [punica_selector.py:20] Using PunicaWrapperGPU.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [lora_model_runner_mixin.py:37] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.
[0;36m(Worker_TP0 pid=4024150)[0;0m INFO 01-09 03:26:58 [punica_selector.py:20] Using PunicaWrapperGPU.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:58 [lora_model_runner_mixin.py:37] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.patch_embed.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0 will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m INFO 01-09 03:26:58 [punica_selector.py:20] Using PunicaWrapperGPU.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2 will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.patch_embed.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0 will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2 will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.patch_embed.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0 will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2 will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:58 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [lora_model_runner_mixin.py:37] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m INFO 01-09 03:26:59 [punica_selector.py:20] Using PunicaWrapperGPU.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.merger.mlp.0 will be ignored.
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.merger.mlp.2 will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.patch_embed.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0 will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2 will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.merger.mlp.0 will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.merger.mlp.2 will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.merger.mlp.0 will be ignored.
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.merger.mlp.2 will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.merger.mlp.0 will be ignored.
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:26:59 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.merger.mlp.2 will be ignored.
[0;36m(Worker_TP0 pid=4024150)[0;0m INFO 01-09 03:26:59 [gpu_model_runner.py:3549] Model loading took 4.0328 GiB memory and 77.910038 seconds
[0;36m(Worker_TP0 pid=4024150)[0;0m INFO 01-09 03:26:59 [gpu_model_runner.py:4306] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
[0;36m(Worker_TP1 pid=4024151)[0;0m INFO 01-09 03:26:59 [gpu_model_runner.py:4306] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
[0;36m(Worker_TP2 pid=4024152)[0;0m INFO 01-09 03:26:59 [gpu_model_runner.py:4306] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
[0;36m(Worker_TP3 pid=4024153)[0;0m INFO 01-09 03:27:00 [gpu_model_runner.py:4306] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
[0;36m(Worker_TP0 pid=4024150)[0;0m INFO 01-09 03:27:20 [backends.py:655] Using cache directory: /home/indrisch/.cache/vllm/torch_compile_cache/97aab168ab/rank_0_0/backbone for vLLM's torch.compile
[0;36m(Worker_TP0 pid=4024150)[0;0m INFO 01-09 03:27:20 [backends.py:715] Dynamo bytecode transform time: 14.33 s
[0;36m(Worker_TP3 pid=4024153)[0;0m [rank3]:W0109 03:27:26.641000 4024153 torch/_inductor/codegen/triton_combo_kernel.py:97] [0/0] ComboKernels: 1 long reduction nodes are separated
[0;36m(Worker_TP1 pid=4024151)[0;0m [rank1]:W0109 03:27:26.641000 4024151 torch/_inductor/codegen/triton_combo_kernel.py:97] [0/0] ComboKernels: 1 long reduction nodes are separated
[0;36m(Worker_TP0 pid=4024150)[0;0m [rank0]:W0109 03:27:26.641000 4024150 torch/_inductor/codegen/triton_combo_kernel.py:97] [0/0] ComboKernels: 1 long reduction nodes are separated
[0;36m(Worker_TP2 pid=4024152)[0;0m [rank2]:W0109 03:27:26.648000 4024152 torch/_inductor/codegen/triton_combo_kernel.py:97] [0/0] ComboKernels: 1 long reduction nodes are separated
[0;36m(Worker_TP3 pid=4024153)[0;0m [rank3]:W0109 03:27:27.412000 4024153 torch/_inductor/codegen/triton_combo_kernel.py:97] [0/0] ComboKernels: 1 long reduction nodes are separated
[0;36m(Worker_TP0 pid=4024150)[0;0m [rank0]:W0109 03:27:27.414000 4024150 torch/_inductor/codegen/triton_combo_kernel.py:97] [0/0] ComboKernels: 1 long reduction nodes are separated
[0;36m(Worker_TP2 pid=4024152)[0;0m [rank2]:W0109 03:27:27.421000 4024152 torch/_inductor/codegen/triton_combo_kernel.py:97] [0/0] ComboKernels: 1 long reduction nodes are separated
[0;36m(Worker_TP1 pid=4024151)[0;0m [rank1]:W0109 03:27:27.424000 4024151 torch/_inductor/codegen/triton_combo_kernel.py:97] [0/0] ComboKernels: 1 long reduction nodes are separated
[0;36m(Worker_TP0 pid=4024150)[0;0m INFO 01-09 03:27:28 [backends.py:257] Cache the graph for dynamic shape for later use
[0;36m(EngineCore_DP0 pid=4024137)[0;0m INFO 01-09 03:27:59 [shm_broadcast.py:501] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization).
[0;36m(Worker_TP0 pid=4024150)[0;0m INFO 01-09 03:28:10 [backends.py:288] Compiling a graph for dynamic shape takes 48.93 s
[0;36m(Worker_TP0 pid=4024150)[0;0m INFO 01-09 03:28:12 [monitor.py:34] torch.compile takes 63.26 s in total
[0;36m(Worker_TP0 pid=4024150)[0;0m INFO 01-09 03:28:14 [gpu_worker.py:359] Available KV cache memory: 34.15 GiB
[0;36m(EngineCore_DP0 pid=4024137)[0;0m INFO 01-09 03:28:14 [kv_cache_utils.py:1286] GPU KV cache size: 2,557,888 tokens
[0;36m(EngineCore_DP0 pid=4024137)[0;0m INFO 01-09 03:28:14 [kv_cache_utils.py:1291] Maximum concurrency for 3,072 tokens per request: 832.65x
[0;36m(Worker_TP0 pid=4024150)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/102 [00:00<?, ?it/s][0;36m(Worker_TP0 pid=4024150)[0;0m WARNING 01-09 03:28:15 [utils.py:250] Using default LoRA kernel configs
[0;36m(Worker_TP3 pid=4024153)[0;0m WARNING 01-09 03:28:15 [utils.py:250] Using default LoRA kernel configs
[0;36m(Worker_TP1 pid=4024151)[0;0m WARNING 01-09 03:28:15 [utils.py:250] Using default LoRA kernel configs
[0;36m(Worker_TP2 pid=4024152)[0;0m WARNING 01-09 03:28:15 [utils.py:250] Using default LoRA kernel configs
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   1%|          | 1/102 [00:04<07:10,  4.27s/it]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|â–Ž         | 3/102 [00:04<01:56,  1.17s/it]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|â–         | 5/102 [00:04<01:00,  1.62it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   7%|â–‹         | 7/102 [00:04<00:37,  2.53it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|â–‰         | 9/102 [00:05<00:26,  3.56it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|â–ˆ         | 11/102 [00:05<00:19,  4.66it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  13%|â–ˆâ–Ž        | 13/102 [00:05<00:15,  5.76it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  15%|â–ˆâ–        | 15/102 [00:05<00:12,  6.80it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|â–ˆâ–‹        | 17/102 [00:05<00:11,  7.61it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|â–ˆâ–Š        | 19/102 [00:05<00:09,  8.40it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|â–ˆâ–ˆ        | 21/102 [00:06<00:08,  9.06it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|â–ˆâ–ˆâ–Ž       | 23/102 [00:06<00:08,  9.58it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|â–ˆâ–ˆâ–       | 25/102 [00:06<00:07,  9.96it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|â–ˆâ–ˆâ–‹       | 27/102 [00:06<00:07, 10.23it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|â–ˆâ–ˆâ–Š       | 29/102 [00:06<00:06, 10.48it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  30%|â–ˆâ–ˆâ–ˆ       | 31/102 [00:07<00:06, 10.60it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|â–ˆâ–ˆâ–ˆâ–      | 33/102 [00:07<00:06, 10.70it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|â–ˆâ–ˆâ–ˆâ–      | 35/102 [00:09<00:27,  2.45it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|â–ˆâ–ˆâ–ˆâ–‹      | 37/102 [00:09<00:20,  3.17it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 39/102 [00:09<00:15,  4.01it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/102 [00:10<00:12,  4.94it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 43/102 [00:10<00:10,  5.90it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 45/102 [00:10<00:08,  6.84it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 47/102 [00:10<00:07,  7.69it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 49/102 [00:10<00:06,  8.44it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/102 [00:10<00:05,  9.04it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 53/102 [00:11<00:05,  9.52it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 55/102 [00:11<00:04,  9.80it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 57/102 [00:11<00:04, 10.07it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 59/102 [00:11<00:04, 10.26it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 61/102 [00:11<00:04, 10.20it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 63/102 [00:12<00:03, 10.32it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 65/102 [00:12<00:03, 10.28it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 67/102 [00:22<00:58,  1.66s/it]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 69/102 [00:24<00:45,  1.38s/it]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 71/102 [00:24<00:30,  1.00it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 73/102 [00:24<00:21,  1.37it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 75/102 [00:25<00:14,  1.86it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 77/102 [00:25<00:10,  2.46it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 79/102 [00:25<00:07,  3.18it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 81/102 [00:25<00:05,  4.00it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 83/102 [00:25<00:03,  4.86it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 85/102 [00:26<00:02,  5.73it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 87/102 [00:26<00:02,  6.58it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 89/102 [00:26<00:01,  7.33it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 91/102 [00:26<00:01,  7.96it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 93/102 [00:26<00:01,  8.45it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 95/102 [00:27<00:00,  8.86it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 97/102 [00:27<00:00,  9.22it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 99/102 [00:27<00:00,  9.43it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 101/102 [00:32<00:00,  1.29it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102/102 [00:32<00:00,  3.17it/s]
[0;36m(Worker_TP0 pid=4024150)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/70 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   1%|â–         | 1/70 [00:00<00:19,  3.59it/s]Capturing CUDA graphs (decode, FULL):   4%|â–         | 3/70 [00:00<00:09,  7.27it/s]Capturing CUDA graphs (decode, FULL):   7%|â–‹         | 5/70 [00:00<00:07,  8.88it/s]Capturing CUDA graphs (decode, FULL):  10%|â–ˆ         | 7/70 [00:00<00:06,  9.79it/s]Capturing CUDA graphs (decode, FULL):  13%|â–ˆâ–Ž        | 9/70 [00:00<00:05, 10.31it/s]Capturing CUDA graphs (decode, FULL):  16%|â–ˆâ–Œ        | 11/70 [00:01<00:05, 10.49it/s]Capturing CUDA graphs (decode, FULL):  19%|â–ˆâ–Š        | 13/70 [00:01<00:05, 10.77it/s]Capturing CUDA graphs (decode, FULL):  21%|â–ˆâ–ˆâ–       | 15/70 [00:01<00:05, 10.96it/s]Capturing CUDA graphs (decode, FULL):  24%|â–ˆâ–ˆâ–       | 17/70 [00:01<00:04, 11.11it/s]Capturing CUDA graphs (decode, FULL):  27%|â–ˆâ–ˆâ–‹       | 19/70 [00:01<00:04, 11.23it/s]Capturing CUDA graphs (decode, FULL):  30%|â–ˆâ–ˆâ–ˆ       | 21/70 [00:02<00:04, 11.25it/s]Capturing CUDA graphs (decode, FULL):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 23/70 [00:02<00:04, 11.33it/s]Capturing CUDA graphs (decode, FULL):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 25/70 [00:02<00:03, 11.37it/s]Capturing CUDA graphs (decode, FULL):  39%|â–ˆâ–ˆâ–ˆâ–Š      | 27/70 [00:02<00:03, 11.40it/s]Capturing CUDA graphs (decode, FULL):  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 29/70 [00:02<00:03, 11.35it/s]Capturing CUDA graphs (decode, FULL):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 31/70 [00:02<00:03, 11.22it/s]Capturing CUDA graphs (decode, FULL):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 33/70 [00:03<00:03, 11.26it/s]Capturing CUDA graphs (decode, FULL):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 35/70 [00:03<00:03, 11.29it/s]Capturing CUDA graphs (decode, FULL):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 37/70 [00:03<00:02, 11.22it/s]Capturing CUDA graphs (decode, FULL):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 39/70 [00:03<00:02, 11.31it/s]Capturing CUDA graphs (decode, FULL):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 41/70 [00:03<00:02, 11.22it/s]Capturing CUDA graphs (decode, FULL):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 43/70 [00:03<00:02, 11.33it/s]Capturing CUDA graphs (decode, FULL):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 45/70 [00:04<00:02, 11.34it/s]Capturing CUDA graphs (decode, FULL):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 47/70 [00:04<00:02, 11.36it/s]Capturing CUDA graphs (decode, FULL):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 49/70 [00:04<00:01, 11.35it/s]Capturing CUDA graphs (decode, FULL):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 51/70 [00:04<00:01, 11.28it/s]Capturing CUDA graphs (decode, FULL):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 53/70 [00:04<00:01, 11.21it/s]Capturing CUDA graphs (decode, FULL):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 55/70 [00:05<00:01, 11.25it/s]Capturing CUDA graphs (decode, FULL):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 57/70 [00:05<00:01, 11.22it/s]Capturing CUDA graphs (decode, FULL):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 59/70 [00:05<00:00, 11.30it/s]Capturing CUDA graphs (decode, FULL):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 61/70 [00:05<00:00, 11.42it/s]Capturing CUDA graphs (decode, FULL):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 63/70 [00:05<00:00, 11.42it/s]Capturing CUDA graphs (decode, FULL):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 65/70 [00:05<00:00, 11.50it/s]Capturing CUDA graphs (decode, FULL):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 67/70 [00:06<00:00, 11.53it/s]Capturing CUDA graphs (decode, FULL):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 69/70 [00:06<00:00, 11.59it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70/70 [00:06<00:00, 11.06it/s]
[0;36m(Worker_TP0 pid=4024150)[0;0m INFO 01-09 03:28:53 [gpu_model_runner.py:4466] Graph capturing finished in 39 secs, took 2.39 GiB
[0;36m(EngineCore_DP0 pid=4024137)[0;0m INFO 01-09 03:28:53 [core.py:254] init engine (profile, create kv cache, warmup model) took 114.32 seconds
[0;36m(EngineCore_DP0 pid=4024137)[0;0m The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.
INFO 01-09 03:28:58 [llm.py:343] Supported tasks: ['generate']
[INFO|2026-01-09 03:28:58] llamafactory.data.loader:143 >> Loading dataset /scratch/indrisch/huggingface/hub/datasets--cvis-tmu--llamafactory-sqa3d-traces-multiimage-vqa/snapshots/ce5c54adc1608d1726730c9ff334e65b6dd70e46/data/...
Converting format of dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<?, ? examples/s]Converting format of dataset (num_proc=32): 102 examples [00:00,  2.90 examples/s]          Converting format of dataset (num_proc=32): 157 examples [00:00, 96.24 examples/s]Converting format of dataset (num_proc=32): 193 examples [00:00, 147.78 examples/s]Converting format of dataset (num_proc=32): 200 examples [00:01, 89.16 examples/s] 
Running tokenizer on dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=32): 104 examples [00:01,  3.26 examples/s]          Running tokenizer on dataset (num_proc=32): 112 examples [00:01, 10.44 examples/s]Running tokenizer on dataset (num_proc=32): 116 examples [00:01, 12.62 examples/s]Running tokenizer on dataset (num_proc=32): 122 examples [00:01, 16.66 examples/s]Running tokenizer on dataset (num_proc=32): 131 examples [00:01, 24.16 examples/s]Running tokenizer on dataset (num_proc=32): 137 examples [00:02, 22.58 examples/s]Running tokenizer on dataset (num_proc=32): 143 examples [00:02, 26.97 examples/s]Running tokenizer on dataset (num_proc=32): 149 examples [00:02, 28.10 examples/s]Running tokenizer on dataset (num_proc=32): 155 examples [00:02, 29.22 examples/s]Running tokenizer on dataset (num_proc=32): 161 examples [00:03, 26.16 examples/s]Running tokenizer on dataset (num_proc=32): 167 examples [00:03, 30.18 examples/s]Running tokenizer on dataset (num_proc=32): 173 examples [00:03, 30.90 examples/s]Running tokenizer on dataset (num_proc=32): 179 examples [00:03, 30.97 examples/s]Running tokenizer on dataset (num_proc=32): 185 examples [00:03, 29.70 examples/s]Running tokenizer on dataset (num_proc=32): 191 examples [00:03, 30.25 examples/s]Running tokenizer on dataset (num_proc=32): 197 examples [00:04, 33.60 examples/s]Running tokenizer on dataset (num_proc=32): 200 examples [00:04, 22.45 examples/s]
training example:
input_ids:
[151644, 8948, 271, 262, 1446, 525, 264, 16585, 11129, 4142, 11528, 17847, 369, 27979, 647, 47522, 389, 264, 3175, 29519, 6109, 6839, 3941, 5248, 5335, 382, 262, 26149, 510, 262, 68252, 1283, 419, 1943, 11, 498, 686, 5258, 451, 6109, 5335, 438, 366, 1805, 29, 9492, 13, 18877, 1105, 438, 2155, 6194, 315, 279, 83490, 6109, 13, 28634, 311, 5335, 553, 220, 16, 5980, 1922, 304, 1973, 315, 11094, 25, 508, 16, 1125, 508, 17, 1125, 4593, 11, 508, 45, 29562, 262, 3596, 35304, 1718, 8575, 38439, 3298, 14017, 510, 262, 7288, 7854, 264, 12966, 220, 18, 35, 10502, 1614, 3941, 6194, 25, 3754, 6171, 3941, 5335, 11, 23583, 8674, 6249, 17040, 11, 323, 990, 5452, 14, 509, 8957, 14688, 21060, 369, 7990, 55916, 624, 262, 7288, 72077, 409, 849, 292, 3793, 26087, 983, 847, 1290, 97089, 17996, 4065, 14, 29898, 484, 32511, 56111, 7612, 1825, 35978, 28455, 448, 5091, 311, 279, 64171, 14, 8092, 13, 1416, 279, 58385, 374, 54761, 11, 1638, 311, 279, 1429, 38219, 8622, 1651, 323, 1977, 773, 624, 262, 7288, 5443, 1172, 9434, 5904, 26, 653, 537, 17023, 63133, 3565, 476, 17188, 389, 9250, 6540, 7797, 6770, 1633, 43898, 3793, 624, 262, 7288, 3197, 5248, 11178, 3000, 11, 8830, 553, 279, 1850, 2432, 311, 27979, 17133, 488, 4274, 1835, 3941, 6194, 13, 1416, 2058, 54761, 1283, 13295, 678, 5335, 11, 1584, 429, 9355, 382, 262, 30990, 320, 327, 32739, 1378, 10010, 11, 304, 419, 1973, 982, 262, 366, 26865, 397, 262, 14822, 14319, 29208, 32711, 304, 220, 18, 4142, 16, 15, 2805, 7354, 13, 356, 632, 5904, 448, 2168, 14937, 320, 68, 1302, 2572, 65750, 18, 5669, 3100, 7579, 18010, 1290, 315, 8718, 26, 508, 16, 5669, 1852, 1894, 11, 42396, 64212, 2823, 63594, 714, 4583, 320, 2640, 37294, 17, 20, 15, 11211, 7241, 5871, 568, 7036, 894, 17796, 8957, 14, 2969, 26745, 487, 323, 1246, 498, 19673, 432, 624, 262, 690, 26865, 397, 262, 366, 9217, 397, 262, 362, 3175, 63594, 4226, 1172, 11, 892, 1231, 387, 510, 262, 7288, 1416, 14016, 25, 3776, 37328, 4226, 11, 4504, 320, 68, 1302, 2572, 1036, 17, 32511, 476, 7414, 14, 2753, 320, 68, 1302, 2572, 1036, 9693, 854, 4292, 262, 7288, 1416, 537, 14016, 25, 362, 11652, 476, 1378, 624, 262, 1416, 4226, 537, 9434, 11, 421, 1052, 374, 902, 2797, 5904, 11, 476, 421, 279, 4226, 374, 35197, 54761, 25, 1036, 33260, 8253, 854, 624, 262, 690, 9217, 1339, 262, 62947, 609, 43797, 50, 510, 262, 7288, 3155, 4183, 13153, 279, 3405, 476, 1140, 5335, 26, 653, 4183, 2550, 4718, 476, 4960, 14158, 624, 262, 7288, 84468, 4285, 1894, 5036, 448, 518, 1429, 825, 22739, 26087, 4145, 3446, 838, 4322, 1574, 58689, 7256, 854, 4292, 262, 7288, 2823, 12966, 3941, 6194, 26, 421, 6194, 28295, 11, 62552, 279, 11299, 15432, 5904, 323, 5185, 432, 304, 366, 26865, 29816, 257, 151645, 198, 151644, 872, 198, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 30, 220, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system

    You are a careful visionâ€“language assistant for spatial VQA on a single indoor scene shown across multiple images.

    INPUT:
    Immediately after this message, you will receive N scene images as <image> tags. Treat them as different views of the SAME scene. Refer to images by 1-based index in order of appearance: [1], [2], â€¦, [N].

    REASONING PRINCIPLES:
    â€¢ Build a consistent 3D mental model across views: track objects across images, infer relative camera pose, and use scale/occlusion/shadows for depth cues.
    â€¢ Interpret deictic terms (â€œto my right/left/in front/behindâ€) EGOCENTRICALLY with respect to the narrator/agent. If the viewpoint is ambiguous, default to the most informative central view and say so.
    â€¢ Use only visible evidence; do not invent unseen details or rely on external knowledge beyond basic object/color terms.
    â€¢ When multiple candidates exist, resolve by the best match to spatial phrase + salience across views. If still ambiguous after checking all images, state that clearly.

    OUTPUT (exactly two blocks, in this order):
    <think>
    Step-by-step reasoning in 3â€“10 short steps. Cite evidence with image indices (e.g., â€œ[3]: light wood desk right of monitor; [1]: same color, confirmsâ€). Be concise but complete (aim â‰¤250 tokens unless necessary). Note any occlusion/ambiguity and how you resolved it.
    </think>
    <answer>
    A single concise answer only, which may be:
    â€¢ If sufficient: One-word answer, Count (e.g., â€œ2â€) or Yes/No (e.g., â€œyesâ€).
    â€¢ If not sufficient: A sentence or two.
    If answer not visible, if there is no clear evidence, or if the answer is genuinely ambiguous: â€œcannot determineâ€.
    </answer>

    STYLE & RULES:
    â€¢ Do NOT repeat the question or list images; do NOT output JSON or extra sections.
    â€¢ Prefer simple color names with at most one modifier (â€œlight/dark/pale/beigeâ€).
    â€¢ Be consistent across views; if views disagree, prioritize the clearest evidence and note it in <think>.
    <|im_end|>
<|im_start|>user
What is on top of the bathroom cabinet that is on my 11 o'clock? <|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|im_end|>
<|im_start|>assistant

label_ids:
[198, 32462, 432, 11, 1077, 594, 1430, 311, 7071, 700, 1128, 594, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 62410, 13, 5512, 11, 1077, 594, 50087, 279, 14852, 6505, 13, 576, 21921, 374, 4363, 264, 4158, 11, 1378, 1737, 95112, 4982, 13, 2411, 220, 16, 16, 297, 62410, 320, 3481, 7950, 3082, 701, 582, 1184, 311, 1779, 429, 7702, 13, 21131, 518, 279, 5335, 11, 5310, 279, 825, 9027, 279, 21921, 3082, 11, 1052, 594, 264, 17846, 3403, 279, 19309, 11, 714, 3783, 11, 902, 2293, 11489, 11, 279, 3405, 374, 911, 279, 21921, 13, 13824, 11, 7196, 279, 330, 66, 75512, 1, 1588, 374, 279, 5819, 4982, 13, 13824, 11, 902, 11, 279, 19309, 702, 264, 21921, 3685, 30, 13824, 11, 902, 11, 279, 1887, 21921, 374, 279, 4158, 61607, 13, 13824, 11, 304, 279, 2168, 1380, 279, 6249, 4933, 279, 19309, 323, 21921, 11, 279, 1909, 315, 279, 21921, 2293, 11489, 11, 902, 11, 7196, 279, 17846, 30, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 2727, 330, 65, 77832, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 3263, 2055, 220, 16, 16, 297, 62410, 1035, 387, 279, 8416, 2115, 3082, 8674, 311, 279, 19309, 13, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 279, 19309, 3082, 13, 2160, 429, 949, 315, 279, 21921, 6505, 30, 13824, 11, 902, 11, 7196, 279, 21921, 374, 279, 4158, 61607, 1212, 279, 19309, 13, 13824, 11, 902, 11, 279, 1909, 315, 429, 21921, 2293, 11489, 11, 902, 11, 304, 1045, 5335, 11, 1052, 594, 264, 17846, 25911, 476, 21810, 13, 13824, 11, 902, 11, 1077, 594, 1779, 279, 1156, 2421, 5335, 13, 576, 17196, 45766, 374, 33866, 11, 1221, 279, 19309, 3082, 13, 13824, 11, 279, 4396, 4226, 25, 3330, 518, 279, 5335, 11, 279, 1633, 389, 1909, 315, 279, 14852, 21921, 320, 1782, 4158, 26482, 4982, 8, 518, 220, 16, 16, 297, 62410, 374, 264, 17846, 13, 13824, 11, 902, 11, 3783, 21677, 17846, 374, 389, 279, 7002, 3403, 279, 19309, 13, 13824, 11, 7196, 358, 5786, 878, 13, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 594, 3405, 25, 330, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 7521, 2055, 1156, 11, 10542, 279, 21921, 13, 576, 4158, 26482, 4982, 3143, 220, 16, 16, 297, 62410, 320, 3481, 7950, 568, 3555, 594, 389, 1909, 30, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 429, 3082, 13, 13824, 11, 9834, 0, 758, 279, 21844, 476, 21810, 11, 1052, 594, 264, 17846, 13, 2055, 279, 4226, 374, 264, 17846, 4192, 1249, 8253, 1128, 374, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 527, 20666, 11, 23643, 279, 27979, 6789, 323, 5424, 304, 279, 5335, 25, 18611, 16, 13, 95210, 279, 1036, 65, 77832, 21921, 854, 304, 3405, 25, 362, 4158, 11, 1378, 481, 26482, 5819, 4982, 374, 9434, 3143, 279, 1909, 481, 2115, 320, 16, 16, 297, 527, 20666, 8, 3082, 8674, 311, 1008, 37664, 1075, 279, 19309, 323, 17196, 13, 2303, 17, 13, 1374, 19991, 6171, 3403, 419, 21921, 25, 758, 279, 5335, 11, 264, 3070, 73225, 334, 374, 21810, 389, 279, 7002, 3403, 419, 21921, 3082, 13, 79226, 54216, 151645, 198]
labels:

Got it, let's try to figure out what's on top of the bathroom cabinet at 11 o'clock. First, let's visualize the bathroom setup. The cabinet is likely a white, two-drawer unit. At 11 o'clock (top-left area), we need to check that spot. Looking at the images, especially the one showing the cabinet area, there's a mirror above the sink, but wait, noâ€”wait, the question is about the cabinet. Wait, maybe the "cabinet" here is the storage unit. Wait, no, the sink has a cabinet below? Wait, no, the main cabinet is the white drawers. Wait, in the image where the camera shows the sink and cabinet, the top of the cabinetâ€”wait, no, maybe the mirror? Wait, no, let's check again. Wait, the user says "bathroom cabinet that is on my 11 o'clock". So 11 o'clock would be the upper left area relative to the sink. Looking at the images, there's a mirror mounted on the wall above the sink area. Is that part of the cabinet setup? Wait, no, maybe the cabinet is the white drawers under the sink. Wait, no, the top of that cabinetâ€”wait, no, in some images, there's a mirror reflected or mounted. Wait, no, let's check the first few images. The shower curtain is colorful, then the sink area. Wait, the correct answer: looking at the images, the object on top of the bathroom cabinet (the white drawer unit) at 11 o'clock is a mirror. Wait, no, waitâ€”the mirror is on the wall above the sink. Wait, maybe I misread. Wait, no, let's check again. Wait, the user's question: "What is on top of the bathroom cabinet that is on my 11 o'clock?" So first, identify the cabinet. The white drawer unit near 11 o'clock (top-left). What's on top? Looking at the images, there's a mirror mounted on the wall above that area. Wait, yes! In the reflection or mounted, there's a mirror. So the answer is a mirror.


To determine what is on top of the bathroom cabinet at 11 oâ€™clock, analyze the spatial layout and elements in the images:  

1. Locate the â€œbathroom cabinetâ€ in question: A white, two - drawer storage unit is visible near the top - left (11 oâ€™clock) area relative to other fixtures like the sink and shower.  
2. Examine objects above this cabinet: In the images, a **mirror** is mounted on the wall above this cabinet area.  


Mirror<|im_end|>

Processing batched inference:   0%|          | 0/1 [00:00<?, ?it/s]
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s][AWARNING 01-09 03:30:22 [input_processor.py:243] vLLM has deprecated support for supporting different tokenizers for different LoRAs. By default, vLLM uses base model's tokenizer. If you are using a LoRA with its own tokenizer, consider specifying `--tokenizer [lora_path]` to use the LoRA tokenizer.
[INFO|image_processing_base.py:316] 2026-01-09 03:30:22,600 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:381] 2026-01-09 03:30:22,601 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[WARNING|logging.py:328] 2026-01-09 03:30:22,601 >> The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.
[INFO|image_processing_base.py:428] 2026-01-09 03:30:22,601 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:1949] 2026-01-09 03:30:22,601 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:30:22,603 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:30:22,603 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:30:22,603 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:30:22,603 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:30:22,603 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:30:22,603 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:30:22,603 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2364] 2026-01-09 03:30:22,890 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:660] 2026-01-09 03:30:22,890 >> Offline mode: forcing local_files_only=True
[INFO|video_processing_utils.py:724] 2026-01-09 03:30:22,891 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|video_processing_utils.py:770] 2026-01-09 03:30:22,891 >> Video processor Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:928] 2026-01-09 03:30:22,891 >> Offline mode: forcing local_files_only=True
[INFO|processing_utils.py:1114] 2026-01-09 03:30:22,892 >> loading configuration file None
[INFO|processing_utils.py:1199] 2026-01-09 03:30:23,333 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2_5_VLProcessor"
}

[INFO|image_processing_base.py:316] 2026-01-09 03:30:23,334 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:381] 2026-01-09 03:30:23,335 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:428] 2026-01-09 03:30:23,335 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:1949] 2026-01-09 03:30:23,335 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:30:23,336 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:30:23,336 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:30:23,336 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:30:23,336 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:30:23,336 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:30:23,336 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:30:23,336 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2364] 2026-01-09 03:30:24,156 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:660] 2026-01-09 03:30:24,156 >> Offline mode: forcing local_files_only=True
[INFO|video_processing_utils.py:724] 2026-01-09 03:30:24,157 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|video_processing_utils.py:770] 2026-01-09 03:30:24,158 >> Video processor Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:928] 2026-01-09 03:30:24,735 >> Offline mode: forcing local_files_only=True
[INFO|processing_utils.py:1114] 2026-01-09 03:30:24,738 >> loading configuration file None
[INFO|processing_utils.py:1199] 2026-01-09 03:30:25,369 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: CachedQwen2TokenizerFast(name_or_path='/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2_5_VLProcessor"
}

Adding requests:   0%|          | 0/100 [00:04<?, ?it/s]
Processing batched inference:   0%|          | 0/1 [01:22<?, ?it/s]
Traceback (most recent call last):
  File "/project/6110552/indrisch/LLaMA-Factory/scripts/vllm_infer.py", line 240, in <module>
    fire.Fire(vllm_infer)
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/project/6110552/indrisch/LLaMA-Factory/scripts/vllm_infer.py", line 220, in vllm_infer
    results = llm.generate(vllm_inputs, sampling_params, lora_request=lora_request)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 434, in generate
    self._validate_and_add_requests(
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 1607, in _validate_and_add_requests
    raise e
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 1595, in _validate_and_add_requests
    request_id = self._add_request(
                 ^^^^^^^^^^^^^^^^^^
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 1694, in _add_request
    engine_request, tokenization_kwargs = self._process_inputs(
                                          ^^^^^^^^^^^^^^^^^^^^^
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 1674, in _process_inputs
    engine_request = self.input_processor.process_inputs(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/vllm/v1/engine/input_processor.py", line 457, in process_inputs
    self._validate_model_inputs(encoder_inputs, decoder_inputs)
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/vllm/v1/engine/input_processor.py", line 532, in _validate_model_inputs
    self._validate_model_input(decoder_inputs, prompt_type="decoder")
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/vllm/v1/engine/input_processor.py", line 606, in _validate_model_input
    raise ValueError(
ValueError: The decoder prompt (length 30533) is longer than the maximum model length of 3072. Make sure that `max_model_len` is no smaller than the number of text tokens plus multimodal tokens. For image inputs, the number of image tokens depends on the number of images, and possibly their aspect ratios as well.
[0;36m(Worker_TP2 pid=4024152)[0;0m INFO 01-09 03:30:27 [multiproc_executor.py:709] Parent process exited, terminating worker
[0;36m(Worker_TP1 pid=4024151)[0;0m INFO 01-09 03:30:27 [multiproc_executor.py:709] Parent process exited, terminating worker
[0;36m(Worker_TP0 pid=4024150)[0;0m INFO 01-09 03:30:27 [multiproc_executor.py:709] Parent process exited, terminating worker
[0;36m(Worker_TP3 pid=4024153)[0;0m INFO 01-09 03:30:27 [multiproc_executor.py:709] Parent process exited, terminating worker
ERROR 01-09 03:30:31 [core_client.py:600] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
