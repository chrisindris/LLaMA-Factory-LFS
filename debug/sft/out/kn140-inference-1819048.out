/project/aip-wangcs/indrisch/LLaMA-Factory/scripts /project/aip-wangcs/indrisch/LLaMA-Factory/debug/sft
[WARNING|2026-01-09 02:58:38] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
[INFO|training_args.py:2222] 2026-01-09 02:58:47,614 >> PyTorch: setting up devices
[INFO|training_args.py:1881] 2026-01-09 02:58:47,805 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
[INFO|hub.py:421] 2026-01-09 02:58:49,990 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2026-01-09 02:58:50,024 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2026-01-09 02:58:50,035 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 02:58:50,035 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2026-01-09 02:58:50,035 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 02:58:50,035 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-09 02:58:50,035 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-09 02:58:50,035 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 02:58:50,035 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2026-01-09 02:58:50,399 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|hub.py:421] 2026-01-09 02:58:50,399 >> Offline mode: forcing local_files_only=True
[INFO|hub.py:421] 2026-01-09 02:58:50,400 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:316] 2026-01-09 02:58:50,403 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2026-01-09 02:58:50,406 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:316] 2026-01-09 02:58:50,414 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2026-01-09 02:58:50,416 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:428] 2026-01-09 02:58:50,432 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:1949] 2026-01-09 02:58:50,432 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2026-01-09 02:58:50,437 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 02:58:50,437 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2026-01-09 02:58:50,437 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 02:58:50,437 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-09 02:58:50,437 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-09 02:58:50,437 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 02:58:50,437 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2026-01-09 02:58:50,698 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:660] 2026-01-09 02:58:50,699 >> Offline mode: forcing local_files_only=True
[INFO|video_processing_utils.py:726] 2026-01-09 02:58:50,702 >> loading configuration file video_preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|video_processing_utils.py:770] 2026-01-09 02:58:50,712 >> Video processor Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:928] 2026-01-09 02:58:50,712 >> Offline mode: forcing local_files_only=True
[INFO|processing_utils.py:1116] 2026-01-09 02:58:50,719 >> loading configuration file processor_config.json from cache at None
[INFO|processing_utils.py:1199] 2026-01-09 02:58:51,165 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2_5_VLProcessor"
}

âš™ï¸  Running in WANDB offline mode
INFO 01-09 02:58:51 [arg_utils.py:589] HF_HUB_OFFLINE is True, replace model_id [Qwen/Qwen2.5-VL-7B-Instruct] to model_path [/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5]
INFO 01-09 02:58:51 [utils.py:253] non-default args: {'trust_remote_code': True, 'seed': None, 'max_model_len': 3072, 'disable_log_stats': True, 'limit_mm_per_prompt': {'image': 4, 'video': 2, 'audio': 2}, 'enable_lora': True, 'model': '/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5'}
[WARNING|configuration_utils.py:697] 2026-01-09 02:58:51,221 >> The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[INFO|configuration_utils.py:763] 2026-01-09 02:58:51,224 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
WARNING 01-09 02:58:51 [arg_utils.py:1175] `seed=None` is equivalent to `seed=0` in V1 Engine. You will no longer be allowed to pass `None` in v0.13.
[INFO|configuration_utils.py:763] 2026-01-09 02:58:51,225 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
[INFO|configuration_utils.py:763] 2026-01-09 02:58:51,226 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
[INFO|configuration_utils.py:839] 2026-01-09 02:58:51,231 >> Model config Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 128000,
  "max_window_layers": 28,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "_name_or_path": "/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5",
    "architectures": [
      "Qwen2_5_VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "dtype": "bfloat16",
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "layer_types": [
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention"
    ],
    "max_position_embeddings": 128000,
    "max_window_layers": 28,
    "model_type": "qwen2_5_vl_text",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "use_cache": true,
    "use_sliding_window": false,
    "vision_token_id": 151654,
    "vocab_size": 152064
  },
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 3584,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

[INFO|hub.py:421] 2026-01-09 02:58:51,243 >> Offline mode: forcing local_files_only=True
INFO 01-09 02:58:51 [model.py:637] Resolved architecture: Qwen2_5_VLForConditionalGeneration
INFO 01-09 02:58:51 [model.py:1750] Using max model len 3072
INFO 01-09 02:58:52 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=8192.
[INFO|hub.py:421] 2026-01-09 02:58:52,337 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2026-01-09 02:58:52,338 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2093] 2026-01-09 02:58:52,339 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 02:58:52,339 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2093] 2026-01-09 02:58:52,339 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 02:58:52,339 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 02:58:52,339 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 02:58:52,339 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 02:58:52,339 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2364] 2026-01-09 02:58:52,592 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:939] 2026-01-09 02:58:52,715 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/generation_config.json
[INFO|configuration_utils.py:986] 2026-01-09 02:58:52,715 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 1e-06
}

[INFO|hub.py:421] 2026-01-09 02:58:53,620 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2026-01-09 02:58:53,622 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2093] 2026-01-09 02:58:53,623 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 02:58:53,623 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2093] 2026-01-09 02:58:53,623 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 02:58:53,623 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 02:58:53,624 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 02:58:53,624 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 02:58:53,624 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2364] 2026-01-09 02:58:53,893 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
WARNING 01-09 02:58:54 [system_utils.py:136] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
[WARNING|2026-01-09 02:59:01] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
[0;36m(EngineCore_DP0 pid=547983)[0;0m INFO 01-09 02:59:05 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5', speculative_config=None, tokenizer='/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=3072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=547983)[0;0m INFO 01-09 02:59:07 [parallel_state.py:1200] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.1.1.140:35903 backend=nccl
[0;36m(EngineCore_DP0 pid=547983)[0;0m INFO 01-09 02:59:07 [parallel_state.py:1408] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=547983)[0;0m The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.
[0;36m(EngineCore_DP0 pid=547983)[0;0m INFO 01-09 02:59:10 [gpu_model_runner.py:3467] Starting to load model /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5...
[0;36m(EngineCore_DP0 pid=547983)[0;0m INFO 01-09 02:59:44 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=547983)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=547983)[0;0m Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:12<00:50, 12.58s/it]
[0;36m(EngineCore_DP0 pid=547983)[0;0m Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:24<00:37, 12.46s/it]
[0;36m(EngineCore_DP0 pid=547983)[0;0m Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:37<00:25, 12.57s/it]
[0;36m(EngineCore_DP0 pid=547983)[0;0m Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:50<00:12, 12.62s/it]
[0;36m(EngineCore_DP0 pid=547983)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:53<00:00,  9.36s/it]
[0;36m(EngineCore_DP0 pid=547983)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:53<00:00, 10.79s/it]
[0;36m(EngineCore_DP0 pid=547983)[0;0m 
[0;36m(EngineCore_DP0 pid=547983)[0;0m INFO 01-09 03:00:38 [default_loader.py:308] Loading weights took 53.98 seconds
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [lora_model_runner_mixin.py:37] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.
[0;36m(EngineCore_DP0 pid=547983)[0;0m INFO 01-09 03:00:38 [punica_selector.py:20] Using PunicaWrapperGPU.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.patch_embed.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0 will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2 will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.merger.mlp.0 will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:00:38 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.merger.mlp.2 will be ignored.
[0;36m(EngineCore_DP0 pid=547983)[0;0m INFO 01-09 03:00:38 [gpu_model_runner.py:3549] Model loading took 15.7031 GiB memory and 87.597487 seconds
[0;36m(EngineCore_DP0 pid=547983)[0;0m INFO 01-09 03:00:39 [gpu_model_runner.py:4306] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
[0;36m(EngineCore_DP0 pid=547983)[0;0m INFO 01-09 03:00:59 [backends.py:655] Using cache directory: /home/indrisch/.cache/vllm/torch_compile_cache/a97a36c1e9/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=547983)[0;0m INFO 01-09 03:00:59 [backends.py:715] Dynamo bytecode transform time: 15.10 s
[0;36m(EngineCore_DP0 pid=547983)[0;0m [rank0]:W0109 03:01:02.337000 547983 torch/_inductor/codegen/triton_combo_kernel.py:97] [0/0] ComboKernels: 1 long reduction nodes are separated
[0;36m(EngineCore_DP0 pid=547983)[0;0m [rank0]:W0109 03:01:03.100000 547983 torch/_inductor/codegen/triton_combo_kernel.py:97] [0/0] ComboKernels: 1 long reduction nodes are separated
[0;36m(EngineCore_DP0 pid=547983)[0;0m INFO 01-09 03:01:03 [backends.py:257] Cache the graph for dynamic shape for later use
[0;36m(EngineCore_DP0 pid=547983)[0;0m INFO 01-09 03:01:40 [backends.py:288] Compiling a graph for dynamic shape takes 40.37 s
[0;36m(EngineCore_DP0 pid=547983)[0;0m INFO 01-09 03:01:42 [monitor.py:34] torch.compile takes 55.47 s in total
[0;36m(EngineCore_DP0 pid=547983)[0;0m INFO 01-09 03:01:44 [gpu_worker.py:359] Available KV cache memory: 21.75 GiB
[0;36m(EngineCore_DP0 pid=547983)[0;0m INFO 01-09 03:01:44 [kv_cache_utils.py:1286] GPU KV cache size: 407,296 tokens
[0;36m(EngineCore_DP0 pid=547983)[0;0m INFO 01-09 03:01:44 [kv_cache_utils.py:1291] Maximum concurrency for 3,072 tokens per request: 132.58x
[0;36m(EngineCore_DP0 pid=547983)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/102 [00:00<?, ?it/s][0;36m(EngineCore_DP0 pid=547983)[0;0m WARNING 01-09 03:01:45 [utils.py:250] Using default LoRA kernel configs
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   1%|          | 1/102 [00:04<06:55,  4.11s/it]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|â–Ž         | 3/102 [00:04<01:51,  1.13s/it]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|â–         | 5/102 [00:04<00:57,  1.70it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   7%|â–‹         | 7/102 [00:04<00:35,  2.68it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|â–‰         | 9/102 [00:04<00:24,  3.80it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|â–ˆ         | 11/102 [00:04<00:18,  5.03it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  13%|â–ˆâ–Ž        | 13/102 [00:05<00:14,  6.28it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  15%|â–ˆâ–        | 15/102 [00:05<00:13,  6.52it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|â–ˆâ–‹        | 17/102 [00:05<00:11,  7.69it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|â–ˆâ–Š        | 19/102 [00:05<00:09,  8.80it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|â–ˆâ–ˆ        | 21/102 [00:05<00:09,  8.19it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|â–ˆâ–ˆâ–Ž       | 23/102 [00:06<00:08,  9.21it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|â–ˆâ–ˆâ–       | 25/102 [00:06<00:07, 10.07it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|â–ˆâ–ˆâ–‹       | 27/102 [00:06<00:08,  8.76it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|â–ˆâ–ˆâ–Š       | 29/102 [00:06<00:07,  9.70it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  30%|â–ˆâ–ˆâ–ˆ       | 31/102 [00:06<00:06, 10.49it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|â–ˆâ–ˆâ–ˆâ–      | 33/102 [00:07<00:06, 10.95it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|â–ˆâ–ˆâ–ˆâ–      | 35/102 [00:09<00:26,  2.57it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|â–ˆâ–ˆâ–ˆâ–‹      | 37/102 [00:09<00:19,  3.39it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 39/102 [00:09<00:14,  4.36it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/102 [00:09<00:11,  5.45it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 43/102 [00:09<00:08,  6.60it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 45/102 [00:09<00:07,  7.76it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 47/102 [00:10<00:06,  8.84it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 49/102 [00:10<00:05,  9.81it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/102 [00:10<00:04, 10.63it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 53/102 [00:10<00:04, 11.27it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 55/102 [00:10<00:03, 11.79it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 57/102 [00:10<00:03, 12.14it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 59/102 [00:11<00:03, 11.94it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 61/102 [00:11<00:03, 12.25it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 63/102 [00:11<00:03, 12.49it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 65/102 [00:11<00:02, 12.67it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 67/102 [00:12<00:09,  3.76it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 69/102 [00:14<00:13,  2.53it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 71/102 [00:14<00:09,  3.34it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 73/102 [00:14<00:06,  4.31it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 75/102 [00:14<00:04,  5.41it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 77/102 [00:14<00:03,  6.59it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 79/102 [00:15<00:02,  7.77it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 81/102 [00:15<00:02,  8.75it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 83/102 [00:15<00:01,  9.74it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 85/102 [00:15<00:01, 10.62it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 87/102 [00:15<00:01, 11.24it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 89/102 [00:15<00:01, 11.68it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 91/102 [00:15<00:00, 12.08it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 93/102 [00:16<00:00, 12.48it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 95/102 [00:16<00:00, 12.92it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 97/102 [00:16<00:00, 13.21it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 99/102 [00:16<00:00, 13.40it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 101/102 [00:22<00:00,  1.13it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102/102 [00:22<00:00,  4.60it/s]
[0;36m(EngineCore_DP0 pid=547983)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/70 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   1%|â–         | 1/70 [00:00<00:22,  3.04it/s]Capturing CUDA graphs (decode, FULL):   4%|â–         | 3/70 [00:00<00:09,  7.06it/s]Capturing CUDA graphs (decode, FULL):   7%|â–‹         | 5/70 [00:00<00:07,  9.15it/s]Capturing CUDA graphs (decode, FULL):  10%|â–ˆ         | 7/70 [00:00<00:06, 10.30it/s]Capturing CUDA graphs (decode, FULL):  13%|â–ˆâ–Ž        | 9/70 [00:00<00:05, 11.21it/s]Capturing CUDA graphs (decode, FULL):  16%|â–ˆâ–Œ        | 11/70 [00:01<00:04, 11.86it/s]Capturing CUDA graphs (decode, FULL):  19%|â–ˆâ–Š        | 13/70 [00:01<00:04, 12.31it/s]Capturing CUDA graphs (decode, FULL):  21%|â–ˆâ–ˆâ–       | 15/70 [00:01<00:04, 12.66it/s]Capturing CUDA graphs (decode, FULL):  24%|â–ˆâ–ˆâ–       | 17/70 [00:01<00:04, 12.86it/s]Capturing CUDA graphs (decode, FULL):  27%|â–ˆâ–ˆâ–‹       | 19/70 [00:01<00:03, 13.03it/s]Capturing CUDA graphs (decode, FULL):  30%|â–ˆâ–ˆâ–ˆ       | 21/70 [00:01<00:03, 13.16it/s]Capturing CUDA graphs (decode, FULL):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 23/70 [00:01<00:03, 13.30it/s]Capturing CUDA graphs (decode, FULL):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 25/70 [00:02<00:03, 13.36it/s]Capturing CUDA graphs (decode, FULL):  39%|â–ˆâ–ˆâ–ˆâ–Š      | 27/70 [00:02<00:03, 13.43it/s]Capturing CUDA graphs (decode, FULL):  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 29/70 [00:02<00:03, 13.45it/s]Capturing CUDA graphs (decode, FULL):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 31/70 [00:02<00:02, 13.45it/s]Capturing CUDA graphs (decode, FULL):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 33/70 [00:02<00:02, 13.44it/s]Capturing CUDA graphs (decode, FULL):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 35/70 [00:02<00:02, 13.50it/s]Capturing CUDA graphs (decode, FULL):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 37/70 [00:03<00:03, 10.26it/s]Capturing CUDA graphs (decode, FULL):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 39/70 [00:03<00:02, 11.07it/s]Capturing CUDA graphs (decode, FULL):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 41/70 [00:03<00:02, 11.80it/s]Capturing CUDA graphs (decode, FULL):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 43/70 [00:03<00:02, 12.40it/s]Capturing CUDA graphs (decode, FULL):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 45/70 [00:03<00:01, 12.85it/s]Capturing CUDA graphs (decode, FULL):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 47/70 [00:03<00:01, 13.16it/s]Capturing CUDA graphs (decode, FULL):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 49/70 [00:04<00:01, 13.42it/s]Capturing CUDA graphs (decode, FULL):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 51/70 [00:04<00:01, 13.60it/s]Capturing CUDA graphs (decode, FULL):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 53/70 [00:04<00:01, 13.71it/s]Capturing CUDA graphs (decode, FULL):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 55/70 [00:04<00:01, 13.86it/s]Capturing CUDA graphs (decode, FULL):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 57/70 [00:04<00:00, 13.96it/s]Capturing CUDA graphs (decode, FULL):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 59/70 [00:04<00:00, 14.05it/s]Capturing CUDA graphs (decode, FULL):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 61/70 [00:04<00:00, 14.16it/s]Capturing CUDA graphs (decode, FULL):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 63/70 [00:05<00:00, 14.23it/s]Capturing CUDA graphs (decode, FULL):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 65/70 [00:05<00:00, 14.32it/s]Capturing CUDA graphs (decode, FULL):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 67/70 [00:05<00:00, 14.44it/s]Capturing CUDA graphs (decode, FULL):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 69/70 [00:05<00:00, 14.57it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70/70 [00:05<00:00, 12.76it/s]
[0;36m(EngineCore_DP0 pid=547983)[0;0m INFO 01-09 03:02:13 [gpu_model_runner.py:4466] Graph capturing finished in 29 secs, took 1.26 GiB
[0;36m(EngineCore_DP0 pid=547983)[0;0m INFO 01-09 03:02:13 [core.py:254] init engine (profile, create kv cache, warmup model) took 94.39 seconds
INFO 01-09 03:02:14 [llm.py:343] Supported tasks: ['generate']
Traceback (most recent call last):
  File "/project/6110552/indrisch/LLaMA-Factory/src/llamafactory/data/parser.py", line 109, in get_dataset_list
    with open(config_path) as f:
         ^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'data/dataset_info.json'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/project/6110552/indrisch/LLaMA-Factory/scripts/vllm_infer.py", line 240, in <module>
    fire.Fire(vllm_infer)
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/project/6110552/indrisch/LLaMA-Factory/scripts/vllm_infer.py", line 131, in vllm_infer
    dataset_module = get_dataset(template_obj, model_args, data_args, training_args, "ppo", **tokenizer_module)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/6110552/indrisch/LLaMA-Factory/src/llamafactory/data/loader.py", line 305, in get_dataset
    dataset = _get_merged_dataset(data_args.dataset, model_args, data_args, training_args, stage)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/6110552/indrisch/LLaMA-Factory/src/llamafactory/data/loader.py", line 179, in _get_merged_dataset
    for dataset_name, dataset_attr in zip(dataset_names, get_dataset_list(dataset_names, data_args.dataset_dir)):
                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/6110552/indrisch/LLaMA-Factory/src/llamafactory/data/parser.py", line 113, in get_dataset_list
    raise ValueError(f"Cannot open {config_path} due to {str(err)}.")
ValueError: Cannot open data/dataset_info.json due to [Errno 2] No such file or directory: 'data/dataset_info.json'.
[rank0]:[W109 03:02:14.621540784 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
ERROR 01-09 03:02:15 [core_client.py:600] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
