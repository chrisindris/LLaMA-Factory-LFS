/project/aip-wangcs/indrisch/LLaMA-Factory/scripts /project/aip-wangcs/indrisch/LLaMA-Factory/debug/sft
[WARNING|2026-01-09 03:06:32] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
[INFO|training_args.py:2222] 2026-01-09 03:06:42,247 >> PyTorch: setting up devices
[INFO|training_args.py:1881] 2026-01-09 03:06:42,443 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
[INFO|hub.py:421] 2026-01-09 03:06:44,703 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2026-01-09 03:06:44,732 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2026-01-09 03:06:44,742 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 03:06:44,742 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2026-01-09 03:06:44,742 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 03:06:44,742 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-09 03:06:44,742 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-09 03:06:44,742 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 03:06:44,742 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2026-01-09 03:06:45,080 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|hub.py:421] 2026-01-09 03:06:45,080 >> Offline mode: forcing local_files_only=True
[INFO|hub.py:421] 2026-01-09 03:06:45,082 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:316] 2026-01-09 03:06:45,085 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2026-01-09 03:06:45,088 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:316] 2026-01-09 03:06:45,096 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2026-01-09 03:06:45,098 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:428] 2026-01-09 03:06:45,113 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:1949] 2026-01-09 03:06:45,114 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2026-01-09 03:06:45,118 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 03:06:45,118 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2026-01-09 03:06:45,119 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 03:06:45,119 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-09 03:06:45,119 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-09 03:06:45,119 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 03:06:45,119 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2026-01-09 03:06:45,381 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:660] 2026-01-09 03:06:45,381 >> Offline mode: forcing local_files_only=True
[INFO|video_processing_utils.py:726] 2026-01-09 03:06:45,385 >> loading configuration file video_preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|video_processing_utils.py:770] 2026-01-09 03:06:45,394 >> Video processor Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:928] 2026-01-09 03:06:45,394 >> Offline mode: forcing local_files_only=True
[INFO|processing_utils.py:1116] 2026-01-09 03:06:45,402 >> loading configuration file processor_config.json from cache at None
[INFO|processing_utils.py:1199] 2026-01-09 03:06:45,845 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2_5_VLProcessor"
}

âš™ï¸  Running in WANDB offline mode
INFO 01-09 03:06:45 [arg_utils.py:589] HF_HUB_OFFLINE is True, replace model_id [Qwen/Qwen2.5-VL-7B-Instruct] to model_path [/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5]
INFO 01-09 03:06:45 [utils.py:253] non-default args: {'trust_remote_code': True, 'seed': None, 'max_model_len': 3072, 'disable_log_stats': True, 'limit_mm_per_prompt': {'image': 4, 'video': 2, 'audio': 2}, 'enable_lora': True, 'model': '/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5'}
[WARNING|configuration_utils.py:697] 2026-01-09 03:06:45,899 >> The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[INFO|configuration_utils.py:763] 2026-01-09 03:06:45,902 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
WARNING 01-09 03:06:45 [arg_utils.py:1175] `seed=None` is equivalent to `seed=0` in V1 Engine. You will no longer be allowed to pass `None` in v0.13.
[INFO|configuration_utils.py:763] 2026-01-09 03:06:45,903 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
[INFO|configuration_utils.py:763] 2026-01-09 03:06:45,904 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
[INFO|configuration_utils.py:839] 2026-01-09 03:06:45,910 >> Model config Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 128000,
  "max_window_layers": 28,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "_name_or_path": "/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5",
    "architectures": [
      "Qwen2_5_VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "dtype": "bfloat16",
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "layer_types": [
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention"
    ],
    "max_position_embeddings": 128000,
    "max_window_layers": 28,
    "model_type": "qwen2_5_vl_text",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "use_cache": true,
    "use_sliding_window": false,
    "vision_token_id": 151654,
    "vocab_size": 152064
  },
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 3584,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

[INFO|hub.py:421] 2026-01-09 03:06:45,921 >> Offline mode: forcing local_files_only=True
INFO 01-09 03:06:45 [model.py:637] Resolved architecture: Qwen2_5_VLForConditionalGeneration
INFO 01-09 03:06:45 [model.py:1750] Using max model len 3072
INFO 01-09 03:06:47 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=8192.
[INFO|hub.py:421] 2026-01-09 03:06:47,034 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2026-01-09 03:06:47,035 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:06:47,036 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:06:47,036 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:06:47,036 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:06:47,036 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:06:47,036 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:06:47,036 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:06:47,037 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2364] 2026-01-09 03:06:47,305 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:939] 2026-01-09 03:06:47,428 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/generation_config.json
[INFO|configuration_utils.py:986] 2026-01-09 03:06:47,428 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 1e-06
}

[INFO|hub.py:421] 2026-01-09 03:06:48,243 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2026-01-09 03:06:48,244 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:06:48,246 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:06:48,246 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:06:48,246 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:06:48,246 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:06:48,246 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:06:48,246 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 03:06:48,246 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2364] 2026-01-09 03:06:48,520 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
WARNING 01-09 03:06:48 [system_utils.py:136] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
[WARNING|2026-01-09 03:06:56] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m INFO 01-09 03:06:59 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5', speculative_config=None, tokenizer='/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=3072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=2091184)[0;0m INFO 01-09 03:07:02 [parallel_state.py:1200] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.1.1.132:38705 backend=nccl
[0;36m(EngineCore_DP0 pid=2091184)[0;0m INFO 01-09 03:07:02 [parallel_state.py:1408] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=2091184)[0;0m The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m INFO 01-09 03:07:05 [gpu_model_runner.py:3467] Starting to load model /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5...
[0;36m(EngineCore_DP0 pid=2091184)[0;0m INFO 01-09 03:07:37 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=2091184)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=2091184)[0;0m Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:12<00:49, 12.25s/it]
[0;36m(EngineCore_DP0 pid=2091184)[0;0m Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:24<00:36, 12.26s/it]
[0;36m(EngineCore_DP0 pid=2091184)[0;0m Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:37<00:25, 12.68s/it]
[0;36m(EngineCore_DP0 pid=2091184)[0;0m Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:50<00:12, 12.65s/it]
[0;36m(EngineCore_DP0 pid=2091184)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:53<00:00,  9.38s/it]
[0;36m(EngineCore_DP0 pid=2091184)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:53<00:00, 10.78s/it]
[0;36m(EngineCore_DP0 pid=2091184)[0;0m 
[0;36m(EngineCore_DP0 pid=2091184)[0;0m INFO 01-09 03:08:31 [default_loader.py:308] Loading weights took 53.90 seconds
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [lora_model_runner_mixin.py:37] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m INFO 01-09 03:08:31 [punica_selector.py:20] Using PunicaWrapperGPU.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.patch_embed.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0 will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2 will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.attn.qkv will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.attn.proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.mlp.gate_up_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.mlp.down_proj will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.merger.mlp.0 will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:31 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.merger.mlp.2 will be ignored.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m INFO 01-09 03:08:31 [gpu_model_runner.py:3549] Model loading took 15.7031 GiB memory and 85.582605 seconds
[0;36m(EngineCore_DP0 pid=2091184)[0;0m INFO 01-09 03:08:31 [gpu_model_runner.py:4306] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
[0;36m(EngineCore_DP0 pid=2091184)[0;0m INFO 01-09 03:08:51 [backends.py:655] Using cache directory: /home/indrisch/.cache/vllm/torch_compile_cache/a97a36c1e9/rank_0_0/backbone for vLLM's torch.compile
[0;36m(EngineCore_DP0 pid=2091184)[0;0m INFO 01-09 03:08:51 [backends.py:715] Dynamo bytecode transform time: 14.48 s
[0;36m(EngineCore_DP0 pid=2091184)[0;0m INFO 01-09 03:08:55 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 3.745 s
[0;36m(EngineCore_DP0 pid=2091184)[0;0m INFO 01-09 03:08:57 [monitor.py:34] torch.compile takes 18.22 s in total
[0;36m(EngineCore_DP0 pid=2091184)[0;0m INFO 01-09 03:08:59 [gpu_worker.py:359] Available KV cache memory: 21.75 GiB
[0;36m(EngineCore_DP0 pid=2091184)[0;0m INFO 01-09 03:08:59 [kv_cache_utils.py:1286] GPU KV cache size: 407,296 tokens
[0;36m(EngineCore_DP0 pid=2091184)[0;0m INFO 01-09 03:08:59 [kv_cache_utils.py:1291] Maximum concurrency for 3,072 tokens per request: 132.58x
[0;36m(EngineCore_DP0 pid=2091184)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/102 [00:00<?, ?it/s][0;36m(EngineCore_DP0 pid=2091184)[0;0m WARNING 01-09 03:08:59 [utils.py:250] Using default LoRA kernel configs
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   1%|          | 1/102 [00:04<07:06,  4.23s/it]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|â–Ž         | 3/102 [00:04<01:54,  1.16s/it]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|â–         | 5/102 [00:04<00:58,  1.66it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   7%|â–‹         | 7/102 [00:04<00:36,  2.62it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|â–‰         | 9/102 [00:04<00:24,  3.72it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|â–ˆ         | 11/102 [00:05<00:18,  4.93it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  13%|â–ˆâ–Ž        | 13/102 [00:05<00:14,  6.18it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  15%|â–ˆâ–        | 15/102 [00:05<00:11,  7.36it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|â–ˆâ–‹        | 17/102 [00:05<00:10,  8.44it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|â–ˆâ–Š        | 19/102 [00:05<00:08,  9.46it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|â–ˆâ–ˆ        | 21/102 [00:05<00:07, 10.28it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|â–ˆâ–ˆâ–Ž       | 23/102 [00:05<00:07, 10.95it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|â–ˆâ–ˆâ–       | 25/102 [00:06<00:06, 11.48it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|â–ˆâ–ˆâ–‹       | 27/102 [00:06<00:06, 11.91it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|â–ˆâ–ˆâ–Š       | 29/102 [00:06<00:06, 12.17it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  30%|â–ˆâ–ˆâ–ˆ       | 31/102 [00:06<00:05, 12.38it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|â–ˆâ–ˆâ–ˆâ–      | 33/102 [00:06<00:05, 12.18it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|â–ˆâ–ˆâ–ˆâ–      | 35/102 [00:08<00:26,  2.57it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|â–ˆâ–ˆâ–ˆâ–‹      | 37/102 [00:09<00:19,  3.39it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 39/102 [00:09<00:14,  4.36it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/102 [00:09<00:11,  5.45it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 43/102 [00:09<00:08,  6.61it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 45/102 [00:09<00:07,  7.77it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 47/102 [00:09<00:06,  8.85it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 49/102 [00:10<00:05,  9.81it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/102 [00:10<00:04, 10.63it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 53/102 [00:10<00:04, 11.30it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 55/102 [00:10<00:03, 11.81it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 57/102 [00:10<00:03, 12.22it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 59/102 [00:10<00:03, 12.04it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 61/102 [00:10<00:03, 12.32it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 63/102 [00:11<00:03, 12.53it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 65/102 [00:11<00:03, 10.05it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 67/102 [00:12<00:09,  3.57it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 69/102 [00:14<00:13,  2.46it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 71/102 [00:14<00:09,  3.25it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 73/102 [00:14<00:06,  4.21it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 75/102 [00:14<00:05,  5.30it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 77/102 [00:14<00:03,  6.48it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 79/102 [00:14<00:03,  7.67it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 81/102 [00:15<00:02,  8.78it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 83/102 [00:15<00:01,  9.79it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 85/102 [00:15<00:01, 10.66it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 87/102 [00:15<00:01, 11.37it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 89/102 [00:15<00:01, 11.93it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 91/102 [00:15<00:00, 12.35it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 93/102 [00:16<00:00, 12.70it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 95/102 [00:16<00:00, 13.06it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 97/102 [00:16<00:00, 13.32it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 99/102 [00:16<00:00, 10.82it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 101/102 [00:21<00:00,  1.37it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102/102 [00:21<00:00,  4.85it/s]
[0;36m(EngineCore_DP0 pid=2091184)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/70 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   1%|â–         | 1/70 [00:00<00:23,  2.97it/s]Capturing CUDA graphs (decode, FULL):   4%|â–         | 3/70 [00:00<00:09,  6.94it/s]Capturing CUDA graphs (decode, FULL):   7%|â–‹         | 5/70 [00:00<00:07,  9.12it/s]Capturing CUDA graphs (decode, FULL):  10%|â–ˆ         | 7/70 [00:00<00:06, 10.47it/s]Capturing CUDA graphs (decode, FULL):  13%|â–ˆâ–Ž        | 9/70 [00:00<00:05, 11.44it/s]Capturing CUDA graphs (decode, FULL):  16%|â–ˆâ–Œ        | 11/70 [00:01<00:04, 12.05it/s]Capturing CUDA graphs (decode, FULL):  19%|â–ˆâ–Š        | 13/70 [00:01<00:04, 12.49it/s]Capturing CUDA graphs (decode, FULL):  21%|â–ˆâ–ˆâ–       | 15/70 [00:01<00:04, 12.82it/s]Capturing CUDA graphs (decode, FULL):  24%|â–ˆâ–ˆâ–       | 17/70 [00:01<00:04, 13.07it/s]Capturing CUDA graphs (decode, FULL):  27%|â–ˆâ–ˆâ–‹       | 19/70 [00:01<00:03, 13.22it/s]Capturing CUDA graphs (decode, FULL):  30%|â–ˆâ–ˆâ–ˆ       | 21/70 [00:01<00:03, 13.35it/s]Capturing CUDA graphs (decode, FULL):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 23/70 [00:01<00:03, 13.46it/s]Capturing CUDA graphs (decode, FULL):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 25/70 [00:02<00:03, 13.51it/s]Capturing CUDA graphs (decode, FULL):  39%|â–ˆâ–ˆâ–ˆâ–Š      | 27/70 [00:02<00:03, 13.59it/s]Capturing CUDA graphs (decode, FULL):  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 29/70 [00:02<00:03, 13.61it/s]Capturing CUDA graphs (decode, FULL):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 31/70 [00:02<00:02, 13.62it/s]Capturing CUDA graphs (decode, FULL):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 33/70 [00:02<00:02, 13.63it/s]Capturing CUDA graphs (decode, FULL):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 35/70 [00:02<00:02, 13.68it/s]Capturing CUDA graphs (decode, FULL):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 37/70 [00:02<00:02, 13.72it/s]Capturing CUDA graphs (decode, FULL):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 39/70 [00:03<00:02, 13.73it/s]Capturing CUDA graphs (decode, FULL):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 41/70 [00:03<00:02, 13.83it/s]Capturing CUDA graphs (decode, FULL):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 43/70 [00:03<00:01, 13.93it/s]Capturing CUDA graphs (decode, FULL):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 45/70 [00:03<00:01, 14.00it/s]Capturing CUDA graphs (decode, FULL):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 47/70 [00:03<00:01, 14.03it/s]Capturing CUDA graphs (decode, FULL):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 49/70 [00:03<00:01, 14.08it/s]Capturing CUDA graphs (decode, FULL):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 51/70 [00:03<00:01, 14.12it/s]Capturing CUDA graphs (decode, FULL):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 53/70 [00:04<00:01, 14.14it/s]Capturing CUDA graphs (decode, FULL):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 55/70 [00:04<00:01, 14.20it/s]Capturing CUDA graphs (decode, FULL):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 57/70 [00:04<00:00, 14.24it/s]Capturing CUDA graphs (decode, FULL):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 59/70 [00:04<00:00, 14.31it/s]Capturing CUDA graphs (decode, FULL):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 61/70 [00:04<00:00, 14.37it/s]Capturing CUDA graphs (decode, FULL):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 63/70 [00:04<00:00, 14.44it/s]Capturing CUDA graphs (decode, FULL):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 65/70 [00:04<00:00, 14.54it/s]Capturing CUDA graphs (decode, FULL):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 67/70 [00:05<00:00, 14.64it/s]Capturing CUDA graphs (decode, FULL):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 69/70 [00:05<00:00, 14.79it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70/70 [00:05<00:00, 13.29it/s]
[0;36m(EngineCore_DP0 pid=2091184)[0;0m INFO 01-09 03:09:26 [gpu_model_runner.py:4466] Graph capturing finished in 27 secs, took 1.26 GiB
[0;36m(EngineCore_DP0 pid=2091184)[0;0m INFO 01-09 03:09:26 [core.py:254] init engine (profile, create kv cache, warmup model) took 54.86 seconds
INFO 01-09 03:09:27 [llm.py:343] Supported tasks: ['generate']
[INFO|2026-01-09 03:09:27] llamafactory.data.loader:143 >> Loading dataset /scratch/indrisch/huggingface/hub/datasets--cvis-tmu--llamafactory-sqa3d-traces-multiimage-vqa/snapshots/ce5c54adc1608d1726730c9ff334e65b6dd70e46/data/...
Setting num_proc from 32 to 2 for the train split as it only contains 2 shards.
[2026-01-09 03:09:27] WARNING builder.py:1680: Setting num_proc from 32 to 2 for the train split as it only contains 2 shards.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 3304 examples [00:00, 13556.27 examples/s]Generating train split: 10706 examples [00:00, 23167.04 examples/s]Generating train split: 18108 examples [00:00, 29128.79 examples/s]Generating train split: 25510 examples [00:01, 24194.85 examples/s]Generating train split: 32912 examples [00:01, 31625.57 examples/s]Generating train split: 33047 examples [00:01, 23690.33 examples/s]
Converting format of dataset (num_proc=32):   0%|          | 0/100 [00:00<?, ? examples/s]Converting format of dataset (num_proc=32):   1%|          | 1/100 [00:00<00:53,  1.84 examples/s]Converting format of dataset (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:00<00:00, 94.94 examples/s]Converting format of dataset (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [00:00<00:00, 180.44 examples/s]Converting format of dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00, 98.85 examples/s]
Running tokenizer on dataset (num_proc=32):   0%|          | 0/100 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=32):   4%|â–         | 4/100 [00:01<00:26,  3.62 examples/s]Running tokenizer on dataset (num_proc=32):  12%|â–ˆâ–        | 12/100 [00:01<00:07, 11.40 examples/s]Running tokenizer on dataset (num_proc=32):  19%|â–ˆâ–‰        | 19/100 [00:01<00:04, 17.31 examples/s]Running tokenizer on dataset (num_proc=32):  25%|â–ˆâ–ˆâ–Œ       | 25/100 [00:01<00:03, 20.94 examples/s]Running tokenizer on dataset (num_proc=32):  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [00:01<00:02, 24.05 examples/s]Running tokenizer on dataset (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [00:01<00:02, 26.62 examples/s]Running tokenizer on dataset (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [00:02<00:01, 28.68 examples/s]Running tokenizer on dataset (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [00:02<00:01, 30.10 examples/s]Running tokenizer on dataset (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:02<00:01, 30.66 examples/s]Running tokenizer on dataset (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:02<00:01, 30.82 examples/s]Running tokenizer on dataset (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:02<00:01, 31.34 examples/s]Running tokenizer on dataset (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:03<00:00, 31.29 examples/s]Running tokenizer on dataset (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:03<00:00, 31.58 examples/s]Running tokenizer on dataset (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:03<00:00, 35.23 examples/s]Running tokenizer on dataset (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:03<00:00, 33.34 examples/s]Running tokenizer on dataset (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [00:03<00:00, 33.48 examples/s]Running tokenizer on dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:04<00:00, 24.59 examples/s]
training example:
input_ids:
[151644, 8948, 271, 262, 1446, 525, 264, 16585, 11129, 4142, 11528, 17847, 369, 27979, 647, 47522, 389, 264, 3175, 29519, 6109, 6839, 3941, 5248, 5335, 382, 262, 26149, 510, 262, 68252, 1283, 419, 1943, 11, 498, 686, 5258, 451, 6109, 5335, 438, 366, 1805, 29, 9492, 13, 18877, 1105, 438, 2155, 6194, 315, 279, 83490, 6109, 13, 28634, 311, 5335, 553, 220, 16, 5980, 1922, 304, 1973, 315, 11094, 25, 508, 16, 1125, 508, 17, 1125, 4593, 11, 508, 45, 29562, 262, 3596, 35304, 1718, 8575, 38439, 3298, 14017, 510, 262, 7288, 7854, 264, 12966, 220, 18, 35, 10502, 1614, 3941, 6194, 25, 3754, 6171, 3941, 5335, 11, 23583, 8674, 6249, 17040, 11, 323, 990, 5452, 14, 509, 8957, 14688, 21060, 369, 7990, 55916, 624, 262, 7288, 72077, 409, 849, 292, 3793, 26087, 983, 847, 1290, 97089, 17996, 4065, 14, 29898, 484, 32511, 56111, 7612, 1825, 35978, 28455, 448, 5091, 311, 279, 64171, 14, 8092, 13, 1416, 279, 58385, 374, 54761, 11, 1638, 311, 279, 1429, 38219, 8622, 1651, 323, 1977, 773, 624, 262, 7288, 5443, 1172, 9434, 5904, 26, 653, 537, 17023, 63133, 3565, 476, 17188, 389, 9250, 6540, 7797, 6770, 1633, 43898, 3793, 624, 262, 7288, 3197, 5248, 11178, 3000, 11, 8830, 553, 279, 1850, 2432, 311, 27979, 17133, 488, 4274, 1835, 3941, 6194, 13, 1416, 2058, 54761, 1283, 13295, 678, 5335, 11, 1584, 429, 9355, 382, 262, 30990, 320, 327, 32739, 1378, 10010, 11, 304, 419, 1973, 982, 262, 366, 26865, 397, 262, 14822, 14319, 29208, 32711, 304, 220, 18, 4142, 16, 15, 2805, 7354, 13, 356, 632, 5904, 448, 2168, 14937, 320, 68, 1302, 2572, 65750, 18, 5669, 3100, 7579, 18010, 1290, 315, 8718, 26, 508, 16, 5669, 1852, 1894, 11, 42396, 64212, 2823, 63594, 714, 4583, 320, 2640, 37294, 17, 20, 15, 11211, 7241, 5871, 568, 7036, 894, 17796, 8957, 14, 2969, 26745, 487, 323, 1246, 498, 19673, 432, 624, 262, 690, 26865, 397, 262, 366, 9217, 397, 262, 362, 3175, 63594, 4226, 1172, 11, 892, 1231, 387, 510, 262, 7288, 1416, 14016, 25, 3776, 37328, 4226, 11, 4504, 320, 68, 1302, 2572, 1036, 17, 32511, 476, 7414, 14, 2753, 320, 68, 1302, 2572, 1036, 9693, 854, 4292, 262, 7288, 1416, 537, 14016, 25, 362, 11652, 476, 1378, 624, 262, 1416, 4226, 537, 9434, 11, 421, 1052, 374, 902, 2797, 5904, 11, 476, 421, 279, 4226, 374, 35197, 54761, 25, 1036, 33260, 8253, 854, 624, 262, 690, 9217, 1339, 262, 62947, 609, 43797, 50, 510, 262, 7288, 3155, 4183, 13153, 279, 3405, 476, 1140, 5335, 26, 653, 4183, 2550, 4718, 476, 4960, 14158, 624, 262, 7288, 84468, 4285, 1894, 5036, 448, 518, 1429, 825, 22739, 26087, 4145, 3446, 838, 4322, 1574, 58689, 7256, 854, 4292, 262, 7288, 2823, 12966, 3941, 6194, 26, 421, 6194, 28295, 11, 62552, 279, 11299, 15432, 5904, 323, 5185, 432, 304, 366, 26865, 29816, 257, 151645, 198, 151644, 872, 198, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 30, 220, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system

    You are a careful visionâ€“language assistant for spatial VQA on a single indoor scene shown across multiple images.

    INPUT:
    Immediately after this message, you will receive N scene images as <image> tags. Treat them as different views of the SAME scene. Refer to images by 1-based index in order of appearance: [1], [2], â€¦, [N].

    REASONING PRINCIPLES:
    â€¢ Build a consistent 3D mental model across views: track objects across images, infer relative camera pose, and use scale/occlusion/shadows for depth cues.
    â€¢ Interpret deictic terms (â€œto my right/left/in front/behindâ€) EGOCENTRICALLY with respect to the narrator/agent. If the viewpoint is ambiguous, default to the most informative central view and say so.
    â€¢ Use only visible evidence; do not invent unseen details or rely on external knowledge beyond basic object/color terms.
    â€¢ When multiple candidates exist, resolve by the best match to spatial phrase + salience across views. If still ambiguous after checking all images, state that clearly.

    OUTPUT (exactly two blocks, in this order):
    <think>
    Step-by-step reasoning in 3â€“10 short steps. Cite evidence with image indices (e.g., â€œ[3]: light wood desk right of monitor; [1]: same color, confirmsâ€). Be concise but complete (aim â‰¤250 tokens unless necessary). Note any occlusion/ambiguity and how you resolved it.
    </think>
    <answer>
    A single concise answer only, which may be:
    â€¢ If sufficient: One-word answer, Count (e.g., â€œ2â€) or Yes/No (e.g., â€œyesâ€).
    â€¢ If not sufficient: A sentence or two.
    If answer not visible, if there is no clear evidence, or if the answer is genuinely ambiguous: â€œcannot determineâ€.
    </answer>

    STYLE & RULES:
    â€¢ Do NOT repeat the question or list images; do NOT output JSON or extra sections.
    â€¢ Prefer simple color names with at most one modifier (â€œlight/dark/pale/beigeâ€).
    â€¢ Be consistent across views; if views disagree, prioritize the clearest evidence and note it in <think>.
    <|im_end|>
<|im_start|>user
What is on top of the bathroom cabinet that is on my 11 o'clock? <|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|im_end|>
<|im_start|>assistant

label_ids:
[198, 32462, 432, 11, 1077, 594, 1430, 311, 7071, 700, 1128, 594, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 62410, 13, 5512, 11, 1077, 594, 50087, 279, 14852, 6505, 13, 576, 21921, 374, 4363, 264, 4158, 11, 1378, 1737, 95112, 4982, 13, 2411, 220, 16, 16, 297, 62410, 320, 3481, 7950, 3082, 701, 582, 1184, 311, 1779, 429, 7702, 13, 21131, 518, 279, 5335, 11, 5310, 279, 825, 9027, 279, 21921, 3082, 11, 1052, 594, 264, 17846, 3403, 279, 19309, 11, 714, 3783, 11, 902, 2293, 11489, 11, 279, 3405, 374, 911, 279, 21921, 13, 13824, 11, 7196, 279, 330, 66, 75512, 1, 1588, 374, 279, 5819, 4982, 13, 13824, 11, 902, 11, 279, 19309, 702, 264, 21921, 3685, 30, 13824, 11, 902, 11, 279, 1887, 21921, 374, 279, 4158, 61607, 13, 13824, 11, 304, 279, 2168, 1380, 279, 6249, 4933, 279, 19309, 323, 21921, 11, 279, 1909, 315, 279, 21921, 2293, 11489, 11, 902, 11, 7196, 279, 17846, 30, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 2727, 330, 65, 77832, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 3263, 2055, 220, 16, 16, 297, 62410, 1035, 387, 279, 8416, 2115, 3082, 8674, 311, 279, 19309, 13, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 279, 19309, 3082, 13, 2160, 429, 949, 315, 279, 21921, 6505, 30, 13824, 11, 902, 11, 7196, 279, 21921, 374, 279, 4158, 61607, 1212, 279, 19309, 13, 13824, 11, 902, 11, 279, 1909, 315, 429, 21921, 2293, 11489, 11, 902, 11, 304, 1045, 5335, 11, 1052, 594, 264, 17846, 25911, 476, 21810, 13, 13824, 11, 902, 11, 1077, 594, 1779, 279, 1156, 2421, 5335, 13, 576, 17196, 45766, 374, 33866, 11, 1221, 279, 19309, 3082, 13, 13824, 11, 279, 4396, 4226, 25, 3330, 518, 279, 5335, 11, 279, 1633, 389, 1909, 315, 279, 14852, 21921, 320, 1782, 4158, 26482, 4982, 8, 518, 220, 16, 16, 297, 62410, 374, 264, 17846, 13, 13824, 11, 902, 11, 3783, 21677, 17846, 374, 389, 279, 7002, 3403, 279, 19309, 13, 13824, 11, 7196, 358, 5786, 878, 13, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 594, 3405, 25, 330, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 7521, 2055, 1156, 11, 10542, 279, 21921, 13, 576, 4158, 26482, 4982, 3143, 220, 16, 16, 297, 62410, 320, 3481, 7950, 568, 3555, 594, 389, 1909, 30, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 429, 3082, 13, 13824, 11, 9834, 0, 758, 279, 21844, 476, 21810, 11, 1052, 594, 264, 17846, 13, 2055, 279, 4226, 374, 264, 17846, 4192, 1249, 8253, 1128, 374, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 527, 20666, 11, 23643, 279, 27979, 6789, 323, 5424, 304, 279, 5335, 25, 18611, 16, 13, 95210, 279, 1036, 65, 77832, 21921, 854, 304, 3405, 25, 362, 4158, 11, 1378, 481, 26482, 5819, 4982, 374, 9434, 3143, 279, 1909, 481, 2115, 320, 16, 16, 297, 527, 20666, 8, 3082, 8674, 311, 1008, 37664, 1075, 279, 19309, 323, 17196, 13, 2303, 17, 13, 1374, 19991, 6171, 3403, 419, 21921, 25, 758, 279, 5335, 11, 264, 3070, 73225, 334, 374, 21810, 389, 279, 7002, 3403, 419, 21921, 3082, 13, 79226, 54216, 151645, 198]
labels:

Got it, let's try to figure out what's on top of the bathroom cabinet at 11 o'clock. First, let's visualize the bathroom setup. The cabinet is likely a white, two-drawer unit. At 11 o'clock (top-left area), we need to check that spot. Looking at the images, especially the one showing the cabinet area, there's a mirror above the sink, but wait, noâ€”wait, the question is about the cabinet. Wait, maybe the "cabinet" here is the storage unit. Wait, no, the sink has a cabinet below? Wait, no, the main cabinet is the white drawers. Wait, in the image where the camera shows the sink and cabinet, the top of the cabinetâ€”wait, no, maybe the mirror? Wait, no, let's check again. Wait, the user says "bathroom cabinet that is on my 11 o'clock". So 11 o'clock would be the upper left area relative to the sink. Looking at the images, there's a mirror mounted on the wall above the sink area. Is that part of the cabinet setup? Wait, no, maybe the cabinet is the white drawers under the sink. Wait, no, the top of that cabinetâ€”wait, no, in some images, there's a mirror reflected or mounted. Wait, no, let's check the first few images. The shower curtain is colorful, then the sink area. Wait, the correct answer: looking at the images, the object on top of the bathroom cabinet (the white drawer unit) at 11 o'clock is a mirror. Wait, no, waitâ€”the mirror is on the wall above the sink. Wait, maybe I misread. Wait, no, let's check again. Wait, the user's question: "What is on top of the bathroom cabinet that is on my 11 o'clock?" So first, identify the cabinet. The white drawer unit near 11 o'clock (top-left). What's on top? Looking at the images, there's a mirror mounted on the wall above that area. Wait, yes! In the reflection or mounted, there's a mirror. So the answer is a mirror.


To determine what is on top of the bathroom cabinet at 11 oâ€™clock, analyze the spatial layout and elements in the images:  

1. Locate the â€œbathroom cabinetâ€ in question: A white, two - drawer storage unit is visible near the top - left (11 oâ€™clock) area relative to other fixtures like the sink and shower.  
2. Examine objects above this cabinet: In the images, a **mirror** is mounted on the wall above this cabinet area.  


Mirror<|im_end|>

Processing batched inference:   0%|          | 0/1 [00:00<?, ?it/s]
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s][AWARNING 01-09 03:10:49 [input_processor.py:243] vLLM has deprecated support for supporting different tokenizers for different LoRAs. By default, vLLM uses base model's tokenizer. If you are using a LoRA with its own tokenizer, consider specifying `--tokenizer [lora_path]` to use the LoRA tokenizer.
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]
Processing batched inference:   0%|          | 0/1 [01:14<?, ?it/s]
Traceback (most recent call last):
  File "/project/6110552/indrisch/LLaMA-Factory/scripts/vllm_infer.py", line 240, in <module>
    fire.Fire(vllm_infer)
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/project/6110552/indrisch/LLaMA-Factory/scripts/vllm_infer.py", line 220, in vllm_infer
    results = llm.generate(vllm_inputs, sampling_params, lora_request=lora_request)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 434, in generate
    self._validate_and_add_requests(
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 1607, in _validate_and_add_requests
    raise e
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 1595, in _validate_and_add_requests
    request_id = self._add_request(
                 ^^^^^^^^^^^^^^^^^^
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 1694, in _add_request
    engine_request, tokenization_kwargs = self._process_inputs(
                                          ^^^^^^^^^^^^^^^^^^^^^
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 1674, in _process_inputs
    engine_request = self.input_processor.process_inputs(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/vllm/v1/engine/input_processor.py", line 441, in process_inputs
    processed_inputs: ProcessorInputs = self.input_preprocessor.preprocess(
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/vllm/inputs/preprocess.py", line 688, in preprocess
    res = self._preprocess(
          ^^^^^^^^^^^^^^^^^
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/vllm/inputs/preprocess.py", line 674, in _preprocess
    return self._process_decoder_only_prompt(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/vllm/inputs/preprocess.py", line 643, in _process_decoder_only_prompt
    prompt_comps = self._prompt_to_llm_inputs(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/vllm/inputs/preprocess.py", line 409, in _prompt_to_llm_inputs
    return self._process_tokens(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/vllm/inputs/preprocess.py", line 341, in _process_tokens
    inputs = self._process_multimodal(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/vllm/inputs/preprocess.py", line 259, in _process_multimodal
    mm_input = mm_processor.apply(
               ^^^^^^^^^^^^^^^^^^^
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/vllm/multimodal/processing.py", line 2124, in apply
    mm_items = self._to_mm_items(mm_data)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/vllm/multimodal/processing.py", line 1379, in _to_mm_items
    self.validate_num_items(modality, len(items))
  File "/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/vllm/multimodal/processing.py", line 1354, in validate_num_items
    raise ValueError(msg)
ValueError: At most 4 image(s) may be provided in one prompt.
[rank0]:[W109 03:10:49.230623892 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
ERROR 01-09 03:10:50 [core_client.py:600] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
