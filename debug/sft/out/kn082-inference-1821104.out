/project/aip-wangcs/indrisch/LLaMA-Factory/scripts /project/aip-wangcs/indrisch/LLaMA-Factory/debug/sft
[WARNING|2026-01-09 13:54:07] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
[INFO|training_args.py:2222] 2026-01-09 13:54:15,020 >> PyTorch: setting up devices
[INFO|training_args.py:1881] 2026-01-09 13:54:15,240 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
[INFO|hub.py:421] 2026-01-09 13:54:17,643 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2026-01-09 13:54:17,675 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2026-01-09 13:54:17,686 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 13:54:17,686 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2026-01-09 13:54:17,686 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 13:54:17,686 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-09 13:54:17,686 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-09 13:54:17,686 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 13:54:17,686 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2026-01-09 13:54:18,030 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|hub.py:421] 2026-01-09 13:54:18,030 >> Offline mode: forcing local_files_only=True
[INFO|hub.py:421] 2026-01-09 13:54:18,031 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:316] 2026-01-09 13:54:18,034 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2026-01-09 13:54:18,037 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:316] 2026-01-09 13:54:18,045 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2026-01-09 13:54:18,047 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:428] 2026-01-09 13:54:18,061 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:1949] 2026-01-09 13:54:18,061 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2026-01-09 13:54:18,066 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 13:54:18,066 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2026-01-09 13:54:18,066 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 13:54:18,067 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-09 13:54:18,067 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-09 13:54:18,067 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 13:54:18,067 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2026-01-09 13:54:18,328 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:660] 2026-01-09 13:54:18,328 >> Offline mode: forcing local_files_only=True
[INFO|video_processing_utils.py:726] 2026-01-09 13:54:18,331 >> loading configuration file video_preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|video_processing_utils.py:770] 2026-01-09 13:54:18,341 >> Video processor Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:928] 2026-01-09 13:54:18,341 >> Offline mode: forcing local_files_only=True
[INFO|processing_utils.py:1116] 2026-01-09 13:54:18,348 >> loading configuration file processor_config.json from cache at None
[INFO|processing_utils.py:1199] 2026-01-09 13:54:18,788 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2_5_VLProcessor"
}

âš™ï¸  Running in WANDB offline mode
INFO 01-09 13:54:18 [arg_utils.py:589] HF_HUB_OFFLINE is True, replace model_id [Qwen/Qwen2.5-VL-7B-Instruct] to model_path [/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5]
INFO 01-09 13:54:18 [utils.py:253] non-default args: {'trust_remote_code': True, 'seed': None, 'max_model_len': 127100, 'tensor_parallel_size': 4, 'disable_log_stats': True, 'limit_mm_per_prompt': {'image': 320}, 'enable_lora': True, 'model': '/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5'}
[WARNING|configuration_utils.py:697] 2026-01-09 13:54:18,844 >> The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[INFO|configuration_utils.py:763] 2026-01-09 13:54:18,847 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
WARNING 01-09 13:54:18 [arg_utils.py:1175] `seed=None` is equivalent to `seed=0` in V1 Engine. You will no longer be allowed to pass `None` in v0.13.
[INFO|configuration_utils.py:763] 2026-01-09 13:54:18,848 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
[INFO|configuration_utils.py:763] 2026-01-09 13:54:18,848 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
[INFO|configuration_utils.py:839] 2026-01-09 13:54:18,854 >> Model config Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 128000,
  "max_window_layers": 28,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "_name_or_path": "/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5",
    "architectures": [
      "Qwen2_5_VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "dtype": "bfloat16",
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "layer_types": [
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention"
    ],
    "max_position_embeddings": 128000,
    "max_window_layers": 28,
    "model_type": "qwen2_5_vl_text",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "use_cache": true,
    "use_sliding_window": false,
    "vision_token_id": 151654,
    "vocab_size": 152064
  },
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 3584,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

[INFO|hub.py:421] 2026-01-09 13:54:18,865 >> Offline mode: forcing local_files_only=True
INFO 01-09 13:54:18 [model.py:637] Resolved architecture: Qwen2_5_VLForConditionalGeneration
INFO 01-09 13:54:18 [model.py:1750] Using max model len 127100
INFO 01-09 13:54:19 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=8192.
[INFO|hub.py:421] 2026-01-09 13:54:19,956 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2026-01-09 13:54:19,957 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2093] 2026-01-09 13:54:19,958 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 13:54:19,958 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2093] 2026-01-09 13:54:19,958 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 13:54:19,958 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 13:54:19,958 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 13:54:19,958 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 13:54:19,958 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2364] 2026-01-09 13:54:20,229 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:939] 2026-01-09 13:54:20,352 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/generation_config.json
[INFO|configuration_utils.py:986] 2026-01-09 13:54:20,353 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 1e-06
}

[INFO|hub.py:421] 2026-01-09 13:54:21,166 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2026-01-09 13:54:21,166 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2093] 2026-01-09 13:54:21,167 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 13:54:21,167 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2093] 2026-01-09 13:54:21,168 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 13:54:21,168 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 13:54:21,168 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 13:54:21,168 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 13:54:21,168 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2364] 2026-01-09 13:54:21,422 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
WARNING 01-09 13:54:21 [system_utils.py:136] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
[WARNING|2026-01-09 13:54:28] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
[0;36m(EngineCore_DP0 pid=3111148)[0;0m INFO 01-09 13:54:32 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5', speculative_config=None, tokenizer='/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=127100, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=3111148)[0;0m WARNING 01-09 13:54:32 [multiproc_executor.py:880] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[WARNING|2026-01-09 13:54:40] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
[WARNING|2026-01-09 13:54:40] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
[WARNING|2026-01-09 13:54:40] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
[WARNING|2026-01-09 13:54:40] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
INFO 01-09 13:54:46 [parallel_state.py:1200] world_size=4 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:42057 backend=nccl
INFO 01-09 13:54:46 [parallel_state.py:1200] world_size=4 rank=2 local_rank=2 distributed_init_method=tcp://127.0.0.1:42057 backend=nccl
INFO 01-09 13:54:46 [parallel_state.py:1200] world_size=4 rank=3 local_rank=3 distributed_init_method=tcp://127.0.0.1:42057 backend=nccl
INFO 01-09 13:54:46 [parallel_state.py:1200] world_size=4 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:42057 backend=nccl
INFO 01-09 13:54:47 [pynccl.py:111] vLLM is using nccl==2.27.7
WARNING 01-09 13:54:47 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 01-09 13:54:47 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 01-09 13:54:47 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 01-09 13:54:47 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 01-09 13:54:47 [custom_all_reduce.py:154] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 01-09 13:54:47 [custom_all_reduce.py:154] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 01-09 13:54:47 [custom_all_reduce.py:154] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 01-09 13:54:47 [custom_all_reduce.py:154] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 01-09 13:54:47 [parallel_state.py:1408] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 3, EP rank 3
INFO 01-09 13:54:47 [parallel_state.py:1408] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 2, EP rank 2
INFO 01-09 13:54:47 [parallel_state.py:1408] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
INFO 01-09 13:54:47 [parallel_state.py:1408] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.
The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.
The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.
The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.
[0;36m(Worker_TP0 pid=3111163)[0;0m INFO 01-09 13:54:50 [gpu_model_runner.py:3467] Starting to load model /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5...
[0;36m(Worker_TP0 pid=3111163)[0;0m INFO 01-09 13:55:24 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(Worker_TP2 pid=3111165)[0;0m INFO 01-09 13:55:24 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(Worker_TP3 pid=3111166)[0;0m INFO 01-09 13:55:24 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(Worker_TP1 pid=3111164)[0;0m INFO 01-09 13:55:24 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(Worker_TP0 pid=3111163)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]
[0;36m(Worker_TP0 pid=3111163)[0;0m Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:07<00:28,  7.19s/it]
[0;36m(Worker_TP0 pid=3111163)[0;0m Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:15<00:22,  7.61s/it]
[0;36m(Worker_TP0 pid=3111163)[0;0m Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:22<00:15,  7.60s/it]
[0;36m(Worker_TP0 pid=3111163)[0;0m Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:30<00:07,  7.66s/it]
[0;36m(Worker_TP0 pid=3111163)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:31<00:00,  5.21s/it]
[0;36m(Worker_TP0 pid=3111163)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:31<00:00,  6.26s/it]
[0;36m(Worker_TP0 pid=3111163)[0;0m 
[0;36m(Worker_TP0 pid=3111163)[0;0m INFO 01-09 13:55:55 [default_loader.py:308] Loading weights took 31.30 seconds
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [lora_model_runner_mixin.py:37] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.
[0;36m(Worker_TP3 pid=3111166)[0;0m INFO 01-09 13:55:55 [punica_selector.py:20] Using PunicaWrapperGPU.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [lora_model_runner_mixin.py:37] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.
[0;36m(Worker_TP0 pid=3111163)[0;0m INFO 01-09 13:55:55 [punica_selector.py:20] Using PunicaWrapperGPU.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.patch_embed.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0 will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2 will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.patch_embed.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0 will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [lora_model_runner_mixin.py:37] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2 will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m INFO 01-09 13:55:55 [punica_selector.py:20] Using PunicaWrapperGPU.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.patch_embed.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0 will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2 will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.attn.proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.attn.qkv will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.merger.mlp.0 will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.mlp.down_proj will be ignored.
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.merger.mlp.2 will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.mlp.down_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.attn.qkv will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.merger.mlp.0 will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.attn.proj will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.merger.mlp.2 will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [lora_model_runner_mixin.py:37] Regarding multimodal models, vLLM currently only supports adding LoRA to language model.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m INFO 01-09 13:55:55 [punica_selector.py:20] Using PunicaWrapperGPU.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.attn.qkv will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.attn.proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.mlp.down_proj will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.merger.mlp.0 will be ignored.
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.merger.mlp.2 will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.patch_embed.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0 will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.0.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.1.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2 will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.2.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.3.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.4.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.5.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.6.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.7.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.8.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.9.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.10.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.11.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.12.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.13.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.14.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.15.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.16.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.17.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.18.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.19.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.20.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.21.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.22.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.23.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.24.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.25.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.26.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.27.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.28.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.29.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.30.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.attn.qkv will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.attn.proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.mlp.gate_up_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.blocks.31.mlp.down_proj will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.merger.mlp.0 will be ignored.
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:55:55 [models.py:510] Regarding multimodal models, vLLM currently only supports adding LoRA to language model, visual.merger.mlp.2 will be ignored.
[0;36m(Worker_TP0 pid=3111163)[0;0m INFO 01-09 13:55:56 [gpu_model_runner.py:3549] Model loading took 4.0333 GiB memory and 64.447138 seconds
[0;36m(Worker_TP2 pid=3111165)[0;0m INFO 01-09 13:55:56 [gpu_model_runner.py:4306] Encoder cache will be initialized with a budget of 114688 tokens, and profiled with 1 video items of the maximum feature size.
[0;36m(Worker_TP3 pid=3111166)[0;0m INFO 01-09 13:55:56 [gpu_model_runner.py:4306] Encoder cache will be initialized with a budget of 114688 tokens, and profiled with 1 video items of the maximum feature size.
[0;36m(Worker_TP0 pid=3111163)[0;0m INFO 01-09 13:55:56 [gpu_model_runner.py:4306] Encoder cache will be initialized with a budget of 114688 tokens, and profiled with 1 video items of the maximum feature size.
[0;36m(Worker_TP1 pid=3111164)[0;0m INFO 01-09 13:55:56 [gpu_model_runner.py:4306] Encoder cache will be initialized with a budget of 114688 tokens, and profiled with 1 video items of the maximum feature size.
[0;36m(Worker_TP0 pid=3111163)[0;0m INFO 01-09 13:56:34 [backends.py:655] Using cache directory: /home/indrisch/.cache/vllm/torch_compile_cache/85e47882d9/rank_0_0/backbone for vLLM's torch.compile
[0;36m(Worker_TP0 pid=3111163)[0;0m INFO 01-09 13:56:34 [backends.py:715] Dynamo bytecode transform time: 14.41 s
[0;36m(Worker_TP0 pid=3111163)[0;0m INFO 01-09 13:56:40 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.792 s
[0;36m(Worker_TP2 pid=3111165)[0;0m INFO 01-09 13:56:40 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.916 s
[0;36m(Worker_TP3 pid=3111166)[0;0m INFO 01-09 13:56:40 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.900 s
[0;36m(Worker_TP1 pid=3111164)[0;0m INFO 01-09 13:56:40 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.967 s
[0;36m(Worker_TP0 pid=3111163)[0;0m INFO 01-09 13:56:42 [monitor.py:34] torch.compile takes 20.20 s in total
[0;36m(Worker_TP0 pid=3111163)[0;0m INFO 01-09 13:56:44 [gpu_worker.py:359] Available KV cache memory: 25.95 GiB
[0;36m(EngineCore_DP0 pid=3111148)[0;0m INFO 01-09 13:56:44 [kv_cache_utils.py:1286] GPU KV cache size: 1,943,712 tokens
[0;36m(EngineCore_DP0 pid=3111148)[0;0m INFO 01-09 13:56:44 [kv_cache_utils.py:1291] Maximum concurrency for 127,100 tokens per request: 15.29x
[0;36m(Worker_TP0 pid=3111163)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/102 [00:00<?, ?it/s][0;36m(Worker_TP0 pid=3111163)[0;0m WARNING 01-09 13:56:45 [utils.py:250] Using default LoRA kernel configs
[0;36m(Worker_TP3 pid=3111166)[0;0m WARNING 01-09 13:56:45 [utils.py:250] Using default LoRA kernel configs
[0;36m(Worker_TP2 pid=3111165)[0;0m WARNING 01-09 13:56:45 [utils.py:250] Using default LoRA kernel configs
[0;36m(Worker_TP1 pid=3111164)[0;0m WARNING 01-09 13:56:45 [utils.py:250] Using default LoRA kernel configs
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   1%|          | 1/102 [00:04<06:56,  4.12s/it]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|â–Ž         | 3/102 [00:04<01:52,  1.14s/it]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|â–         | 5/102 [00:04<00:58,  1.67it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   7%|â–‹         | 7/102 [00:04<00:36,  2.60it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|â–‰         | 9/102 [00:04<00:25,  3.65it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|â–ˆ         | 11/102 [00:05<00:19,  4.74it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  13%|â–ˆâ–Ž        | 13/102 [00:05<00:15,  5.85it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  15%|â–ˆâ–        | 15/102 [00:05<00:12,  6.90it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|â–ˆâ–‹        | 17/102 [00:05<00:11,  7.72it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|â–ˆâ–Š        | 19/102 [00:05<00:09,  8.49it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|â–ˆâ–ˆ        | 21/102 [00:05<00:08,  9.15it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|â–ˆâ–ˆâ–Ž       | 23/102 [00:06<00:08,  9.63it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|â–ˆâ–ˆâ–       | 25/102 [00:06<00:07,  9.98it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|â–ˆâ–ˆâ–‹       | 27/102 [00:06<00:07, 10.24it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|â–ˆâ–ˆâ–Š       | 29/102 [00:06<00:07, 10.40it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  30%|â–ˆâ–ˆâ–ˆ       | 31/102 [00:06<00:06, 10.38it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|â–ˆâ–ˆâ–ˆâ–      | 33/102 [00:07<00:06, 10.58it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|â–ˆâ–ˆâ–ˆâ–      | 35/102 [00:09<00:27,  2.43it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|â–ˆâ–ˆâ–ˆâ–‹      | 37/102 [00:09<00:20,  3.16it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 39/102 [00:09<00:15,  4.02it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/102 [00:09<00:12,  4.93it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 43/102 [00:10<00:10,  5.86it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 45/102 [00:10<00:08,  6.80it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 47/102 [00:10<00:07,  7.60it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 49/102 [00:10<00:06,  8.31it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/102 [00:10<00:05,  8.92it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 53/102 [00:11<00:05,  9.45it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 55/102 [00:11<00:05,  8.13it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 57/102 [00:11<00:06,  7.45it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 59/102 [00:11<00:05,  8.14it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 61/102 [00:12<00:05,  7.36it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 63/102 [00:12<00:04,  8.13it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 64/102 [00:12<00:05,  7.26it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 65/102 [00:12<00:04,  7.49it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 67/102 [00:14<00:12,  2.85it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 69/102 [00:15<00:15,  2.08it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 71/102 [00:15<00:10,  2.83it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 73/102 [00:16<00:07,  3.66it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 75/102 [00:16<00:05,  4.58it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 77/102 [00:16<00:04,  5.53it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 79/102 [00:16<00:03,  6.38it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 81/102 [00:16<00:02,  7.22it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 83/102 [00:17<00:02,  7.91it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 85/102 [00:17<00:02,  8.48it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 87/102 [00:17<00:01,  8.93it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 89/102 [00:17<00:01,  9.32it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 91/102 [00:17<00:01,  9.44it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 93/102 [00:18<00:00,  9.69it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 95/102 [00:18<00:00,  9.81it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 97/102 [00:18<00:00,  9.77it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 99/102 [00:18<00:00,  9.59it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 101/102 [00:23<00:00,  1.29it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102/102 [00:23<00:00,  4.36it/s]
[0;36m(Worker_TP0 pid=3111163)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/70 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   1%|â–         | 1/70 [00:00<00:17,  3.99it/s]Capturing CUDA graphs (decode, FULL):   4%|â–         | 3/70 [00:00<00:08,  7.52it/s]Capturing CUDA graphs (decode, FULL):   7%|â–‹         | 5/70 [00:00<00:07,  8.60it/s]Capturing CUDA graphs (decode, FULL):  10%|â–ˆ         | 7/70 [00:00<00:06,  9.52it/s]Capturing CUDA graphs (decode, FULL):  13%|â–ˆâ–Ž        | 9/70 [00:01<00:06,  9.93it/s]Capturing CUDA graphs (decode, FULL):  16%|â–ˆâ–Œ        | 11/70 [00:01<00:05, 10.29it/s]Capturing CUDA graphs (decode, FULL):  19%|â–ˆâ–Š        | 13/70 [00:01<00:05, 10.54it/s]Capturing CUDA graphs (decode, FULL):  21%|â–ˆâ–ˆâ–       | 15/70 [00:01<00:05, 10.77it/s]Capturing CUDA graphs (decode, FULL):  24%|â–ˆâ–ˆâ–       | 17/70 [00:01<00:04, 10.94it/s]Capturing CUDA graphs (decode, FULL):  27%|â–ˆâ–ˆâ–‹       | 19/70 [00:01<00:04, 11.06it/s]Capturing CUDA graphs (decode, FULL):  30%|â–ˆâ–ˆâ–ˆ       | 21/70 [00:02<00:04, 11.15it/s]Capturing CUDA graphs (decode, FULL):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 23/70 [00:02<00:04, 11.27it/s]Capturing CUDA graphs (decode, FULL):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 25/70 [00:02<00:03, 11.37it/s]Capturing CUDA graphs (decode, FULL):  39%|â–ˆâ–ˆâ–ˆâ–Š      | 27/70 [00:02<00:03, 11.43it/s]Capturing CUDA graphs (decode, FULL):  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 29/70 [00:02<00:03, 11.22it/s]Capturing CUDA graphs (decode, FULL):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 31/70 [00:02<00:03, 11.34it/s]Capturing CUDA graphs (decode, FULL):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 33/70 [00:03<00:03, 11.34it/s]Capturing CUDA graphs (decode, FULL):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 35/70 [00:03<00:03, 11.38it/s]Capturing CUDA graphs (decode, FULL):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 37/70 [00:03<00:02, 11.20it/s]Capturing CUDA graphs (decode, FULL):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 39/70 [00:03<00:02, 11.30it/s]Capturing CUDA graphs (decode, FULL):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 41/70 [00:03<00:02, 11.33it/s]Capturing CUDA graphs (decode, FULL):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 43/70 [00:04<00:02, 11.43it/s]Capturing CUDA graphs (decode, FULL):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 45/70 [00:04<00:02, 11.30it/s]Capturing CUDA graphs (decode, FULL):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 47/70 [00:04<00:02, 11.14it/s]Capturing CUDA graphs (decode, FULL):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 49/70 [00:04<00:01, 11.29it/s]Capturing CUDA graphs (decode, FULL):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 51/70 [00:04<00:01, 11.39it/s]Capturing CUDA graphs (decode, FULL):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 53/70 [00:04<00:01, 11.35it/s]Capturing CUDA graphs (decode, FULL):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 55/70 [00:05<00:01, 11.24it/s]Capturing CUDA graphs (decode, FULL):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 57/70 [00:05<00:01, 11.30it/s]Capturing CUDA graphs (decode, FULL):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 59/70 [00:05<00:01, 10.91it/s]Capturing CUDA graphs (decode, FULL):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 61/70 [00:05<00:00, 10.81it/s]Capturing CUDA graphs (decode, FULL):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 63/70 [00:05<00:00, 10.99it/s]Capturing CUDA graphs (decode, FULL):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 65/70 [00:05<00:00, 11.08it/s]Capturing CUDA graphs (decode, FULL):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 67/70 [00:06<00:00, 11.21it/s]Capturing CUDA graphs (decode, FULL):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 69/70 [00:06<00:00, 11.31it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70/70 [00:06<00:00, 10.95it/s]
[0;36m(Worker_TP0 pid=3111163)[0;0m INFO 01-09 13:57:15 [gpu_model_runner.py:4466] Graph capturing finished in 31 secs, took 2.39 GiB
[0;36m(EngineCore_DP0 pid=3111148)[0;0m INFO 01-09 13:57:15 [core.py:254] init engine (profile, create kv cache, warmup model) took 79.23 seconds
[0;36m(EngineCore_DP0 pid=3111148)[0;0m The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.
INFO 01-09 13:57:20 [llm.py:343] Supported tasks: ['generate']
[INFO|2026-01-09 13:57:20] llamafactory.data.loader:143 >> Loading dataset /scratch/indrisch/huggingface/hub/datasets--cvis-tmu--llamafactory-sqa3d-traces-multiimage-vqa/snapshots/ce5c54adc1608d1726730c9ff334e65b6dd70e46/data/...
Converting format of dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<?, ? examples/s]Converting format of dataset (num_proc=32): 102 examples [00:00,  2.63 examples/s]          Converting format of dataset (num_proc=32): 155 examples [00:00, 85.94 examples/s]Converting format of dataset (num_proc=32): 196 examples [00:00, 144.51 examples/s]Converting format of dataset (num_proc=32): 200 examples [00:01, 83.76 examples/s] 
Running tokenizer on dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=32): 104 examples [00:01,  2.96 examples/s]          Running tokenizer on dataset (num_proc=32): 112 examples [00:01,  9.65 examples/s]Running tokenizer on dataset (num_proc=32): 119 examples [00:01, 15.18 examples/s]Running tokenizer on dataset (num_proc=32): 125 examples [00:01, 19.26 examples/s]Running tokenizer on dataset (num_proc=32): 131 examples [00:02, 22.25 examples/s]Running tokenizer on dataset (num_proc=32): 137 examples [00:02, 27.96 examples/s]Running tokenizer on dataset (num_proc=32): 143 examples [00:02, 25.77 examples/s]Running tokenizer on dataset (num_proc=32): 149 examples [00:02, 28.03 examples/s]Running tokenizer on dataset (num_proc=32): 155 examples [00:02, 29.35 examples/s]Running tokenizer on dataset (num_proc=32): 161 examples [00:02, 29.92 examples/s]Running tokenizer on dataset (num_proc=32): 167 examples [00:03, 31.12 examples/s]Running tokenizer on dataset (num_proc=32): 173 examples [00:03, 32.15 examples/s]Running tokenizer on dataset (num_proc=32): 179 examples [00:03, 32.76 examples/s]Running tokenizer on dataset (num_proc=32): 185 examples [00:03, 36.37 examples/s]Running tokenizer on dataset (num_proc=32): 191 examples [00:03, 34.72 examples/s]Running tokenizer on dataset (num_proc=32): 197 examples [00:03, 34.37 examples/s]Running tokenizer on dataset (num_proc=32): 200 examples [00:04, 23.45 examples/s]
training example:
input_ids:
[151644, 8948, 271, 262, 1446, 525, 264, 16585, 11129, 4142, 11528, 17847, 369, 27979, 647, 47522, 389, 264, 3175, 29519, 6109, 6839, 3941, 5248, 5335, 382, 262, 26149, 510, 262, 68252, 1283, 419, 1943, 11, 498, 686, 5258, 451, 6109, 5335, 438, 366, 1805, 29, 9492, 13, 18877, 1105, 438, 2155, 6194, 315, 279, 83490, 6109, 13, 28634, 311, 5335, 553, 220, 16, 5980, 1922, 304, 1973, 315, 11094, 25, 508, 16, 1125, 508, 17, 1125, 4593, 11, 508, 45, 29562, 262, 3596, 35304, 1718, 8575, 38439, 3298, 14017, 510, 262, 7288, 7854, 264, 12966, 220, 18, 35, 10502, 1614, 3941, 6194, 25, 3754, 6171, 3941, 5335, 11, 23583, 8674, 6249, 17040, 11, 323, 990, 5452, 14, 509, 8957, 14688, 21060, 369, 7990, 55916, 624, 262, 7288, 72077, 409, 849, 292, 3793, 26087, 983, 847, 1290, 97089, 17996, 4065, 14, 29898, 484, 32511, 56111, 7612, 1825, 35978, 28455, 448, 5091, 311, 279, 64171, 14, 8092, 13, 1416, 279, 58385, 374, 54761, 11, 1638, 311, 279, 1429, 38219, 8622, 1651, 323, 1977, 773, 624, 262, 7288, 5443, 1172, 9434, 5904, 26, 653, 537, 17023, 63133, 3565, 476, 17188, 389, 9250, 6540, 7797, 6770, 1633, 43898, 3793, 624, 262, 7288, 3197, 5248, 11178, 3000, 11, 8830, 553, 279, 1850, 2432, 311, 27979, 17133, 488, 4274, 1835, 3941, 6194, 13, 1416, 2058, 54761, 1283, 13295, 678, 5335, 11, 1584, 429, 9355, 382, 262, 30990, 320, 327, 32739, 1378, 10010, 11, 304, 419, 1973, 982, 262, 366, 26865, 397, 262, 14822, 14319, 29208, 32711, 304, 220, 18, 4142, 16, 15, 2805, 7354, 13, 356, 632, 5904, 448, 2168, 14937, 320, 68, 1302, 2572, 65750, 18, 5669, 3100, 7579, 18010, 1290, 315, 8718, 26, 508, 16, 5669, 1852, 1894, 11, 42396, 64212, 2823, 63594, 714, 4583, 320, 2640, 37294, 17, 20, 15, 11211, 7241, 5871, 568, 7036, 894, 17796, 8957, 14, 2969, 26745, 487, 323, 1246, 498, 19673, 432, 624, 262, 690, 26865, 397, 262, 366, 9217, 397, 262, 362, 3175, 63594, 4226, 1172, 11, 892, 1231, 387, 510, 262, 7288, 1416, 14016, 25, 3776, 37328, 4226, 11, 4504, 320, 68, 1302, 2572, 1036, 17, 32511, 476, 7414, 14, 2753, 320, 68, 1302, 2572, 1036, 9693, 854, 4292, 262, 7288, 1416, 537, 14016, 25, 362, 11652, 476, 1378, 624, 262, 1416, 4226, 537, 9434, 11, 421, 1052, 374, 902, 2797, 5904, 11, 476, 421, 279, 4226, 374, 35197, 54761, 25, 1036, 33260, 8253, 854, 624, 262, 690, 9217, 1339, 262, 62947, 609, 43797, 50, 510, 262, 7288, 3155, 4183, 13153, 279, 3405, 476, 1140, 5335, 26, 653, 4183, 2550, 4718, 476, 4960, 14158, 624, 262, 7288, 84468, 4285, 1894, 5036, 448, 518, 1429, 825, 22739, 26087, 4145, 3446, 838, 4322, 1574, 58689, 7256, 854, 4292, 262, 7288, 2823, 12966, 3941, 6194, 26, 421, 6194, 28295, 11, 62552, 279, 11299, 15432, 5904, 323, 5185, 432, 304, 366, 26865, 29816, 257, 151645, 198, 151644, 872, 198, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 30, 220, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system

    You are a careful visionâ€“language assistant for spatial VQA on a single indoor scene shown across multiple images.

    INPUT:
    Immediately after this message, you will receive N scene images as <image> tags. Treat them as different views of the SAME scene. Refer to images by 1-based index in order of appearance: [1], [2], â€¦, [N].

    REASONING PRINCIPLES:
    â€¢ Build a consistent 3D mental model across views: track objects across images, infer relative camera pose, and use scale/occlusion/shadows for depth cues.
    â€¢ Interpret deictic terms (â€œto my right/left/in front/behindâ€) EGOCENTRICALLY with respect to the narrator/agent. If the viewpoint is ambiguous, default to the most informative central view and say so.
    â€¢ Use only visible evidence; do not invent unseen details or rely on external knowledge beyond basic object/color terms.
    â€¢ When multiple candidates exist, resolve by the best match to spatial phrase + salience across views. If still ambiguous after checking all images, state that clearly.

    OUTPUT (exactly two blocks, in this order):
    <think>
    Step-by-step reasoning in 3â€“10 short steps. Cite evidence with image indices (e.g., â€œ[3]: light wood desk right of monitor; [1]: same color, confirmsâ€). Be concise but complete (aim â‰¤250 tokens unless necessary). Note any occlusion/ambiguity and how you resolved it.
    </think>
    <answer>
    A single concise answer only, which may be:
    â€¢ If sufficient: One-word answer, Count (e.g., â€œ2â€) or Yes/No (e.g., â€œyesâ€).
    â€¢ If not sufficient: A sentence or two.
    If answer not visible, if there is no clear evidence, or if the answer is genuinely ambiguous: â€œcannot determineâ€.
    </answer>

    STYLE & RULES:
    â€¢ Do NOT repeat the question or list images; do NOT output JSON or extra sections.
    â€¢ Prefer simple color names with at most one modifier (â€œlight/dark/pale/beigeâ€).
    â€¢ Be consistent across views; if views disagree, prioritize the clearest evidence and note it in <think>.
    <|im_end|>
<|im_start|>user
What is on top of the bathroom cabinet that is on my 11 o'clock? <|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|im_end|>
<|im_start|>assistant

label_ids:
[198, 32462, 432, 11, 1077, 594, 1430, 311, 7071, 700, 1128, 594, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 62410, 13, 5512, 11, 1077, 594, 50087, 279, 14852, 6505, 13, 576, 21921, 374, 4363, 264, 4158, 11, 1378, 1737, 95112, 4982, 13, 2411, 220, 16, 16, 297, 62410, 320, 3481, 7950, 3082, 701, 582, 1184, 311, 1779, 429, 7702, 13, 21131, 518, 279, 5335, 11, 5310, 279, 825, 9027, 279, 21921, 3082, 11, 1052, 594, 264, 17846, 3403, 279, 19309, 11, 714, 3783, 11, 902, 2293, 11489, 11, 279, 3405, 374, 911, 279, 21921, 13, 13824, 11, 7196, 279, 330, 66, 75512, 1, 1588, 374, 279, 5819, 4982, 13, 13824, 11, 902, 11, 279, 19309, 702, 264, 21921, 3685, 30, 13824, 11, 902, 11, 279, 1887, 21921, 374, 279, 4158, 61607, 13, 13824, 11, 304, 279, 2168, 1380, 279, 6249, 4933, 279, 19309, 323, 21921, 11, 279, 1909, 315, 279, 21921, 2293, 11489, 11, 902, 11, 7196, 279, 17846, 30, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 2727, 330, 65, 77832, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 3263, 2055, 220, 16, 16, 297, 62410, 1035, 387, 279, 8416, 2115, 3082, 8674, 311, 279, 19309, 13, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 279, 19309, 3082, 13, 2160, 429, 949, 315, 279, 21921, 6505, 30, 13824, 11, 902, 11, 7196, 279, 21921, 374, 279, 4158, 61607, 1212, 279, 19309, 13, 13824, 11, 902, 11, 279, 1909, 315, 429, 21921, 2293, 11489, 11, 902, 11, 304, 1045, 5335, 11, 1052, 594, 264, 17846, 25911, 476, 21810, 13, 13824, 11, 902, 11, 1077, 594, 1779, 279, 1156, 2421, 5335, 13, 576, 17196, 45766, 374, 33866, 11, 1221, 279, 19309, 3082, 13, 13824, 11, 279, 4396, 4226, 25, 3330, 518, 279, 5335, 11, 279, 1633, 389, 1909, 315, 279, 14852, 21921, 320, 1782, 4158, 26482, 4982, 8, 518, 220, 16, 16, 297, 62410, 374, 264, 17846, 13, 13824, 11, 902, 11, 3783, 21677, 17846, 374, 389, 279, 7002, 3403, 279, 19309, 13, 13824, 11, 7196, 358, 5786, 878, 13, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 594, 3405, 25, 330, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 7521, 2055, 1156, 11, 10542, 279, 21921, 13, 576, 4158, 26482, 4982, 3143, 220, 16, 16, 297, 62410, 320, 3481, 7950, 568, 3555, 594, 389, 1909, 30, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 429, 3082, 13, 13824, 11, 9834, 0, 758, 279, 21844, 476, 21810, 11, 1052, 594, 264, 17846, 13, 2055, 279, 4226, 374, 264, 17846, 4192, 1249, 8253, 1128, 374, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 527, 20666, 11, 23643, 279, 27979, 6789, 323, 5424, 304, 279, 5335, 25, 18611, 16, 13, 95210, 279, 1036, 65, 77832, 21921, 854, 304, 3405, 25, 362, 4158, 11, 1378, 481, 26482, 5819, 4982, 374, 9434, 3143, 279, 1909, 481, 2115, 320, 16, 16, 297, 527, 20666, 8, 3082, 8674, 311, 1008, 37664, 1075, 279, 19309, 323, 17196, 13, 2303, 17, 13, 1374, 19991, 6171, 3403, 419, 21921, 25, 758, 279, 5335, 11, 264, 3070, 73225, 334, 374, 21810, 389, 279, 7002, 3403, 419, 21921, 3082, 13, 79226, 54216, 151645, 198]
labels:

Got it, let's try to figure out what's on top of the bathroom cabinet at 11 o'clock. First, let's visualize the bathroom setup. The cabinet is likely a white, two-drawer unit. At 11 o'clock (top-left area), we need to check that spot. Looking at the images, especially the one showing the cabinet area, there's a mirror above the sink, but wait, noâ€”wait, the question is about the cabinet. Wait, maybe the "cabinet" here is the storage unit. Wait, no, the sink has a cabinet below? Wait, no, the main cabinet is the white drawers. Wait, in the image where the camera shows the sink and cabinet, the top of the cabinetâ€”wait, no, maybe the mirror? Wait, no, let's check again. Wait, the user says "bathroom cabinet that is on my 11 o'clock". So 11 o'clock would be the upper left area relative to the sink. Looking at the images, there's a mirror mounted on the wall above the sink area. Is that part of the cabinet setup? Wait, no, maybe the cabinet is the white drawers under the sink. Wait, no, the top of that cabinetâ€”wait, no, in some images, there's a mirror reflected or mounted. Wait, no, let's check the first few images. The shower curtain is colorful, then the sink area. Wait, the correct answer: looking at the images, the object on top of the bathroom cabinet (the white drawer unit) at 11 o'clock is a mirror. Wait, no, waitâ€”the mirror is on the wall above the sink. Wait, maybe I misread. Wait, no, let's check again. Wait, the user's question: "What is on top of the bathroom cabinet that is on my 11 o'clock?" So first, identify the cabinet. The white drawer unit near 11 o'clock (top-left). What's on top? Looking at the images, there's a mirror mounted on the wall above that area. Wait, yes! In the reflection or mounted, there's a mirror. So the answer is a mirror.


To determine what is on top of the bathroom cabinet at 11 oâ€™clock, analyze the spatial layout and elements in the images:  

1. Locate the â€œbathroom cabinetâ€ in question: A white, two - drawer storage unit is visible near the top - left (11 oâ€™clock) area relative to other fixtures like the sink and shower.  
2. Examine objects above this cabinet: In the images, a **mirror** is mounted on the wall above this cabinet area.  


Mirror<|im_end|>

Processing batched inference:   0%|          | 0/1 [00:00<?, ?it/s]
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s][AWARNING 01-09 13:58:25 [input_processor.py:243] vLLM has deprecated support for supporting different tokenizers for different LoRAs. By default, vLLM uses base model's tokenizer. If you are using a LoRA with its own tokenizer, consider specifying `--tokenizer [lora_path]` to use the LoRA tokenizer.
[INFO|image_processing_base.py:316] 2026-01-09 13:58:25,327 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:381] 2026-01-09 13:58:25,327 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[WARNING|logging.py:328] 2026-01-09 13:58:25,327 >> The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.
[INFO|image_processing_base.py:428] 2026-01-09 13:58:25,328 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:1949] 2026-01-09 13:58:25,328 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2093] 2026-01-09 13:58:25,329 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 13:58:25,329 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2093] 2026-01-09 13:58:25,329 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 13:58:25,329 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 13:58:25,329 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 13:58:25,329 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 13:58:25,329 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2364] 2026-01-09 13:58:25,601 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:660] 2026-01-09 13:58:25,601 >> Offline mode: forcing local_files_only=True
[INFO|video_processing_utils.py:724] 2026-01-09 13:58:25,602 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|video_processing_utils.py:770] 2026-01-09 13:58:25,602 >> Video processor Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:928] 2026-01-09 13:58:25,602 >> Offline mode: forcing local_files_only=True
[INFO|processing_utils.py:1114] 2026-01-09 13:58:25,603 >> loading configuration file None
[INFO|processing_utils.py:1199] 2026-01-09 13:58:26,047 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2_5_VLProcessor"
}

[INFO|image_processing_base.py:316] 2026-01-09 13:58:26,048 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:381] 2026-01-09 13:58:26,049 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:428] 2026-01-09 13:58:26,049 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:1949] 2026-01-09 13:58:26,049 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2093] 2026-01-09 13:58:26,049 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 13:58:26,049 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2093] 2026-01-09 13:58:26,049 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 13:58:26,050 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 13:58:26,050 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 13:58:26,050 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 13:58:26,050 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2364] 2026-01-09 13:58:26,832 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:660] 2026-01-09 13:58:26,832 >> Offline mode: forcing local_files_only=True
[INFO|video_processing_utils.py:724] 2026-01-09 13:58:26,832 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|video_processing_utils.py:770] 2026-01-09 13:58:26,833 >> Video processor Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:928] 2026-01-09 13:58:27,400 >> Offline mode: forcing local_files_only=True
[INFO|processing_utils.py:1114] 2026-01-09 13:58:27,402 >> loading configuration file None
[INFO|processing_utils.py:1199] 2026-01-09 13:58:27,980 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: CachedQwen2TokenizerFast(name_or_path='/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2_5_VLProcessor"
}


Adding requests:   1%|          | 1/100 [00:04<06:54,  4.19s/it][A
Adding requests:   2%|â–         | 2/100 [00:05<03:49,  2.34s/it][A
Adding requests:   3%|â–Ž         | 3/100 [00:10<05:50,  3.62s/it][A
Adding requests:   4%|â–         | 4/100 [00:11<03:59,  2.50s/it][A
Adding requests:   5%|â–Œ         | 5/100 [00:13<03:49,  2.41s/it][A
Adding requests:   6%|â–Œ         | 6/100 [00:15<03:34,  2.29s/it][A
Adding requests:   7%|â–‹         | 7/100 [00:16<02:41,  1.73s/it][A
Adding requests:   8%|â–Š         | 8/100 [00:17<02:21,  1.54s/it][A
Adding requests:   9%|â–‰         | 9/100 [00:19<02:36,  1.72s/it][A
Adding requests:  10%|â–ˆ         | 10/100 [00:23<03:38,  2.43s/it][A
Adding requests:  11%|â–ˆ         | 11/100 [00:24<02:49,  1.90s/it][A
Adding requests:  12%|â–ˆâ–        | 12/100 [00:26<03:01,  2.06s/it][A
Adding requests:  13%|â–ˆâ–Ž        | 13/100 [00:29<03:25,  2.36s/it][A
Adding requests:  14%|â–ˆâ–        | 14/100 [00:31<03:08,  2.19s/it][A
Adding requests:  15%|â–ˆâ–Œ        | 15/100 [00:32<02:35,  1.83s/it][A
Adding requests:  16%|â–ˆâ–Œ        | 16/100 [00:33<02:15,  1.61s/it][A
Adding requests:  18%|â–ˆâ–Š        | 18/100 [00:34<01:41,  1.24s/it][A
Adding requests:  19%|â–ˆâ–‰        | 19/100 [00:37<02:03,  1.52s/it][A
Adding requests:  20%|â–ˆâ–ˆ        | 20/100 [00:40<02:34,  1.93s/it][A
Adding requests:  21%|â–ˆâ–ˆ        | 21/100 [00:42<02:32,  1.94s/it][A
Adding requests:  22%|â–ˆâ–ˆâ–       | 22/100 [00:43<02:15,  1.74s/it][A
Adding requests:  23%|â–ˆâ–ˆâ–Ž       | 23/100 [00:46<02:42,  2.11s/it][A
Adding requests:  24%|â–ˆâ–ˆâ–       | 24/100 [00:47<02:18,  1.83s/it][A
Adding requests:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [00:49<02:03,  1.65s/it][A
Adding requests:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [00:50<02:06,  1.71s/it][A
Adding requests:  27%|â–ˆâ–ˆâ–‹       | 27/100 [00:55<03:14,  2.66s/it][A
Adding requests:  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:56<02:39,  2.22s/it][A
Adding requests:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [00:59<02:03,  1.77s/it][A
Adding requests:  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [01:00<01:50,  1.60s/it][A
Adding requests:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [01:02<01:56,  1.71s/it][A
Adding requests:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [01:03<01:47,  1.61s/it][A
Adding requests:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [01:06<02:04,  1.89s/it][A
Adding requests:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [01:07<01:49,  1.69s/it][A
Adding requests:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [01:10<02:10,  2.04s/it][A
Adding requests:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [01:12<02:11,  2.08s/it][A
Adding requests:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [01:14<01:54,  1.84s/it][A
Adding requests:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [01:16<02:04,  2.04s/it][A
Adding requests:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [01:17<01:47,  1.79s/it][A
Adding requests:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [01:20<01:59,  2.02s/it][A
Adding requests:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [01:22<01:53,  1.96s/it][A
Adding requests:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [01:23<01:46,  1.86s/it][A
Adding requests:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [01:23<00:56,  1.03s/it][A
Adding requests:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [01:25<01:00,  1.12s/it][A
Adding requests:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [01:28<01:31,  1.72s/it][A
Adding requests:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [01:29<01:20,  1.55s/it][A
Adding requests:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [01:31<01:22,  1.62s/it][A
Adding requests:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [01:33<01:28,  1.78s/it][A
Adding requests:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [01:35<01:20,  1.64s/it][A
Adding requests:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [01:35<00:58,  1.21s/it][A
Adding requests:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [01:42<02:19,  2.96s/it][A
Adding requests:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [01:43<01:53,  2.48s/it][A
Adding requests:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [01:45<01:48,  2.41s/it][A
Adding requests:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [01:48<01:45,  2.40s/it][A
Adding requests:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [01:51<01:46,  2.48s/it][A
Adding requests:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [01:52<01:27,  2.09s/it][A
Adding requests:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [01:55<01:43,  2.53s/it][A
Adding requests:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [01:58<01:38,  2.47s/it][A
Adding requests:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [01:59<01:23,  2.13s/it][A
Adding requests:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [02:01<01:15,  1.99s/it][A
Adding requests:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [02:05<01:42,  2.76s/it][A
Adding requests:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [02:07<01:24,  2.35s/it][A
Adding requests:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [02:08<01:09,  1.99s/it][A
Adding requests:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [02:11<01:25,  2.53s/it][A
Adding requests:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [02:12<01:05,  1.98s/it][A
Adding requests:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [02:14<00:58,  1.81s/it][A
Adding requests:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [02:15<00:53,  1.73s/it][A
Adding requests:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [02:17<00:54,  1.81s/it][A
Adding requests:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [02:22<01:16,  2.63s/it][A
Adding requests:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [02:23<01:03,  2.27s/it][A
Adding requests:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [02:25<01:00,  2.24s/it][A
Adding requests:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [02:27<00:54,  2.11s/it][A
Adding requests:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [02:28<00:44,  1.80s/it][A
Adding requests:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [02:30<00:41,  1.75s/it][A
Adding requests:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [02:31<00:35,  1.54s/it][A
Adding requests:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [02:33<00:38,  1.77s/it][A
Adding requests:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [02:35<00:35,  1.70s/it][A
Adding requests:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [02:37<00:39,  1.98s/it][A
Adding requests:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [02:39<00:37,  1.99s/it][A
Adding requests:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [02:41<00:31,  1.76s/it][A
Adding requests:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [02:42<00:27,  1.63s/it][A
Adding requests:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [02:44<00:29,  1.87s/it][A
Adding requests:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [02:46<00:28,  1.87s/it][A
Adding requests:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [02:48<00:24,  1.77s/it][A
Adding requests:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [02:49<00:22,  1.69s/it][A
Adding requests:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [02:51<00:20,  1.67s/it][A
Adding requests:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [02:53<00:18,  1.72s/it][A
Adding requests:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [02:55<00:19,  1.95s/it][A
Adding requests:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [03:00<00:26,  2.97s/it][A
Adding requests:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [03:02<00:19,  2.41s/it][A
Adding requests:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [03:05<00:19,  2.76s/it][A
Adding requests:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [03:06<00:13,  2.18s/it][A
Adding requests:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [03:07<00:05,  1.38s/it][A
Adding requests:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [03:08<00:03,  1.26s/it][A
Adding requests:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [03:11<00:03,  1.84s/it][A
Adding requests:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [03:11<00:01,  1.37s/it][A
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [03:13<00:00,  1.50s/it][AAdding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [03:13<00:00,  1.94s/it]

Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:04<00:00, 18.03it/s, est. speed input: 129803.57 toks/s, output: 4322.31 toks/s][A
Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:05<00:00, 15.72it/s, est. speed input: 118382.01 toks/s, output: 4121.25 toks/s][A
Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:05<00:00, 14.46it/s, est. speed input: 115017.30 toks/s, output: 3887.50 toks/s][A
Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:05<00:00, 14.52it/s, est. speed input: 115624.58 toks/s, output: 3857.87 toks/s][A
Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [00:06<00:00, 12.54it/s, est. speed input: 108928.75 toks/s, output: 3676.99 toks/s][A
Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [00:07<00:00,  7.54it/s, est. speed input: 93701.75 toks/s, output: 3272.17 toks/s] [A
Processed prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [00:12<00:01,  2.40it/s, est. speed input: 58668.15 toks/s, output: 2183.24 toks/s][A
Processed prompts:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [00:12<00:00,  2.29it/s, est. speed input: 56998.42 toks/s, output: 2126.38 toks/s][A
Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [00:12<00:00,  2.41it/s, est. speed input: 56201.28 toks/s, output: 2150.40 toks/s][A
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:16<00:00,  1.13it/s, est. speed input: 43403.73 toks/s, output: 1709.01 toks/s][A
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:16<00:00,  1.13it/s, est. speed input: 43403.73 toks/s, output: 1709.01 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:16<00:00,  5.92it/s, est. speed input: 43403.73 toks/s, output: 1709.01 toks/s]
Processing batched inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:29<00:00, 269.78s/it]Processing batched inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:29<00:00, 269.78s/it]
[0;36m(Worker_TP0 pid=3111163)[0;0m INFO 01-09 14:01:56 [multiproc_executor.py:709] Parent process exited, terminating worker
[0;36m(Worker_TP3 pid=3111166)[0;0m INFO 01-09 14:01:56 [multiproc_executor.py:709] Parent process exited, terminating worker
[0;36m(Worker_TP1 pid=3111164)[0;0m INFO 01-09 14:01:56 [multiproc_executor.py:709] Parent process exited, terminating worker
[0;36m(Worker_TP2 pid=3111165)[0;0m INFO 01-09 14:01:56 [multiproc_executor.py:709] Parent process exited, terminating worker
**********************************************************************
100 total generated results have been saved at /project/aip-wangcs/indrisch/LLaMA-Factory/debug/sft/out/generated_predictions_Qwen25VL7BInstruct_SQA3Dep1.jsonl.
**********************************************************************
[WARNING|2026-01-09 14:02:12] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
[INFO|training_args.py:2222] 2026-01-09 14:02:15,769 >> PyTorch: setting up devices
[INFO|training_args.py:1881] 2026-01-09 14:02:15,928 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
[INFO|hub.py:421] 2026-01-09 14:02:17,013 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2026-01-09 14:02:17,025 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2026-01-09 14:02:17,036 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 14:02:17,036 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2026-01-09 14:02:17,036 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 14:02:17,036 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-09 14:02:17,036 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-09 14:02:17,036 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 14:02:17,036 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2026-01-09 14:02:17,305 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|hub.py:421] 2026-01-09 14:02:17,305 >> Offline mode: forcing local_files_only=True
[INFO|hub.py:421] 2026-01-09 14:02:17,306 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:316] 2026-01-09 14:02:17,308 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2026-01-09 14:02:17,312 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:316] 2026-01-09 14:02:17,316 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2026-01-09 14:02:17,319 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:428] 2026-01-09 14:02:17,326 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:1949] 2026-01-09 14:02:17,326 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2026-01-09 14:02:17,333 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 14:02:17,333 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2026-01-09 14:02:17,333 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 14:02:17,333 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-09 14:02:17,333 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-09 14:02:17,333 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2026-01-09 14:02:17,333 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2026-01-09 14:02:17,594 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:660] 2026-01-09 14:02:17,594 >> Offline mode: forcing local_files_only=True
[INFO|video_processing_utils.py:726] 2026-01-09 14:02:17,598 >> loading configuration file video_preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|video_processing_utils.py:770] 2026-01-09 14:02:17,603 >> Video processor Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:928] 2026-01-09 14:02:17,603 >> Offline mode: forcing local_files_only=True
[INFO|processing_utils.py:1116] 2026-01-09 14:02:17,609 >> loading configuration file processor_config.json from cache at None
[INFO|processing_utils.py:1199] 2026-01-09 14:02:18,049 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2_5_VLProcessor"
}

âš™ï¸  Running in WANDB offline mode
INFO 01-09 14:02:18 [arg_utils.py:589] HF_HUB_OFFLINE is True, replace model_id [Qwen/Qwen2.5-VL-7B-Instruct] to model_path [/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5]
INFO 01-09 14:02:18 [utils.py:253] non-default args: {'trust_remote_code': True, 'seed': None, 'max_model_len': 127100, 'tensor_parallel_size': 4, 'disable_log_stats': True, 'limit_mm_per_prompt': {'image': 320}, 'model': '/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5'}
[WARNING|configuration_utils.py:697] 2026-01-09 14:02:18,134 >> The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[INFO|configuration_utils.py:763] 2026-01-09 14:02:18,137 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
WARNING 01-09 14:02:18 [arg_utils.py:1175] `seed=None` is equivalent to `seed=0` in V1 Engine. You will no longer be allowed to pass `None` in v0.13.
[INFO|configuration_utils.py:763] 2026-01-09 14:02:18,139 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
[INFO|configuration_utils.py:763] 2026-01-09 14:02:18,140 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
[INFO|configuration_utils.py:839] 2026-01-09 14:02:18,144 >> Model config Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 128000,
  "max_window_layers": 28,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "_name_or_path": "/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5",
    "architectures": [
      "Qwen2_5_VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "dtype": "bfloat16",
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "layer_types": [
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention"
    ],
    "max_position_embeddings": 128000,
    "max_window_layers": 28,
    "model_type": "qwen2_5_vl_text",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "use_cache": true,
    "use_sliding_window": false,
    "vision_token_id": 151654,
    "vocab_size": 152064
  },
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 3584,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

[INFO|hub.py:421] 2026-01-09 14:02:18,154 >> Offline mode: forcing local_files_only=True
INFO 01-09 14:02:18 [model.py:637] Resolved architecture: Qwen2_5_VLForConditionalGeneration
INFO 01-09 14:02:18 [model.py:1750] Using max model len 127100
INFO 01-09 14:02:18 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=8192.
[INFO|hub.py:421] 2026-01-09 14:02:18,563 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2026-01-09 14:02:18,564 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2093] 2026-01-09 14:02:18,564 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 14:02:18,564 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2093] 2026-01-09 14:02:18,564 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 14:02:18,565 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 14:02:18,565 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 14:02:18,565 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 14:02:18,565 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2364] 2026-01-09 14:02:18,809 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:939] 2026-01-09 14:02:18,930 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/generation_config.json
[INFO|configuration_utils.py:986] 2026-01-09 14:02:18,931 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 1e-06
}

[INFO|hub.py:421] 2026-01-09 14:02:19,245 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2026-01-09 14:02:19,245 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2093] 2026-01-09 14:02:19,246 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 14:02:19,246 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2093] 2026-01-09 14:02:19,246 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 14:02:19,246 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 14:02:19,246 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 14:02:19,246 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 14:02:19,246 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2364] 2026-01-09 14:02:19,489 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
WARNING 01-09 14:02:19 [system_utils.py:136] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
[WARNING|2026-01-09 14:02:26] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
[0;36m(EngineCore_DP0 pid=3115364)[0;0m INFO 01-09 14:02:29 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5', speculative_config=None, tokenizer='/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=127100, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=3115364)[0;0m WARNING 01-09 14:02:29 [multiproc_executor.py:880] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[WARNING|2026-01-09 14:02:36] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
[WARNING|2026-01-09 14:02:36] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
[WARNING|2026-01-09 14:02:36] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
[WARNING|2026-01-09 14:02:36] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
INFO 01-09 14:02:41 [parallel_state.py:1200] world_size=4 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:49583 backend=nccl
INFO 01-09 14:02:41 [parallel_state.py:1200] world_size=4 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:49583 backend=nccl
INFO 01-09 14:02:41 [parallel_state.py:1200] world_size=4 rank=3 local_rank=3 distributed_init_method=tcp://127.0.0.1:49583 backend=nccl
INFO 01-09 14:02:41 [parallel_state.py:1200] world_size=4 rank=2 local_rank=2 distributed_init_method=tcp://127.0.0.1:49583 backend=nccl
INFO 01-09 14:02:41 [pynccl.py:111] vLLM is using nccl==2.27.7
WARNING 01-09 14:02:42 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 01-09 14:02:42 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 01-09 14:02:42 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 01-09 14:02:42 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 01-09 14:02:42 [custom_all_reduce.py:154] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 01-09 14:02:42 [custom_all_reduce.py:154] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 01-09 14:02:42 [custom_all_reduce.py:154] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 01-09 14:02:42 [custom_all_reduce.py:154] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 01-09 14:02:42 [parallel_state.py:1408] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 3, EP rank 3
INFO 01-09 14:02:42 [parallel_state.py:1408] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 2, EP rank 2
INFO 01-09 14:02:42 [parallel_state.py:1408] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
INFO 01-09 14:02:42 [parallel_state.py:1408] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.
The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.
The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.
The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.
[0;36m(Worker_TP0 pid=3115376)[0;0m INFO 01-09 14:02:45 [gpu_model_runner.py:3467] Starting to load model /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5...
[0;36m(Worker_TP1 pid=3115377)[0;0m INFO 01-09 14:02:46 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(Worker_TP3 pid=3115379)[0;0m INFO 01-09 14:02:46 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(Worker_TP0 pid=3115376)[0;0m INFO 01-09 14:02:46 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(Worker_TP0 pid=3115376)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]
[0;36m(Worker_TP2 pid=3115378)[0;0m INFO 01-09 14:02:46 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(Worker_TP0 pid=3115376)[0;0m Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:01,  3.11it/s]
[0;36m(Worker_TP0 pid=3115376)[0;0m Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:00<00:01,  2.67it/s]
[0;36m(Worker_TP0 pid=3115376)[0;0m Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:01<00:00,  2.57it/s]
[0;36m(Worker_TP0 pid=3115376)[0;0m Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:01<00:00,  2.49it/s]
[0;36m(Worker_TP0 pid=3115376)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:01<00:00,  3.03it/s]
[0;36m(Worker_TP0 pid=3115376)[0;0m 
[0;36m(Worker_TP0 pid=3115376)[0;0m INFO 01-09 14:02:48 [default_loader.py:308] Loading weights took 1.66 seconds
[0;36m(Worker_TP0 pid=3115376)[0;0m INFO 01-09 14:02:48 [gpu_model_runner.py:3549] Model loading took 3.9983 GiB memory and 2.191613 seconds
[0;36m(Worker_TP3 pid=3115379)[0;0m INFO 01-09 14:02:49 [gpu_model_runner.py:4306] Encoder cache will be initialized with a budget of 114688 tokens, and profiled with 1 video items of the maximum feature size.
[0;36m(Worker_TP1 pid=3115377)[0;0m INFO 01-09 14:02:49 [gpu_model_runner.py:4306] Encoder cache will be initialized with a budget of 114688 tokens, and profiled with 1 video items of the maximum feature size.
[0;36m(Worker_TP0 pid=3115376)[0;0m INFO 01-09 14:02:49 [gpu_model_runner.py:4306] Encoder cache will be initialized with a budget of 114688 tokens, and profiled with 1 video items of the maximum feature size.
[0;36m(Worker_TP2 pid=3115378)[0;0m INFO 01-09 14:02:49 [gpu_model_runner.py:4306] Encoder cache will be initialized with a budget of 114688 tokens, and profiled with 1 video items of the maximum feature size.
[0;36m(Worker_TP0 pid=3115376)[0;0m INFO 01-09 14:03:19 [backends.py:655] Using cache directory: /home/indrisch/.cache/vllm/torch_compile_cache/36fd6c62d9/rank_0_0/backbone for vLLM's torch.compile
[0;36m(Worker_TP0 pid=3115376)[0;0m INFO 01-09 14:03:19 [backends.py:715] Dynamo bytecode transform time: 7.64 s
[0;36m(Worker_TP3 pid=3115379)[0;0m [rank3]:W0109 14:03:21.702000 3115379 torch/_inductor/codegen/triton_combo_kernel.py:97] [0/0] ComboKernels: 1 long reduction nodes are separated
[0;36m(Worker_TP1 pid=3115377)[0;0m [rank1]:W0109 14:03:21.702000 3115377 torch/_inductor/codegen/triton_combo_kernel.py:97] [0/0] ComboKernels: 1 long reduction nodes are separated
[0;36m(Worker_TP0 pid=3115376)[0;0m [rank0]:W0109 14:03:21.739000 3115376 torch/_inductor/codegen/triton_combo_kernel.py:97] [0/0] ComboKernels: 1 long reduction nodes are separated
[0;36m(Worker_TP2 pid=3115378)[0;0m [rank2]:W0109 14:03:21.788000 3115378 torch/_inductor/codegen/triton_combo_kernel.py:97] [0/0] ComboKernels: 1 long reduction nodes are separated
[0;36m(Worker_TP3 pid=3115379)[0;0m [rank3]:W0109 14:03:22.045000 3115379 torch/_inductor/codegen/triton_combo_kernel.py:97] [0/0] ComboKernels: 1 long reduction nodes are separated
[0;36m(Worker_TP1 pid=3115377)[0;0m [rank1]:W0109 14:03:22.048000 3115377 torch/_inductor/codegen/triton_combo_kernel.py:97] [0/0] ComboKernels: 1 long reduction nodes are separated
[0;36m(Worker_TP0 pid=3115376)[0;0m [rank0]:W0109 14:03:22.089000 3115376 torch/_inductor/codegen/triton_combo_kernel.py:97] [0/0] ComboKernels: 1 long reduction nodes are separated
[0;36m(Worker_TP2 pid=3115378)[0;0m [rank2]:W0109 14:03:22.154000 3115378 torch/_inductor/codegen/triton_combo_kernel.py:97] [0/0] ComboKernels: 1 long reduction nodes are separated
[0;36m(Worker_TP0 pid=3115376)[0;0m INFO 01-09 14:03:22 [backends.py:257] Cache the graph for dynamic shape for later use
[0;36m(Worker_TP0 pid=3115376)[0;0m INFO 01-09 14:03:44 [backends.py:288] Compiling a graph for dynamic shape takes 24.60 s
[0;36m(Worker_TP0 pid=3115376)[0;0m INFO 01-09 14:03:46 [monitor.py:34] torch.compile takes 32.24 s in total
[0;36m(Worker_TP0 pid=3115376)[0;0m INFO 01-09 14:03:47 [gpu_worker.py:359] Available KV cache memory: 25.99 GiB
[0;36m(EngineCore_DP0 pid=3115364)[0;0m INFO 01-09 14:03:48 [kv_cache_utils.py:1286] GPU KV cache size: 1,946,336 tokens
[0;36m(EngineCore_DP0 pid=3115364)[0;0m INFO 01-09 14:03:48 [kv_cache_utils.py:1291] Maximum concurrency for 127,100 tokens per request: 15.31x
[0;36m(Worker_TP0 pid=3115376)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 2/51 [00:00<00:03, 14.26it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|â–Š         | 4/51 [00:00<00:03, 15.05it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 6/51 [00:00<00:02, 15.21it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|â–ˆâ–Œ        | 8/51 [00:00<00:02, 15.49it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|â–ˆâ–‰        | 10/51 [00:00<00:02, 15.41it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–Ž       | 12/51 [00:00<00:02, 15.64it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 14/51 [00:00<00:02, 16.17it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 16/51 [00:01<00:02, 16.55it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/51 [00:01<00:01, 16.72it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 20/51 [00:01<00:01, 16.97it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 22/51 [00:01<00:01, 17.04it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 24/51 [00:01<00:01, 17.17it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 26/51 [00:01<00:01, 17.17it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/51 [00:01<00:01, 17.09it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 30/51 [00:01<00:01, 17.23it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 32/51 [00:01<00:01, 17.03it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 34/51 [00:02<00:01, 16.81it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 36/51 [00:02<00:00, 16.84it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 38/51 [00:02<00:00, 16.76it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 40/51 [00:02<00:00, 16.78it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/51 [00:02<00:00, 16.52it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 44/51 [00:02<00:00, 16.54it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 46/51 [00:02<00:00, 16.66it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 48/51 [00:02<00:00, 16.72it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 50/51 [00:03<00:00, 16.79it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:03<00:00, 16.51it/s]
[0;36m(Worker_TP0 pid=3115376)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   6%|â–Œ         | 2/35 [00:00<00:02, 15.14it/s]Capturing CUDA graphs (decode, FULL):  11%|â–ˆâ–        | 4/35 [00:00<00:01, 17.08it/s]Capturing CUDA graphs (decode, FULL):  17%|â–ˆâ–‹        | 6/35 [00:00<00:01, 17.49it/s]Capturing CUDA graphs (decode, FULL):  23%|â–ˆâ–ˆâ–Ž       | 8/35 [00:00<00:01, 17.98it/s]Capturing CUDA graphs (decode, FULL):  29%|â–ˆâ–ˆâ–Š       | 10/35 [00:00<00:01, 18.23it/s]Capturing CUDA graphs (decode, FULL):  34%|â–ˆâ–ˆâ–ˆâ–      | 12/35 [00:00<00:01, 18.33it/s]Capturing CUDA graphs (decode, FULL):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 14/35 [00:00<00:01, 18.47it/s]Capturing CUDA graphs (decode, FULL):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 16/35 [00:00<00:01, 18.45it/s]Capturing CUDA graphs (decode, FULL):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/35 [00:00<00:00, 18.47it/s]Capturing CUDA graphs (decode, FULL):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 20/35 [00:01<00:00, 18.44it/s]Capturing CUDA graphs (decode, FULL):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 22/35 [00:01<00:00, 18.44it/s]Capturing CUDA graphs (decode, FULL):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 24/35 [00:01<00:00, 18.44it/s]Capturing CUDA graphs (decode, FULL):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 26/35 [00:01<00:00, 18.47it/s]Capturing CUDA graphs (decode, FULL):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 28/35 [00:01<00:00, 18.56it/s]Capturing CUDA graphs (decode, FULL):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 30/35 [00:01<00:00, 18.57it/s]Capturing CUDA graphs (decode, FULL):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 32/35 [00:01<00:00, 18.70it/s]Capturing CUDA graphs (decode, FULL):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 34/35 [00:01<00:00, 18.74it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:01<00:00, 18.37it/s]
[0;36m(Worker_TP0 pid=3115376)[0;0m INFO 01-09 14:03:54 [gpu_model_runner.py:4466] Graph capturing finished in 6 secs, took 1.11 GiB
[0;36m(EngineCore_DP0 pid=3115364)[0;0m INFO 01-09 14:03:54 [core.py:254] init engine (profile, create kv cache, warmup model) took 64.88 seconds
[0;36m(EngineCore_DP0 pid=3115364)[0;0m The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.
INFO 01-09 14:03:58 [llm.py:343] Supported tasks: ['generate']
[INFO|2026-01-09 14:03:58] llamafactory.data.loader:143 >> Loading dataset /scratch/indrisch/huggingface/hub/datasets--cvis-tmu--llamafactory-sqa3d-traces-multiimage-vqa/snapshots/ce5c54adc1608d1726730c9ff334e65b6dd70e46/data/...
Converting format of dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<?, ? examples/s]Converting format of dataset (num_proc=32): 104 examples [00:00,  5.09 examples/s]          Converting format of dataset (num_proc=32): 178 examples [00:00, 116.42 examples/s]Converting format of dataset (num_proc=32): 200 examples [00:01, 87.02 examples/s] 
Running tokenizer on dataset (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=32): 104 examples [00:01,  2.98 examples/s]          Running tokenizer on dataset (num_proc=32): 112 examples [00:01,  9.64 examples/s]Running tokenizer on dataset (num_proc=32): 119 examples [00:01, 15.22 examples/s]Running tokenizer on dataset (num_proc=32): 125 examples [00:01, 19.13 examples/s]Running tokenizer on dataset (num_proc=32): 131 examples [00:02, 21.99 examples/s]Running tokenizer on dataset (num_proc=32): 137 examples [00:02, 26.78 examples/s]Running tokenizer on dataset (num_proc=32): 143 examples [00:02, 28.06 examples/s]Running tokenizer on dataset (num_proc=32): 149 examples [00:02, 30.05 examples/s]Running tokenizer on dataset (num_proc=32): 155 examples [00:02, 27.46 examples/s]Running tokenizer on dataset (num_proc=32): 161 examples [00:03, 28.43 examples/s]Running tokenizer on dataset (num_proc=32): 167 examples [00:03, 28.71 examples/s]Running tokenizer on dataset (num_proc=32): 173 examples [00:03, 30.00 examples/s]Running tokenizer on dataset (num_proc=32): 179 examples [00:03, 30.76 examples/s]Running tokenizer on dataset (num_proc=32): 185 examples [00:03, 35.51 examples/s]Running tokenizer on dataset (num_proc=32): 191 examples [00:03, 31.55 examples/s]Running tokenizer on dataset (num_proc=32): 197 examples [00:04, 35.44 examples/s]Running tokenizer on dataset (num_proc=32): 200 examples [00:04, 23.19 examples/s]
training example:
input_ids:
[151644, 8948, 271, 262, 1446, 525, 264, 16585, 11129, 4142, 11528, 17847, 369, 27979, 647, 47522, 389, 264, 3175, 29519, 6109, 6839, 3941, 5248, 5335, 382, 262, 26149, 510, 262, 68252, 1283, 419, 1943, 11, 498, 686, 5258, 451, 6109, 5335, 438, 366, 1805, 29, 9492, 13, 18877, 1105, 438, 2155, 6194, 315, 279, 83490, 6109, 13, 28634, 311, 5335, 553, 220, 16, 5980, 1922, 304, 1973, 315, 11094, 25, 508, 16, 1125, 508, 17, 1125, 4593, 11, 508, 45, 29562, 262, 3596, 35304, 1718, 8575, 38439, 3298, 14017, 510, 262, 7288, 7854, 264, 12966, 220, 18, 35, 10502, 1614, 3941, 6194, 25, 3754, 6171, 3941, 5335, 11, 23583, 8674, 6249, 17040, 11, 323, 990, 5452, 14, 509, 8957, 14688, 21060, 369, 7990, 55916, 624, 262, 7288, 72077, 409, 849, 292, 3793, 26087, 983, 847, 1290, 97089, 17996, 4065, 14, 29898, 484, 32511, 56111, 7612, 1825, 35978, 28455, 448, 5091, 311, 279, 64171, 14, 8092, 13, 1416, 279, 58385, 374, 54761, 11, 1638, 311, 279, 1429, 38219, 8622, 1651, 323, 1977, 773, 624, 262, 7288, 5443, 1172, 9434, 5904, 26, 653, 537, 17023, 63133, 3565, 476, 17188, 389, 9250, 6540, 7797, 6770, 1633, 43898, 3793, 624, 262, 7288, 3197, 5248, 11178, 3000, 11, 8830, 553, 279, 1850, 2432, 311, 27979, 17133, 488, 4274, 1835, 3941, 6194, 13, 1416, 2058, 54761, 1283, 13295, 678, 5335, 11, 1584, 429, 9355, 382, 262, 30990, 320, 327, 32739, 1378, 10010, 11, 304, 419, 1973, 982, 262, 366, 26865, 397, 262, 14822, 14319, 29208, 32711, 304, 220, 18, 4142, 16, 15, 2805, 7354, 13, 356, 632, 5904, 448, 2168, 14937, 320, 68, 1302, 2572, 65750, 18, 5669, 3100, 7579, 18010, 1290, 315, 8718, 26, 508, 16, 5669, 1852, 1894, 11, 42396, 64212, 2823, 63594, 714, 4583, 320, 2640, 37294, 17, 20, 15, 11211, 7241, 5871, 568, 7036, 894, 17796, 8957, 14, 2969, 26745, 487, 323, 1246, 498, 19673, 432, 624, 262, 690, 26865, 397, 262, 366, 9217, 397, 262, 362, 3175, 63594, 4226, 1172, 11, 892, 1231, 387, 510, 262, 7288, 1416, 14016, 25, 3776, 37328, 4226, 11, 4504, 320, 68, 1302, 2572, 1036, 17, 32511, 476, 7414, 14, 2753, 320, 68, 1302, 2572, 1036, 9693, 854, 4292, 262, 7288, 1416, 537, 14016, 25, 362, 11652, 476, 1378, 624, 262, 1416, 4226, 537, 9434, 11, 421, 1052, 374, 902, 2797, 5904, 11, 476, 421, 279, 4226, 374, 35197, 54761, 25, 1036, 33260, 8253, 854, 624, 262, 690, 9217, 1339, 262, 62947, 609, 43797, 50, 510, 262, 7288, 3155, 4183, 13153, 279, 3405, 476, 1140, 5335, 26, 653, 4183, 2550, 4718, 476, 4960, 14158, 624, 262, 7288, 84468, 4285, 1894, 5036, 448, 518, 1429, 825, 22739, 26087, 4145, 3446, 838, 4322, 1574, 58689, 7256, 854, 4292, 262, 7288, 2823, 12966, 3941, 6194, 26, 421, 6194, 28295, 11, 62552, 279, 11299, 15432, 5904, 323, 5185, 432, 304, 366, 26865, 29816, 257, 151645, 198, 151644, 872, 198, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 30, 220, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151652, 151655, 151653, 151645, 198, 151644, 77091, 198]
inputs:
<|im_start|>system

    You are a careful visionâ€“language assistant for spatial VQA on a single indoor scene shown across multiple images.

    INPUT:
    Immediately after this message, you will receive N scene images as <image> tags. Treat them as different views of the SAME scene. Refer to images by 1-based index in order of appearance: [1], [2], â€¦, [N].

    REASONING PRINCIPLES:
    â€¢ Build a consistent 3D mental model across views: track objects across images, infer relative camera pose, and use scale/occlusion/shadows for depth cues.
    â€¢ Interpret deictic terms (â€œto my right/left/in front/behindâ€) EGOCENTRICALLY with respect to the narrator/agent. If the viewpoint is ambiguous, default to the most informative central view and say so.
    â€¢ Use only visible evidence; do not invent unseen details or rely on external knowledge beyond basic object/color terms.
    â€¢ When multiple candidates exist, resolve by the best match to spatial phrase + salience across views. If still ambiguous after checking all images, state that clearly.

    OUTPUT (exactly two blocks, in this order):
    <think>
    Step-by-step reasoning in 3â€“10 short steps. Cite evidence with image indices (e.g., â€œ[3]: light wood desk right of monitor; [1]: same color, confirmsâ€). Be concise but complete (aim â‰¤250 tokens unless necessary). Note any occlusion/ambiguity and how you resolved it.
    </think>
    <answer>
    A single concise answer only, which may be:
    â€¢ If sufficient: One-word answer, Count (e.g., â€œ2â€) or Yes/No (e.g., â€œyesâ€).
    â€¢ If not sufficient: A sentence or two.
    If answer not visible, if there is no clear evidence, or if the answer is genuinely ambiguous: â€œcannot determineâ€.
    </answer>

    STYLE & RULES:
    â€¢ Do NOT repeat the question or list images; do NOT output JSON or extra sections.
    â€¢ Prefer simple color names with at most one modifier (â€œlight/dark/pale/beigeâ€).
    â€¢ Be consistent across views; if views disagree, prioritize the clearest evidence and note it in <think>.
    <|im_end|>
<|im_start|>user
What is on top of the bathroom cabinet that is on my 11 o'clock? <|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|im_end|>
<|im_start|>assistant

label_ids:
[198, 32462, 432, 11, 1077, 594, 1430, 311, 7071, 700, 1128, 594, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 62410, 13, 5512, 11, 1077, 594, 50087, 279, 14852, 6505, 13, 576, 21921, 374, 4363, 264, 4158, 11, 1378, 1737, 95112, 4982, 13, 2411, 220, 16, 16, 297, 62410, 320, 3481, 7950, 3082, 701, 582, 1184, 311, 1779, 429, 7702, 13, 21131, 518, 279, 5335, 11, 5310, 279, 825, 9027, 279, 21921, 3082, 11, 1052, 594, 264, 17846, 3403, 279, 19309, 11, 714, 3783, 11, 902, 2293, 11489, 11, 279, 3405, 374, 911, 279, 21921, 13, 13824, 11, 7196, 279, 330, 66, 75512, 1, 1588, 374, 279, 5819, 4982, 13, 13824, 11, 902, 11, 279, 19309, 702, 264, 21921, 3685, 30, 13824, 11, 902, 11, 279, 1887, 21921, 374, 279, 4158, 61607, 13, 13824, 11, 304, 279, 2168, 1380, 279, 6249, 4933, 279, 19309, 323, 21921, 11, 279, 1909, 315, 279, 21921, 2293, 11489, 11, 902, 11, 7196, 279, 17846, 30, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 2727, 330, 65, 77832, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 3263, 2055, 220, 16, 16, 297, 62410, 1035, 387, 279, 8416, 2115, 3082, 8674, 311, 279, 19309, 13, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 279, 19309, 3082, 13, 2160, 429, 949, 315, 279, 21921, 6505, 30, 13824, 11, 902, 11, 7196, 279, 21921, 374, 279, 4158, 61607, 1212, 279, 19309, 13, 13824, 11, 902, 11, 279, 1909, 315, 429, 21921, 2293, 11489, 11, 902, 11, 304, 1045, 5335, 11, 1052, 594, 264, 17846, 25911, 476, 21810, 13, 13824, 11, 902, 11, 1077, 594, 1779, 279, 1156, 2421, 5335, 13, 576, 17196, 45766, 374, 33866, 11, 1221, 279, 19309, 3082, 13, 13824, 11, 279, 4396, 4226, 25, 3330, 518, 279, 5335, 11, 279, 1633, 389, 1909, 315, 279, 14852, 21921, 320, 1782, 4158, 26482, 4982, 8, 518, 220, 16, 16, 297, 62410, 374, 264, 17846, 13, 13824, 11, 902, 11, 3783, 21677, 17846, 374, 389, 279, 7002, 3403, 279, 19309, 13, 13824, 11, 7196, 358, 5786, 878, 13, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 594, 3405, 25, 330, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 7521, 2055, 1156, 11, 10542, 279, 21921, 13, 576, 4158, 26482, 4982, 3143, 220, 16, 16, 297, 62410, 320, 3481, 7950, 568, 3555, 594, 389, 1909, 30, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 429, 3082, 13, 13824, 11, 9834, 0, 758, 279, 21844, 476, 21810, 11, 1052, 594, 264, 17846, 13, 2055, 279, 4226, 374, 264, 17846, 4192, 1249, 8253, 1128, 374, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 527, 20666, 11, 23643, 279, 27979, 6789, 323, 5424, 304, 279, 5335, 25, 18611, 16, 13, 95210, 279, 1036, 65, 77832, 21921, 854, 304, 3405, 25, 362, 4158, 11, 1378, 481, 26482, 5819, 4982, 374, 9434, 3143, 279, 1909, 481, 2115, 320, 16, 16, 297, 527, 20666, 8, 3082, 8674, 311, 1008, 37664, 1075, 279, 19309, 323, 17196, 13, 2303, 17, 13, 1374, 19991, 6171, 3403, 419, 21921, 25, 758, 279, 5335, 11, 264, 3070, 73225, 334, 374, 21810, 389, 279, 7002, 3403, 419, 21921, 3082, 13, 79226, 54216, 151645, 198]
labels:

Got it, let's try to figure out what's on top of the bathroom cabinet at 11 o'clock. First, let's visualize the bathroom setup. The cabinet is likely a white, two-drawer unit. At 11 o'clock (top-left area), we need to check that spot. Looking at the images, especially the one showing the cabinet area, there's a mirror above the sink, but wait, noâ€”wait, the question is about the cabinet. Wait, maybe the "cabinet" here is the storage unit. Wait, no, the sink has a cabinet below? Wait, no, the main cabinet is the white drawers. Wait, in the image where the camera shows the sink and cabinet, the top of the cabinetâ€”wait, no, maybe the mirror? Wait, no, let's check again. Wait, the user says "bathroom cabinet that is on my 11 o'clock". So 11 o'clock would be the upper left area relative to the sink. Looking at the images, there's a mirror mounted on the wall above the sink area. Is that part of the cabinet setup? Wait, no, maybe the cabinet is the white drawers under the sink. Wait, no, the top of that cabinetâ€”wait, no, in some images, there's a mirror reflected or mounted. Wait, no, let's check the first few images. The shower curtain is colorful, then the sink area. Wait, the correct answer: looking at the images, the object on top of the bathroom cabinet (the white drawer unit) at 11 o'clock is a mirror. Wait, no, waitâ€”the mirror is on the wall above the sink. Wait, maybe I misread. Wait, no, let's check again. Wait, the user's question: "What is on top of the bathroom cabinet that is on my 11 o'clock?" So first, identify the cabinet. The white drawer unit near 11 o'clock (top-left). What's on top? Looking at the images, there's a mirror mounted on the wall above that area. Wait, yes! In the reflection or mounted, there's a mirror. So the answer is a mirror.


To determine what is on top of the bathroom cabinet at 11 oâ€™clock, analyze the spatial layout and elements in the images:  

1. Locate the â€œbathroom cabinetâ€ in question: A white, two - drawer storage unit is visible near the top - left (11 oâ€™clock) area relative to other fixtures like the sink and shower.  
2. Examine objects above this cabinet: In the images, a **mirror** is mounted on the wall above this cabinet area.  


Mirror<|im_end|>

Processing batched inference:   0%|          | 0/1 [00:00<?, ?it/s]
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s][A[INFO|image_processing_base.py:316] 2026-01-09 14:04:52,483 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:381] 2026-01-09 14:04:52,484 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[WARNING|logging.py:328] 2026-01-09 14:04:52,484 >> The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.
[INFO|image_processing_base.py:428] 2026-01-09 14:04:52,484 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:1949] 2026-01-09 14:04:52,484 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2093] 2026-01-09 14:04:52,485 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 14:04:52,485 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2093] 2026-01-09 14:04:52,485 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 14:04:52,485 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 14:04:52,485 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 14:04:52,485 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 14:04:52,485 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2364] 2026-01-09 14:04:52,756 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:660] 2026-01-09 14:04:52,756 >> Offline mode: forcing local_files_only=True
[INFO|video_processing_utils.py:724] 2026-01-09 14:04:52,756 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|video_processing_utils.py:770] 2026-01-09 14:04:52,757 >> Video processor Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:928] 2026-01-09 14:04:52,757 >> Offline mode: forcing local_files_only=True
[INFO|processing_utils.py:1114] 2026-01-09 14:04:52,757 >> loading configuration file None
[INFO|processing_utils.py:1199] 2026-01-09 14:04:53,202 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2_5_VLProcessor"
}

[INFO|image_processing_base.py:316] 2026-01-09 14:04:53,203 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:381] 2026-01-09 14:04:53,204 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:428] 2026-01-09 14:04:53,204 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:1949] 2026-01-09 14:04:53,204 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2093] 2026-01-09 14:04:53,205 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 14:04:53,205 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2093] 2026-01-09 14:04:53,205 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 14:04:53,205 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 14:04:53,205 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 14:04:53,205 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2093] 2026-01-09 14:04:53,205 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2364] 2026-01-09 14:04:53,988 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:660] 2026-01-09 14:04:53,988 >> Offline mode: forcing local_files_only=True
[INFO|video_processing_utils.py:724] 2026-01-09 14:04:53,988 >> loading configuration file /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|video_processing_utils.py:770] 2026-01-09 14:04:53,989 >> Video processor Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:928] 2026-01-09 14:04:54,552 >> Offline mode: forcing local_files_only=True
[INFO|processing_utils.py:1114] 2026-01-09 14:04:54,555 >> loading configuration file None
[INFO|processing_utils.py:1199] 2026-01-09 14:04:55,130 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: CachedQwen2TokenizerFast(name_or_path='/scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2_5_VLProcessor"
}


Adding requests:   1%|          | 1/100 [00:03<06:19,  3.83s/it][A
Adding requests:   2%|â–         | 2/100 [00:04<03:39,  2.24s/it][A
Adding requests:   3%|â–Ž         | 3/100 [00:09<05:21,  3.31s/it][A
Adding requests:   4%|â–         | 4/100 [00:10<03:43,  2.33s/it][A
Adding requests:   5%|â–Œ         | 5/100 [00:12<03:40,  2.32s/it][A
Adding requests:   6%|â–Œ         | 6/100 [00:14<03:31,  2.25s/it][A
Adding requests:   7%|â–‹         | 7/100 [00:15<02:43,  1.76s/it][A
Adding requests:   8%|â–Š         | 8/100 [00:16<02:30,  1.64s/it][A
Adding requests:   9%|â–‰         | 9/100 [00:19<02:46,  1.83s/it][A
Adding requests:  10%|â–ˆ         | 10/100 [00:23<03:47,  2.53s/it][A
Adding requests:  11%|â–ˆ         | 11/100 [00:23<02:55,  1.97s/it][A
Adding requests:  12%|â–ˆâ–        | 12/100 [00:26<03:05,  2.11s/it][A
Adding requests:  13%|â–ˆâ–Ž        | 13/100 [00:30<04:00,  2.77s/it][A
Adding requests:  14%|â–ˆâ–        | 14/100 [00:32<03:31,  2.46s/it][A
Adding requests:  15%|â–ˆâ–Œ        | 15/100 [00:33<02:51,  2.01s/it][A
Adding requests:  16%|â–ˆâ–Œ        | 16/100 [00:34<02:25,  1.73s/it][A
Adding requests:  18%|â–ˆâ–Š        | 18/100 [00:36<01:46,  1.30s/it][A
Adding requests:  19%|â–ˆâ–‰        | 19/100 [00:38<02:07,  1.57s/it][A
Adding requests:  20%|â–ˆâ–ˆ        | 20/100 [00:41<02:37,  1.96s/it][A
Adding requests:  21%|â–ˆâ–ˆ        | 21/100 [00:43<02:33,  1.94s/it][A
Adding requests:  22%|â–ˆâ–ˆâ–       | 22/100 [00:44<02:15,  1.73s/it][A
Adding requests:  23%|â–ˆâ–ˆâ–Ž       | 23/100 [00:47<02:41,  2.10s/it][A
Adding requests:  24%|â–ˆâ–ˆâ–       | 24/100 [00:48<02:16,  1.79s/it][A
Adding requests:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [00:49<02:01,  1.62s/it][A
Adding requests:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [00:52<02:11,  1.78s/it][A
Adding requests:  27%|â–ˆâ–ˆâ–‹       | 27/100 [00:57<03:19,  2.73s/it][A
Adding requests:  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:58<02:42,  2.26s/it][A
Adding requests:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [01:00<02:04,  1.79s/it][A
Adding requests:  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [01:01<01:50,  1.61s/it][A
Adding requests:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [01:03<01:57,  1.73s/it][A
Adding requests:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [01:05<01:48,  1.62s/it][A
Adding requests:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [01:07<02:05,  1.91s/it][A
Adding requests:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [01:08<01:50,  1.70s/it][A
Adding requests:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [01:11<02:11,  2.06s/it][A
Adding requests:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [01:14<02:13,  2.11s/it][A
Adding requests:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [01:15<01:55,  1.87s/it][A
Adding requests:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [01:17<02:06,  2.07s/it][A
Adding requests:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [01:19<01:48,  1.81s/it][A
Adding requests:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [01:21<01:59,  2.03s/it][A
Adding requests:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [01:23<01:52,  1.94s/it][A
Adding requests:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [01:25<01:45,  1.85s/it][A
Adding requests:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [01:25<00:56,  1.02s/it][A
Adding requests:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [01:26<01:03,  1.18s/it][A
Adding requests:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [01:30<01:33,  1.77s/it][A
Adding requests:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [01:31<01:22,  1.59s/it][A
Adding requests:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [01:33<01:23,  1.65s/it][A
Adding requests:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [01:35<01:30,  1.80s/it][A
Adding requests:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [01:36<01:20,  1.65s/it][A
Adding requests:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [01:36<00:58,  1.23s/it][A
Adding requests:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [01:43<02:18,  2.95s/it][A
Adding requests:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [01:45<01:53,  2.46s/it][A
Adding requests:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [01:47<01:48,  2.41s/it][A
Adding requests:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [01:49<01:45,  2.41s/it][A
Adding requests:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [01:52<01:47,  2.49s/it][A
Adding requests:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [01:53<01:28,  2.12s/it][A
Adding requests:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [01:57<01:44,  2.54s/it][A
Adding requests:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [01:59<01:38,  2.47s/it][A
Adding requests:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [02:01<01:23,  2.14s/it][A
Adding requests:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [02:02<01:15,  1.99s/it][A
Adding requests:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [02:07<01:42,  2.76s/it][A
Adding requests:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [02:08<01:24,  2.35s/it][A
Adding requests:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [02:09<01:09,  1.98s/it][A
Adding requests:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [02:13<01:25,  2.52s/it][A
Adding requests:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [02:14<01:05,  1.98s/it][A
Adding requests:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [02:15<00:58,  1.81s/it][A
Adding requests:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [02:17<00:53,  1.73s/it][A
Adding requests:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [02:19<00:54,  1.82s/it][A
Adding requests:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [02:23<01:16,  2.63s/it][A
Adding requests:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [02:25<01:03,  2.27s/it][A
Adding requests:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [02:27<01:00,  2.24s/it][A
Adding requests:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [02:29<00:55,  2.12s/it][A
Adding requests:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [02:30<00:44,  1.80s/it][A
Adding requests:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [02:31<00:41,  1.75s/it][A
Adding requests:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [02:32<00:35,  1.54s/it][A
Adding requests:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [02:35<00:39,  1.78s/it][A
Adding requests:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [02:36<00:35,  1.70s/it][A
Adding requests:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [02:39<00:39,  1.99s/it][A
Adding requests:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [02:41<00:37,  1.99s/it][A
Adding requests:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [02:42<00:32,  1.78s/it][A
Adding requests:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [02:44<00:28,  1.65s/it][A
Adding requests:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [02:46<00:30,  1.88s/it][A
Adding requests:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [02:48<00:28,  1.88s/it][A
Adding requests:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [02:49<00:24,  1.76s/it][A
Adding requests:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [02:51<00:21,  1.68s/it][A
Adding requests:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [02:52<00:20,  1.67s/it][A
Adding requests:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [02:54<00:18,  1.71s/it][A
Adding requests:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [02:57<00:19,  1.93s/it][A
Adding requests:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [03:02<00:26,  2.94s/it][A
Adding requests:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [03:03<00:19,  2.39s/it][A
Adding requests:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [03:07<00:19,  2.75s/it][A
Adding requests:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [03:08<00:13,  2.18s/it][A
Adding requests:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [03:08<00:05,  1.37s/it][A
Adding requests:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [03:09<00:03,  1.25s/it][A
Adding requests:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [03:13<00:03,  1.83s/it][A
Adding requests:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [03:13<00:01,  1.37s/it][A
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [03:15<00:00,  1.49s/it][AAdding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [03:15<00:00,  1.95s/it]

Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:04<00:00, 22.75it/s, est. speed input: 167855.17 toks/s, output: 3989.84 toks/s][A
Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [00:05<00:00, 15.87it/s, est. speed input: 128688.67 toks/s, output: 3039.41 toks/s][A
Processed prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [00:06<00:00, 13.41it/s, est. speed input: 116977.84 toks/s, output: 2903.57 toks/s][A
Processed prompts:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [00:06<00:00, 11.68it/s, est. speed input: 109182.32 toks/s, output: 2753.26 toks/s][A
Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [00:12<00:00,  3.17it/s, est. speed input: 57567.18 toks/s, output: 1524.77 toks/s] [A
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:13<00:00,  2.94it/s, est. speed input: 54672.28 toks/s, output: 1511.73 toks/s][A
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:13<00:00,  2.94it/s, est. speed input: 54672.28 toks/s, output: 1511.73 toks/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:13<00:00,  7.45it/s, est. speed input: 54672.28 toks/s, output: 1511.73 toks/s]
Processing batched inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:16<00:00, 256.49s/it]Processing batched inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:16<00:00, 256.49s/it]
[0;36m(Worker_TP0 pid=3115376)[0;0m INFO 01-09 14:08:21 [multiproc_executor.py:709] Parent process exited, terminating worker
[0;36m(Worker_TP1 pid=3115377)[0;0m INFO 01-09 14:08:21 [multiproc_executor.py:709] Parent process exited, terminating worker
[0;36m(Worker_TP2 pid=3115378)[0;0m INFO 01-09 14:08:21 [multiproc_executor.py:709] Parent process exited, terminating worker
[0;36m(Worker_TP3 pid=3115379)[0;0m INFO 01-09 14:08:21 [multiproc_executor.py:709] Parent process exited, terminating worker
[W109 14:08:22.871763276 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=64, addr=[localhost]:44686, remote=[localhost]:49583): Failed to recv, got 0 bytes. Connection was likely closed. Did the remote server shutdown or crash?
Exception raised from recvBytes at /tmp/build_wheels_tmp.6038/python-3.12/torch/torch/csrc/distributed/c10d/Utils.hpp:697 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x15553e3918b0 in /project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5d9a9d1 (0x155532eb89d1 in /project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x5d9bdcd (0x155532eb9dcd in /project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x5d9c97a (0x155532eba97a in /project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x30e (0x155532eb548e in /project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::ProcessGroupNCCL::HeartbeatMonitor::runLoop() + 0x3dd (0x155512c7d9ed in /project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0xe07f0 (0x15554f4e07f0 in /cvmfs/soft.computecanada.ca/gentoo/2023/x86-64-v3/usr/lib/gcc/x86_64-pc-linux-gnu/14/libstdc++.so.6)
frame #7: <unknown function> + 0x84a9d (0x155554bd7a9d in /cvmfs/soft.computecanada.ca/gentoo/2023/x86-64-v3/usr/lib64/libc.so.6)
frame #8: <unknown function> + 0x10509c (0x155554c5809c in /cvmfs/soft.computecanada.ca/gentoo/2023/x86-64-v3/usr/lib64/libc.so.6)

[W109 14:08:22.871741158 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=63, addr=[localhost]:44718, remote=[localhost]:49583): Failed to recv, got 0 bytes. Connection was likely closed. Did the remote server shutdown or crash?
Exception raised from recvBytes at /tmp/build_wheels_tmp.6038/python-3.12/torch/torch/csrc/distributed/c10d/Utils.hpp:697 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x15553e3918b0 in /project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5d9a9d1 (0x155532eb89d1 in /project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x5d9bdcd (0x155532eb9dcd in /project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x5d9c97a (0x155532eba97a in /project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x30e (0x155532eb548e in /project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::ProcessGroupNCCL::HeartbeatMonitor::runLoop() + 0x3dd (0x155512c7d9ed in /project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0xe07f0 (0x15554f4e07f0 in /cvmfs/soft.computecanada.ca/gentoo/2023/x86-64-v3/usr/lib/gcc/x86_64-pc-linux-gnu/14/libstdc++.so.6)
frame #7: <unknown function> + 0x84a9d (0x155554bd7a9d in /cvmfs/soft.computecanada.ca/gentoo/2023/x86-64-v3/usr/lib64/libc.so.6)
frame #8: <unknown function> + 0x10509c (0x155554c5809c in /cvmfs/soft.computecanada.ca/gentoo/2023/x86-64-v3/usr/lib64/libc.so.6)

[W109 14:08:22.871756134 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=63, addr=[localhost]:44712, remote=[localhost]:49583): Failed to recv, got 0 bytes. Connection was likely closed. Did the remote server shutdown or crash?
Exception raised from recvBytes at /tmp/build_wheels_tmp.6038/python-3.12/torch/torch/csrc/distributed/c10d/Utils.hpp:697 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x15553e3918b0 in /project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5d9a9d1 (0x155532eb89d1 in /project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x5d9bdcd (0x155532eb9dcd in /project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x5d9c97a (0x155532eba97a in /project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x30e (0x155532eb548e in /project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::ProcessGroupNCCL::HeartbeatMonitor::runLoop() + 0x3dd (0x155512c7d9ed in /project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0xe07f0 (0x15554f4e07f0 in /cvmfs/soft.computecanada.ca/gentoo/2023/x86-64-v3/usr/lib/gcc/x86_64-pc-linux-gnu/14/libstdc++.so.6)
frame #7: <unknown function> + 0x84a9d (0x155554bd7a9d in /cvmfs/soft.computecanada.ca/gentoo/2023/x86-64-v3/usr/lib64/libc.so.6)
frame #8: <unknown function> + 0x10509c (0x155554c5809c in /cvmfs/soft.computecanada.ca/gentoo/2023/x86-64-v3/usr/lib64/libc.so.6)

[W109 14:08:22.890932890 ProcessGroupNCCL.cpp:1771] [PG ID 0 PG GUID 0 Rank 1] Failed to check the "should dump" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: Failed to recv, got 0 bytes. Connection was likely closed. Did the remote server shutdown or crash?
[W109 14:08:22.890931356 ProcessGroupNCCL.cpp:1771] [PG ID 0 PG GUID 0 Rank 3] Failed to check the "should dump" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: Failed to recv, got 0 bytes. Connection was likely closed. Did the remote server shutdown or crash?
[W109 14:08:22.890946937 ProcessGroupNCCL.cpp:1771] [PG ID 0 PG GUID 0 Rank 2] Failed to check the "should dump" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: Failed to recv, got 0 bytes. Connection was likely closed. Did the remote server shutdown or crash?
**********************************************************************
100 total generated results have been saved at /project/aip-wangcs/indrisch/LLaMA-Factory/debug/sft/out/generated_predictions_Qwen25VL7BInstruct.jsonl.
**********************************************************************
