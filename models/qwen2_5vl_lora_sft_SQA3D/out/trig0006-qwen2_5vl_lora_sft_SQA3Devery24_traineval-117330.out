
==========
== CUDA ==
==========

CUDA Version 12.4.1

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

[INFO|2025-12-01 05:25:27] llamafactory.launcher:143 >> Initializing 4 distributed tasks at: 127.0.0.1:60459
W1201 05:25:28.782000 2152667 site-packages/torch/distributed/run.py:792] 
W1201 05:25:28.782000 2152667 site-packages/torch/distributed/run.py:792] *****************************************
W1201 05:25:28.782000 2152667 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1201 05:25:28.782000 2152667 site-packages/torch/distributed/run.py:792] *****************************************
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
[2025-12-01 05:25:36,869] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-01 05:25:36,885] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-01 05:25:37,097] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-01 05:25:37,309] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2025-12-01 05:25:40,084] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-01 05:25:40,084] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-01 05:25:40,084] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-12-01 05:25:40,084] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-01 05:25:40,084] [INFO] [comm.py:669:init_distributed] cdb=None
[INFO|2025-12-01 05:25:40] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.
[INFO|2025-12-01 05:25:40] llamafactory.hparams.parser:423 >> Process rank: 0, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|hub.py:421] 2025-12-01 05:25:40,436 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2025-12-01 05:25:40,451 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2025-12-01 05:25:40,466 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-12-01 05:25:40,466 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-12-01 05:25:40,466 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-12-01 05:25:40,466 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-01 05:25:40,466 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-01 05:25:40,466 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-12-01 05:25:40,466 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-12-01 05:25:40,686 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|hub.py:421] 2025-12-01 05:25:40,686 >> Offline mode: forcing local_files_only=True
[INFO|hub.py:421] 2025-12-01 05:25:40,687 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:316] 2025-12-01 05:25:40,689 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2025-12-01 05:25:40,692 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:316] 2025-12-01 05:25:40,694 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2025-12-01 05:25:40,696 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:428] 2025-12-01 05:25:40,703 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:1949] 2025-12-01 05:25:40,703 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2025-12-01 05:25:40,708 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-12-01 05:25:40,708 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-12-01 05:25:40,708 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-12-01 05:25:40,708 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-01 05:25:40,708 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-01 05:25:40,708 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-12-01 05:25:40,708 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-12-01 05:25:40,869 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:660] 2025-12-01 05:25:40,870 >> Offline mode: forcing local_files_only=True
[INFO|video_processing_utils.py:726] 2025-12-01 05:25:40,873 >> loading configuration file video_preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|video_processing_utils.py:770] 2025-12-01 05:25:40,876 >> Video processor Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:928] 2025-12-01 05:25:40,877 >> Offline mode: forcing local_files_only=True
[INFO|processing_utils.py:1116] 2025-12-01 05:25:40,883 >> loading configuration file processor_config.json from cache at None
[INFO|processing_utils.py:1199] 2025-12-01 05:25:41,189 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2_5_VLProcessor"
}

[INFO|2025-12-01 05:25:41] llamafactory.data.loader:143 >> Loading dataset /scratch/indrisch/huggingface/hub/datasets--cvis-tmu--llamafactory-sqa3d-traces-multiimage-vqa/snapshots/ce5c54adc1608d1726730c9ff334e65b6dd70e46/data/...
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
Converting format of dataset (num_proc=32): 100%|██████████| 33047/33047 [00:00<?, ? examples/s][INFO|2025-12-01 05:25:41] llamafactory.hparams.parser:423 >> Process rank: 2, world size: 4, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-12-01 05:25:41] llamafactory.hparams.parser:423 >> Process rank: 1, world size: 4, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-12-01 05:25:41] llamafactory.hparams.parser:423 >> Process rank: 3, world size: 4, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
Converting format of dataset (num_proc=32): 33049 examples [00:00,  4.42 examples/s]            Converting format of dataset (num_proc=32): 33095 examples [00:00, 113.62 examples/s]Converting format of dataset (num_proc=32): 33166 examples [00:00, 263.72 examples/s]Converting format of dataset (num_proc=32): 33271 examples [00:00, 469.50 examples/s]Converting format of dataset (num_proc=32): 33379 examples [00:00, 631.89 examples/s]Converting format of dataset (num_proc=32): 33522 examples [00:00, 853.45 examples/s]Converting format of dataset (num_proc=32): 33680 examples [00:01, 1051.38 examples/s]Converting format of dataset (num_proc=32): 33839 examples [00:01, 1201.09 examples/s]Converting format of dataset (num_proc=32): 34145 examples [00:01, 1738.62 examples/s]Converting format of dataset (num_proc=32): 34416 examples [00:01, 2019.54 examples/s][rank2]:[W1201 05:25:42.031476387 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W1201 05:25:42.033295480 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W1201 05:25:42.036683906 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Converting format of dataset (num_proc=32): 35027 examples [00:01, 3189.01 examples/s]Converting format of dataset (num_proc=32): 36225 examples [00:01, 5707.44 examples/s]Converting format of dataset (num_proc=32): 40400 examples [00:01, 16275.83 examples/s]Converting format of dataset (num_proc=32): 46261 examples [00:01, 28716.97 examples/s]Converting format of dataset (num_proc=32): 52161 examples [00:01, 37669.06 examples/s]Converting format of dataset (num_proc=32): 58153 examples [00:01, 44218.28 examples/s]Converting format of dataset (num_proc=32): 63442 examples [00:02, 46742.95 examples/s]Converting format of dataset (num_proc=32): 66094 examples [00:02, 13171.32 examples/s]
[rank0]:[W1201 05:25:44.345462358 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
Running tokenizer on dataset (num_proc=32): 100%|██████████| 33047/33047 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=32): 34047 examples [15:35,  1.07 examples/s]            Running tokenizer on dataset (num_proc=32): 35047 examples [15:47,  2.55 examples/s]Running tokenizer on dataset (num_proc=32): 36047 examples [15:48,  4.69 examples/s]Running tokenizer on dataset (num_proc=32): 37047 examples [15:50,  7.68 examples/s]Running tokenizer on dataset (num_proc=32): 38047 examples [15:51, 11.99 examples/s]Running tokenizer on dataset (num_proc=32): 39047 examples [15:51, 18.09 examples/s]Running tokenizer on dataset (num_proc=32): 40047 examples [15:54, 26.22 examples/s]Running tokenizer on dataset (num_proc=32): 41047 examples [15:54, 38.38 examples/s]Running tokenizer on dataset (num_proc=32): 42047 examples [15:56, 54.73 examples/s]Running tokenizer on dataset (num_proc=32): 43047 examples [15:56, 78.13 examples/s]Running tokenizer on dataset (num_proc=32): 44047 examples [15:59, 103.49 examples/s]Running tokenizer on dataset (num_proc=32): 45047 examples [16:03, 122.82 examples/s]Running tokenizer on dataset (num_proc=32): 46047 examples [16:04, 173.19 examples/s]Running tokenizer on dataset (num_proc=32): 47047 examples [16:04, 241.56 examples/s]Running tokenizer on dataset (num_proc=32): 48080 examples [16:09, 225.18 examples/s]Running tokenizer on dataset (num_proc=32): 50080 examples [16:10, 372.68 examples/s]Running tokenizer on dataset (num_proc=32): 51080 examples [16:11, 488.50 examples/s]Running tokenizer on dataset (num_proc=32): 52080 examples [16:12, 547.44 examples/s]Running tokenizer on dataset (num_proc=32): 53080 examples [16:13, 608.66 examples/s]Running tokenizer on dataset (num_proc=32): 55080 examples [16:15, 702.07 examples/s]Running tokenizer on dataset (num_proc=32): 56112 examples [16:18, 593.93 examples/s]Running tokenizer on dataset (num_proc=32): 57144 examples [16:19, 695.38 examples/s]Running tokenizer on dataset (num_proc=32): 58144 examples [16:19, 780.95 examples/s]Running tokenizer on dataset (num_proc=32): 59177 examples [16:20, 950.50 examples/s]Running tokenizer on dataset (num_proc=32): 60210 examples [16:21, 931.27 examples/s]Running tokenizer on dataset (num_proc=32): 61275 examples [16:23, 707.89 examples/s]Running tokenizer on dataset (num_proc=32): 61406 examples [16:28, 338.48 examples/s]Running tokenizer on dataset (num_proc=32): 62406 examples [16:28, 496.27 examples/s]Running tokenizer on dataset (num_proc=32): 63406 examples [16:31, 450.47 examples/s]Running tokenizer on dataset (num_proc=32): 64439 examples [16:31, 590.57 examples/s]Running tokenizer on dataset (num_proc=32): 64603 examples [16:40, 196.49 examples/s]Running tokenizer on dataset (num_proc=32): 65669 examples [16:43, 232.88 examples/s]Running tokenizer on dataset (num_proc=32): 65767 examples [16:45, 185.72 examples/s]Running tokenizer on dataset (num_proc=32): 65832 examples [16:52, 105.48 examples/s]Running tokenizer on dataset (num_proc=32): 65898 examples [16:53, 95.50 examples/s] Running tokenizer on dataset (num_proc=32): 65931 examples [16:54, 94.26 examples/s]Running tokenizer on dataset (num_proc=32): 65964 examples [16:56, 72.21 examples/s]Running tokenizer on dataset (num_proc=32): 65996 examples [16:58, 57.95 examples/s]Running tokenizer on dataset (num_proc=32): 66061 examples [16:58, 66.55 examples/s]Running tokenizer on dataset (num_proc=32): 66094 examples [17:10, 18.86 examples/s]Running tokenizer on dataset (num_proc=32): 66094 examples [17:10, 32.06 examples/s]
training example:
input_ids:
[151644, 8948, 271, 262, 1446, 525, 264, 16585, 11129, 4142, 11528, 17847, 369, 27979, 647, 47522, 389, 264, 3175, 29519, 6109, 6839, 3941, 5248, 5335, 382, 262, 26149, 510, 262, 68252, 1283, 419, 1943, 11, 498, 686, 5258, 451, 6109, 5335, 438, 366, 1805, 29, 9492, 13, 18877, 1105, 438, 2155, 6194, 315, 279, 83490, 6109, 13, 28634, 311, 5335, 553, 220, 16, 5980, 1922, 304, 1973, 315, 11094, 25, 508, 16, 1125, 508, 17, 1125, 4593, 11, 508, 45, 29562, 262, 3596, 35304, 1718, 8575, 38439, 3298, 14017, 510, 262, 7288, 7854, 264, 12966, 220, 18, 35, 10502, 1614, 3941, 6194, 25, 3754, 6171, 3941, 5335, 11, 23583, 8674, 6249, 17040, 11, 323, 990, 5452, 14, 509, 8957, 14688, 21060, 369, 7990, 55916, 624, 262, 7288, 72077, 409, 849, 292, 3793, 26087, 983, 847, 1290, 97089, 17996, 4065, 14, 29898, 484, 32511, 56111, 7612, 1825, 35978, 28455, 448, 5091, 311, 279, 64171, 14, 8092, 13, 1416, 279, 58385, 374, 54761, 11, 1638, 311, 279, 1429, 38219, 8622, 1651, 323, 1977, 773, 624, 262, 7288, 5443, 1172, 9434, 5904, 26, 653, 537, 17023, 63133, 3565, 476, 17188, 389, 9250, 6540, 7797, 6770, 1633, 43898, 3793, 624, 262, 7288, 3197, 5248, 11178, 3000, 11, 8830, 553, 279, 1850, 2432, 311, 27979, 17133, 488, 4274, 1835, 3941, 6194, 13, 1416, 2058, 54761, 1283, 13295, 678, 5335, 11, 1584, 429, 9355, 382, 262, 30990, 320, 327, 32739, 1378, 10010, 11, 304, 419, 1973, 982, 262, 366, 26865, 397, 262, 14822, 14319, 29208, 32711, 304, 220, 18, 4142, 16, 15, 2805, 7354, 13, 356, 632, 5904, 448, 2168, 14937, 320, 68, 1302, 2572, 65750, 18, 5669, 3100, 7579, 18010, 1290, 315, 8718, 26, 508, 16, 5669, 1852, 1894, 11, 42396, 64212, 2823, 63594, 714, 4583, 320, 2640, 37294, 17, 20, 15, 11211, 7241, 5871, 568, 7036, 894, 17796, 8957, 14, 2969, 26745, 487, 323, 1246, 498, 19673, 432, 624, 262, 690, 26865, 397, 262, 366, 9217, 397, 262, 362, 3175, 63594, 4226, 1172, 11, 892, 1231, 387, 510, 262, 7288, 1416, 14016, 25, 3776, 37328, 4226, 11, 4504, 320, 68, 1302, 2572, 1036, 17, 32511, 476, 7414, 14, 2753, 320, 68, 1302, 2572, 1036, 9693, 854, 4292, 262, 7288, 1416, 537, 14016, 25, 362, 11652, 476, 1378, 624, 262, 1416, 4226, 537, 9434, 11, 421, 1052, 374, 902, 2797, 5904, 11, 476, 421, 279, 4226, 374, 35197, 54761, 25, 1036, 33260, 8253, 854, 624, 262, 690, 9217, 1339, 262, 62947, 609, 43797, 50, 510, 262, 7288, 3155, 4183, 13153, 279, 3405, 476, 1140, 5335, 26, 653, 4183, 2550, 4718, 476, 4960, 14158, 624, 262, 7288, 84468, 4285, 1894, 5036, 448, 518, 1429, 825, 22739, 26087, 4145, 3446, 838, 4322, 1574, 58689, 7256, 854, 4292, 262, 7288, 2823, 12966, 3941, 6194, 26, 421, 6194, 28295, 11, 62552, 279, 11299, 15432, 5904, 323, 5185, 432, 304, 366, 26865, 29816, 257, 151645, 198, 151644, 872, 198, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 30, 220, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151645, 198, 151644, 77091, 198, 198, 32462, 432, 11, 1077, 594, 1430, 311, 7071, 700, 1128, 594, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 62410, 13, 5512, 11, 1077, 594, 50087, 279, 14852, 6505, 13, 576, 21921, 374, 4363, 264, 4158, 11, 1378, 1737, 95112, 4982, 13, 2411, 220, 16, 16, 297, 62410, 320, 3481, 7950, 3082, 701, 582, 1184, 311, 1779, 429, 7702, 13, 21131, 518, 279, 5335, 11, 5310, 279, 825, 9027, 279, 21921, 3082, 11, 1052, 594, 264, 17846, 3403, 279, 19309, 11, 714, 3783, 11, 902, 2293, 11489, 11, 279, 3405, 374, 911, 279, 21921, 13, 13824, 11, 7196, 279, 330, 66, 75512, 1, 1588, 374, 279, 5819, 4982, 13, 13824, 11, 902, 11, 279, 19309, 702, 264, 21921, 3685, 30, 13824, 11, 902, 11, 279, 1887, 21921, 374, 279, 4158, 61607, 13, 13824, 11, 304, 279, 2168, 1380, 279, 6249, 4933, 279, 19309, 323, 21921, 11, 279, 1909, 315, 279, 21921, 2293, 11489, 11, 902, 11, 7196, 279, 17846, 30, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 2727, 330, 65, 77832, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 3263, 2055, 220, 16, 16, 297, 62410, 1035, 387, 279, 8416, 2115, 3082, 8674, 311, 279, 19309, 13, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 279, 19309, 3082, 13, 2160, 429, 949, 315, 279, 21921, 6505, 30, 13824, 11, 902, 11, 7196, 279, 21921, 374, 279, 4158, 61607, 1212, 279, 19309, 13, 13824, 11, 902, 11, 279, 1909, 315, 429, 21921, 2293, 11489, 11, 902, 11, 304, 1045, 5335, 11, 1052, 594, 264, 17846, 25911, 476, 21810, 13, 13824, 11, 902, 11, 1077, 594, 1779, 279, 1156, 2421, 5335, 13, 576, 17196, 45766, 374, 33866, 11, 1221, 279, 19309, 3082, 13, 13824, 11, 279, 4396, 4226, 25, 3330, 518, 279, 5335, 11, 279, 1633, 389, 1909, 315, 279, 14852, 21921, 320, 1782, 4158, 26482, 4982, 8, 518, 220, 16, 16, 297, 62410, 374, 264, 17846, 13, 13824, 11, 902, 11, 3783, 21677, 17846, 374, 389, 279, 7002, 3403, 279, 19309, 13, 13824, 11, 7196, 358, 5786, 878, 13, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 594, 3405, 25, 330, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 7521, 2055, 1156, 11, 10542, 279, 21921, 13, 576, 4158, 26482, 4982, 3143, 220, 16, 16, 297, 62410, 320, 3481, 7950, 568, 3555, 594, 389, 1909, 30, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 429, 3082, 13, 13824, 11, 9834, 0, 758, 279, 21844, 476, 21810, 11, 1052, 594, 264, 17846, 13, 2055, 279, 4226, 374, 264, 17846, 4192, 1249, 8253, 1128, 374, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 527, 20666, 11, 23643, 279, 27979, 6789, 323, 5424, 304, 279, 5335, 25, 18611, 16, 13, 95210, 279, 1036, 65, 77832, 21921, 854, 304, 3405, 25, 362, 4158, 11, 1378, 481, 26482, 5819, 4982, 374, 9434, 3143, 279, 1909, 481, 2115, 320, 16, 16, 297, 527, 20666, 8, 3082, 8674, 311, 1008, 37664, 1075, 279, 19309, 323, 17196, 13, 2303, 17, 13, 1374, 19991, 6171, 3403, 419, 21921, 25, 758, 279, 5335, 11, 264, 3070, 73225, 334, 374, 21810, 389, 279, 7002, 3403, 419, 21921, 3082, 13, 79226, 54216, 151645, 198]
inputs:
<|im_start|>system

    You are a careful vision–language assistant for spatial VQA on a single indoor scene shown across multiple images.

    INPUT:
    Immediately after this message, you will receive N scene images as <image> tags. Treat them as different views of the SAME scene. Refer to images by 1-based index in order of appearance: [1], [2], …, [N].

    REASONING PRINCIPLES:
    • Build a consistent 3D mental model across views: track objects across images, infer relative camera pose, and use scale/occlusion/shadows for depth cues.
    • Interpret deictic terms (“to my right/left/in front/behind”) EGOCENTRICALLY with respect to the narrator/agent. If the viewpoint is ambiguous, default to the most informative central view and say so.
    • Use only visible evidence; do not invent unseen details or rely on external knowledge beyond basic object/color terms.
    • When multiple candidates exist, resolve by the best match to spatial phrase + salience across views. If still ambiguous after checking all images, state that clearly.

    OUTPUT (exactly two blocks, in this order):
    <think>
    Step-by-step reasoning in 3–10 short steps. Cite evidence with image indices (e.g., “[3]: light wood desk right of monitor; [1]: same color, confirms”). Be concise but complete (aim ≤250 tokens unless necessary). Note any occlusion/ambiguity and how you resolved it.
    </think>
    <answer>
    A single concise answer only, which may be:
    • If sufficient: One-word answer, Count (e.g., “2”) or Yes/No (e.g., “yes”).
    • If not sufficient: A sentence or two.
    If answer not visible, if there is no clear evidence, or if the answer is genuinely ambiguous: “cannot determine”.
    </answer>

    STYLE & RULES:
    • Do NOT repeat the question or list images; do NOT output JSON or extra sections.
    • Prefer simple color names with at most one modifier (“light/dark/pale/beige”).
    • Be consistent across views; if views disagree, prioritize the clearest evidence and note it in <think>.
    <|im_end|>
<|im_start|>user
What is on top of the bathroom cabinet that is on my 11 o'clock? <|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|im_end|>
<|im_start|>assistant

Got it, let's try to figure out what's on top of the bathroom cabinet at 11 o'clock. First, let's visualize the bathroom setup. The cabinet is likely a white, two-drawer unit. At 11 o'clock (top-left area), we need to check that spot. Looking at the images, especially the one showing the cabinet area, there's a mirror above the sink, but wait, no—wait, the question is about the cabinet. Wait, maybe the "cabinet" here is the storage unit. Wait, no, the sink has a cabinet below? Wait, no, the main cabinet is the white drawers. Wait, in the image where the camera shows the sink and cabinet, the top of the cabinet—wait, no, maybe the mirror? Wait, no, let's check again. Wait, the user says "bathroom cabinet that is on my 11 o'clock". So 11 o'clock would be the upper left area relative to the sink. Looking at the images, there's a mirror mounted on the wall above the sink area. Is that part of the cabinet setup? Wait, no, maybe the cabinet is the white drawers under the sink. Wait, no, the top of that cabinet—wait, no, in some images, there's a mirror reflected or mounted. Wait, no, let's check the first few images. The shower curtain is colorful, then the sink area. Wait, the correct answer: looking at the images, the object on top of the bathroom cabinet (the white drawer unit) at 11 o'clock is a mirror. Wait, no, wait—the mirror is on the wall above the sink. Wait, maybe I misread. Wait, no, let's check again. Wait, the user's question: "What is on top of the bathroom cabinet that is on my 11 o'clock?" So first, identify the cabinet. The white drawer unit near 11 o'clock (top-left). What's on top? Looking at the images, there's a mirror mounted on the wall above that area. Wait, yes! In the reflection or mounted, there's a mirror. So the answer is a mirror.


To determine what is on top of the bathroom cabinet at 11 o’clock, analyze the spatial layout and elements in the images:  

1. Locate the “bathroom cabinet” in question: A white, two - drawer storage unit is visible near the top - left (11 o’clock) area relative to other fixtures like the sink and shower.  
2. Examine objects above this cabinet: In the images, a **mirror** is mounted on the wall above this cabinet area.  


Mirror<|im_end|>

label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 198, 32462, 432, 11, 1077, 594, 1430, 311, 7071, 700, 1128, 594, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 62410, 13, 5512, 11, 1077, 594, 50087, 279, 14852, 6505, 13, 576, 21921, 374, 4363, 264, 4158, 11, 1378, 1737, 95112, 4982, 13, 2411, 220, 16, 16, 297, 62410, 320, 3481, 7950, 3082, 701, 582, 1184, 311, 1779, 429, 7702, 13, 21131, 518, 279, 5335, 11, 5310, 279, 825, 9027, 279, 21921, 3082, 11, 1052, 594, 264, 17846, 3403, 279, 19309, 11, 714, 3783, 11, 902, 2293, 11489, 11, 279, 3405, 374, 911, 279, 21921, 13, 13824, 11, 7196, 279, 330, 66, 75512, 1, 1588, 374, 279, 5819, 4982, 13, 13824, 11, 902, 11, 279, 19309, 702, 264, 21921, 3685, 30, 13824, 11, 902, 11, 279, 1887, 21921, 374, 279, 4158, 61607, 13, 13824, 11, 304, 279, 2168, 1380, 279, 6249, 4933, 279, 19309, 323, 21921, 11, 279, 1909, 315, 279, 21921, 2293, 11489, 11, 902, 11, 7196, 279, 17846, 30, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 2727, 330, 65, 77832, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 3263, 2055, 220, 16, 16, 297, 62410, 1035, 387, 279, 8416, 2115, 3082, 8674, 311, 279, 19309, 13, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 279, 19309, 3082, 13, 2160, 429, 949, 315, 279, 21921, 6505, 30, 13824, 11, 902, 11, 7196, 279, 21921, 374, 279, 4158, 61607, 1212, 279, 19309, 13, 13824, 11, 902, 11, 279, 1909, 315, 429, 21921, 2293, 11489, 11, 902, 11, 304, 1045, 5335, 11, 1052, 594, 264, 17846, 25911, 476, 21810, 13, 13824, 11, 902, 11, 1077, 594, 1779, 279, 1156, 2421, 5335, 13, 576, 17196, 45766, 374, 33866, 11, 1221, 279, 19309, 3082, 13, 13824, 11, 279, 4396, 4226, 25, 3330, 518, 279, 5335, 11, 279, 1633, 389, 1909, 315, 279, 14852, 21921, 320, 1782, 4158, 26482, 4982, 8, 518, 220, 16, 16, 297, 62410, 374, 264, 17846, 13, 13824, 11, 902, 11, 3783, 21677, 17846, 374, 389, 279, 7002, 3403, 279, 19309, 13, 13824, 11, 7196, 358, 5786, 878, 13, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 594, 3405, 25, 330, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 7521, 2055, 1156, 11, 10542, 279, 21921, 13, 576, 4158, 26482, 4982, 3143, 220, 16, 16, 297, 62410, 320, 3481, 7950, 568, 3555, 594, 389, 1909, 30, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 429, 3082, 13, 13824, 11, 9834, 0, 758, 279, 21844, 476, 21810, 11, 1052, 594, 264, 17846, 13, 2055, 279, 4226, 374, 264, 17846, 4192, 1249, 8253, 1128, 374, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 527, 20666, 11, 23643, 279, 27979, 6789, 323, 5424, 304, 279, 5335, 25, 18611, 16, 13, 95210, 279, 1036, 65, 77832, 21921, 854, 304, 3405, 25, 362, 4158, 11, 1378, 481, 26482, 5819, 4982, 374, 9434, 3143, 279, 1909, 481, 2115, 320, 16, 16, 297, 527, 20666, 8, 3082, 8674, 311, 1008, 37664, 1075, 279, 19309, 323, 17196, 13, 2303, 17, 13, 1374, 19991, 6171, 3403, 419, 21921, 25, 758, 279, 5335, 11, 264, 3070, 73225, 334, 374, 21810, 389, 279, 7002, 3403, 419, 21921, 3082, 13, 79226, 54216, 151645, 198]
labels:

Got it, let's try to figure out what's on top of the bathroom cabinet at 11 o'clock. First, let's visualize the bathroom setup. The cabinet is likely a white, two-drawer unit. At 11 o'clock (top-left area), we need to check that spot. Looking at the images, especially the one showing the cabinet area, there's a mirror above the sink, but wait, no—wait, the question is about the cabinet. Wait, maybe the "cabinet" here is the storage unit. Wait, no, the sink has a cabinet below? Wait, no, the main cabinet is the white drawers. Wait, in the image where the camera shows the sink and cabinet, the top of the cabinet—wait, no, maybe the mirror? Wait, no, let's check again. Wait, the user says "bathroom cabinet that is on my 11 o'clock". So 11 o'clock would be the upper left area relative to the sink. Looking at the images, there's a mirror mounted on the wall above the sink area. Is that part of the cabinet setup? Wait, no, maybe the cabinet is the white drawers under the sink. Wait, no, the top of that cabinet—wait, no, in some images, there's a mirror reflected or mounted. Wait, no, let's check the first few images. The shower curtain is colorful, then the sink area. Wait, the correct answer: looking at the images, the object on top of the bathroom cabinet (the white drawer unit) at 11 o'clock is a mirror. Wait, no, wait—the mirror is on the wall above the sink. Wait, maybe I misread. Wait, no, let's check again. Wait, the user's question: "What is on top of the bathroom cabinet that is on my 11 o'clock?" So first, identify the cabinet. The white drawer unit near 11 o'clock (top-left). What's on top? Looking at the images, there's a mirror mounted on the wall above that area. Wait, yes! In the reflection or mounted, there's a mirror. So the answer is a mirror.


To determine what is on top of the bathroom cabinet at 11 o’clock, analyze the spatial layout and elements in the images:  

1. Locate the “bathroom cabinet” in question: A white, two - drawer storage unit is visible near the top - left (11 o’clock) area relative to other fixtures like the sink and shower.  
2. Examine objects above this cabinet: In the images, a **mirror** is mounted on the wall above this cabinet area.  


Mirror<|im_end|>

[INFO|hub.py:421] 2025-12-01 05:42:58,395 >> Offline mode: forcing local_files_only=True
[INFO|configuration_utils.py:765] 2025-12-01 05:42:58,401 >> loading configuration file config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
[INFO|configuration_utils.py:839] 2025-12-01 05:42:58,405 >> Model config Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 128000,
  "max_window_layers": 28,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "_name_or_path": "Qwen/Qwen2.5-VL-7B-Instruct",
    "architectures": [
      "Qwen2_5_VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "dtype": "bfloat16",
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "layer_types": [
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention"
    ],
    "max_position_embeddings": 128000,
    "max_window_layers": 28,
    "model_type": "qwen2_5_vl_text",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "use_cache": true,
    "use_sliding_window": false,
    "vision_token_id": 151654,
    "vocab_size": 152064
  },
  "tie_word_embeddings": false,
  "transformers_version": "4.57.0",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 3584,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

[INFO|2025-12-01 05:42:58] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
[INFO|2025-12-01 05:42:58] llamafactory.model.model_utils.liger_kernel:143 >> Liger kernel has been applied to the model.
[INFO|hub.py:421] 2025-12-01 05:42:58,627 >> Offline mode: forcing local_files_only=True
[INFO|hub.py:421] 2025-12-01 05:42:58,629 >> Offline mode: forcing local_files_only=True
[INFO|modeling_utils.py:4840] 2025-12-01 05:42:58,630 >> Offline mode: forcing local_files_only=True
[INFO|modeling_utils.py:1176] 2025-12-01 05:42:58,634 >> loading weights file model.safetensors from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/model.safetensors.index.json
[INFO|modeling_utils.py:4377] 2025-12-01 05:42:58,647 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[2025-12-01 05:42:58,647] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
[WARNING|logging.py:328] 2025-12-01 05:42:58,658 >> Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLForConditionalGeneration is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[INFO|configuration_utils.py:986] 2025-12-01 05:42:58,661 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

[WARNING|logging.py:328] 2025-12-01 05:42:58,663 >> Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[INFO|modeling_utils.py:2345] 2025-12-01 05:42:58,664 >> Instantiating Qwen2_5_VisionTransformerPretrainedModel model under default dtype torch.float32.
[WARNING|logging.py:328] 2025-12-01 05:42:58,666 >> Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VisionTransformerPretrainedModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[2025-12-01 05:42:58,952] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
[2025-12-01 05:42:58,960] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLForConditionalGeneration is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[2025-12-01 05:42:58,966] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VisionTransformerPretrainedModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLForConditionalGeneration is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VisionTransformerPretrainedModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLForConditionalGeneration is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VisionTransformerPretrainedModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[INFO|modeling_utils.py:2345] 2025-12-01 05:42:59,518 >> Instantiating Qwen2_5_VLTextModel model under default dtype torch.float32.
[WARNING|logging.py:328] 2025-12-01 05:42:59,521 >> Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLTextModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLTextModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLTextModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLTextModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[2025-12-01 05:43:03,364] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 729, num_elems = 8.29B
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:14<00:58, 14.68s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:14<00:58, 14.67s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:14<00:58, 14.68s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:14<00:59, 14.78s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:28<00:42, 14.00s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:28<00:41, 14.00s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:28<00:42, 14.00s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:28<00:42, 14.05s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:43<00:28, 14.47s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:43<00:28, 14.47s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:43<00:28, 14.47s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:43<00:28, 14.49s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:56<00:13, 13.97s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:56<00:13, 13.97s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:56<00:13, 13.97s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:56<00:13, 13.98s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:00<00:00, 10.54s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:00<00:00, 10.54s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:00<00:00, 10.54s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:00<00:00, 12.18s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [01:00<00:00, 12.18s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [01:00<00:00, 12.18s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [01:00<00:00, 10.53s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:00<00:00, 12.19s/it]
[INFO|configuration_utils.py:941] 2025-12-01 05:44:04,326 >> loading configuration file generation_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/generation_config.json
[INFO|configuration_utils.py:986] 2025-12-01 05:44:04,326 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 1e-06
}

[INFO|dynamic_module_utils.py:423] 2025-12-01 05:44:04,328 >> Could not locate the custom_generate/generate.py inside Qwen/Qwen2.5-VL-7B-Instruct.
[INFO|2025-12-01 05:44:04] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2025-12-01 05:44:04] llamafactory.model.model_utils.attention:143 >> Using FlashAttention-2 for faster training and inference.
[INFO|2025-12-01 05:44:04] llamafactory.model.adapter:143 >> DeepSpeed ZeRO3 detected, remaining trainable params in float32.
[INFO|2025-12-01 05:44:04] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA
[INFO|2025-12-01 05:44:04] llamafactory.model.model_utils.misc:143 >> Found linear modules: k_proj,q_proj,v_proj,o_proj,down_proj,gate_proj,up_proj
[INFO|2025-12-01 05:44:04] llamafactory.model.model_utils.visual:143 >> Set vision model not trainable: ['visual.patch_embed', 'visual.blocks'].
[INFO|2025-12-01 05:44:04] llamafactory.model.model_utils.visual:143 >> Set multi model projector not trainable: visual.merger.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
[INFO|2025-12-01 05:44:04] llamafactory.model.loader:143 >> trainable params: 20,185,088 || all params: 8,312,351,744 || trainable%: 0.2428
name: base_model.model.model.visual.patch_embed.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.ln_q.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.0.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.0.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.embed_tokens.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.lm_head.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
[INFO|trainer.py:749] 2025-12-01 05:44:04,930 >> Using auto half precision backend
[WARNING|2025-12-01 05:44:04] llamafactory.train.callbacks:154 >> Previous trainer log in this folder will be deleted.
[WARNING|trainer.py:982] 2025-12-01 05:44:04,933 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Currently training with a batch size of: 2
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Currently training with a batch size of: 2
[DEBUG|trainer.py:2373] 2025-12-01 05:44:05,152 >> Currently training with a batch size of: 2
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 8. Using DeepSpeed's value.
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
[INFO|deepspeed.py:383] 2025-12-01 05:44:05,167 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Currently training with a batch size of: 2
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Using /dev/shm/slurm.indrisch.117330/.cache/torch_extensions as PyTorch extensions root...
Creating extension directory /dev/shm/slurm.indrisch.117330/.cache/torch_extensions/cpu_adam...
Detected CUDA files, patching ldflags
Emitting ninja build file /dev/shm/slurm.indrisch.117330/.cache/torch_extensions/cpu_adam/build.ninja...
/opt/conda/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module cpu_adam...
Using envvar MAX_JOBS (16) as the number of workers...
Using /dev/shm/slurm.indrisch.117330/.cache/torch_extensions as PyTorch extensions root...
Using /dev/shm/slurm.indrisch.117330/.cache/torch_extensions as PyTorch extensions root...
Using /dev/shm/slurm.indrisch.117330/.cache/torch_extensions as PyTorch extensions root...
[1/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.11/site-packages/torch/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o 
[2/3] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.11/site-packages/torch/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o 
[3/3] c++ cpu_adam.o cpu_adam_impl.o -shared -lcurand -L/usr/local/cuda/lib64 -L/opt/conda/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so
Loading extension module cpu_adam...
Time to load cpu_adam op: 23.46071767807007 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-12-01 05:44:29,789] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
Loading extension module cpu_adam...
Time to load cpu_adam op: 23.37930393218994 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-12-01 05:44:29,829] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
Loading extension module cpu_adam...
Loading extension module cpu_adam...
Time to load cpu_adam op: 23.517503261566162 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-12-01 05:44:29,871] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.9, git-hash=unknown, git-branch=unknown
[2025-12-01 05:44:29,871] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
Time to load cpu_adam op: 23.51781177520752 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-12-01 05:44:29,871] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
[2025-12-01 05:44:29,897] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-12-01 05:44:29,900] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-12-01 05:44:29,900] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-12-01 05:44:29,921] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2025-12-01 05:44:29,921] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2025-12-01 05:44:29,921] [INFO] [logging.py:107:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2025-12-01 05:44:29,921] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2025-12-01 05:44:30,122] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[2025-12-01 05:44:30,123] [INFO] [utils.py:782:see_memory_usage] MA 0.04 GB         Max_MA 3.05 GB         CA 0.08 GB         Max_CA 3 GB 
[2025-12-01 05:44:30,123] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.27 GB, percent = 8.9%
[2025-12-01 05:44:30,128] [INFO] [stage3.py:170:__init__] Reduce bucket size 12845056
[2025-12-01 05:44:30,128] [INFO] [stage3.py:171:__init__] Prefetch bucket size 11560550
[2025-12-01 05:44:30,322] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2025-12-01 05:44:30,322] [INFO] [utils.py:782:see_memory_usage] MA 0.04 GB         Max_MA 0.04 GB         CA 0.08 GB         Max_CA 0 GB 
[2025-12-01 05:44:30,322] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.27 GB, percent = 8.9%
Parameter Offload: Total persistent parameters: 8303616 in 676 params
[2025-12-01 05:44:31,139] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2025-12-01 05:44:31,140] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.04 GB         CA 0.08 GB         Max_CA 0 GB 
[2025-12-01 05:44:31,140] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.39 GB, percent = 8.9%
[2025-12-01 05:44:31,420] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[2025-12-01 05:44:31,420] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.08 GB         Max_CA 0 GB 
[2025-12-01 05:44:31,420] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.35 GB, percent = 8.9%
[2025-12-01 05:44:31,711] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 1
[2025-12-01 05:44:31,711] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.08 GB         Max_CA 0 GB 
[2025-12-01 05:44:31,711] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.39 GB, percent = 8.9%
[2025-12-01 05:44:31,982] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[2025-12-01 05:44:31,983] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.08 GB         Max_CA 0 GB 
[2025-12-01 05:44:31,983] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.39 GB, percent = 8.9%
[2025-12-01 05:44:32,264] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[2025-12-01 05:44:32,265] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.08 GB         Max_CA 0 GB 
[2025-12-01 05:44:32,265] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.41 GB, percent = 8.9%
[2025-12-01 05:44:32,537] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-12-01 05:44:32,537] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.08 GB         Max_CA 0 GB 
[2025-12-01 05:44:32,537] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.49 GB, percent = 8.9%
[2025-12-01 05:44:32,856] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-12-01 05:44:32,857] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.08 GB         Max_CA 0 GB 
[2025-12-01 05:44:32,857] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.53 GB, percent = 8.9%
[2025-12-01 05:44:32,858] [INFO] [stage3.py:534:_setup_for_real_optimizer] optimizer state initialized
Attempting to resume from /scratch/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465/
Attempting to resume from /scratch/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465/
Attempting to resume from /scratch/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465/
[2025-12-01 05:44:32,999] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /scratch/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/zero_pp_rank_1_mp_rank_00_model_states.pt...
[2025-12-01 05:44:32,999] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /scratch/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/zero_pp_rank_3_mp_rank_00_model_states.pt...
[2025-12-01 05:44:32,999] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /scratch/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/zero_pp_rank_2_mp_rank_00_model_states.pt...
[2025-12-01 05:44:33,009] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /scratch/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/zero_pp_rank_2_mp_rank_00_model_states.pt.
[2025-12-01 05:44:33,009] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /scratch/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/zero_pp_rank_2_mp_rank_00_model_states.pt...
[2025-12-01 05:44:33,010] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /scratch/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/zero_pp_rank_1_mp_rank_00_model_states.pt.
[2025-12-01 05:44:33,010] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /scratch/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/zero_pp_rank_1_mp_rank_00_model_states.pt...
[2025-12-01 05:44:33,013] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /scratch/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/zero_pp_rank_3_mp_rank_00_model_states.pt.
[2025-12-01 05:44:33,013] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /scratch/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/zero_pp_rank_3_mp_rank_00_model_states.pt...
[2025-12-01 05:44:33,017] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /scratch/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/zero_pp_rank_2_mp_rank_00_model_states.pt.
[2025-12-01 05:44:33,018] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /scratch/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/zero_pp_rank_1_mp_rank_00_model_states.pt.
[2025-12-01 05:44:33,021] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /scratch/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/zero_pp_rank_3_mp_rank_00_model_states.pt.
[2025-12-01 05:44:33,031] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /scratch/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2025-12-01 05:44:33,032] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /scratch/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2025-12-01 05:44:33,036] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /scratch/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2025-12-01 05:44:33,114] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /scratch/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2025-12-01 05:44:33,114] [INFO] [engine.py:3241:_get_all_zero_checkpoint_state_dicts] successfully read 4 ZeRO state_dicts for rank 2
[2025-12-01 05:44:33,123] [INFO] [engine.py:3191:_load_zero_checkpoint] loading 4 zero partition checkpoints for rank 2
[2025-12-01 05:44:33,136] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /scratch/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2025-12-01 05:44:33,136] [INFO] [engine.py:3241:_get_all_zero_checkpoint_state_dicts] successfully read 4 ZeRO state_dicts for rank 3
[2025-12-01 05:44:33,143] [INFO] [engine.py:3191:_load_zero_checkpoint] loading 4 zero partition checkpoints for rank 3
[2025-12-01 05:44:33,173] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /scratch/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2025-12-01 05:44:33,173] [INFO] [engine.py:3241:_get_all_zero_checkpoint_state_dicts] successfully read 4 ZeRO state_dicts for rank 1
[2025-12-01 05:44:33,180] [INFO] [engine.py:3191:_load_zero_checkpoint] loading 4 zero partition checkpoints for rank 1
[2025-12-01 05:44:33,265] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-12-01 05:44:33,266] [INFO] [utils.py:782:see_memory_usage] MA 0.02 GB         Max_MA 0.02 GB         CA 0.12 GB         Max_CA 0 GB 
[2025-12-01 05:44:33,266] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.66 GB, percent = 9.0%
[2025-12-01 05:44:33,266] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[2025-12-01 05:44:33,266] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-12-01 05:44:33,266] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-12-01 05:44:33,266] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2025-12-01 05:44:33,271] [INFO] [config.py:1003:print] DeepSpeedEngine configuration:
[2025-12-01 05:44:33,271] [INFO] [config.py:1007:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-12-01 05:44:33,271] [INFO] [config.py:1007:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-12-01 05:44:33,271] [INFO] [config.py:1007:print]   amp_enabled .................. False
[2025-12-01 05:44:33,271] [INFO] [config.py:1007:print]   amp_params ................... False
[2025-12-01 05:44:33,271] [INFO] [config.py:1007:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-12-01 05:44:33,271] [INFO] [config.py:1007:print]   bfloat16_enabled ............. True
[2025-12-01 05:44:33,271] [INFO] [config.py:1007:print]   bfloat16_immediate_grad_update  True
[2025-12-01 05:44:33,271] [INFO] [config.py:1007:print]   checkpoint_parallel_write_pipeline  False
[2025-12-01 05:44:33,271] [INFO] [config.py:1007:print]   checkpoint_tag_validation_enabled  True
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   checkpoint_tag_validation_fail  False
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1482ced83090>
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   communication_data_type ...... None
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   curriculum_enabled_legacy .... False
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   curriculum_params_legacy ..... False
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   data_efficiency_enabled ...... False
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   dataloader_drop_last ......... False
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   disable_allgather ............ False
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   dump_state ................... False
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   dynamic_loss_scale_args ...... None
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   eigenvalue_enabled ........... False
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   eigenvalue_gas_boundary_resolution  1
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   eigenvalue_layer_num ......... 0
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   eigenvalue_max_iter .......... 100
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   eigenvalue_stability ......... 1e-06
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   eigenvalue_tol ............... 0.01
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   eigenvalue_verbose ........... False
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   elasticity_enabled ........... False
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   fp16_auto_cast ............... None
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   fp16_enabled ................. False
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   fp16_master_weights_and_gradients  False
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   global_rank .................. 0
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   grad_accum_dtype ............. None
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   gradient_accumulation_steps .. 8
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   gradient_clipping ............ 1.0
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   gradient_predivide_factor .... 1.0
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   graph_harvesting ............. False
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   initial_dynamic_scale ........ 1
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   load_universal_checkpoint .... False
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   loss_scale ................... 1.0
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   memory_breakdown ............. False
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   mics_hierarchial_params_gather  False
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   mics_shard_size .............. -1
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   optimizer_legacy_fusion ...... False
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   optimizer_name ............... None
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   optimizer_params ............. None
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   pld_enabled .................. False
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   pld_params ................... False
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   prescale_gradients ........... False
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   scheduler_name ............... None
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   scheduler_params ............. None
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   seq_parallel_communication_data_type  torch.float32
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   sparse_attention ............. None
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   sparse_gradients_enabled ..... False
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   steps_per_print .............. inf
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   timers_config ................ enabled=True synchronized=True
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   train_batch_size ............. 64
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   train_micro_batch_size_per_gpu  2
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   use_data_before_expert_parallel_  False
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   use_node_local_storage ....... False
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   wall_clock_breakdown ......... False
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   weight_quantization_config ... None
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   world_size ................... 4
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   zero_allow_untested_optimizer  True
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=12845056 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=11560550 param_persistence_threshold=35840 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   zero_enabled ................. True
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   zero_force_ds_cpu_optimizer .. True
[2025-12-01 05:44:33,272] [INFO] [config.py:1007:print]   zero_optimization_stage ...... 3
[2025-12-01 05:44:33,272] [INFO] [config.py:993:print_user_config]   json = {
    "train_batch_size": 64, 
    "train_micro_batch_size_per_gpu": 2, 
    "gradient_accumulation_steps": 8, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "offload_param": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "overlap_comm": false, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 1.284506e+07, 
        "stage3_prefetch_bucket_size": 1.156055e+07, 
        "stage3_param_persistence_threshold": 3.584000e+04, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "steps_per_print": inf
}
[INFO|deepspeed.py:495] 2025-12-01 05:44:33,274 >> Attempting to resume from /scratch/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465/
[2025-12-01 05:44:33,289] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /scratch/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-12-01 05:44:33,300] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /scratch/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-12-01 05:44:33,300] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /scratch/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-12-01 05:44:33,308] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /scratch/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-12-01 05:44:33,321] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /scratch/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-12-01 05:44:33,656] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /scratch/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465/global_step465/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-12-01 05:44:33,656] [INFO] [engine.py:3241:_get_all_zero_checkpoint_state_dicts] successfully read 4 ZeRO state_dicts for rank 0
[2025-12-01 05:44:33,662] [INFO] [engine.py:3191:_load_zero_checkpoint] loading 4 zero partition checkpoints for rank 0
Detected torch version: 2.6.0+cu124
Detected torch version: 2.6.0+cu124
Detected torch version: 2.6.0+cu124
[DEBUG|import_utils.py:92] 2025-12-01 05:44:33,688 >> Detected torch version: 2.6.0+cu124
***** Running training *****
***** Running training *****
***** Running training *****
[INFO|trainer.py:2519] 2025-12-01 05:44:33,691 >> ***** Running training *****
  Num examples = 29,742
  Num examples = 29,742
  Num examples = 29,742
  Num Epochs = 2
[INFO|trainer.py:2520] 2025-12-01 05:44:33,691 >>   Num examples = 29,742
  Num Epochs = 2
  Num Epochs = 2
[INFO|trainer.py:2521] 2025-12-01 05:44:33,691 >>   Num Epochs = 2
  Instantaneous batch size per device = 2
  Instantaneous batch size per device = 2
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2522] 2025-12-01 05:44:33,691 >>   Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 8
  Gradient Accumulation steps = 8
[INFO|trainer.py:2525] 2025-12-01 05:44:33,691 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 8
  Total optimization steps = 930
  Total optimization steps = 930
  Total optimization steps = 930
[INFO|trainer.py:2526] 2025-12-01 05:44:33,691 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2527] 2025-12-01 05:44:33,691 >>   Total optimization steps = 930
[INFO|trainer.py:2528] 2025-12-01 05:44:33,696 >>   Number of trainable parameters = 20,185,088
  Number of trainable parameters = 20,185,088
  Number of trainable parameters = 20,185,088
[INFO|trainer.py:2549] 2025-12-01 05:44:33,696 >>   Continuing training from checkpoint, will skip to saved global_step
  Number of trainable parameters = 20,185,088
[INFO|trainer.py:2550] 2025-12-01 05:44:33,696 >>   Continuing training from epoch 1
[INFO|trainer.py:2551] 2025-12-01 05:44:33,696 >>   Continuing training from global step 465
[INFO|trainer.py:2553] 2025-12-01 05:44:33,696 >>   Will skip the first 1 epochs then the first 0 batches in the first epoch.
  Continuing training from checkpoint, will skip to saved global_step
  Continuing training from epoch 1
  Continuing training from global step 465
  Will skip the first 1 epochs then the first 0 batches in the first epoch.
  Continuing training from checkpoint, will skip to saved global_step
  Continuing training from epoch 1
  Continuing training from global step 465
  Will skip the first 1 epochs then the first 0 batches in the first epoch.
  Continuing training from checkpoint, will skip to saved global_step
  Continuing training from epoch 1
  Continuing training from global step 465
  Will skip the first 1 epochs then the first 0 batches in the first epoch.
  0%|          | 0/930 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /dev/shm/slurm.indrisch.117330/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /dev/shm/slurm.indrisch.117330/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /dev/shm/slurm.indrisch.117330/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /dev/shm/slurm.indrisch.117330/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
 50%|█████     | 466/930 [01:46<01:45,  4.39it/s] 50%|█████     | 467/930 [03:13<03:48,  2.03it/s] 50%|█████     | 468/930 [04:51<07:04,  1.09it/s] 50%|█████     | 469/930 [06:31<11:44,  1.53s/it] 51%|█████     | 470/930 [07:50<16:53,  2.20s/it]                                                 {'loss': 0.722, 'grad_norm': 0.4169086813926697, 'learning_rate': 5.794218338977854e-05, 'epoch': 1.01}
 51%|█████     | 470/930 [07:50<16:53,  2.20s/it] 51%|█████     | 471/930 [09:24<25:31,  3.34s/it] 51%|█████     | 472/930 [10:55<37:02,  4.85s/it] 51%|█████     | 473/930 [12:28<53:11,  6.98s/it] 51%|█████     | 474/930 [13:38<1:09:01,  9.08s/it] 51%|█████     | 475/930 [15:14<1:38:54, 13.04s/it] 51%|█████     | 476/930 [16:58<2:20:54, 18.62s/it] 51%|█████▏    | 477/930 [18:38<3:09:50, 25.14s/it] 51%|█████▏    | 478/930 [20:14<4:03:41, 32.35s/it] 52%|█████▏    | 479/930 [21:34<4:48:46, 38.42s/it] 52%|█████▏    | 480/930 [23:08<5:53:03, 47.07s/it]                                                   {'loss': 0.7245, 'grad_norm': 0.4173602759838104, 'learning_rate': 5.6084153515520134e-05, 'epoch': 1.03}
 52%|█████▏    | 480/930 [23:08<5:53:03, 47.07s/it] 52%|█████▏    | 481/930 [24:23<6:30:41, 52.21s/it] 52%|█████▏    | 482/930 [25:59<7:35:54, 61.06s/it] 52%|█████▏    | 483/930 [27:22<8:13:02, 66.18s/it] 52%|█████▏    | 484/930 [28:41<8:35:18, 69.32s/it] 52%|█████▏    | 485/930 [30:10<9:11:40, 74.38s/it] 52%|█████▏    | 486/930 [31:47<9:54:12, 80.30s/it] 52%|█████▏    | 487/930 [33:27<10:33:22, 85.79s/it] 52%|█████▏    | 488/930 [34:55<10:37:46, 86.58s/it] 53%|█████▎    | 489/930 [36:26<10:46:27, 87.95s/it] 53%|█████▎    | 490/930 [37:54<10:43:55, 87.81s/it]                                                    {'loss': 0.7566, 'grad_norm': 0.4097922146320343, 'learning_rate': 5.4217553302152237e-05, 'epoch': 1.05}
 53%|█████▎    | 490/930 [37:54<10:43:55, 87.81s/it] 53%|█████▎    | 491/930 [39:19<10:36:28, 86.99s/it] 53%|█████▎    | 492/930 [40:48<10:39:54, 87.66s/it] 53%|█████▎    | 493/930 [42:24<10:55:35, 90.01s/it] 53%|█████▎    | 494/930 [44:02<11:11:56, 92.47s/it] 53%|█████▎    | 495/930 [45:31<11:02:19, 91.36s/it] 53%|█████▎    | 496/930 [47:05<11:06:13, 92.11s/it] 53%|█████▎    | 497/930 [48:40<11:11:22, 93.03s/it] 54%|█████▎    | 498/930 [50:08<10:58:43, 91.49s/it] 54%|█████▎    | 499/930 [51:52<11:24:57, 95.35s/it] 54%|█████▍    | 500/930 [53:20<11:07:09, 93.09s/it]                                                    {'loss': 0.7424, 'grad_norm': 0.40367022156715393, 'learning_rate': 5.23450121042383e-05, 'epoch': 1.08}
 54%|█████▍    | 500/930 [53:20<11:07:09, 93.09s/it]
***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1
[INFO|trainer.py:4643] 2025-12-01 06:37:54,118 >> 
***** Running Evaluation *****
[INFO|trainer.py:4645] 2025-12-01 06:37:54,118 >>   Num examples = 3305
[INFO|trainer.py:4648] 2025-12-01 06:37:54,118 >>   Batch size = 1

  0%|          | 0/827 [00:00<?, ?it/s][A
  0%|          | 2/827 [00:03<27:04,  1.97s/it][A
  0%|          | 3/827 [00:06<32:10,  2.34s/it][A
  0%|          | 4/827 [00:08<31:16,  2.28s/it][A
  1%|          | 5/827 [00:12<36:55,  2.69s/it][A
  1%|          | 6/827 [00:15<37:51,  2.77s/it][A
  1%|          | 7/827 [00:18<39:45,  2.91s/it][A
  1%|          | 8/827 [00:21<40:22,  2.96s/it][A
  1%|          | 9/827 [00:27<50:38,  3.71s/it][A
  1%|          | 10/827 [00:33<1:01:35,  4.52s/it][A
  1%|▏         | 11/827 [00:36<54:20,  4.00s/it]  [A
  1%|▏         | 12/827 [00:39<49:38,  3.65s/it][A
  2%|▏         | 13/827 [00:41<44:40,  3.29s/it][A
  2%|▏         | 14/827 [00:44<42:11,  3.11s/it][A
  2%|▏         | 15/827 [00:47<43:17,  3.20s/it][A
  2%|▏         | 16/827 [00:50<42:20,  3.13s/it][A
  2%|▏         | 17/827 [00:53<39:55,  2.96s/it][A
  2%|▏         | 18/827 [00:55<38:59,  2.89s/it][A
  2%|▏         | 19/827 [00:58<38:24,  2.85s/it][A
  2%|▏         | 20/827 [01:03<44:43,  3.33s/it][A
  3%|▎         | 21/827 [01:06<43:53,  3.27s/it][A
  3%|▎         | 22/827 [01:10<48:39,  3.63s/it][A
  3%|▎         | 23/827 [01:12<43:10,  3.22s/it][A
  3%|▎         | 24/827 [01:15<41:07,  3.07s/it][A
  3%|▎         | 25/827 [01:19<43:45,  3.27s/it][A
  3%|▎         | 26/827 [01:24<51:48,  3.88s/it][A
  3%|▎         | 27/827 [01:28<53:09,  3.99s/it][A
  3%|▎         | 28/827 [01:31<46:42,  3.51s/it][A
  4%|▎         | 29/827 [01:34<43:30,  3.27s/it][A
  4%|▎         | 30/827 [01:38<48:19,  3.64s/it][A
  4%|▎         | 31/827 [01:42<49:11,  3.71s/it][A
  4%|▍         | 32/827 [01:44<44:34,  3.36s/it][A
  4%|▍         | 33/827 [01:49<51:07,  3.86s/it][A
  4%|▍         | 34/827 [01:52<47:05,  3.56s/it][A
  4%|▍         | 35/827 [01:56<46:23,  3.51s/it][A
  4%|▍         | 36/827 [01:58<42:51,  3.25s/it][A
  4%|▍         | 37/827 [02:01<39:10,  2.97s/it][A
  5%|▍         | 38/827 [02:06<47:43,  3.63s/it][A
  5%|▍         | 39/827 [02:10<48:35,  3.70s/it][A
  5%|▍         | 40/827 [02:15<53:32,  4.08s/it][A
  5%|▍         | 41/827 [02:17<47:17,  3.61s/it][A
  5%|▌         | 42/827 [02:21<46:17,  3.54s/it][A
  5%|▌         | 43/827 [02:24<44:48,  3.43s/it][A
  5%|▌         | 44/827 [02:26<41:12,  3.16s/it][A
  5%|▌         | 45/827 [02:30<41:23,  3.18s/it][A
  6%|▌         | 46/827 [02:34<46:08,  3.54s/it][A
  6%|▌         | 47/827 [02:38<50:01,  3.85s/it][A
  6%|▌         | 48/827 [02:41<45:28,  3.50s/it][A
  6%|▌         | 49/827 [02:44<42:50,  3.30s/it][A
  6%|▌         | 50/827 [02:47<40:01,  3.09s/it][A
  6%|▌         | 51/827 [02:49<38:09,  2.95s/it][A
  6%|▋         | 52/827 [02:55<48:08,  3.73s/it][A
  6%|▋         | 53/827 [02:57<43:56,  3.41s/it][A
  7%|▋         | 54/827 [03:00<39:19,  3.05s/it][A
  7%|▋         | 55/827 [03:03<39:12,  3.05s/it][A
  7%|▋         | 56/827 [03:05<35:01,  2.73s/it][A
  7%|▋         | 57/827 [03:08<38:11,  2.98s/it][A
  7%|▋         | 58/827 [03:11<36:44,  2.87s/it][A
  7%|▋         | 59/827 [03:14<37:37,  2.94s/it][A
  7%|▋         | 60/827 [03:16<34:52,  2.73s/it][A
  7%|▋         | 61/827 [03:21<41:36,  3.26s/it][A
  7%|▋         | 62/827 [03:23<38:22,  3.01s/it][A
  8%|▊         | 63/827 [03:26<36:38,  2.88s/it][A
  8%|▊         | 64/827 [03:28<35:28,  2.79s/it][A
  8%|▊         | 65/827 [03:31<35:30,  2.80s/it][A
  8%|▊         | 66/827 [03:34<37:39,  2.97s/it][A
  8%|▊         | 67/827 [03:39<43:43,  3.45s/it][A
  8%|▊         | 68/827 [03:41<38:47,  3.07s/it][A
  8%|▊         | 69/827 [03:44<37:07,  2.94s/it][A
  8%|▊         | 70/827 [03:46<36:00,  2.85s/it][A
  9%|▊         | 71/827 [03:49<35:02,  2.78s/it][A
  9%|▊         | 72/827 [03:54<41:37,  3.31s/it][A
  9%|▉         | 73/827 [03:57<40:10,  3.20s/it][A
  9%|▉         | 74/827 [04:00<40:43,  3.24s/it][A
  9%|▉         | 75/827 [04:05<46:02,  3.67s/it][A
  9%|▉         | 76/827 [04:07<41:09,  3.29s/it][A
  9%|▉         | 77/827 [04:10<39:32,  3.16s/it][A
  9%|▉         | 78/827 [04:13<38:53,  3.12s/it][A
 10%|▉         | 79/827 [04:16<37:22,  3.00s/it][A
 10%|▉         | 80/827 [04:18<35:03,  2.82s/it][A
 10%|▉         | 81/827 [04:22<41:03,  3.30s/it][A
 10%|▉         | 82/827 [04:26<41:29,  3.34s/it][A
 10%|█         | 83/827 [04:29<39:26,  3.18s/it][A
 10%|█         | 84/827 [04:32<41:06,  3.32s/it][A
 10%|█         | 85/827 [04:36<42:46,  3.46s/it][A
 10%|█         | 86/827 [04:39<39:45,  3.22s/it][A
 11%|█         | 87/827 [04:41<36:11,  2.93s/it][A
 11%|█         | 88/827 [04:48<51:58,  4.22s/it][A
 11%|█         | 89/827 [04:54<56:23,  4.58s/it][A
 11%|█         | 90/827 [04:59<58:45,  4.78s/it][A
 11%|█         | 91/827 [05:02<51:15,  4.18s/it][A
 11%|█         | 92/827 [05:05<47:58,  3.92s/it][A
 11%|█         | 93/827 [05:08<43:38,  3.57s/it][A
 11%|█▏        | 94/827 [05:10<40:16,  3.30s/it][A
 11%|█▏        | 95/827 [05:13<37:26,  3.07s/it][A
 12%|█▏        | 96/827 [05:16<37:10,  3.05s/it][A
 12%|█▏        | 97/827 [05:18<34:25,  2.83s/it][A
 12%|█▏        | 98/827 [05:20<32:08,  2.65s/it][A
 12%|█▏        | 99/827 [05:23<31:45,  2.62s/it][A
 12%|█▏        | 100/827 [05:25<30:22,  2.51s/it][A
 12%|█▏        | 101/827 [05:28<31:29,  2.60s/it][A
 12%|█▏        | 102/827 [05:31<32:37,  2.70s/it][A
 12%|█▏        | 103/827 [05:34<32:33,  2.70s/it][A
 13%|█▎        | 104/827 [05:36<32:28,  2.69s/it][A
 13%|█▎        | 105/827 [05:39<32:55,  2.74s/it][A
 13%|█▎        | 106/827 [05:42<33:39,  2.80s/it][A
 13%|█▎        | 107/827 [05:48<45:55,  3.83s/it][A
 13%|█▎        | 108/827 [05:52<43:25,  3.62s/it][A
 13%|█▎        | 109/827 [05:55<43:31,  3.64s/it][A
 13%|█▎        | 110/827 [05:58<40:34,  3.39s/it][A
 13%|█▎        | 111/827 [06:01<39:03,  3.27s/it][A
 14%|█▎        | 112/827 [06:06<43:59,  3.69s/it][A
 14%|█▎        | 113/827 [06:09<42:26,  3.57s/it][A
 14%|█▍        | 114/827 [06:12<39:44,  3.34s/it][A
 14%|█▍        | 115/827 [06:14<37:11,  3.13s/it][A
 14%|█▍        | 116/827 [06:18<37:15,  3.14s/it][A
 14%|█▍        | 117/827 [06:20<35:49,  3.03s/it][A
 14%|█▍        | 118/827 [06:25<40:38,  3.44s/it][A
 14%|█▍        | 119/827 [06:29<43:42,  3.70s/it][A
 15%|█▍        | 120/827 [06:34<46:04,  3.91s/it][A
 15%|█▍        | 121/827 [06:36<42:44,  3.63s/it][A
 15%|█▍        | 122/827 [06:39<40:17,  3.43s/it][A
 15%|█▍        | 123/827 [06:42<35:48,  3.05s/it][A
 15%|█▍        | 124/827 [06:44<34:43,  2.96s/it][A
 15%|█▌        | 125/827 [06:47<34:52,  2.98s/it][A
 15%|█▌        | 126/827 [06:50<34:46,  2.98s/it][A
 15%|█▌        | 127/827 [06:53<34:32,  2.96s/it][A
 15%|█▌        | 128/827 [06:57<35:49,  3.08s/it][A
 16%|█▌        | 129/827 [06:59<34:28,  2.96s/it][A
 16%|█▌        | 130/827 [07:02<35:01,  3.02s/it][A
 16%|█▌        | 131/827 [07:05<33:48,  2.91s/it][A
 16%|█▌        | 132/827 [07:09<36:43,  3.17s/it][A
 16%|█▌        | 133/827 [07:12<37:50,  3.27s/it][A
 16%|█▌        | 134/827 [07:16<38:02,  3.29s/it][A
 16%|█▋        | 135/827 [07:18<35:23,  3.07s/it][A
 16%|█▋        | 136/827 [07:24<43:09,  3.75s/it][A
 17%|█▋        | 137/827 [07:27<40:12,  3.50s/it][A
 17%|█▋        | 138/827 [07:30<39:39,  3.45s/it][A
 17%|█▋        | 139/827 [07:35<46:00,  4.01s/it][A
 17%|█▋        | 140/827 [07:39<44:37,  3.90s/it][A
 17%|█▋        | 141/827 [07:42<43:08,  3.77s/it][A
 17%|█▋        | 142/827 [07:45<39:30,  3.46s/it][A
 17%|█▋        | 143/827 [07:48<37:58,  3.33s/it][A
 17%|█▋        | 144/827 [07:51<34:55,  3.07s/it][A
 18%|█▊        | 145/827 [07:53<33:10,  2.92s/it][A
 18%|█▊        | 146/827 [07:57<34:46,  3.06s/it][A
 18%|█▊        | 147/827 [07:59<32:58,  2.91s/it][A
 18%|█▊        | 148/827 [08:02<32:33,  2.88s/it][A
 18%|█▊        | 149/827 [08:04<31:31,  2.79s/it][A
 18%|█▊        | 150/827 [08:07<31:24,  2.78s/it][A
 18%|█▊        | 151/827 [08:09<29:02,  2.58s/it][A
 18%|█▊        | 152/827 [08:12<29:17,  2.60s/it][A
 19%|█▊        | 153/827 [08:15<30:33,  2.72s/it][A
 19%|█▊        | 154/827 [08:18<32:31,  2.90s/it][A
 19%|█▊        | 155/827 [08:21<33:07,  2.96s/it][A
 19%|█▉        | 156/827 [08:26<37:46,  3.38s/it][A
 19%|█▉        | 157/827 [08:28<34:22,  3.08s/it][A
 19%|█▉        | 158/827 [08:31<34:28,  3.09s/it][A
 19%|█▉        | 159/827 [08:34<32:29,  2.92s/it][A
 19%|█▉        | 160/827 [08:38<37:06,  3.34s/it][A
 19%|█▉        | 161/827 [08:40<33:46,  3.04s/it][A
 20%|█▉        | 162/827 [08:43<31:21,  2.83s/it][A
 20%|█▉        | 163/827 [08:47<34:20,  3.10s/it][A
 20%|█▉        | 164/827 [08:50<34:02,  3.08s/it][A
 20%|█▉        | 165/827 [08:52<32:20,  2.93s/it][A
 20%|██        | 166/827 [08:55<32:30,  2.95s/it][A
 20%|██        | 167/827 [09:01<43:21,  3.94s/it][A
 20%|██        | 168/827 [09:07<48:12,  4.39s/it][A
 20%|██        | 169/827 [09:09<42:16,  3.86s/it][A
 21%|██        | 170/827 [09:12<38:27,  3.51s/it][A
 21%|██        | 171/827 [09:16<38:40,  3.54s/it][A
 21%|██        | 172/827 [09:19<36:37,  3.36s/it][A
 21%|██        | 173/827 [09:21<33:10,  3.04s/it][A
 21%|██        | 174/827 [09:24<31:55,  2.93s/it][A
 21%|██        | 175/827 [09:27<33:51,  3.12s/it][A
 21%|██▏       | 176/827 [09:30<31:13,  2.88s/it][A
 21%|██▏       | 177/827 [09:32<30:38,  2.83s/it][A
 22%|██▏       | 178/827 [09:34<28:02,  2.59s/it][A
 22%|██▏       | 179/827 [09:39<34:45,  3.22s/it][A
 22%|██▏       | 180/827 [09:42<34:46,  3.22s/it][A
 22%|██▏       | 181/827 [09:45<32:58,  3.06s/it][A
 22%|██▏       | 182/827 [09:49<35:48,  3.33s/it][A
 22%|██▏       | 183/827 [09:52<34:07,  3.18s/it][A
 22%|██▏       | 184/827 [09:54<32:16,  3.01s/it][A
 22%|██▏       | 185/827 [09:58<35:39,  3.33s/it][A
 22%|██▏       | 186/827 [10:01<33:53,  3.17s/it][A
 23%|██▎       | 187/827 [10:04<31:20,  2.94s/it][A
 23%|██▎       | 188/827 [10:06<30:58,  2.91s/it][A
 23%|██▎       | 189/827 [10:10<34:15,  3.22s/it][A
 23%|██▎       | 190/827 [10:13<33:18,  3.14s/it][A
 23%|██▎       | 191/827 [10:17<34:04,  3.21s/it][A
 23%|██▎       | 192/827 [10:20<32:54,  3.11s/it][A
 23%|██▎       | 193/827 [10:24<35:58,  3.40s/it][A
 23%|██▎       | 194/827 [10:27<35:11,  3.34s/it][A
 24%|██▎       | 195/827 [10:30<33:45,  3.20s/it][A
 24%|██▎       | 196/827 [10:32<30:49,  2.93s/it][A
 24%|██▍       | 197/827 [10:35<29:58,  2.85s/it][A
 24%|██▍       | 198/827 [10:37<27:51,  2.66s/it][A
 24%|██▍       | 199/827 [10:40<30:19,  2.90s/it][A
 24%|██▍       | 200/827 [10:44<31:29,  3.01s/it][A
 24%|██▍       | 201/827 [10:48<35:49,  3.43s/it][A
 24%|██▍       | 202/827 [10:52<36:18,  3.49s/it][A
 25%|██▍       | 203/827 [10:56<37:52,  3.64s/it][A
 25%|██▍       | 204/827 [11:01<42:46,  4.12s/it][A
 25%|██▍       | 205/827 [11:03<37:47,  3.65s/it][A
 25%|██▍       | 206/827 [11:07<36:29,  3.53s/it][A
 25%|██▌       | 207/827 [11:10<34:40,  3.36s/it][A
 25%|██▌       | 208/827 [11:12<32:59,  3.20s/it][A
 25%|██▌       | 209/827 [11:16<32:34,  3.16s/it][A
 25%|██▌       | 210/827 [11:18<30:15,  2.94s/it][A
 26%|██▌       | 211/827 [11:21<30:45,  3.00s/it][A
 26%|██▌       | 212/827 [11:25<34:16,  3.34s/it][A
 26%|██▌       | 213/827 [11:30<38:39,  3.78s/it][A
 26%|██▌       | 214/827 [11:33<35:37,  3.49s/it][A
 26%|██▌       | 215/827 [11:36<33:07,  3.25s/it][A
 26%|██▌       | 216/827 [11:38<31:03,  3.05s/it][A
 26%|██▌       | 217/827 [11:42<32:55,  3.24s/it][A
 26%|██▋       | 218/827 [11:45<34:10,  3.37s/it][A
 26%|██▋       | 219/827 [11:48<32:54,  3.25s/it][A
 27%|██▋       | 220/827 [11:51<30:30,  3.02s/it][A
 27%|██▋       | 221/827 [11:55<35:06,  3.48s/it][A
 27%|██▋       | 222/827 [11:58<33:43,  3.34s/it][A
 27%|██▋       | 223/827 [12:02<33:08,  3.29s/it][A
 27%|██▋       | 224/827 [12:04<31:08,  3.10s/it][A
 27%|██▋       | 225/827 [12:07<31:17,  3.12s/it][A
 27%|██▋       | 226/827 [12:10<28:47,  2.87s/it][A
 27%|██▋       | 227/827 [12:12<26:57,  2.70s/it][A
 28%|██▊       | 228/827 [12:17<32:39,  3.27s/it][A
 28%|██▊       | 229/827 [12:19<30:16,  3.04s/it][A
 28%|██▊       | 230/827 [12:23<33:07,  3.33s/it][A
 28%|██▊       | 231/827 [12:26<32:34,  3.28s/it][A
 28%|██▊       | 232/827 [12:29<30:21,  3.06s/it][A
 28%|██▊       | 233/827 [12:32<29:21,  2.97s/it][A
 28%|██▊       | 234/827 [12:36<33:44,  3.41s/it][A
 28%|██▊       | 235/827 [12:41<38:49,  3.94s/it][A
 29%|██▊       | 236/827 [12:44<35:57,  3.65s/it][A
 29%|██▊       | 237/827 [12:47<34:34,  3.52s/it][A
 29%|██▉       | 238/827 [12:53<40:11,  4.09s/it][A
 29%|██▉       | 239/827 [12:56<36:05,  3.68s/it][A
 29%|██▉       | 240/827 [13:01<41:28,  4.24s/it][A
 29%|██▉       | 241/827 [13:05<41:02,  4.20s/it][A
 29%|██▉       | 242/827 [13:09<39:30,  4.05s/it][A
 29%|██▉       | 243/827 [13:12<37:03,  3.81s/it][A
 30%|██▉       | 244/827 [13:17<41:00,  4.22s/it][A
 30%|██▉       | 245/827 [13:20<37:03,  3.82s/it][A
 30%|██▉       | 246/827 [13:24<36:13,  3.74s/it][A
 30%|██▉       | 247/827 [13:26<31:53,  3.30s/it][A
 30%|██▉       | 248/827 [13:31<35:32,  3.68s/it][A
 30%|███       | 249/827 [13:35<37:54,  3.93s/it][A
 30%|███       | 250/827 [13:37<33:06,  3.44s/it][A
 30%|███       | 251/827 [13:40<29:06,  3.03s/it][A
 30%|███       | 252/827 [13:44<33:28,  3.49s/it][A
 31%|███       | 253/827 [13:47<30:49,  3.22s/it][A
 31%|███       | 254/827 [13:49<27:41,  2.90s/it][A
 31%|███       | 255/827 [13:55<37:24,  3.92s/it][A
 31%|███       | 256/827 [13:59<38:00,  3.99s/it][A
 31%|███       | 257/827 [14:03<38:11,  4.02s/it][A
 31%|███       | 258/827 [14:06<34:12,  3.61s/it][A
 31%|███▏      | 259/827 [14:10<36:10,  3.82s/it][A
 31%|███▏      | 260/827 [14:14<36:59,  3.91s/it][A
 32%|███▏      | 261/827 [14:19<37:45,  4.00s/it][A
 32%|███▏      | 262/827 [14:22<34:21,  3.65s/it][A
 32%|███▏      | 263/827 [14:28<41:08,  4.38s/it][A
 32%|███▏      | 264/827 [14:30<35:45,  3.81s/it][A
 32%|███▏      | 265/827 [14:32<31:30,  3.36s/it][A
 32%|███▏      | 266/827 [14:35<29:59,  3.21s/it][A
 32%|███▏      | 267/827 [14:38<27:37,  2.96s/it][A
 32%|███▏      | 268/827 [14:40<27:12,  2.92s/it][A
 33%|███▎      | 269/827 [14:43<27:22,  2.94s/it][A
 33%|███▎      | 270/827 [14:48<32:05,  3.46s/it][A
 33%|███▎      | 271/827 [14:51<29:34,  3.19s/it][A
 33%|███▎      | 272/827 [14:54<29:50,  3.23s/it][A
 33%|███▎      | 273/827 [14:57<28:00,  3.03s/it][A
 33%|███▎      | 274/827 [15:03<37:04,  4.02s/it][A
 33%|███▎      | 275/827 [15:08<39:57,  4.34s/it][A
 33%|███▎      | 276/827 [15:11<36:47,  4.01s/it][A
 33%|███▎      | 277/827 [15:15<35:11,  3.84s/it][A
 34%|███▎      | 278/827 [15:18<34:04,  3.72s/it][A
 34%|███▎      | 279/827 [15:21<31:07,  3.41s/it][A
 34%|███▍      | 280/827 [15:23<29:06,  3.19s/it][A
 34%|███▍      | 281/827 [15:26<27:27,  3.02s/it][A
 34%|███▍      | 282/827 [15:30<28:34,  3.15s/it][A
 34%|███▍      | 283/827 [15:32<27:56,  3.08s/it][A
 34%|███▍      | 284/827 [15:36<29:03,  3.21s/it][A
 34%|███▍      | 285/827 [15:39<28:58,  3.21s/it][A
 35%|███▍      | 286/827 [15:44<33:14,  3.69s/it][A
 35%|███▍      | 287/827 [15:46<29:35,  3.29s/it][A
 35%|███▍      | 288/827 [15:49<28:17,  3.15s/it][A
 35%|███▍      | 289/827 [15:52<27:06,  3.02s/it][A
 35%|███▌      | 290/827 [15:55<27:24,  3.06s/it][A
 35%|███▌      | 291/827 [15:58<26:06,  2.92s/it][A
 35%|███▌      | 292/827 [16:00<25:26,  2.85s/it][A
 35%|███▌      | 293/827 [16:03<25:12,  2.83s/it][A
 36%|███▌      | 294/827 [16:06<25:17,  2.85s/it][A
 36%|███▌      | 295/827 [16:09<26:47,  3.02s/it][A
 36%|███▌      | 296/827 [16:13<27:45,  3.14s/it][A
 36%|███▌      | 297/827 [16:16<28:30,  3.23s/it][A
 36%|███▌      | 298/827 [16:19<27:32,  3.12s/it][A
 36%|███▌      | 299/827 [16:22<26:10,  2.97s/it][A
 36%|███▋      | 300/827 [16:25<25:57,  2.96s/it][A
 36%|███▋      | 301/827 [16:27<23:30,  2.68s/it][A
 37%|███▋      | 302/827 [16:30<24:48,  2.83s/it][A
 37%|███▋      | 303/827 [16:32<23:22,  2.68s/it][A
 37%|███▋      | 304/827 [16:35<22:43,  2.61s/it][A
 37%|███▋      | 305/827 [16:38<23:38,  2.72s/it][A
 37%|███▋      | 306/827 [16:42<27:17,  3.14s/it][A
 37%|███▋      | 307/827 [16:44<25:59,  3.00s/it][A
 37%|███▋      | 308/827 [16:47<24:58,  2.89s/it][A
 37%|███▋      | 309/827 [16:50<25:04,  2.91s/it][A
 37%|███▋      | 310/827 [16:53<24:19,  2.82s/it][A
 38%|███▊      | 311/827 [16:55<22:54,  2.66s/it][A
 38%|███▊      | 312/827 [16:58<23:11,  2.70s/it][A
 38%|███▊      | 313/827 [17:02<27:12,  3.18s/it][A
 38%|███▊      | 314/827 [17:04<24:39,  2.88s/it][A
 38%|███▊      | 315/827 [17:08<26:02,  3.05s/it][A
 38%|███▊      | 316/827 [17:11<26:31,  3.11s/it][A
 38%|███▊      | 317/827 [17:15<29:43,  3.50s/it][A
 38%|███▊      | 318/827 [17:20<32:10,  3.79s/it][A
 39%|███▊      | 319/827 [17:23<30:55,  3.65s/it][A
 39%|███▊      | 320/827 [17:26<28:34,  3.38s/it][A
 39%|███▉      | 321/827 [17:29<28:31,  3.38s/it][A
 39%|███▉      | 322/827 [17:33<29:32,  3.51s/it][A
 39%|███▉      | 323/827 [17:36<27:27,  3.27s/it][A
 39%|███▉      | 324/827 [17:40<30:30,  3.64s/it][A
 39%|███▉      | 325/827 [17:43<28:54,  3.46s/it][A
 39%|███▉      | 326/827 [17:46<26:51,  3.22s/it][A
 40%|███▉      | 327/827 [17:50<28:57,  3.47s/it][A
 40%|███▉      | 328/827 [17:53<26:46,  3.22s/it][A
 40%|███▉      | 329/827 [17:57<29:50,  3.59s/it][A
 40%|███▉      | 330/827 [18:01<31:12,  3.77s/it][A
 40%|████      | 331/827 [18:06<32:58,  3.99s/it][A
 40%|████      | 332/827 [18:08<29:04,  3.52s/it][A
 40%|████      | 333/827 [18:13<31:37,  3.84s/it][A
 40%|████      | 334/827 [18:16<28:51,  3.51s/it][A
 41%|████      | 335/827 [18:19<28:47,  3.51s/it][A
 41%|████      | 336/827 [18:21<25:28,  3.11s/it][A
 41%|████      | 337/827 [18:25<26:17,  3.22s/it][A
 41%|████      | 338/827 [18:27<24:59,  3.07s/it][A
 41%|████      | 339/827 [18:30<24:19,  2.99s/it][A
 41%|████      | 340/827 [18:33<22:46,  2.80s/it][A
 41%|████      | 341/827 [18:36<24:15,  2.99s/it][A
 41%|████▏     | 342/827 [18:39<24:19,  3.01s/it][A
 41%|████▏     | 343/827 [18:42<23:58,  2.97s/it][A
 42%|████▏     | 344/827 [18:45<24:56,  3.10s/it][A
 42%|████▏     | 345/827 [18:50<27:57,  3.48s/it][A
 42%|████▏     | 346/827 [18:53<26:52,  3.35s/it][A
 42%|████▏     | 347/827 [18:56<26:03,  3.26s/it][A
 42%|████▏     | 348/827 [18:59<25:53,  3.24s/it][A
 42%|████▏     | 349/827 [19:02<24:12,  3.04s/it][A
 42%|████▏     | 350/827 [19:06<27:43,  3.49s/it][A
 42%|████▏     | 351/827 [19:09<26:31,  3.34s/it][A
 43%|████▎     | 352/827 [19:13<26:42,  3.37s/it][A
 43%|████▎     | 353/827 [19:15<25:11,  3.19s/it][A
 43%|████▎     | 354/827 [19:18<24:10,  3.07s/it][A
 43%|████▎     | 355/827 [19:21<24:14,  3.08s/it][A
 43%|████▎     | 356/827 [19:26<27:43,  3.53s/it][A
 43%|████▎     | 357/827 [19:29<27:42,  3.54s/it][A
 43%|████▎     | 358/827 [19:32<25:35,  3.27s/it][A
 43%|████▎     | 359/827 [19:35<23:49,  3.05s/it][A
 44%|████▎     | 360/827 [19:38<24:03,  3.09s/it][A
 44%|████▎     | 361/827 [19:42<26:23,  3.40s/it][A
 44%|████▍     | 362/827 [19:44<23:18,  3.01s/it][A
 44%|████▍     | 363/827 [19:47<22:54,  2.96s/it][A
 44%|████▍     | 364/827 [19:51<24:27,  3.17s/it][A
 44%|████▍     | 365/827 [19:53<23:12,  3.01s/it][A
 44%|████▍     | 366/827 [19:57<24:40,  3.21s/it][A
 44%|████▍     | 367/827 [20:01<27:15,  3.55s/it][A
 44%|████▍     | 368/827 [20:04<24:59,  3.27s/it][A
 45%|████▍     | 369/827 [20:07<23:50,  3.12s/it][A
 45%|████▍     | 370/827 [20:09<23:07,  3.04s/it][A
 45%|████▍     | 371/827 [20:13<24:31,  3.23s/it][A
 45%|████▍     | 372/827 [20:16<23:57,  3.16s/it][A
 45%|████▌     | 373/827 [20:19<23:07,  3.06s/it][A
 45%|████▌     | 374/827 [20:24<26:56,  3.57s/it][A
 45%|████▌     | 375/827 [20:28<28:55,  3.84s/it][A
 45%|████▌     | 376/827 [20:35<36:10,  4.81s/it][A
 46%|████▌     | 377/827 [20:39<34:25,  4.59s/it][A
 46%|████▌     | 378/827 [20:42<30:11,  4.04s/it][A
 46%|████▌     | 379/827 [20:49<35:39,  4.78s/it][A
 46%|████▌     | 380/827 [20:54<36:39,  4.92s/it][A
 46%|████▌     | 381/827 [20:57<32:50,  4.42s/it][A
 46%|████▌     | 382/827 [21:00<29:32,  3.98s/it][A
 46%|████▋     | 383/827 [21:04<30:14,  4.09s/it][A
 46%|████▋     | 384/827 [21:07<27:37,  3.74s/it][A
 47%|████▋     | 385/827 [21:10<25:38,  3.48s/it][A
 47%|████▋     | 386/827 [21:13<24:09,  3.29s/it][A
 47%|████▋     | 387/827 [21:16<23:33,  3.21s/it][A
 47%|████▋     | 388/827 [21:19<23:01,  3.15s/it][A
 47%|████▋     | 389/827 [21:23<24:09,  3.31s/it][A
 47%|████▋     | 390/827 [21:26<23:35,  3.24s/it][A
 47%|████▋     | 391/827 [21:28<22:22,  3.08s/it][A
 47%|████▋     | 392/827 [21:31<21:16,  2.93s/it][A
 48%|████▊     | 393/827 [21:35<23:42,  3.28s/it][A
 48%|████▊     | 394/827 [21:39<24:02,  3.33s/it][A
 48%|████▊     | 395/827 [21:43<25:38,  3.56s/it][A
 48%|████▊     | 396/827 [21:47<27:26,  3.82s/it][A
 48%|████▊     | 397/827 [21:49<24:15,  3.38s/it][A
 48%|████▊     | 398/827 [21:53<23:32,  3.29s/it][A
 48%|████▊     | 399/827 [21:56<23:12,  3.25s/it][A
 48%|████▊     | 400/827 [21:59<22:19,  3.14s/it][A
 48%|████▊     | 401/827 [22:03<24:14,  3.41s/it][A
 49%|████▊     | 402/827 [22:06<22:59,  3.25s/it][A
 49%|████▊     | 403/827 [22:10<25:34,  3.62s/it][A
 49%|████▉     | 404/827 [22:12<23:00,  3.26s/it][A
 49%|████▉     | 405/827 [22:15<21:22,  3.04s/it][A
 49%|████▉     | 406/827 [22:18<22:09,  3.16s/it][A
 49%|████▉     | 407/827 [22:21<21:01,  3.00s/it][A
 49%|████▉     | 408/827 [22:24<20:09,  2.89s/it][A
 49%|████▉     | 409/827 [22:27<20:36,  2.96s/it][A
 50%|████▉     | 410/827 [22:29<19:22,  2.79s/it][A
 50%|████▉     | 411/827 [22:32<19:10,  2.77s/it][A
 50%|████▉     | 412/827 [22:35<19:08,  2.77s/it][A
 50%|████▉     | 413/827 [22:38<20:33,  2.98s/it][A
 50%|█████     | 414/827 [22:43<23:35,  3.43s/it][A
 50%|█████     | 415/827 [22:45<22:15,  3.24s/it][A
 50%|█████     | 416/827 [22:48<20:09,  2.94s/it][A
 50%|█████     | 417/827 [22:53<24:06,  3.53s/it][A
 51%|█████     | 418/827 [22:57<26:33,  3.90s/it][A
 51%|█████     | 419/827 [23:00<23:50,  3.51s/it][A
 51%|█████     | 420/827 [23:04<25:30,  3.76s/it][A
 51%|█████     | 421/827 [23:08<25:29,  3.77s/it][A
 51%|█████     | 422/827 [23:11<22:58,  3.40s/it][A
 51%|█████     | 423/827 [23:13<21:10,  3.15s/it][A
 51%|█████▏    | 424/827 [23:16<20:12,  3.01s/it][A
 51%|█████▏    | 425/827 [23:19<20:06,  3.00s/it][A
 52%|█████▏    | 426/827 [23:23<23:23,  3.50s/it][A
 52%|█████▏    | 427/827 [23:26<21:57,  3.29s/it][A
 52%|█████▏    | 428/827 [23:31<24:44,  3.72s/it][A
 52%|█████▏    | 429/827 [23:35<24:29,  3.69s/it][A
 52%|█████▏    | 430/827 [23:37<22:38,  3.42s/it][A
 52%|█████▏    | 431/827 [23:41<23:54,  3.62s/it][A
 52%|█████▏    | 432/827 [23:46<25:37,  3.89s/it][A
 52%|█████▏    | 433/827 [23:52<30:06,  4.58s/it][A
 52%|█████▏    | 434/827 [23:57<30:26,  4.65s/it][A
 53%|█████▎    | 435/827 [23:59<25:59,  3.98s/it][A
 53%|█████▎    | 436/827 [24:02<23:59,  3.68s/it][A
 53%|█████▎    | 437/827 [24:06<23:50,  3.67s/it][A
 53%|█████▎    | 438/827 [24:09<22:12,  3.43s/it][A
 53%|█████▎    | 439/827 [24:12<20:49,  3.22s/it][A
 53%|█████▎    | 440/827 [24:16<21:59,  3.41s/it][A
 53%|█████▎    | 441/827 [24:18<21:03,  3.27s/it][A
 53%|█████▎    | 442/827 [24:21<18:41,  2.91s/it][A
 54%|█████▎    | 443/827 [24:24<19:16,  3.01s/it][A
 54%|█████▎    | 444/827 [24:26<18:25,  2.89s/it][A
 54%|█████▍    | 445/827 [24:31<21:47,  3.42s/it][A
 54%|█████▍    | 446/827 [24:34<20:55,  3.30s/it][A
 54%|█████▍    | 447/827 [24:37<21:01,  3.32s/it][A
 54%|█████▍    | 448/827 [24:41<20:59,  3.32s/it][A
 54%|█████▍    | 449/827 [24:46<24:45,  3.93s/it][A
 54%|█████▍    | 450/827 [24:51<25:58,  4.13s/it][A
 55%|█████▍    | 451/827 [24:54<23:30,  3.75s/it][A
 55%|█████▍    | 452/827 [24:57<21:56,  3.51s/it][A
 55%|█████▍    | 453/827 [25:02<25:03,  4.02s/it][A
 55%|█████▍    | 454/827 [25:05<23:17,  3.75s/it][A
 55%|█████▌    | 455/827 [25:08<21:37,  3.49s/it][A
 55%|█████▌    | 456/827 [25:12<23:48,  3.85s/it][A
 55%|█████▌    | 457/827 [25:16<22:58,  3.73s/it][A
 55%|█████▌    | 458/827 [25:19<21:53,  3.56s/it][A
 56%|█████▌    | 459/827 [25:23<21:51,  3.56s/it][A
 56%|█████▌    | 460/827 [25:26<21:29,  3.51s/it][A
 56%|█████▌    | 461/827 [25:31<24:46,  4.06s/it][A
 56%|█████▌    | 462/827 [25:36<25:55,  4.26s/it][A
 56%|█████▌    | 463/827 [25:39<22:49,  3.76s/it][A
 56%|█████▌    | 464/827 [25:41<20:46,  3.43s/it][A
 56%|█████▌    | 465/827 [25:46<22:32,  3.74s/it][A
 56%|█████▋    | 466/827 [25:49<20:48,  3.46s/it][A
 56%|█████▋    | 467/827 [25:52<21:24,  3.57s/it][A
 57%|█████▋    | 468/827 [25:56<21:01,  3.52s/it][A
 57%|█████▋    | 469/827 [26:00<22:51,  3.83s/it][A
 57%|█████▋    | 470/827 [26:03<20:07,  3.38s/it][A
 57%|█████▋    | 471/827 [26:06<20:14,  3.41s/it][A
 57%|█████▋    | 472/827 [26:11<22:27,  3.79s/it][A
 57%|█████▋    | 473/827 [26:15<22:57,  3.89s/it][A
 57%|█████▋    | 474/827 [26:19<22:29,  3.82s/it][A
 57%|█████▋    | 475/827 [26:22<20:58,  3.58s/it][A
 58%|█████▊    | 476/827 [26:24<18:26,  3.15s/it][A
 58%|█████▊    | 477/827 [26:26<16:47,  2.88s/it][A
 58%|█████▊    | 478/827 [26:28<15:52,  2.73s/it][A
 58%|█████▊    | 479/827 [26:33<18:56,  3.27s/it][A
 58%|█████▊    | 480/827 [26:35<17:26,  3.02s/it][A
 58%|█████▊    | 481/827 [26:40<19:51,  3.44s/it][A
 58%|█████▊    | 482/827 [26:43<18:56,  3.30s/it][A
 58%|█████▊    | 483/827 [26:45<17:21,  3.03s/it][A
 59%|█████▊    | 484/827 [26:48<16:29,  2.88s/it][A
 59%|█████▊    | 485/827 [26:50<16:14,  2.85s/it][A
 59%|█████▉    | 486/827 [26:53<16:01,  2.82s/it][A
 59%|█████▉    | 487/827 [26:56<16:28,  2.91s/it][A
 59%|█████▉    | 488/827 [27:00<17:10,  3.04s/it][A
 59%|█████▉    | 489/827 [27:03<18:23,  3.26s/it][A
 59%|█████▉    | 490/827 [27:08<19:57,  3.55s/it][A
 59%|█████▉    | 491/827 [27:10<18:10,  3.25s/it][A
 59%|█████▉    | 492/827 [27:13<17:43,  3.18s/it][A
 60%|█████▉    | 493/827 [27:16<16:33,  2.97s/it][A
 60%|█████▉    | 494/827 [27:19<16:30,  2.98s/it][A
 60%|█████▉    | 495/827 [27:22<17:35,  3.18s/it][A
 60%|█████▉    | 496/827 [27:26<18:02,  3.27s/it][A
 60%|██████    | 497/827 [27:29<16:58,  3.09s/it][A
 60%|██████    | 498/827 [27:31<16:30,  3.01s/it][A
 60%|██████    | 499/827 [27:35<17:00,  3.11s/it][A
 60%|██████    | 500/827 [27:38<17:07,  3.14s/it][A
 61%|██████    | 501/827 [27:40<16:02,  2.95s/it][A
 61%|██████    | 502/827 [27:43<14:56,  2.76s/it][A
 61%|██████    | 503/827 [27:47<17:47,  3.30s/it][A
 61%|██████    | 504/827 [27:50<16:47,  3.12s/it][A
 61%|██████    | 505/827 [27:53<16:20,  3.04s/it][A
 61%|██████    | 506/827 [27:56<16:00,  2.99s/it][A
 61%|██████▏   | 507/827 [27:59<16:45,  3.14s/it][A
 61%|██████▏   | 508/827 [28:06<21:42,  4.08s/it][A
 62%|██████▏   | 509/827 [28:08<18:56,  3.57s/it][A
 62%|██████▏   | 510/827 [28:11<18:00,  3.41s/it][A
 62%|██████▏   | 511/827 [28:14<17:09,  3.26s/it][A
 62%|██████▏   | 512/827 [28:16<15:44,  3.00s/it][A
 62%|██████▏   | 513/827 [28:19<15:09,  2.90s/it][A
 62%|██████▏   | 514/827 [28:22<15:56,  3.06s/it][A
 62%|██████▏   | 515/827 [28:25<14:42,  2.83s/it][A
 62%|██████▏   | 516/827 [28:28<15:16,  2.95s/it][A
 63%|██████▎   | 517/827 [28:31<15:17,  2.96s/it][A
 63%|██████▎   | 518/827 [28:33<14:30,  2.82s/it][A
 63%|██████▎   | 519/827 [28:36<14:37,  2.85s/it][A
 63%|██████▎   | 520/827 [28:38<13:38,  2.67s/it][A
 63%|██████▎   | 521/827 [28:42<14:56,  2.93s/it][A
 63%|██████▎   | 522/827 [28:46<17:11,  3.38s/it][A
 63%|██████▎   | 523/827 [28:49<16:21,  3.23s/it][A
 63%|██████▎   | 524/827 [28:52<16:13,  3.21s/it][A
 63%|██████▎   | 525/827 [28:56<16:18,  3.24s/it][A
 64%|██████▎   | 526/827 [29:01<19:18,  3.85s/it][A
 64%|██████▎   | 527/827 [29:04<17:47,  3.56s/it][A
 64%|██████▍   | 528/827 [29:08<18:05,  3.63s/it][A
 64%|██████▍   | 529/827 [29:11<17:09,  3.46s/it][A
 64%|██████▍   | 530/827 [29:15<18:25,  3.72s/it][A
 64%|██████▍   | 531/827 [29:18<16:22,  3.32s/it][A
 64%|██████▍   | 532/827 [29:21<16:23,  3.33s/it][A
 64%|██████▍   | 533/827 [29:24<15:49,  3.23s/it][A
 65%|██████▍   | 534/827 [29:28<17:37,  3.61s/it][A
 65%|██████▍   | 535/827 [29:33<19:11,  3.94s/it][A
 65%|██████▍   | 536/827 [29:36<18:10,  3.75s/it][A
 65%|██████▍   | 537/827 [29:39<16:32,  3.42s/it][A
 65%|██████▌   | 538/827 [29:42<15:10,  3.15s/it][A
 65%|██████▌   | 539/827 [29:44<14:29,  3.02s/it][A
 65%|██████▌   | 540/827 [29:47<14:22,  3.01s/it][A
 65%|██████▌   | 541/827 [29:50<14:32,  3.05s/it][A
 66%|██████▌   | 542/827 [29:54<14:45,  3.11s/it][A
 66%|██████▌   | 543/827 [29:57<15:15,  3.22s/it][A
 66%|██████▌   | 544/827 [30:00<15:21,  3.26s/it][A
 66%|██████▌   | 545/827 [30:04<16:03,  3.42s/it][A
 66%|██████▌   | 546/827 [30:08<15:53,  3.39s/it][A
 66%|██████▌   | 547/827 [30:12<16:47,  3.60s/it][A
 66%|██████▋   | 548/827 [30:14<15:18,  3.29s/it][A
 66%|██████▋   | 549/827 [30:16<13:43,  2.96s/it][A
 67%|██████▋   | 550/827 [30:20<14:18,  3.10s/it][A
 67%|██████▋   | 551/827 [30:23<14:44,  3.21s/it][A
 67%|██████▋   | 552/827 [30:26<14:37,  3.19s/it][A
 67%|██████▋   | 553/827 [30:30<14:54,  3.27s/it][A
 67%|██████▋   | 554/827 [30:32<13:19,  2.93s/it][A
 67%|██████▋   | 555/827 [30:35<13:03,  2.88s/it][A
 67%|██████▋   | 556/827 [30:38<13:13,  2.93s/it][A
 67%|██████▋   | 557/827 [30:40<12:40,  2.82s/it][A
 67%|██████▋   | 558/827 [30:43<12:06,  2.70s/it][A
 68%|██████▊   | 559/827 [30:47<13:29,  3.02s/it][A
 68%|██████▊   | 560/827 [30:50<13:23,  3.01s/it][A
 68%|██████▊   | 561/827 [30:53<13:16,  3.00s/it][A
 68%|██████▊   | 562/827 [30:56<13:48,  3.13s/it][A
 68%|██████▊   | 563/827 [30:59<13:10,  2.99s/it][A
 68%|██████▊   | 564/827 [31:02<13:08,  3.00s/it][A
 68%|██████▊   | 565/827 [31:04<12:36,  2.89s/it][A
 68%|██████▊   | 566/827 [31:09<14:39,  3.37s/it][A
 69%|██████▊   | 567/827 [31:11<13:27,  3.11s/it][A
 69%|██████▊   | 568/827 [31:15<13:42,  3.18s/it][A
 69%|██████▉   | 569/827 [31:17<13:09,  3.06s/it][A
 69%|██████▉   | 570/827 [31:21<13:17,  3.10s/it][A
 69%|██████▉   | 571/827 [31:25<14:39,  3.44s/it][A
 69%|██████▉   | 572/827 [31:29<15:54,  3.74s/it][A
 69%|██████▉   | 573/827 [31:34<16:52,  3.99s/it][A
 69%|██████▉   | 574/827 [31:37<15:49,  3.75s/it][A
 70%|██████▉   | 575/827 [31:41<16:15,  3.87s/it][A
 70%|██████▉   | 576/827 [31:45<15:28,  3.70s/it][A
 70%|██████▉   | 577/827 [31:47<14:16,  3.43s/it][A
 70%|██████▉   | 578/827 [31:51<14:06,  3.40s/it][A
 70%|███████   | 579/827 [31:53<13:08,  3.18s/it][A
 70%|███████   | 580/827 [31:56<12:23,  3.01s/it][A
 70%|███████   | 581/827 [32:01<15:13,  3.71s/it][A
 70%|███████   | 582/827 [32:05<15:14,  3.73s/it][A
 70%|███████   | 583/827 [32:08<14:00,  3.44s/it][A
 71%|███████   | 584/827 [32:11<13:16,  3.28s/it][A
 71%|███████   | 585/827 [32:13<12:27,  3.09s/it][A
 71%|███████   | 586/827 [32:16<11:57,  2.98s/it][A
 71%|███████   | 587/827 [32:20<12:27,  3.12s/it][A
 71%|███████   | 588/827 [32:27<17:10,  4.31s/it][A
 71%|███████   | 589/827 [32:30<16:18,  4.11s/it][A
 71%|███████▏  | 590/827 [32:34<15:49,  4.01s/it][A
 71%|███████▏  | 591/827 [32:40<17:52,  4.55s/it][A
 72%|███████▏  | 592/827 [32:43<16:08,  4.12s/it][A
 72%|███████▏  | 593/827 [32:45<14:11,  3.64s/it][A
 72%|███████▏  | 594/827 [32:49<13:30,  3.48s/it][A
 72%|███████▏  | 595/827 [32:51<12:11,  3.16s/it][A
 72%|███████▏  | 596/827 [32:55<13:11,  3.43s/it][A
 72%|███████▏  | 597/827 [33:00<15:21,  4.01s/it][A
 72%|███████▏  | 598/827 [33:03<13:54,  3.65s/it][A
 72%|███████▏  | 599/827 [33:06<12:49,  3.37s/it][A
 73%|███████▎  | 600/827 [33:09<12:11,  3.22s/it][A
 73%|███████▎  | 601/827 [33:13<12:48,  3.40s/it][A
 73%|███████▎  | 602/827 [33:16<13:00,  3.47s/it][A
 73%|███████▎  | 603/827 [33:19<11:50,  3.17s/it][A
 73%|███████▎  | 604/827 [33:22<11:40,  3.14s/it][A
 73%|███████▎  | 605/827 [33:24<11:00,  2.98s/it][A
 73%|███████▎  | 606/827 [33:27<11:03,  3.00s/it][A
 73%|███████▎  | 607/827 [33:31<11:39,  3.18s/it][A
 74%|███████▎  | 608/827 [33:34<11:07,  3.05s/it][A
 74%|███████▎  | 609/827 [33:37<11:10,  3.07s/it][A
 74%|███████▍  | 610/827 [33:43<14:31,  4.01s/it][A
 74%|███████▍  | 611/827 [33:46<13:32,  3.76s/it][A
 74%|███████▍  | 612/827 [33:50<13:03,  3.64s/it][A
 74%|███████▍  | 613/827 [33:53<12:43,  3.57s/it][A
 74%|███████▍  | 614/827 [33:57<12:59,  3.66s/it][A
 74%|███████▍  | 615/827 [34:00<12:04,  3.42s/it][A
 74%|███████▍  | 616/827 [34:03<11:50,  3.37s/it][A
 75%|███████▍  | 617/827 [34:05<10:24,  2.97s/it][A
 75%|███████▍  | 618/827 [34:11<12:59,  3.73s/it][A
 75%|███████▍  | 619/827 [34:14<12:19,  3.56s/it][A
 75%|███████▍  | 620/827 [34:17<11:34,  3.36s/it][A
 75%|███████▌  | 621/827 [34:20<11:01,  3.21s/it][A
 75%|███████▌  | 622/827 [34:23<10:45,  3.15s/it][A
 75%|███████▌  | 623/827 [34:25<10:13,  3.01s/it][A
 75%|███████▌  | 624/827 [34:29<10:42,  3.17s/it][A
 76%|███████▌  | 625/827 [34:32<10:48,  3.21s/it][A
 76%|███████▌  | 626/827 [34:35<10:54,  3.26s/it][A
 76%|███████▌  | 627/827 [34:39<10:52,  3.26s/it][A
 76%|███████▌  | 628/827 [34:41<10:11,  3.07s/it][A
 76%|███████▌  | 629/827 [34:44<09:29,  2.87s/it][A
 76%|███████▌  | 630/827 [34:47<09:32,  2.91s/it][A
 76%|███████▋  | 631/827 [34:50<10:00,  3.07s/it][A
 76%|███████▋  | 632/827 [34:55<11:17,  3.47s/it][A
 77%|███████▋  | 633/827 [34:58<10:59,  3.40s/it][A
 77%|███████▋  | 634/827 [35:04<13:40,  4.25s/it][A
 77%|███████▋  | 635/827 [35:07<12:18,  3.85s/it][A
 77%|███████▋  | 636/827 [35:10<11:09,  3.51s/it][A
 77%|███████▋  | 637/827 [35:13<11:03,  3.49s/it][A
 77%|███████▋  | 638/827 [35:18<11:53,  3.77s/it][A
 77%|███████▋  | 639/827 [35:21<11:03,  3.53s/it][A
 77%|███████▋  | 640/827 [35:23<09:49,  3.15s/it][A
 78%|███████▊  | 641/827 [35:26<09:49,  3.17s/it][A
 78%|███████▊  | 642/827 [35:29<09:57,  3.23s/it][A
 78%|███████▊  | 643/827 [35:32<09:06,  2.97s/it][A
 78%|███████▊  | 644/827 [35:36<10:26,  3.42s/it][A
 78%|███████▊  | 645/827 [35:40<10:21,  3.41s/it][A
 78%|███████▊  | 646/827 [35:43<09:52,  3.27s/it][A
 78%|███████▊  | 647/827 [35:45<09:18,  3.10s/it][A
 78%|███████▊  | 648/827 [35:48<08:54,  2.99s/it][A
 78%|███████▊  | 649/827 [35:51<09:12,  3.10s/it][A
 79%|███████▊  | 650/827 [35:54<08:49,  2.99s/it][A
 79%|███████▊  | 651/827 [35:57<08:46,  2.99s/it][A
 79%|███████▉  | 652/827 [36:00<08:25,  2.89s/it][A
 79%|███████▉  | 653/827 [36:03<08:24,  2.90s/it][A
 79%|███████▉  | 654/827 [36:06<08:40,  3.01s/it][A
 79%|███████▉  | 655/827 [36:11<10:39,  3.72s/it][A
 79%|███████▉  | 656/827 [36:13<09:11,  3.22s/it][A
 79%|███████▉  | 657/827 [36:16<08:51,  3.13s/it][A
 80%|███████▉  | 658/827 [36:19<08:41,  3.09s/it][A
 80%|███████▉  | 659/827 [36:24<10:26,  3.73s/it][A
 80%|███████▉  | 660/827 [36:30<11:33,  4.15s/it][A
 80%|███████▉  | 661/827 [36:34<11:38,  4.21s/it][A
 80%|████████  | 662/827 [36:37<10:51,  3.95s/it][A
 80%|████████  | 663/827 [36:40<09:42,  3.55s/it][A
 80%|████████  | 664/827 [36:44<10:19,  3.80s/it][A
 80%|████████  | 665/827 [36:47<09:28,  3.51s/it][A
 81%|████████  | 666/827 [36:51<09:55,  3.70s/it][A
 81%|████████  | 667/827 [36:54<08:58,  3.37s/it][A
 81%|████████  | 668/827 [36:57<08:57,  3.38s/it][A
 81%|████████  | 669/827 [37:00<08:08,  3.09s/it][A
 81%|████████  | 670/827 [37:02<07:30,  2.87s/it][A
 81%|████████  | 671/827 [37:05<07:19,  2.82s/it][A
 81%|████████▏ | 672/827 [37:08<07:50,  3.03s/it][A
 81%|████████▏ | 673/827 [37:11<07:24,  2.89s/it][A
 81%|████████▏ | 674/827 [37:14<07:28,  2.93s/it][A
 82%|████████▏ | 675/827 [37:17<07:17,  2.88s/it][A
 82%|████████▏ | 676/827 [37:20<07:21,  2.92s/it][A
 82%|████████▏ | 677/827 [37:24<08:31,  3.41s/it][A
 82%|████████▏ | 678/827 [37:27<08:00,  3.22s/it][A
 82%|████████▏ | 679/827 [37:31<08:50,  3.59s/it][A
 82%|████████▏ | 680/827 [37:34<07:59,  3.26s/it][A
 82%|████████▏ | 681/827 [37:37<07:30,  3.08s/it][A
 82%|████████▏ | 682/827 [37:42<09:06,  3.77s/it][A
 83%|████████▎ | 683/827 [37:45<08:17,  3.46s/it][A
 83%|████████▎ | 684/827 [37:47<07:46,  3.27s/it][A
 83%|████████▎ | 685/827 [37:50<06:53,  2.91s/it][A
 83%|████████▎ | 686/827 [37:52<06:45,  2.87s/it][A
 83%|████████▎ | 687/827 [37:58<08:51,  3.80s/it][A
 83%|████████▎ | 688/827 [38:01<08:21,  3.60s/it][A
 83%|████████▎ | 689/827 [38:06<09:03,  3.94s/it][A
 83%|████████▎ | 690/827 [38:09<08:27,  3.70s/it][A
 84%|████████▎ | 691/827 [38:12<07:39,  3.38s/it][A
 84%|████████▎ | 692/827 [38:15<07:36,  3.38s/it][A
 84%|████████▍ | 693/827 [38:18<07:09,  3.20s/it][A
 84%|████████▍ | 694/827 [38:21<06:47,  3.07s/it][A
 84%|████████▍ | 695/827 [38:24<06:39,  3.02s/it][A
 84%|████████▍ | 696/827 [38:28<07:26,  3.41s/it][A
 84%|████████▍ | 697/827 [38:31<06:54,  3.19s/it][A
 84%|████████▍ | 698/827 [38:34<06:38,  3.09s/it][A
 85%|████████▍ | 699/827 [38:36<06:18,  2.96s/it][A
 85%|████████▍ | 700/827 [38:40<06:41,  3.16s/it][A
 85%|████████▍ | 701/827 [38:45<07:58,  3.80s/it][A
 85%|████████▍ | 702/827 [38:48<07:33,  3.63s/it][A
 85%|████████▌ | 703/827 [38:52<07:26,  3.60s/it][A
 85%|████████▌ | 704/827 [38:55<07:02,  3.43s/it][A
 85%|████████▌ | 705/827 [38:57<06:11,  3.04s/it][A
 85%|████████▌ | 706/827 [39:00<05:54,  2.93s/it][A
 85%|████████▌ | 707/827 [39:03<05:43,  2.86s/it][A
 86%|████████▌ | 708/827 [39:06<05:58,  3.01s/it][A
 86%|████████▌ | 709/827 [39:09<06:13,  3.17s/it][A
 86%|████████▌ | 710/827 [39:12<05:47,  2.97s/it][A
 86%|████████▌ | 711/827 [39:15<05:48,  3.00s/it][A
 86%|████████▌ | 712/827 [39:18<05:39,  2.95s/it][A
 86%|████████▌ | 713/827 [39:21<05:29,  2.89s/it][A
 86%|████████▋ | 714/827 [39:24<05:34,  2.96s/it][A
 86%|████████▋ | 715/827 [39:28<06:23,  3.42s/it][A
 87%|████████▋ | 716/827 [39:31<05:50,  3.16s/it][A
 87%|████████▋ | 717/827 [39:35<06:27,  3.52s/it][A
 87%|████████▋ | 718/827 [39:40<07:00,  3.85s/it][A
 87%|████████▋ | 719/827 [39:42<06:12,  3.45s/it][A
 87%|████████▋ | 720/827 [39:45<05:58,  3.35s/it][A
 87%|████████▋ | 721/827 [39:48<05:38,  3.19s/it][A
 87%|████████▋ | 722/827 [39:51<05:27,  3.12s/it][A
 87%|████████▋ | 723/827 [39:54<05:14,  3.02s/it][A
 88%|████████▊ | 724/827 [39:57<04:59,  2.91s/it][A
 88%|████████▊ | 725/827 [39:59<04:35,  2.70s/it][A
 88%|████████▊ | 726/827 [40:04<05:54,  3.51s/it][A
 88%|████████▊ | 727/827 [40:08<05:56,  3.57s/it][A
 88%|████████▊ | 728/827 [40:11<05:38,  3.42s/it][A
 88%|████████▊ | 729/827 [40:13<04:54,  3.00s/it][A
 88%|████████▊ | 730/827 [40:16<04:53,  3.03s/it][A
 88%|████████▊ | 731/827 [40:20<05:21,  3.34s/it][A
 89%|████████▊ | 732/827 [40:25<05:51,  3.70s/it][A
 89%|████████▊ | 733/827 [40:29<06:04,  3.87s/it][A
 89%|████████▉ | 734/827 [40:31<05:17,  3.41s/it][A
 89%|████████▉ | 735/827 [40:34<04:53,  3.19s/it][A
 89%|████████▉ | 736/827 [40:37<04:55,  3.25s/it][A
 89%|████████▉ | 737/827 [40:42<05:22,  3.58s/it][A
 89%|████████▉ | 738/827 [40:44<04:53,  3.30s/it][A
 89%|████████▉ | 739/827 [40:47<04:33,  3.11s/it][A
 89%|████████▉ | 740/827 [40:52<05:12,  3.59s/it][A
 90%|████████▉ | 741/827 [40:55<04:54,  3.42s/it][A
 90%|████████▉ | 742/827 [40:58<04:48,  3.40s/it][A
 90%|████████▉ | 743/827 [41:03<05:16,  3.77s/it][A
 90%|████████▉ | 744/827 [41:06<05:09,  3.73s/it][A
 90%|█████████ | 745/827 [41:10<05:09,  3.77s/it][A
 90%|█████████ | 746/827 [41:13<04:41,  3.47s/it][A
 90%|█████████ | 747/827 [41:18<05:22,  4.03s/it][A
 90%|█████████ | 748/827 [41:21<04:42,  3.57s/it][A
 91%|█████████ | 749/827 [41:24<04:34,  3.52s/it][A
 91%|█████████ | 750/827 [41:29<04:54,  3.82s/it][A
 91%|█████████ | 751/827 [41:32<04:29,  3.54s/it][A
 91%|█████████ | 752/827 [41:34<04:05,  3.28s/it][A
 91%|█████████ | 753/827 [41:37<03:48,  3.09s/it][A
 91%|█████████ | 754/827 [41:39<03:28,  2.86s/it][A
 91%|█████████▏| 755/827 [41:42<03:25,  2.85s/it][A
 91%|█████████▏| 756/827 [41:47<04:03,  3.42s/it][A
 92%|█████████▏| 757/827 [41:49<03:35,  3.08s/it][A
 92%|█████████▏| 758/827 [41:53<03:57,  3.44s/it][A
 92%|█████████▏| 759/827 [41:56<03:37,  3.20s/it][A
 92%|█████████▏| 760/827 [41:59<03:18,  2.96s/it][A
 92%|█████████▏| 761/827 [42:05<04:19,  3.93s/it][A
 92%|█████████▏| 762/827 [42:07<03:51,  3.57s/it][A
 92%|█████████▏| 763/827 [42:13<04:36,  4.31s/it][A
 92%|█████████▏| 764/827 [42:16<03:56,  3.75s/it][A
 93%|█████████▎| 765/827 [42:19<03:39,  3.55s/it][A
 93%|█████████▎| 766/827 [42:22<03:25,  3.36s/it][A
 93%|█████████▎| 767/827 [42:25<03:18,  3.31s/it][A
 93%|█████████▎| 768/827 [42:28<03:02,  3.10s/it][A
 93%|█████████▎| 769/827 [42:32<03:21,  3.48s/it][A
 93%|█████████▎| 770/827 [42:35<03:16,  3.45s/it][A
 93%|█████████▎| 771/827 [42:39<03:08,  3.36s/it][A
 93%|█████████▎| 772/827 [42:41<02:52,  3.14s/it][A
 93%|█████████▎| 773/827 [42:44<02:49,  3.14s/it][A
 94%|█████████▎| 774/827 [42:47<02:45,  3.13s/it][A
 94%|█████████▎| 775/827 [42:50<02:40,  3.08s/it][A
 94%|█████████▍| 776/827 [42:54<02:40,  3.16s/it][A
 94%|█████████▍| 777/827 [42:57<02:38,  3.17s/it][A
 94%|█████████▍| 778/827 [43:00<02:35,  3.18s/it][A
 94%|█████████▍| 779/827 [43:03<02:23,  2.98s/it][A
 94%|█████████▍| 780/827 [43:05<02:15,  2.88s/it][A
 94%|█████████▍| 781/827 [43:08<02:12,  2.88s/it][A
 95%|█████████▍| 782/827 [43:11<02:09,  2.88s/it][A
 95%|█████████▍| 783/827 [43:14<02:09,  2.95s/it][A
 95%|█████████▍| 784/827 [43:16<01:57,  2.73s/it][A
 95%|█████████▍| 785/827 [43:20<01:59,  2.85s/it][A
 95%|█████████▌| 786/827 [43:22<01:56,  2.85s/it][A
 95%|█████████▌| 787/827 [43:28<02:22,  3.55s/it][A
 95%|█████████▌| 788/827 [43:31<02:10,  3.35s/it][A
 95%|█████████▌| 789/827 [43:34<02:03,  3.26s/it][A
 96%|█████████▌| 790/827 [43:37<02:04,  3.36s/it][A
 96%|█████████▌| 791/827 [43:40<01:56,  3.23s/it][A
 96%|█████████▌| 792/827 [43:42<01:44,  2.98s/it][A
 96%|█████████▌| 793/827 [43:46<01:43,  3.03s/it][A
 96%|█████████▌| 794/827 [43:51<02:04,  3.78s/it][A
 96%|█████████▌| 795/827 [43:55<02:00,  3.78s/it][A
 96%|█████████▋| 796/827 [43:59<02:03,  3.99s/it][A
 96%|█████████▋| 797/827 [44:02<01:47,  3.59s/it][A
 96%|█████████▋| 798/827 [44:05<01:37,  3.38s/it][A
 97%|█████████▋| 799/827 [44:07<01:24,  3.01s/it][A
 97%|█████████▋| 800/827 [44:10<01:16,  2.83s/it][A
 97%|█████████▋| 801/827 [44:17<01:47,  4.13s/it][A
 97%|█████████▋| 802/827 [44:21<01:44,  4.19s/it][A
 97%|█████████▋| 803/827 [44:24<01:32,  3.83s/it][A
 97%|█████████▋| 804/827 [44:29<01:34,  4.13s/it][A
 97%|█████████▋| 805/827 [44:32<01:22,  3.76s/it][A
 97%|█████████▋| 806/827 [44:34<01:10,  3.34s/it][A
 98%|█████████▊| 807/827 [44:37<01:02,  3.13s/it][A
 98%|█████████▊| 808/827 [44:40<00:59,  3.11s/it][A
 98%|█████████▊| 809/827 [44:43<00:54,  3.01s/it][A
 98%|█████████▊| 810/827 [44:45<00:50,  2.95s/it][A
 98%|█████████▊| 811/827 [44:48<00:47,  2.98s/it][A
 98%|█████████▊| 812/827 [44:51<00:43,  2.88s/it][A
 98%|█████████▊| 813/827 [44:56<00:47,  3.38s/it][A
 98%|█████████▊| 814/827 [45:01<00:50,  3.92s/it][A
 99%|█████████▊| 815/827 [45:03<00:42,  3.50s/it][A
 99%|█████████▊| 816/827 [45:07<00:40,  3.70s/it][A
 99%|█████████▉| 817/827 [45:10<00:34,  3.41s/it][A
 99%|█████████▉| 818/827 [45:13<00:29,  3.32s/it][A
 99%|█████████▉| 819/827 [45:18<00:29,  3.72s/it][A
 99%|█████████▉| 820/827 [45:22<00:27,  3.93s/it][A
 99%|█████████▉| 821/827 [45:25<00:21,  3.58s/it][A
 99%|█████████▉| 822/827 [45:28<00:17,  3.43s/it][A
100%|█████████▉| 823/827 [45:31<00:12,  3.19s/it][A
100%|█████████▉| 824/827 [45:33<00:08,  2.85s/it][A
100%|█████████▉| 825/827 [45:36<00:05,  2.94s/it][A
100%|█████████▉| 826/827 [45:41<00:03,  3.67s/it][A
100%|██████████| 827/827 [45:44<00:00,  3.47s/it][A                                                    
                                                 [A{'eval_loss': 0.7248924970626831, 'eval_runtime': 2750.766, 'eval_samples_per_second': 1.201, 'eval_steps_per_second': 0.301, 'epoch': 1.08}
 54%|█████▍    | 500/930 [1:39:11<11:07:09, 93.09s/it]
100%|██████████| 827/827 [45:45<00:00,  3.47s/it][A
                                                 [A[INFO|trainer.py:4309] 2025-12-01 07:23:47,854 >> Saving model checkpoint to saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500
/opt/conda/lib/python3.11/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in Qwen/Qwen2.5-VL-7B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2421] 2025-12-01 07:23:47,957 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-01 07:23:47,960 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-01 07:23:47,962 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/special_tokens_map.json
[2025-12-01 07:23:48,205] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step500 is about to be saved!
[2025-12-01 07:23:48,223] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/global_step500/zero_pp_rank_3_mp_rank_00_model_states.pt...
[2025-12-01 07:23:48,224] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/global_step500/zero_pp_rank_2_mp_rank_00_model_states.pt...
[2025-12-01 07:23:48,224] [INFO] [logging.py:107:log_dist] [Rank 1] Saving model checkpoint: saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/global_step500/zero_pp_rank_1_mp_rank_00_model_states.pt
[2025-12-01 07:23:48,224] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/global_step500/zero_pp_rank_1_mp_rank_00_model_states.pt...
[2025-12-01 07:23:48,226] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/global_step500/zero_pp_rank_0_mp_rank_00_model_states.pt
[2025-12-01 07:23:48,226] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/global_step500/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2025-12-01 07:23:48,241] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/global_step500/zero_pp_rank_3_mp_rank_00_model_states.pt.
[2025-12-01 07:23:48,242] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/global_step500/zero_pp_rank_2_mp_rank_00_model_states.pt.
[2025-12-01 07:23:48,243] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/global_step500/zero_pp_rank_1_mp_rank_00_model_states.pt.
[2025-12-01 07:23:48,246] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/global_step500/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2025-12-01 07:23:48,248] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/global_step500/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2025-12-01 07:23:48,248] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/global_step500/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2025-12-01 07:23:48,248] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/global_step500/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2025-12-01 07:23:48,249] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/global_step500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-12-01 07:23:48,336] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/global_step500/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2025-12-01 07:23:48,336] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/global_step500/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2025-12-01 07:23:48,342] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/global_step500/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2025-12-01 07:23:48,342] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/global_step500/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2025-12-01 07:23:48,347] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/global_step500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-12-01 07:23:48,351] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/global_step500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-12-01 07:23:48,352] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/global_step500/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2025-12-01 07:23:48,352] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/global_step500/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2025-12-01 07:23:48,378] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step500 is ready now!
[2025-12-01 07:23:48,378] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step500 is ready now!
[2025-12-01 07:23:48,378] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step500 is ready now!
[2025-12-01 07:23:48,381] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step500 is ready now!
[INFO|image_processing_base.py:253] 2025-12-01 07:23:48,395 >> Image processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/preprocessor_config.json
[INFO|tokenization_utils_base.py:2421] 2025-12-01 07:23:48,397 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-01 07:23:48,400 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-01 07:23:48,403 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/special_tokens_map.json
[INFO|video_processing_utils.py:600] 2025-12-01 07:23:48,526 >> Video processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/video_preprocessor_config.json
[INFO|processing_utils.py:814] 2025-12-01 07:23:48,528 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-500/chat_template.jinja
[rank2]: Traceback (most recent call last):
[rank2]:   File "/app/src/llamafactory/launcher.py", line 180, in <module>
[rank2]:     run_exp()
[rank2]:   File "/app/src/llamafactory/train/tuner.py", line 110, in run_exp
[rank2]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank2]:   File "/app/src/llamafactory/train/tuner.py", line 72, in _training_function
[rank2]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank2]:   File "/app/src/llamafactory/train/sft/workflow.py", line 96, in run_sft
[rank2]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank2]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/transformers/trainer.py", line 2325, in train
[rank2]:     return inner_training_loop(
[rank2]:            ^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank2]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank2]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/transformers/trainer.py", line 4020, in training_step
[rank2]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/app/src/llamafactory/train/sft/trainer.py", line 117, in compute_loss
[rank2]:     return super().compute_loss(model, inputs, *args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/transformers/trainer.py", line 4110, in compute_loss
[rank2]:     outputs = model(**inputs)
[rank2]:               ^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank2]:     ret_val = func(*args, **kwargs)
[rank2]:               ^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2054, in forward
[rank2]:     loss = self.module(*inputs, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank2]:     return inner()
[rank2]:            ^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/peft/peft_model.py", line 1850, in forward
[rank2]:     return self.base_model(
[rank2]:            ^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank2]:     return inner()
[rank2]:            ^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/peft/tuners/tuners_utils.py", line 222, in forward
[rank2]:     return self.model.forward(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py", line 918, in wrapper
[rank2]:     output = func(self, *args, **kwargs)
[rank2]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/liger_kernel/transformers/model/qwen2_5_vl.py", line 92, in lce_forward
[rank2]:     outputs = self.model(
[rank2]:               ^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank2]:     return inner()
[rank2]:            ^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1305, in forward
[rank2]:     outputs = self.language_model(
[rank2]:               ^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank2]:     return inner()
[rank2]:            ^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 891, in forward
[rank2]:     layer_outputs = decoder_layer(
[rank2]:                     ^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/transformers/modeling_layers.py", line 93, in __call__
[rank2]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/app/src/llamafactory/model/model_utils/checkpointing.py", line 97, in custom_gradient_checkpointing_func
[rank2]:     return gradient_checkpointing_func(func, *args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/_compile.py", line 32, in inner
[rank2]:     return disable_fn(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank2]:     return fn(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py", line 489, in checkpoint
[rank2]:     return CheckpointFunction.apply(function, preserve, *args)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/autograd/function.py", line 575, in apply
[rank2]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py", line 264, in forward
[rank2]:     outputs = run_function(*args)
[rank2]:               ^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank2]:     return inner()
[rank2]:            ^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[rank2]:     return func(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 741, in forward
[rank2]:     hidden_states, self_attn_weights = self.self_attn(
[rank2]:                                        ^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank2]:     return inner()
[rank2]:            ^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[rank2]:     return func(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 645, in forward
[rank2]:     query_states = self.q_proj(hidden_states)
[rank2]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank2]:     return inner()
[rank2]:            ^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1782, in inner
[rank2]:     args_result = hook(self, args)
[rank2]:                   ^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank2]:     return fn(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/parameter_offload.py", line 297, in _pre_forward_module_hook
[rank2]:     self.pre_sub_module_forward_function(module)
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank2]:     return func(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/parameter_offload.py", line 472, in pre_sub_module_forward_function
[rank2]:     param_coordinator.fetch_sub_module(sub_module, forward=True)
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank2]:     return fn(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank2]:     ret_val = func(*args, **kwargs)
[rank2]:               ^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank2]:     return func(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py", line 366, in fetch_sub_module
[rank2]:     raise RuntimeError(
[rank2]: RuntimeError: tracing error at step 1411: 
[rank2]: module id: 2530, training: True
[rank2]: expected the next 1 parameters in the parameter fetch queue to be ({'id': 'name=base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_A.default.weight id=1093', 'status': 'AVAILABLE', 'numel': 28672, 'ds_numel': 28672, 'shape': (8, 3584), 'ds_shape': (8, 3584), 'requires_grad': True, 'grad_shape': None, 'persist': True, 'active_sub_modules': {2530}, 'ds_tensor.shape': torch.Size([7168])},) 
[rank2]: but got 
[rank2]:  ({'id': 'name=base_model.model.model.language_model.layers.26.self_attn.q_proj.base_layer.weight id=703', 'status': 'NOT_AVAILABLE', 'numel': 0, 'ds_numel': 12845056, 'shape': (0,), 'ds_shape': (3584, 3584), 'requires_grad': False, 'grad_shape': None, 'persist': False, 'active_sub_modules': set(), 'ds_tensor.shape': torch.Size([3211264])},).
[rank1]: Traceback (most recent call last):
[rank1]:   File "/app/src/llamafactory/launcher.py", line 180, in <module>
[rank1]:     run_exp()
[rank1]:   File "/app/src/llamafactory/train/tuner.py", line 110, in run_exp
[rank1]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank1]:   File "/app/src/llamafactory/train/tuner.py", line 72, in _training_function
[rank1]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank1]:   File "/app/src/llamafactory/train/sft/workflow.py", line 96, in run_sft
[rank1]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/transformers/trainer.py", line 2325, in train
[rank1]:     return inner_training_loop(
[rank1]:            ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank1]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/transformers/trainer.py", line 4020, in training_step
[rank1]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/app/src/llamafactory/train/sft/trainer.py", line 117, in compute_loss
[rank1]:     return super().compute_loss(model, inputs, *args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/transformers/trainer.py", line 4110, in compute_loss
[rank1]:     outputs = model(**inputs)
[rank1]:               ^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank1]:     ret_val = func(*args, **kwargs)
[rank1]:               ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2054, in forward
[rank1]:     loss = self.module(*inputs, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank1]:     return inner()
[rank1]:            ^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/peft/peft_model.py", line 1850, in forward
[rank1]:     return self.base_model(
[rank1]:            ^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank1]:     return inner()
[rank1]:            ^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/peft/tuners/tuners_utils.py", line 222, in forward
[rank1]:     return self.model.forward(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py", line 918, in wrapper
[rank1]:     output = func(self, *args, **kwargs)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/liger_kernel/transformers/model/qwen2_5_vl.py", line 92, in lce_forward
[rank1]:     outputs = self.model(
[rank1]:               ^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank1]:     return inner()
[rank1]:            ^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1305, in forward
[rank1]:     outputs = self.language_model(
[rank1]:               ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank1]:     return inner()
[rank1]:            ^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 891, in forward
[rank1]:     layer_outputs = decoder_layer(
[rank1]:                     ^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/transformers/modeling_layers.py", line 93, in __call__
[rank1]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/app/src/llamafactory/model/model_utils/checkpointing.py", line 97, in custom_gradient_checkpointing_func
[rank1]:     return gradient_checkpointing_func(func, *args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/_compile.py", line 32, in inner
[rank1]:     return disable_fn(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank1]:     return fn(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py", line 489, in checkpoint
[rank1]:     return CheckpointFunction.apply(function, preserve, *args)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/autograd/function.py", line 575, in apply
[rank1]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py", line 264, in forward
[rank1]:     outputs = run_function(*args)
[rank1]:               ^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank1]:     return inner()
[rank1]:            ^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[rank1]:     return func(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 741, in forward
[rank1]:     hidden_states, self_attn_weights = self.self_attn(
[rank1]:                                        ^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank1]:     return inner()
[rank1]:            ^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[rank1]:     return func(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 645, in forward
[rank1]:     query_states = self.q_proj(hidden_states)
[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank1]:     return inner()
[rank1]:            ^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1782, in inner
[rank1]:     args_result = hook(self, args)
[rank1]:                   ^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank1]:     return fn(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/parameter_offload.py", line 297, in _pre_forward_module_hook
[rank1]:     self.pre_sub_module_forward_function(module)
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank1]:     return func(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/parameter_offload.py", line 472, in pre_sub_module_forward_function
[rank1]:     param_coordinator.fetch_sub_module(sub_module, forward=True)
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank1]:     return fn(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank1]:     ret_val = func(*args, **kwargs)
[rank1]:               ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank1]:     return func(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py", line 366, in fetch_sub_module
[rank1]:     raise RuntimeError(
[rank1]: RuntimeError: tracing error at step 1411: 
[rank1]: module id: 2530, training: True
[rank1]: expected the next 1 parameters in the parameter fetch queue to be ({'id': 'name=base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_A.default.weight id=1093', 'status': 'AVAILABLE', 'numel': 28672, 'ds_numel': 28672, 'shape': (8, 3584), 'ds_shape': (8, 3584), 'requires_grad': True, 'grad_shape': None, 'persist': True, 'active_sub_modules': {2530}, 'ds_tensor.shape': torch.Size([7168])},) 
[rank1]: but got 
[rank1]:  ({'id': 'name=base_model.model.model.language_model.layers.26.self_attn.q_proj.base_layer.weight id=703', 'status': 'NOT_AVAILABLE', 'numel': 0, 'ds_numel': 12845056, 'shape': (0,), 'ds_shape': (3584, 3584), 'requires_grad': False, 'grad_shape': None, 'persist': False, 'active_sub_modules': set(), 'ds_tensor.shape': torch.Size([3211264])},).
[rank3]: Traceback (most recent call last):
[rank3]:   File "/app/src/llamafactory/launcher.py", line 180, in <module>
[rank3]:     run_exp()
[rank3]:   File "/app/src/llamafactory/train/tuner.py", line 110, in run_exp
[rank3]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank3]:   File "/app/src/llamafactory/train/tuner.py", line 72, in _training_function
[rank3]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank3]:   File "/app/src/llamafactory/train/sft/workflow.py", line 96, in run_sft
[rank3]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank3]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/transformers/trainer.py", line 2325, in train
[rank3]:     return inner_training_loop(
[rank3]:            ^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank3]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank3]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/transformers/trainer.py", line 4020, in training_step
[rank3]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/app/src/llamafactory/train/sft/trainer.py", line 117, in compute_loss
[rank3]:     return super().compute_loss(model, inputs, *args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/transformers/trainer.py", line 4110, in compute_loss
[rank3]:     outputs = model(**inputs)
[rank3]:               ^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank3]:     ret_val = func(*args, **kwargs)
[rank3]:               ^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2054, in forward
[rank3]:     loss = self.module(*inputs, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank3]:     return inner()
[rank3]:            ^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/peft/peft_model.py", line 1850, in forward
[rank3]:     return self.base_model(
[rank3]:            ^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank3]:     return inner()
[rank3]:            ^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/peft/tuners/tuners_utils.py", line 222, in forward
[rank3]:     return self.model.forward(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py", line 918, in wrapper
[rank3]:     output = func(self, *args, **kwargs)
[rank3]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/liger_kernel/transformers/model/qwen2_5_vl.py", line 92, in lce_forward
[rank3]:     outputs = self.model(
[rank3]:               ^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank3]:     return inner()
[rank3]:            ^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1305, in forward
[rank3]:     outputs = self.language_model(
[rank3]:               ^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank3]:     return inner()
[rank3]:            ^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 891, in forward
[rank3]:     layer_outputs = decoder_layer(
[rank3]:                     ^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/transformers/modeling_layers.py", line 93, in __call__
[rank3]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/app/src/llamafactory/model/model_utils/checkpointing.py", line 97, in custom_gradient_checkpointing_func
[rank3]:     return gradient_checkpointing_func(func, *args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/_compile.py", line 32, in inner
[rank3]:     return disable_fn(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank3]:     return fn(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py", line 489, in checkpoint
[rank3]:     return CheckpointFunction.apply(function, preserve, *args)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/autograd/function.py", line 575, in apply
[rank3]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py", line 264, in forward
[rank3]:     outputs = run_function(*args)
[rank3]:               ^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank3]:     return inner()
[rank3]:            ^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[rank3]:     return func(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 741, in forward
[rank3]:     hidden_states, self_attn_weights = self.self_attn(
[rank3]:                                        ^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank3]:     return inner()
[rank3]:            ^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[rank3]:     return func(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 645, in forward
[rank3]:     query_states = self.q_proj(hidden_states)
[rank3]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank3]:     return inner()
[rank3]:            ^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1782, in inner
[rank3]:     args_result = hook(self, args)
[rank3]:                   ^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank3]:     return fn(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/parameter_offload.py", line 297, in _pre_forward_module_hook
[rank3]:     self.pre_sub_module_forward_function(module)
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank3]:     return func(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/parameter_offload.py", line 472, in pre_sub_module_forward_function
[rank3]:     param_coordinator.fetch_sub_module(sub_module, forward=True)
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank3]:     return fn(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank3]:     ret_val = func(*args, **kwargs)
[rank3]:               ^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank3]:     return func(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py", line 366, in fetch_sub_module
[rank3]:     raise RuntimeError(
[rank3]: RuntimeError: tracing error at step 1411: 
[rank3]: module id: 2530, training: True
[rank3]: expected the next 1 parameters in the parameter fetch queue to be ({'id': 'name=base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_A.default.weight id=1093', 'status': 'AVAILABLE', 'numel': 28672, 'ds_numel': 28672, 'shape': (8, 3584), 'ds_shape': (8, 3584), 'requires_grad': True, 'grad_shape': None, 'persist': True, 'active_sub_modules': {2530}, 'ds_tensor.shape': torch.Size([7168])},) 
[rank3]: but got 
[rank3]:  ({'id': 'name=base_model.model.model.language_model.layers.26.self_attn.q_proj.base_layer.weight id=703', 'status': 'NOT_AVAILABLE', 'numel': 0, 'ds_numel': 12845056, 'shape': (0,), 'ds_shape': (3584, 3584), 'requires_grad': False, 'grad_shape': None, 'persist': False, 'active_sub_modules': set(), 'ds_tensor.shape': torch.Size([3211264])},).
[rank0]: Traceback (most recent call last):
[rank0]:   File "/app/src/llamafactory/launcher.py", line 180, in <module>
[rank0]:     run_exp()
[rank0]:   File "/app/src/llamafactory/train/tuner.py", line 110, in run_exp
[rank0]:     _training_function(config={"args": args, "callbacks": callbacks})
[rank0]:   File "/app/src/llamafactory/train/tuner.py", line 72, in _training_function
[rank0]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank0]:   File "/app/src/llamafactory/train/sft/workflow.py", line 96, in run_sft
[rank0]:     train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/transformers/trainer.py", line 2325, in train
[rank0]:     return inner_training_loop(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/transformers/trainer.py", line 4020, in training_step
[rank0]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/app/src/llamafactory/train/sft/trainer.py", line 117, in compute_loss
[rank0]:     return super().compute_loss(model, inputs, *args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/transformers/trainer.py", line 4110, in compute_loss
[rank0]:     outputs = model(**inputs)
[rank0]:               ^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2054, in forward
[rank0]:     loss = self.module(*inputs, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/peft/peft_model.py", line 1850, in forward
[rank0]:     return self.base_model(
[rank0]:            ^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/peft/tuners/tuners_utils.py", line 222, in forward
[rank0]:     return self.model.forward(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py", line 918, in wrapper
[rank0]:     output = func(self, *args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/liger_kernel/transformers/model/qwen2_5_vl.py", line 92, in lce_forward
[rank0]:     outputs = self.model(
[rank0]:               ^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1305, in forward
[rank0]:     outputs = self.language_model(
[rank0]:               ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 891, in forward
[rank0]:     layer_outputs = decoder_layer(
[rank0]:                     ^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/transformers/modeling_layers.py", line 93, in __call__
[rank0]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/app/src/llamafactory/model/model_utils/checkpointing.py", line 97, in custom_gradient_checkpointing_func
[rank0]:     return gradient_checkpointing_func(func, *args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/_compile.py", line 32, in inner
[rank0]:     return disable_fn(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py", line 489, in checkpoint
[rank0]:     return CheckpointFunction.apply(function, preserve, *args)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/autograd/function.py", line 575, in apply
[rank0]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py", line 264, in forward
[rank0]:     outputs = run_function(*args)
[rank0]:               ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 741, in forward
[rank0]:     hidden_states, self_attn_weights = self.self_attn(
[rank0]:                                        ^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1793, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 645, in forward
[rank0]:     query_states = self.q_proj(hidden_states)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1845, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1782, in inner
[rank0]:     args_result = hook(self, args)
[rank0]:                   ^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/parameter_offload.py", line 297, in _pre_forward_module_hook
[rank0]:     self.pre_sub_module_forward_function(module)
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/parameter_offload.py", line 472, in pre_sub_module_forward_function
[rank0]:     param_coordinator.fetch_sub_module(sub_module, forward=True)
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py", line 366, in fetch_sub_module
[rank0]:     raise RuntimeError(
[rank0]: RuntimeError: tracing error at step 1411: 
[rank0]: module id: 2530, training: True
[rank0]: expected the next 1 parameters in the parameter fetch queue to be ({'id': 'name=base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_A.default.weight id=1093', 'status': 'AVAILABLE', 'numel': 28672, 'ds_numel': 28672, 'shape': (8, 3584), 'ds_shape': (8, 3584), 'requires_grad': True, 'grad_shape': None, 'persist': True, 'active_sub_modules': {2530}, 'ds_tensor.shape': torch.Size([7168])},) 
[rank0]: but got 
[rank0]:  ({'id': 'name=base_model.model.model.language_model.layers.26.self_attn.q_proj.base_layer.weight id=703', 'status': 'NOT_AVAILABLE', 'numel': 0, 'ds_numel': 12845056, 'shape': (0,), 'ds_shape': (3584, 3584), 'requires_grad': False, 'grad_shape': None, 'persist': False, 'active_sub_modules': set(), 'ds_tensor.shape': torch.Size([3211264])},).
 54%|█████▍    | 500/930 [1:39:21<1:25:26, 11.92s/it] 
W1201 07:23:59.824000 2152667 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2152760 closing signal SIGTERM
W1201 07:23:59.825000 2152667 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2152761 closing signal SIGTERM
W1201 07:23:59.825000 2152667 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2152763 closing signal SIGTERM
E1201 07:24:04.044000 2152667 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 2 (pid: 2152762) of binary: /opt/conda/bin/python3.11
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/opt/conda/lib/python3.11/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/opt/conda/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/app/src/llamafactory/launcher.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-12-01_07:23:59
  host      : trig0006.scinet.local
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 2152762)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Traceback (most recent call last):
  File "/opt/conda/bin/llamafactory-cli", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/app/src/llamafactory/cli.py", line 24, in main
    launcher.launch()
  File "/app/src/llamafactory/launcher.py", line 110, in launch
    process = subprocess.run(
              ^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['torchrun', '--nnodes', '1', '--node_rank', '0', '--nproc_per_node', '4', '--master_addr', '127.0.0.1', '--master_port', '60459', '/app/src/llamafactory/launcher.py', '/scratch/indrisch/LLaMA-Factory/examples/train_lora/qwen2_5vl_lora_sft_SQA3Devery24_traineval_resumefromcheckpoint_epoch2.yaml']' returned non-zero exit status 1.

scontrol show job 117330
JobId=117330 JobName=slurm_qwen2_5vl_lora_sft_SQA3Devery24_traineval_resumefromcheckpoint_epoch2.sh
   UserId=indrisch(3122343) GroupId=indrisch(3122343) MCS_label=N/A
   Priority=150242 Nice=0 Account=def-wangcs QOS=normal
   JobState=COMPLETING Reason=NonZeroExitCode Dependency=(null)
   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=1:0
   RunTime=01:59:01 TimeLimit=16:10:00 TimeMin=N/A
   SubmitTime=2025-12-01T04:29:27 EligibleTime=2025-12-01T04:29:27
   AccrueTime=2025-12-01T04:29:27
   StartTime=2025-12-01T05:25:04 EndTime=2025-12-01T07:24:05 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2025-12-01T05:25:04 Scheduler=Backfill
   Partition=compute_full_node AllocNode:Sid=trig-login01:1090123
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=trig0006
   BatchHost=trig0006
   NumNodes=1 NumCPUs=96 NumTasks=1 CPUs/Task=96 ReqB:S:C:T=0:0:*:*
   ReqTRES=cpu=96,mem=770000M,node=1,billing=4,gres/gpu=4
   AllocTRES=cpu=96,mem=770000M,node=1,billing=4,gres/gpu=4
   Socks/Node=* NtasksPerN:B:S:C=1:0:*:* CoreSpec=*
   MinCPUsNode=96 MinMemoryNode=770000M MinTmpDiskNode=0
   Features=(null) DelayBoot=00:00:00
   OverSubscribe=NO Contiguous=0 Licenses=(null) Network=(null)
   Command=./slurm_qwen2_5vl_lora_sft_SQA3Devery24_traineval_resumefromcheckpoint_epoch2.sh
   WorkDir=/scratch/indrisch/LLaMA-Factory/preliminaries/qwen2_5vl_lora_sft_SQA3D
   Comment=/opt/slurm/bin/sbatch --export=NONE --get-user-env=L ./slurm_qwen2_5vl_lora_sft_SQA3Devery24_traineval_resumefromcheckpoint_epoch2.sh 
   StdErr=/scratch/indrisch/LLaMA-Factory/preliminaries/qwen2_5vl_lora_sft_SQA3D/%N-qwen2_5vl_lora_sft_SQA3Devery24_traineval-117330.out
   StdIn=/dev/null
   StdOut=/scratch/indrisch/LLaMA-Factory/preliminaries/qwen2_5vl_lora_sft_SQA3D/%N-qwen2_5vl_lora_sft_SQA3Devery24_traineval-117330.out
   TresPerNode=gres/gpu:h100:4
   TresPerTask=cpu=96
   

sacct -j 117330
JobID           JobName    Account    Elapsed  MaxVMSize     MaxRSS  SystemCPU    UserCPU ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- ---------- ---------- -------- 
117330       slurm_qwe+ def-wangcs   01:59:01                         00:00:00   00:00:00      0:0 
117330.batch      batch def-wangcs   01:59:01                         00:00:00   00:00:00      0:0 
117330.exte+     extern def-wangcs   01:59:01                         00:00:00   00:00:00      0:0 

