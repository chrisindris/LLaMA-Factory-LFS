
==========
== CUDA ==
==========

CUDA Version 12.4.1

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

[INFO|2026-01-02 18:25:53] llamafactory.launcher:143 >> Initializing 4 distributed tasks at: 127.0.0.1:34713
W0102 18:25:54.901000 274839 site-packages/torch/distributed/run.py:792] 
W0102 18:25:54.901000 274839 site-packages/torch/distributed/run.py:792] *****************************************
W0102 18:25:54.901000 274839 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0102 18:25:54.901000 274839 site-packages/torch/distributed/run.py:792] *****************************************
⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode

[2026-01-02 18:26:02,691] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-02 18:26:02,707] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-02 18:26:02,785] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-02 18:26:03,128] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2026-01-02 18:26:07,133] [INFO] [comm.py:669:init_distributed] cdb=None
[2026-01-02 18:26:07,133] [INFO] [comm.py:669:init_distributed] cdb=None
[2026-01-02 18:26:07,133] [INFO] [comm.py:669:init_distributed] cdb=None
[2026-01-02 18:26:07,135] [INFO] [comm.py:669:init_distributed] cdb=None
[2026-01-02 18:26:07,135] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[INFO|2026-01-02 18:26:07] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.
[INFO|2026-01-02 18:26:07] llamafactory.hparams.parser:423 >> Process rank: 0, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|hub.py:421] 2026-01-02 18:26:07,686 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2026-01-02 18:26:07,701 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2026-01-02 18:26:07,708 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2026-01-02 18:26:07,708 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2026-01-02 18:26:07,708 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2026-01-02 18:26:07,708 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-02 18:26:07,708 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-02 18:26:07,708 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2026-01-02 18:26:07,708 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2026-01-02 18:26:07,908 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|hub.py:421] 2026-01-02 18:26:07,908 >> Offline mode: forcing local_files_only=True
[INFO|hub.py:421] 2026-01-02 18:26:07,909 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:316] 2026-01-02 18:26:07,911 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2026-01-02 18:26:07,914 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:316] 2026-01-02 18:26:07,916 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2026-01-02 18:26:07,918 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:428] 2026-01-02 18:26:07,925 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:1949] 2026-01-02 18:26:07,925 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2026-01-02 18:26:07,927 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2026-01-02 18:26:07,927 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2026-01-02 18:26:07,927 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2026-01-02 18:26:07,927 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-02 18:26:07,927 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-02 18:26:07,927 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2026-01-02 18:26:07,927 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2026-01-02 18:26:08,087 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:660] 2026-01-02 18:26:08,089 >> Offline mode: forcing local_files_only=True
[INFO|video_processing_utils.py:726] 2026-01-02 18:26:08,092 >> loading configuration file video_preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|video_processing_utils.py:770] 2026-01-02 18:26:08,095 >> Video processor Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:928] 2026-01-02 18:26:08,095 >> Offline mode: forcing local_files_only=True
[INFO|processing_utils.py:1116] 2026-01-02 18:26:08,099 >> loading configuration file processor_config.json from cache at None
[INFO|processing_utils.py:1199] 2026-01-02 18:26:08,391 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2_5_VLProcessor"
}

[INFO|2026-01-02 18:26:08] llamafactory.data.loader:143 >> Loading dataset /scratch/indrisch/huggingface/hub/datasets--cvis-tmu--llamafactory-sqa3d-traces-multiimage-vqa_R0.0_C1.0_F0.0_X0.0/snapshots/cd6673598d0ac0c9d3e27e56ca6e6cc39b5b4c8a/data/...
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
Setting num_proc from 32 to 2 for the train split as it only contains 2 shards.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 3304 examples [00:00, 20898.56 examples/s]Generating train split: 12252 examples [00:00, 29580.06 examples/s]Generating train split: 21200 examples [00:00, 44741.27 examples/s]Generating train split: 33047 examples [00:00, 41160.53 examples/s]
[INFO|2026-01-02 18:26:09] llamafactory.hparams.parser:423 >> Process rank: 2, world size: 4, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
[INFO|2026-01-02 18:26:09] llamafactory.hparams.parser:423 >> Process rank: 1, world size: 4, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2026-01-02 18:26:09] llamafactory.hparams.parser:423 >> Process rank: 3, world size: 4, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
Converting format of dataset (num_proc=32):   0%|          | 0/33047 [00:00<?, ? examples/s]Converting format of dataset (num_proc=32):   0%|          | 1/33047 [00:00<4:49:00,  1.91 examples/s]Converting format of dataset (num_proc=32):   0%|          | 62/33047 [00:00<04:09, 131.97 examples/s]Converting format of dataset (num_proc=32):   1%|          | 362/33047 [00:00<00:40, 800.55 examples/s][rank1]:[W102 18:26:10.638369158 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W102 18:26:10.652187329 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W102 18:26:10.658666754 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Converting format of dataset (num_proc=32):   3%|▎         | 940/33047 [00:00<00:15, 2036.27 examples/s]Converting format of dataset (num_proc=32):   7%|▋         | 2167/33047 [00:00<00:06, 4690.78 examples/s]Converting format of dataset (num_proc=32):  23%|██▎       | 7495/33047 [00:01<00:01, 17841.42 examples/s]Converting format of dataset (num_proc=32):  41%|████      | 13564/33047 [00:01<00:00, 29757.74 examples/s]Converting format of dataset (num_proc=32):  59%|█████▉    | 19614/33047 [00:01<00:00, 38420.24 examples/s]Converting format of dataset (num_proc=32):  78%|███████▊  | 25750/33047 [00:01<00:00, 44993.44 examples/s]Converting format of dataset (num_proc=32):  93%|█████████▎| 30646/33047 [00:01<00:00, 45089.90 examples/s]Converting format of dataset (num_proc=32): 100%|██████████| 33047/33047 [00:01<00:00, 20277.22 examples/s]
[rank0]:[W102 18:26:11.708080559 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
Running tokenizer on dataset (num_proc=32):   0%|          | 0/33047 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=32):   3%|▎         | 1000/33047 [15:13<8:08:08,  1.09 examples/s]Running tokenizer on dataset (num_proc=32):   6%|▌         | 2000/33047 [15:32<3:20:19,  2.58 examples/s]Running tokenizer on dataset (num_proc=32):   9%|▉         | 3000/33047 [15:36<1:46:15,  4.71 examples/s]Running tokenizer on dataset (num_proc=32):  12%|█▏        | 4000/33047 [15:36<1:02:13,  7.78 examples/s]Running tokenizer on dataset (num_proc=32):  15%|█▌        | 5000/33047 [15:37<38:36, 12.11 examples/s]  Running tokenizer on dataset (num_proc=32):  18%|█▊        | 6033/33047 [15:44<25:13, 17.85 examples/s]Running tokenizer on dataset (num_proc=32):  21%|██▏       | 7033/33047 [15:49<17:09, 25.28 examples/s]Running tokenizer on dataset (num_proc=32):  24%|██▍       | 8033/33047 [15:55<12:01, 34.68 examples/s]Running tokenizer on dataset (num_proc=32):  27%|██▋       | 9033/33047 [15:56<08:10, 48.95 examples/s]Running tokenizer on dataset (num_proc=32):  30%|███       | 10033/33047 [15:58<05:38, 67.95 examples/s]Running tokenizer on dataset (num_proc=32):  33%|███▎      | 11033/33047 [15:59<03:47, 96.95 examples/s]Running tokenizer on dataset (num_proc=32):  36%|███▋      | 12033/33047 [16:01<02:49, 124.03 examples/s]Running tokenizer on dataset (num_proc=32):  40%|███▉      | 13066/33047 [16:03<02:02, 163.75 examples/s]Running tokenizer on dataset (num_proc=32):  46%|████▌     | 15066/33047 [16:04<01:01, 294.77 examples/s]Running tokenizer on dataset (num_proc=32):  49%|████▊     | 16099/33047 [16:04<00:45, 376.07 examples/s]Running tokenizer on dataset (num_proc=32):  52%|█████▏    | 17131/33047 [16:07<00:40, 393.52 examples/s]Running tokenizer on dataset (num_proc=32):  55%|█████▍    | 18131/33047 [16:09<00:35, 415.88 examples/s]Running tokenizer on dataset (num_proc=32):  58%|█████▊    | 19164/33047 [16:10<00:30, 448.02 examples/s]Running tokenizer on dataset (num_proc=32):  61%|██████    | 20164/33047 [16:14<00:34, 373.36 examples/s]Running tokenizer on dataset (num_proc=32):  64%|██████▍   | 21164/33047 [16:15<00:24, 489.77 examples/s]Running tokenizer on dataset (num_proc=32):  67%|██████▋   | 22197/33047 [16:16<00:19, 549.53 examples/s]Running tokenizer on dataset (num_proc=32):  70%|███████   | 23197/33047 [16:16<00:13, 728.03 examples/s]Running tokenizer on dataset (num_proc=32):  73%|███████▎  | 24197/33047 [16:17<00:11, 770.23 examples/s]Running tokenizer on dataset (num_proc=32):  76%|███████▋  | 25230/33047 [16:20<00:13, 565.07 examples/s]Running tokenizer on dataset (num_proc=32):  79%|███████▉  | 26262/33047 [16:24<00:15, 447.91 examples/s]Running tokenizer on dataset (num_proc=32):  82%|████████▏ | 27262/33047 [16:25<00:10, 555.03 examples/s]Running tokenizer on dataset (num_proc=32):  86%|████████▌ | 28295/33047 [16:25<00:06, 705.95 examples/s]Running tokenizer on dataset (num_proc=32):  89%|████████▊ | 29295/33047 [16:27<00:05, 676.07 examples/s]Running tokenizer on dataset (num_proc=32):  92%|█████████▏| 30328/33047 [16:30<00:05, 525.32 examples/s]Running tokenizer on dataset (num_proc=32):  92%|█████████▏| 30426/33047 [16:33<00:07, 330.94 examples/s]Running tokenizer on dataset (num_proc=32):  95%|█████████▌| 31459/33047 [16:34<00:03, 443.13 examples/s]Running tokenizer on dataset (num_proc=32):  95%|█████████▌| 31557/33047 [16:40<00:07, 192.43 examples/s]Running tokenizer on dataset (num_proc=32):  96%|█████████▌| 31623/33047 [16:41<00:08, 166.57 examples/s]Running tokenizer on dataset (num_proc=32):  99%|█████████▊| 32623/33047 [16:42<00:01, 294.70 examples/s]Running tokenizer on dataset (num_proc=32):  99%|█████████▉| 32720/33047 [16:46<00:01, 182.62 examples/s]Running tokenizer on dataset (num_proc=32):  99%|█████████▉| 32786/33047 [16:49<00:02, 119.41 examples/s]Running tokenizer on dataset (num_proc=32):  99%|█████████▉| 32852/33047 [16:50<00:01, 111.60 examples/s]Running tokenizer on dataset (num_proc=32): 100%|█████████▉| 32884/33047 [16:51<00:01, 104.68 examples/s]Running tokenizer on dataset (num_proc=32): 100%|█████████▉| 32917/33047 [16:51<00:01, 100.31 examples/s]Running tokenizer on dataset (num_proc=32): 100%|█████████▉| 32950/33047 [16:52<00:00, 103.18 examples/s]Running tokenizer on dataset (num_proc=32): 100%|█████████▉| 32983/33047 [17:03<00:02, 21.52 examples/s] Running tokenizer on dataset (num_proc=32): 100%|█████████▉| 33015/33047 [17:05<00:01, 19.36 examples/s]Running tokenizer on dataset (num_proc=32): 100%|██████████| 33047/33047 [17:12<00:00, 13.40 examples/s]Running tokenizer on dataset (num_proc=32): 100%|██████████| 33047/33047 [17:12<00:00, 32.01 examples/s]
training example:
input_ids:
[151644, 8948, 271, 262, 1446, 525, 264, 16585, 11129, 4142, 11528, 17847, 369, 27979, 647, 47522, 389, 264, 3175, 29519, 6109, 6839, 3941, 5248, 5335, 382, 262, 26149, 510, 262, 68252, 1283, 419, 1943, 11, 498, 686, 5258, 451, 6109, 5335, 438, 366, 1805, 29, 9492, 13, 18877, 1105, 438, 2155, 6194, 315, 279, 83490, 6109, 13, 28634, 311, 5335, 553, 220, 16, 5980, 1922, 304, 1973, 315, 11094, 25, 508, 16, 1125, 508, 17, 1125, 4593, 11, 508, 45, 29562, 262, 3596, 35304, 1718, 8575, 38439, 3298, 14017, 510, 262, 7288, 7854, 264, 12966, 220, 18, 35, 10502, 1614, 3941, 6194, 25, 3754, 6171, 3941, 5335, 11, 23583, 8674, 6249, 17040, 11, 323, 990, 5452, 14, 509, 8957, 14688, 21060, 369, 7990, 55916, 624, 262, 7288, 72077, 409, 849, 292, 3793, 26087, 983, 847, 1290, 97089, 17996, 4065, 14, 29898, 484, 32511, 56111, 7612, 1825, 35978, 28455, 448, 5091, 311, 279, 64171, 14, 8092, 13, 1416, 279, 58385, 374, 54761, 11, 1638, 311, 279, 1429, 38219, 8622, 1651, 323, 1977, 773, 624, 262, 7288, 5443, 1172, 9434, 5904, 26, 653, 537, 17023, 63133, 3565, 476, 17188, 389, 9250, 6540, 7797, 6770, 1633, 43898, 3793, 624, 262, 7288, 3197, 5248, 11178, 3000, 11, 8830, 553, 279, 1850, 2432, 311, 27979, 17133, 488, 4274, 1835, 3941, 6194, 13, 1416, 2058, 54761, 1283, 13295, 678, 5335, 11, 1584, 429, 9355, 382, 262, 30990, 320, 327, 32739, 1378, 10010, 11, 304, 419, 1973, 982, 262, 366, 26865, 397, 262, 14822, 14319, 29208, 32711, 304, 220, 18, 4142, 16, 15, 2805, 7354, 13, 356, 632, 5904, 448, 2168, 14937, 320, 68, 1302, 2572, 65750, 18, 5669, 3100, 7579, 18010, 1290, 315, 8718, 26, 508, 16, 5669, 1852, 1894, 11, 42396, 64212, 2823, 63594, 714, 4583, 320, 2640, 37294, 17, 20, 15, 11211, 7241, 5871, 568, 7036, 894, 17796, 8957, 14, 2969, 26745, 487, 323, 1246, 498, 19673, 432, 624, 262, 690, 26865, 397, 262, 366, 9217, 397, 262, 362, 3175, 63594, 4226, 1172, 11, 892, 1231, 387, 510, 262, 7288, 1416, 14016, 25, 3776, 37328, 4226, 11, 4504, 320, 68, 1302, 2572, 1036, 17, 32511, 476, 7414, 14, 2753, 320, 68, 1302, 2572, 1036, 9693, 854, 4292, 262, 7288, 1416, 537, 14016, 25, 362, 11652, 476, 1378, 624, 262, 1416, 4226, 537, 9434, 11, 421, 1052, 374, 902, 2797, 5904, 11, 476, 421, 279, 4226, 374, 35197, 54761, 25, 1036, 33260, 8253, 854, 624, 262, 690, 9217, 1339, 262, 62947, 609, 43797, 50, 510, 262, 7288, 3155, 4183, 13153, 279, 3405, 476, 1140, 5335, 26, 653, 4183, 2550, 4718, 476, 4960, 14158, 624, 262, 7288, 84468, 4285, 1894, 5036, 448, 518, 1429, 825, 22739, 26087, 4145, 3446, 838, 4322, 1574, 58689, 7256, 854, 4292, 262, 7288, 2823, 12966, 3941, 6194, 26, 421, 6194, 28295, 11, 62552, 279, 11299, 15432, 5904, 323, 5185, 432, 304, 366, 26865, 29816, 257, 151645, 198, 151644, 872, 198, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 30, 220, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151645, 198, 151644, 77091, 198, 198, 4416, 11, 1077, 594, 1430, 311, 7071, 700, 1128, 594, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 62410, 13, 5512, 11, 358, 1184, 311, 50087, 279, 6789, 13, 576, 14852, 702, 264, 19309, 21921, 11, 323, 582, 1184, 311, 10542, 1128, 594, 389, 1909, 315, 432, 518, 279, 220, 16, 16, 297, 62410, 2309, 13, 4710, 22464, 518, 279, 5335, 11, 279, 21921, 374, 4158, 448, 1378, 61607, 13, 6771, 594, 1779, 279, 3082, 2163, 429, 21921, 13, 758, 279, 5335, 11, 1052, 594, 264, 17846, 3403, 279, 19309, 11, 714, 279, 3405, 374, 911, 279, 21921, 13, 13824, 11, 7196, 279, 21921, 374, 3143, 279, 7002, 13, 6771, 594, 1779, 279, 3565, 13, 4710, 14190, 11, 279, 5335, 1473, 264, 14852, 448, 5257, 5424, 25, 264, 33866, 17196, 45766, 11, 264, 19309, 448, 264, 21921, 3685, 11, 52884, 389, 264, 29659, 11, 4992, 13, 576, 21921, 304, 3405, 2293, 1149, 594, 1477, 1181, 3728, 13, 2411, 220, 16, 16, 297, 62410, 11, 892, 374, 279, 8416, 2115, 3082, 8674, 311, 279, 19309, 13, 13824, 11, 902, 11, 220, 16, 16, 297, 62410, 304, 264, 14852, 6505, 2293, 54440, 279, 8416, 949, 13, 13824, 11, 7196, 279, 21921, 702, 3589, 389, 1909, 13, 6771, 594, 1779, 382, 22464, 518, 825, 315, 279, 5335, 11, 1052, 594, 264, 27645, 476, 3589, 389, 1909, 315, 279, 21921, 13, 13824, 11, 304, 279, 2168, 1380, 279, 6249, 9210, 4933, 279, 21921, 11, 1052, 594, 4302, 389, 1909, 13, 13824, 11, 902, 2293, 11489, 11, 7196, 358, 5786, 878, 13, 13824, 11, 279, 3405, 374, 330, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 7521, 2055, 220, 16, 16, 297, 62410, 5106, 13, 6771, 594, 2415, 279, 17961, 13, 220, 16, 17, 297, 62410, 374, 7678, 8305, 320, 83, 70739, 279, 80088, 701, 220, 18, 297, 62410, 374, 1290, 320, 83, 70739, 279, 26214, 701, 220, 24, 297, 62410, 374, 2115, 320, 83, 70739, 279, 52884, 568, 2055, 220, 16, 16, 297, 62410, 1035, 387, 10078, 311, 279, 2115, 315, 220, 16, 17, 297, 62410, 13, 13824, 11, 902, 11, 5297, 8866, 9892, 25, 220, 16, 17, 374, 4065, 11, 220, 18, 1290, 11, 220, 21, 1182, 11, 220, 24, 2115, 13, 2055, 220, 16, 16, 297, 62410, 374, 1101, 1573, 220, 16, 17, 11, 773, 8416, 2115, 8674, 311, 279, 4065, 382, 14190, 11, 3330, 518, 279, 5335, 11, 279, 21921, 3143, 279, 19309, 2293, 18532, 594, 264, 17846, 3403, 432, 30, 2308, 11, 279, 17846, 374, 389, 279, 7002, 3403, 279, 19309, 13, 13824, 11, 7196, 279, 21921, 5086, 702, 3589, 13, 13824, 11, 304, 1045, 5335, 11, 1052, 594, 264, 33866, 42613, 476, 27292, 389, 1909, 315, 279, 21921, 13, 13824, 11, 902, 2293, 11489, 11, 1077, 594, 1779, 1549, 13, 8670, 0, 13824, 11, 304, 825, 315, 279, 5335, 11, 1052, 594, 264, 33866, 1509, 389, 1909, 315, 279, 21921, 13, 13824, 11, 902, 11, 7196, 537, 13, 13824, 11, 279, 4396, 4226, 25, 3330, 518, 279, 5335, 11, 1052, 594, 4302, 389, 1909, 315, 429, 3151, 21921, 13, 13824, 11, 902, 2293, 11489, 11, 7196, 358, 2776, 7402, 13, 13824, 11, 902, 11, 1077, 594, 1744, 1549, 382, 14190, 11, 902, 11, 279, 4396, 21930, 25, 304, 279, 5335, 11, 1052, 594, 902, 9434, 1509, 389, 1909, 315, 429, 21921, 13, 1988, 429, 646, 944, 387, 13, 13824, 11, 902, 2293, 11489, 11, 7196, 358, 1865, 264, 16523, 13, 13824, 11, 1077, 594, 1779, 678, 5335, 13, 576, 19309, 21921, 2293, 20363, 518, 279, 5335, 11, 1052, 594, 4302, 389, 1909, 315, 432, 13, 1988, 429, 594, 537, 3204, 13, 13824, 11, 902, 11, 3783, 21677, 3405, 2578, 387, 22023, 311, 2441, 21921, 30, 2308, 11, 330, 65, 77832, 21921, 3263, 13824, 11, 7196, 279, 4226, 374, 330, 41212, 3263, 1988, 429, 4977, 10322, 13, 13824, 11, 902, 2293, 11489, 11, 1077, 594, 1779, 1549, 382, 14190, 11, 902, 11, 304, 825, 315, 279, 5335, 11, 1052, 594, 264, 33866, 42613, 476, 2494, 30, 2308, 13, 13824, 11, 7196, 358, 2776, 916, 5689, 415, 1095, 13, 6771, 594, 1401, 518, 279, 5335, 1549, 13, 576, 19309, 21921, 702, 1378, 61607, 13, 1913, 1909, 315, 432, 2293, 18532, 594, 4302, 9434, 13, 1988, 429, 646, 944, 387, 1290, 13, 13824, 11, 902, 2293, 11489, 11, 7196, 279, 4226, 374, 330, 3862, 374, 4302, 389, 1909, 315, 279, 14852, 21921, 1189, 1988, 1077, 594, 7683, 382, 14190, 11, 678, 5335, 1473, 429, 3082, 448, 902, 3589, 389, 1909, 315, 279, 21921, 13, 2055, 279, 4226, 374, 429, 1052, 594, 4302, 389, 1909, 315, 432, 518, 220, 16, 16, 297, 62410, 320, 269, 12379, 568, 1988, 1077, 594, 1281, 2704, 382, 9454, 11, 1283, 13295, 678, 9760, 5335, 11, 1052, 594, 4302, 389, 1909, 315, 429, 3151, 14852, 21921, 4192, 1249, 8253, 1128, 374, 389, 1909, 315, 279, 14852, 21921, 518, 279, 220, 16, 16, 297, 527, 20666, 2309, 11, 23643, 279, 27979, 6789, 323, 9124, 3565, 504, 279, 3897, 5335, 25, 18611, 16, 13, 3070, 90618, 9608, 95518, 64547, 279, 1036, 65, 77832, 21921, 854, 323, 1181, 3728, 320, 51659, 279, 19309, 568, 2303, 17, 13, 3070, 9594, 65946, 95518, 1374, 19991, 678, 14087, 369, 6171, 34228, 45988, 419, 21921, 304, 1181, 220, 16, 16, 297, 527, 20666, 72845, 3082, 320, 20432, 311, 14260, 14852, 16725, 568, 2303, 18, 13, 3070, 43434, 95518, 2240, 315, 279, 5335, 1473, 894, 1509, 40119, 389, 1909, 315, 419, 3151, 14852, 21921, 2878, 429, 27979, 10143, 13, 79226, 3862, 374, 4302, 389, 1909, 315, 429, 14852, 21921, 518, 697, 220, 16, 16, 297, 527, 20666, 2309, 13, 151645, 198]
inputs:
<|im_start|>system

    You are a careful vision–language assistant for spatial VQA on a single indoor scene shown across multiple images.

    INPUT:
    Immediately after this message, you will receive N scene images as <image> tags. Treat them as different views of the SAME scene. Refer to images by 1-based index in order of appearance: [1], [2], …, [N].

    REASONING PRINCIPLES:
    • Build a consistent 3D mental model across views: track objects across images, infer relative camera pose, and use scale/occlusion/shadows for depth cues.
    • Interpret deictic terms (“to my right/left/in front/behind”) EGOCENTRICALLY with respect to the narrator/agent. If the viewpoint is ambiguous, default to the most informative central view and say so.
    • Use only visible evidence; do not invent unseen details or rely on external knowledge beyond basic object/color terms.
    • When multiple candidates exist, resolve by the best match to spatial phrase + salience across views. If still ambiguous after checking all images, state that clearly.

    OUTPUT (exactly two blocks, in this order):
    <think>
    Step-by-step reasoning in 3–10 short steps. Cite evidence with image indices (e.g., “[3]: light wood desk right of monitor; [1]: same color, confirms”). Be concise but complete (aim ≤250 tokens unless necessary). Note any occlusion/ambiguity and how you resolved it.
    </think>
    <answer>
    A single concise answer only, which may be:
    • If sufficient: One-word answer, Count (e.g., “2”) or Yes/No (e.g., “yes”).
    • If not sufficient: A sentence or two.
    If answer not visible, if there is no clear evidence, or if the answer is genuinely ambiguous: “cannot determine”.
    </answer>

    STYLE & RULES:
    • Do NOT repeat the question or list images; do NOT output JSON or extra sections.
    • Prefer simple color names with at most one modifier (“light/dark/pale/beige”).
    • Be consistent across views; if views disagree, prioritize the clearest evidence and note it in <think>.
    <|im_end|>
<|im_start|>user
What is on top of the bathroom cabinet that is on my 11 o'clock? <|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|im_end|>
<|im_start|>assistant

So, let's try to figure out what's on top of the bathroom cabinet at 11 o'clock. First, I need to visualize the layout. The bathroom has a sink cabinet, and we need to identify what's on top of it at the 11 o'clock position. 

Looking at the images, the cabinet is white with two drawers. Let's check the area around that cabinet. In the images, there's a mirror above the sink, but the question is about the cabinet. Wait, maybe the cabinet is near the wall. Let's check the details. 

Wait, the images show a bathroom with various elements: a colorful shower curtain, a sink with a cabinet below, towels on a rack, etc. The cabinet in question—let's find its location. At 11 o'clock, which is the upper left area relative to the sink. Wait, no, 11 o'clock in a bathroom setup—probably the upper part. Wait, maybe the cabinet has items on top. Let's check.

Looking at one of the images, there's a shelf or items on top of the cabinet. Wait, in the image where the camera angle shows the cabinet, there's nothing on top. Wait, no—wait, maybe I misread. Wait, the question is "What is on top of the bathroom cabinet that is on my 11 o'clock?" So 11 o'clock direction. Let's map the directions. 12 o'clock is straight ahead (towards the bathtub), 3 o'clock is right (towards the toilet), 9 o'clock is left (towards the towels). So 11 o'clock would be slightly to the left of 12 o'clock. Wait, no, standard clock positions: 12 is front, 3 right, 6 back, 9 left. So 11 o'clock is just before 12, so upper left relative to the front.

Wait, looking at the images, the cabinet near the sink—there's a mirror above it? No, the mirror is on the wall above the sink. Wait, maybe the cabinet itself has items. Wait, in some images, there's a colorful towel or cloth on top of the cabinet. Wait, no—wait, let's check again. Oh! Wait, in one of the images, there's a colorful item on top of the cabinet. Wait, no, maybe not. Wait, the correct answer: looking at the images, there's nothing on top of that specific cabinet. Wait, no—wait, maybe I'm missing. Wait, no, let's think again.

Wait, no, the correct observation: in the images, there's no visible item on top of that cabinet. But that can't be. Wait, no—wait, maybe I made a mistake. Wait, let's check all images. The sink cabinet—looking at the images, there's nothing on top of it. But that's not possible. Wait, no, wait—the question might be referring to another cabinet? No, "bathroom cabinet". Wait, maybe the answer is "nothing". But that seems odd. Wait, no—wait, let's check again.

Wait, no, in one of the images, there's a colorful towel or something? No. Wait, maybe I'm overcomplicating. Let's look at the images again. The sink cabinet has two drawers. On top of it—there's nothing visible. But that can't be right. Wait, no—wait, maybe the answer is "There is nothing on top of the bathroom cabinet." But let's confirm.

Wait, all images show that area with no items on top of the cabinet. So the answer is that there's nothing on top of it at 11 o'clock (or anywhere). But let's make sure.

Yes, after checking all relevant images, there's nothing on top of that specific bathroom cabinet.


To determine what is on top of the bathroom cabinet at the 11 o’clock position, analyze the spatial layout and visual details from the provided images:  

1. **Spatial Context**: Identify the “bathroom cabinet” and its location (near the sink).  
2. **Visual Inspection**: Examine all frames for objects positioned atop this cabinet in its 11 o’clock directional area (relative to typical bathroom orientation).  
3. **Conclusion**: None of the images show any item resting on top of this specific bathroom cabinet within that spatial zone.  


There is nothing on top of that bathroom cabinet at your 11 o’clock position.<|im_end|>

label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 198, 4416, 11, 1077, 594, 1430, 311, 7071, 700, 1128, 594, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 62410, 13, 5512, 11, 358, 1184, 311, 50087, 279, 6789, 13, 576, 14852, 702, 264, 19309, 21921, 11, 323, 582, 1184, 311, 10542, 1128, 594, 389, 1909, 315, 432, 518, 279, 220, 16, 16, 297, 62410, 2309, 13, 4710, 22464, 518, 279, 5335, 11, 279, 21921, 374, 4158, 448, 1378, 61607, 13, 6771, 594, 1779, 279, 3082, 2163, 429, 21921, 13, 758, 279, 5335, 11, 1052, 594, 264, 17846, 3403, 279, 19309, 11, 714, 279, 3405, 374, 911, 279, 21921, 13, 13824, 11, 7196, 279, 21921, 374, 3143, 279, 7002, 13, 6771, 594, 1779, 279, 3565, 13, 4710, 14190, 11, 279, 5335, 1473, 264, 14852, 448, 5257, 5424, 25, 264, 33866, 17196, 45766, 11, 264, 19309, 448, 264, 21921, 3685, 11, 52884, 389, 264, 29659, 11, 4992, 13, 576, 21921, 304, 3405, 2293, 1149, 594, 1477, 1181, 3728, 13, 2411, 220, 16, 16, 297, 62410, 11, 892, 374, 279, 8416, 2115, 3082, 8674, 311, 279, 19309, 13, 13824, 11, 902, 11, 220, 16, 16, 297, 62410, 304, 264, 14852, 6505, 2293, 54440, 279, 8416, 949, 13, 13824, 11, 7196, 279, 21921, 702, 3589, 389, 1909, 13, 6771, 594, 1779, 382, 22464, 518, 825, 315, 279, 5335, 11, 1052, 594, 264, 27645, 476, 3589, 389, 1909, 315, 279, 21921, 13, 13824, 11, 304, 279, 2168, 1380, 279, 6249, 9210, 4933, 279, 21921, 11, 1052, 594, 4302, 389, 1909, 13, 13824, 11, 902, 2293, 11489, 11, 7196, 358, 5786, 878, 13, 13824, 11, 279, 3405, 374, 330, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 7521, 2055, 220, 16, 16, 297, 62410, 5106, 13, 6771, 594, 2415, 279, 17961, 13, 220, 16, 17, 297, 62410, 374, 7678, 8305, 320, 83, 70739, 279, 80088, 701, 220, 18, 297, 62410, 374, 1290, 320, 83, 70739, 279, 26214, 701, 220, 24, 297, 62410, 374, 2115, 320, 83, 70739, 279, 52884, 568, 2055, 220, 16, 16, 297, 62410, 1035, 387, 10078, 311, 279, 2115, 315, 220, 16, 17, 297, 62410, 13, 13824, 11, 902, 11, 5297, 8866, 9892, 25, 220, 16, 17, 374, 4065, 11, 220, 18, 1290, 11, 220, 21, 1182, 11, 220, 24, 2115, 13, 2055, 220, 16, 16, 297, 62410, 374, 1101, 1573, 220, 16, 17, 11, 773, 8416, 2115, 8674, 311, 279, 4065, 382, 14190, 11, 3330, 518, 279, 5335, 11, 279, 21921, 3143, 279, 19309, 2293, 18532, 594, 264, 17846, 3403, 432, 30, 2308, 11, 279, 17846, 374, 389, 279, 7002, 3403, 279, 19309, 13, 13824, 11, 7196, 279, 21921, 5086, 702, 3589, 13, 13824, 11, 304, 1045, 5335, 11, 1052, 594, 264, 33866, 42613, 476, 27292, 389, 1909, 315, 279, 21921, 13, 13824, 11, 902, 2293, 11489, 11, 1077, 594, 1779, 1549, 13, 8670, 0, 13824, 11, 304, 825, 315, 279, 5335, 11, 1052, 594, 264, 33866, 1509, 389, 1909, 315, 279, 21921, 13, 13824, 11, 902, 11, 7196, 537, 13, 13824, 11, 279, 4396, 4226, 25, 3330, 518, 279, 5335, 11, 1052, 594, 4302, 389, 1909, 315, 429, 3151, 21921, 13, 13824, 11, 902, 2293, 11489, 11, 7196, 358, 2776, 7402, 13, 13824, 11, 902, 11, 1077, 594, 1744, 1549, 382, 14190, 11, 902, 11, 279, 4396, 21930, 25, 304, 279, 5335, 11, 1052, 594, 902, 9434, 1509, 389, 1909, 315, 429, 21921, 13, 1988, 429, 646, 944, 387, 13, 13824, 11, 902, 2293, 11489, 11, 7196, 358, 1865, 264, 16523, 13, 13824, 11, 1077, 594, 1779, 678, 5335, 13, 576, 19309, 21921, 2293, 20363, 518, 279, 5335, 11, 1052, 594, 4302, 389, 1909, 315, 432, 13, 1988, 429, 594, 537, 3204, 13, 13824, 11, 902, 11, 3783, 21677, 3405, 2578, 387, 22023, 311, 2441, 21921, 30, 2308, 11, 330, 65, 77832, 21921, 3263, 13824, 11, 7196, 279, 4226, 374, 330, 41212, 3263, 1988, 429, 4977, 10322, 13, 13824, 11, 902, 2293, 11489, 11, 1077, 594, 1779, 1549, 382, 14190, 11, 902, 11, 304, 825, 315, 279, 5335, 11, 1052, 594, 264, 33866, 42613, 476, 2494, 30, 2308, 13, 13824, 11, 7196, 358, 2776, 916, 5689, 415, 1095, 13, 6771, 594, 1401, 518, 279, 5335, 1549, 13, 576, 19309, 21921, 702, 1378, 61607, 13, 1913, 1909, 315, 432, 2293, 18532, 594, 4302, 9434, 13, 1988, 429, 646, 944, 387, 1290, 13, 13824, 11, 902, 2293, 11489, 11, 7196, 279, 4226, 374, 330, 3862, 374, 4302, 389, 1909, 315, 279, 14852, 21921, 1189, 1988, 1077, 594, 7683, 382, 14190, 11, 678, 5335, 1473, 429, 3082, 448, 902, 3589, 389, 1909, 315, 279, 21921, 13, 2055, 279, 4226, 374, 429, 1052, 594, 4302, 389, 1909, 315, 432, 518, 220, 16, 16, 297, 62410, 320, 269, 12379, 568, 1988, 1077, 594, 1281, 2704, 382, 9454, 11, 1283, 13295, 678, 9760, 5335, 11, 1052, 594, 4302, 389, 1909, 315, 429, 3151, 14852, 21921, 4192, 1249, 8253, 1128, 374, 389, 1909, 315, 279, 14852, 21921, 518, 279, 220, 16, 16, 297, 527, 20666, 2309, 11, 23643, 279, 27979, 6789, 323, 9124, 3565, 504, 279, 3897, 5335, 25, 18611, 16, 13, 3070, 90618, 9608, 95518, 64547, 279, 1036, 65, 77832, 21921, 854, 323, 1181, 3728, 320, 51659, 279, 19309, 568, 2303, 17, 13, 3070, 9594, 65946, 95518, 1374, 19991, 678, 14087, 369, 6171, 34228, 45988, 419, 21921, 304, 1181, 220, 16, 16, 297, 527, 20666, 72845, 3082, 320, 20432, 311, 14260, 14852, 16725, 568, 2303, 18, 13, 3070, 43434, 95518, 2240, 315, 279, 5335, 1473, 894, 1509, 40119, 389, 1909, 315, 419, 3151, 14852, 21921, 2878, 429, 27979, 10143, 13, 79226, 3862, 374, 4302, 389, 1909, 315, 429, 14852, 21921, 518, 697, 220, 16, 16, 297, 527, 20666, 2309, 13, 151645, 198]
labels:

So, let's try to figure out what's on top of the bathroom cabinet at 11 o'clock. First, I need to visualize the layout. The bathroom has a sink cabinet, and we need to identify what's on top of it at the 11 o'clock position. 

Looking at the images, the cabinet is white with two drawers. Let's check the area around that cabinet. In the images, there's a mirror above the sink, but the question is about the cabinet. Wait, maybe the cabinet is near the wall. Let's check the details. 

Wait, the images show a bathroom with various elements: a colorful shower curtain, a sink with a cabinet below, towels on a rack, etc. The cabinet in question—let's find its location. At 11 o'clock, which is the upper left area relative to the sink. Wait, no, 11 o'clock in a bathroom setup—probably the upper part. Wait, maybe the cabinet has items on top. Let's check.

Looking at one of the images, there's a shelf or items on top of the cabinet. Wait, in the image where the camera angle shows the cabinet, there's nothing on top. Wait, no—wait, maybe I misread. Wait, the question is "What is on top of the bathroom cabinet that is on my 11 o'clock?" So 11 o'clock direction. Let's map the directions. 12 o'clock is straight ahead (towards the bathtub), 3 o'clock is right (towards the toilet), 9 o'clock is left (towards the towels). So 11 o'clock would be slightly to the left of 12 o'clock. Wait, no, standard clock positions: 12 is front, 3 right, 6 back, 9 left. So 11 o'clock is just before 12, so upper left relative to the front.

Wait, looking at the images, the cabinet near the sink—there's a mirror above it? No, the mirror is on the wall above the sink. Wait, maybe the cabinet itself has items. Wait, in some images, there's a colorful towel or cloth on top of the cabinet. Wait, no—wait, let's check again. Oh! Wait, in one of the images, there's a colorful item on top of the cabinet. Wait, no, maybe not. Wait, the correct answer: looking at the images, there's nothing on top of that specific cabinet. Wait, no—wait, maybe I'm missing. Wait, no, let's think again.

Wait, no, the correct observation: in the images, there's no visible item on top of that cabinet. But that can't be. Wait, no—wait, maybe I made a mistake. Wait, let's check all images. The sink cabinet—looking at the images, there's nothing on top of it. But that's not possible. Wait, no, wait—the question might be referring to another cabinet? No, "bathroom cabinet". Wait, maybe the answer is "nothing". But that seems odd. Wait, no—wait, let's check again.

Wait, no, in one of the images, there's a colorful towel or something? No. Wait, maybe I'm overcomplicating. Let's look at the images again. The sink cabinet has two drawers. On top of it—there's nothing visible. But that can't be right. Wait, no—wait, maybe the answer is "There is nothing on top of the bathroom cabinet." But let's confirm.

Wait, all images show that area with no items on top of the cabinet. So the answer is that there's nothing on top of it at 11 o'clock (or anywhere). But let's make sure.

Yes, after checking all relevant images, there's nothing on top of that specific bathroom cabinet.


To determine what is on top of the bathroom cabinet at the 11 o’clock position, analyze the spatial layout and visual details from the provided images:  

1. **Spatial Context**: Identify the “bathroom cabinet” and its location (near the sink).  
2. **Visual Inspection**: Examine all frames for objects positioned atop this cabinet in its 11 o’clock directional area (relative to typical bathroom orientation).  
3. **Conclusion**: None of the images show any item resting on top of this specific bathroom cabinet within that spatial zone.  


There is nothing on top of that bathroom cabinet at your 11 o’clock position.<|im_end|>

[INFO|hub.py:421] 2026-01-02 18:43:27,097 >> Offline mode: forcing local_files_only=True
[INFO|configuration_utils.py:765] 2026-01-02 18:43:27,101 >> loading configuration file config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
[INFO|configuration_utils.py:839] 2026-01-02 18:43:27,104 >> Model config Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 128000,
  "max_window_layers": 28,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "_name_or_path": "Qwen/Qwen2.5-VL-7B-Instruct",
    "architectures": [
      "Qwen2_5_VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "dtype": "bfloat16",
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "layer_types": [
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention"
    ],
    "max_position_embeddings": 128000,
    "max_window_layers": 28,
    "model_type": "qwen2_5_vl_text",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "use_cache": true,
    "use_sliding_window": false,
    "vision_token_id": 151654,
    "vocab_size": 152064
  },
  "tie_word_embeddings": false,
  "transformers_version": "4.57.0",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 3584,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

[INFO|2026-01-02 18:43:27] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
[INFO|2026-01-02 18:43:27] llamafactory.model.model_utils.liger_kernel:143 >> Liger kernel has been applied to the model.
[INFO|hub.py:421] 2026-01-02 18:43:27,312 >> Offline mode: forcing local_files_only=True
[WARNING|logging.py:328] 2026-01-02 18:43:27,314 >> `torch_dtype` is deprecated! Use `dtype` instead!
[INFO|hub.py:421] 2026-01-02 18:43:27,314 >> Offline mode: forcing local_files_only=True
[INFO|modeling_utils.py:4840] 2026-01-02 18:43:27,315 >> Offline mode: forcing local_files_only=True
[INFO|modeling_utils.py:1176] 2026-01-02 18:43:27,318 >> loading weights file model.safetensors from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/model.safetensors.index.json
[INFO|modeling_utils.py:2345] 2026-01-02 18:43:27,323 >> Instantiating Qwen2_5_VLForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:986] 2026-01-02 18:43:27,329 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

[INFO|modeling_utils.py:2345] 2026-01-02 18:43:27,332 >> Instantiating Qwen2_5_VisionTransformerPretrainedModel model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:2345] 2026-01-02 18:43:27,359 >> Instantiating Qwen2_5_VLTextModel model under default dtype torch.bfloat16.
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:19<01:19, 19.96s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:19<01:18, 19.54s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:20<01:20, 20.09s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:19<01:18, 19.74s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:37<00:55, 18.52s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:37<00:56, 18.69s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:37<00:55, 18.46s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:37<00:56, 18.67s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:57<00:38, 19.28s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:57<00:38, 19.19s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:57<00:38, 19.31s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:57<00:38, 19.24s/it]Loading checkpoint shards:  80%|████████  | 4/5 [01:17<00:19, 19.40s/it]Loading checkpoint shards:  80%|████████  | 4/5 [01:17<00:19, 19.44s/it]Loading checkpoint shards:  80%|████████  | 4/5 [01:17<00:19, 19.49s/it]Loading checkpoint shards:  80%|████████  | 4/5 [01:17<00:19, 19.49s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:23<00:00, 14.51s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:23<00:00, 16.64s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [01:22<00:00, 14.49s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:22<00:00, 16.59s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [01:23<00:00, 14.52s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:23<00:00, 16.67s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [01:22<00:00, 14.48s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:22<00:00, 16.56s/it]
[INFO|configuration_utils.py:941] 2026-01-02 18:44:50,771 >> loading configuration file generation_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/generation_config.json
[INFO|configuration_utils.py:986] 2026-01-02 18:44:50,772 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 1e-06
}

[INFO|dynamic_module_utils.py:423] 2026-01-02 18:44:50,774 >> Could not locate the custom_generate/generate.py inside Qwen/Qwen2.5-VL-7B-Instruct.
[INFO|2026-01-02 18:44:50] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2026-01-02 18:44:50] llamafactory.model.model_utils.attention:143 >> Using FlashAttention-2 for faster training and inference.
[INFO|2026-01-02 18:44:50] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.
[INFO|2026-01-02 18:44:50] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA
[INFO|2026-01-02 18:44:50] llamafactory.model.model_utils.misc:143 >> Found linear modules: k_proj,up_proj,v_proj,o_proj,gate_proj,q_proj,down_proj
[INFO|2026-01-02 18:44:50] llamafactory.model.model_utils.visual:143 >> Set vision model not trainable: ['visual.patch_embed', 'visual.blocks'].
[INFO|2026-01-02 18:44:50] llamafactory.model.model_utils.visual:143 >> Set multi model projector not trainable: visual.merger.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Currently training with a batch size of: 2
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Currently training with a batch size of: 2
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
[INFO|2026-01-02 18:44:51] llamafactory.model.loader:143 >> trainable params: 20,185,088 || all params: 8,312,351,744 || trainable%: 0.2428
name: base_model.model.model.visual.patch_embed.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.ln_q.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.0.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.0.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.embed_tokens.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.lm_head.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
[INFO|trainer.py:749] 2026-01-02 18:44:51,445 >> Using auto half precision backend
[WARNING|trainer.py:982] 2026-01-02 18:44:51,447 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Currently training with a batch size of: 2
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
[DEBUG|trainer.py:2373] 2026-01-02 18:44:51,714 >> Currently training with a batch size of: 2
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 8. Using DeepSpeed's value.
[INFO|deepspeed.py:383] 2026-01-02 18:44:51,729 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Using /dev/shm/slurm.indrisch.177487/.cache/torch_extensions as PyTorch extensions root...
Creating extension directory /dev/shm/slurm.indrisch.177487/.cache/torch_extensions/cpu_adam...
Using /dev/shm/slurm.indrisch.177487/.cache/torch_extensions as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /dev/shm/slurm.indrisch.177487/.cache/torch_extensions/cpu_adam/build.ninja...
/opt/conda/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module cpu_adam...
Using envvar MAX_JOBS (16) as the number of workers...
Using /dev/shm/slurm.indrisch.177487/.cache/torch_extensions as PyTorch extensions root...
Using /dev/shm/slurm.indrisch.177487/.cache/torch_extensions as PyTorch extensions root...
[1/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.11/site-packages/torch/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o 
[2/3] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.11/site-packages/torch/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o 
[3/3] c++ cpu_adam.o cpu_adam_impl.o -shared -lcurand -L/usr/local/cuda/lib64 -L/opt/conda/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so
Loading extension module cpu_adam...
Time to load cpu_adam op: 23.425624132156372 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2026-01-02 18:45:15,954] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
Loading extension module cpu_adam...
Time to load cpu_adam op: 23.096998929977417 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2026-01-02 18:45:16,006] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.9, git-hash=unknown, git-branch=unknown
[2026-01-02 18:45:16,007] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
Loading extension module cpu_adam...
Time to load cpu_adam op: 23.317291498184204 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2026-01-02 18:45:16,034] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
Loading extension module cpu_adam...
Time to load cpu_adam op: 23.50238275527954 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2026-01-02 18:45:16,038] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
[2026-01-02 18:45:16,981] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2026-01-02 18:45:16,984] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2026-01-02 18:45:16,984] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2026-01-02 18:45:17,005] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2026-01-02 18:45:17,005] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2026-01-02 18:45:17,005] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2026-01-02 18:45:17,005] [INFO] [stage_1_and_2.py:150:__init__] Reduce bucket size 500000000
[2026-01-02 18:45:17,005] [INFO] [stage_1_and_2.py:151:__init__] Allgather bucket size 500000000
[2026-01-02 18:45:17,005] [INFO] [stage_1_and_2.py:152:__init__] CPU Offload: True
[2026-01-02 18:45:17,005] [INFO] [stage_1_and_2.py:153:__init__] Round robin gradient partitioning: True
***** Running training *****
  Num examples = 29,742
  Num Epochs = 1
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 8
  Total optimization steps = 465
***** Running training *****
  Num examples = 29,742
  Num Epochs = 1
***** Running training *****
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 8
  Num examples = 29,742
  Total optimization steps = 465
  Num Epochs = 1
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 8
  Total optimization steps = 465
  Number of trainable parameters = 20,185,088
  Number of trainable parameters = 20,185,088
  Number of trainable parameters = 20,185,088
[2026-01-02 18:45:17,326] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2026-01-02 18:45:17,326] [INFO] [utils.py:782:see_memory_usage] MA 15.48 GB         Max_MA 15.48 GB         CA 15.49 GB         Max_CA 15 GB 
[2026-01-02 18:45:17,326] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 32.15 GB, percent = 4.3%
[2026-01-02 18:45:17,549] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2026-01-02 18:45:17,550] [INFO] [utils.py:782:see_memory_usage] MA 15.48 GB         Max_MA 15.48 GB         CA 15.49 GB         Max_CA 15 GB 
[2026-01-02 18:45:17,550] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 32.18 GB, percent = 4.3%
[2026-01-02 18:45:17,550] [INFO] [stage_1_and_2.py:557:__init__] optimizer state initialized
[2026-01-02 18:45:17,749] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2026-01-02 18:45:17,749] [INFO] [utils.py:782:see_memory_usage] MA 15.48 GB         Max_MA 15.48 GB         CA 15.49 GB         Max_CA 15 GB 
[2026-01-02 18:45:17,749] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 32.17 GB, percent = 4.3%
[2026-01-02 18:45:17,752] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2026-01-02 18:45:17,752] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2026-01-02 18:45:17,752] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2026-01-02 18:45:17,752] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2026-01-02 18:45:17,756] [INFO] [config.py:1003:print] DeepSpeedEngine configuration:
[2026-01-02 18:45:17,756] [INFO] [config.py:1007:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2026-01-02 18:45:17,756] [INFO] [config.py:1007:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2026-01-02 18:45:17,756] [INFO] [config.py:1007:print]   amp_enabled .................. False
[2026-01-02 18:45:17,756] [INFO] [config.py:1007:print]   amp_params ................... False
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   bfloat16_enabled ............. True
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   bfloat16_immediate_grad_update  True
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   checkpoint_parallel_write_pipeline  False
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   checkpoint_tag_validation_enabled  True
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   checkpoint_tag_validation_fail  False
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x146a854bb210>
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   communication_data_type ...... None
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   curriculum_enabled_legacy .... False
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   curriculum_params_legacy ..... False
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   data_efficiency_enabled ...... False
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   dataloader_drop_last ......... False
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   disable_allgather ............ False
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   dump_state ................... False
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   dynamic_loss_scale_args ...... None
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   eigenvalue_enabled ........... False
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   eigenvalue_gas_boundary_resolution  1
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   eigenvalue_layer_num ......... 0
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   eigenvalue_max_iter .......... 100
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   eigenvalue_stability ......... 1e-06
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   eigenvalue_tol ............... 0.01
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   eigenvalue_verbose ........... False
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   elasticity_enabled ........... False
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   fp16_auto_cast ............... None
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   fp16_enabled ................. False
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   fp16_master_weights_and_gradients  False
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   global_rank .................. 0
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   grad_accum_dtype ............. None
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   gradient_accumulation_steps .. 8
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   gradient_clipping ............ 1.0
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   gradient_predivide_factor .... 1.0
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   graph_harvesting ............. False
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   initial_dynamic_scale ........ 1
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   load_universal_checkpoint .... False
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   loss_scale ................... 1.0
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   memory_breakdown ............. False
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   mics_hierarchial_params_gather  False
[2026-01-02 18:45:17,757] [INFO] [config.py:1007:print]   mics_shard_size .............. -1
[2026-01-02 18:45:17,758] [INFO] [config.py:1007:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2026-01-02 18:45:17,758] [INFO] [config.py:1007:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2026-01-02 18:45:17,758] [INFO] [config.py:1007:print]   optimizer_legacy_fusion ...... False
[2026-01-02 18:45:17,758] [INFO] [config.py:1007:print]   optimizer_name ............... None
[2026-01-02 18:45:17,758] [INFO] [config.py:1007:print]   optimizer_params ............. None
[2026-01-02 18:45:17,758] [INFO] [config.py:1007:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2026-01-02 18:45:17,758] [INFO] [config.py:1007:print]   pld_enabled .................. False
[2026-01-02 18:45:17,758] [INFO] [config.py:1007:print]   pld_params ................... False
[2026-01-02 18:45:17,758] [INFO] [config.py:1007:print]   prescale_gradients ........... False
[2026-01-02 18:45:17,758] [INFO] [config.py:1007:print]   scheduler_name ............... None
[2026-01-02 18:45:17,758] [INFO] [config.py:1007:print]   scheduler_params ............. None
[2026-01-02 18:45:17,758] [INFO] [config.py:1007:print]   seq_parallel_communication_data_type  torch.float32
[2026-01-02 18:45:17,758] [INFO] [config.py:1007:print]   sparse_attention ............. None
[2026-01-02 18:45:17,758] [INFO] [config.py:1007:print]   sparse_gradients_enabled ..... False
[2026-01-02 18:45:17,758] [INFO] [config.py:1007:print]   steps_per_print .............. inf
[2026-01-02 18:45:17,758] [INFO] [config.py:1007:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2026-01-02 18:45:17,758] [INFO] [config.py:1007:print]   timers_config ................ enabled=True synchronized=True
[2026-01-02 18:45:17,758] [INFO] [config.py:1007:print]   train_batch_size ............. 64
[2026-01-02 18:45:17,758] [INFO] [config.py:1007:print]   train_micro_batch_size_per_gpu  2
[2026-01-02 18:45:17,758] [INFO] [config.py:1007:print]   use_data_before_expert_parallel_  False
[2026-01-02 18:45:17,758] [INFO] [config.py:1007:print]   use_node_local_storage ....... False
[2026-01-02 18:45:17,758] [INFO] [config.py:1007:print]   wall_clock_breakdown ......... False
[2026-01-02 18:45:17,758] [INFO] [config.py:1007:print]   weight_quantization_config ... None
[2026-01-02 18:45:17,758] [INFO] [config.py:1007:print]   world_size ................... 4
[2026-01-02 18:45:17,758] [INFO] [config.py:1007:print]   zero_allow_untested_optimizer  True
[2026-01-02 18:45:17,758] [INFO] [config.py:1007:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=True zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2026-01-02 18:45:17,758] [INFO] [config.py:1007:print]   zero_enabled ................. True
[2026-01-02 18:45:17,758] [INFO] [config.py:1007:print]   zero_force_ds_cpu_optimizer .. True
[2026-01-02 18:45:17,758] [INFO] [config.py:1007:print]   zero_optimization_stage ...... 2
[2026-01-02 18:45:17,758] [INFO] [config.py:993:print_user_config]   json = {
    "train_batch_size": 64, 
    "train_micro_batch_size_per_gpu": 2, 
    "gradient_accumulation_steps": 8, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": false, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true, 
        "round_robin_gradients": true
    }, 
    "steps_per_print": inf
}
[INFO|trainer.py:2519] 2026-01-02 18:45:17,759 >> ***** Running training *****
[INFO|trainer.py:2520] 2026-01-02 18:45:17,759 >>   Num examples = 29,742
[INFO|trainer.py:2521] 2026-01-02 18:45:17,759 >>   Num Epochs = 1
[INFO|trainer.py:2522] 2026-01-02 18:45:17,759 >>   Instantaneous batch size per device = 2
[INFO|trainer.py:2525] 2026-01-02 18:45:17,759 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2526] 2026-01-02 18:45:17,759 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2527] 2026-01-02 18:45:17,759 >>   Total optimization steps = 465
[INFO|trainer.py:2528] 2026-01-02 18:45:17,763 >>   Number of trainable parameters = 20,185,088
[INFO|integration_utils.py:867] 2026-01-02 18:45:17,765 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Tracking run with wandb version 0.23.1
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /scratch/indrisch/LLaMA-Factory/wandb/wandb/offline-run-20260102_184517-63x2u7jd
  0%|          | 0/465 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /dev/shm/slurm.indrisch.177487/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /dev/shm/slurm.indrisch.177487/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /dev/shm/slurm.indrisch.177487/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /dev/shm/slurm.indrisch.177487/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
  0%|          | 1/465 [01:43<13:24:02, 103.97s/it]                                                   {'loss': 1.6415, 'grad_norm': 0.07194175571203232, 'learning_rate': 0.0, 'epoch': 0.0}
  0%|          | 1/465 [01:43<13:24:02, 103.97s/it]  0%|          | 2/465 [03:10<12:04:51, 93.93s/it]                                                   {'loss': 1.6916, 'grad_norm': 0.09002505987882614, 'learning_rate': 2.1276595744680853e-06, 'epoch': 0.0}
  0%|          | 2/465 [03:10<12:04:51, 93.93s/it]  1%|          | 3/465 [05:10<13:32:47, 105.56s/it]                                                   {'loss': 1.716, 'grad_norm': 0.18125952780246735, 'learning_rate': 4.255319148936171e-06, 'epoch': 0.01}
  1%|          | 3/465 [05:10<13:32:47, 105.56s/it]  1%|          | 4/465 [06:55<13:29:15, 105.33s/it]                                                   {'loss': 1.7299, 'grad_norm': 0.11944915354251862, 'learning_rate': 6.3829787234042555e-06, 'epoch': 0.01}
  1%|          | 4/465 [06:55<13:29:15, 105.33s/it]  1%|          | 5/465 [08:38<13:22:24, 104.66s/it]                                                   {'loss': 1.636, 'grad_norm': 0.09151788800954819, 'learning_rate': 8.510638297872341e-06, 'epoch': 0.01}
  1%|          | 5/465 [08:38<13:22:24, 104.66s/it]  1%|▏         | 6/465 [10:05<12:34:39, 98.65s/it]                                                   {'loss': 1.7067, 'grad_norm': 0.11241075396537781, 'learning_rate': 1.0638297872340426e-05, 'epoch': 0.01}
  1%|▏         | 6/465 [10:05<12:34:39, 98.65s/it]  2%|▏         | 7/465 [11:27<11:50:54, 93.13s/it]                                                  {'loss': 1.733, 'grad_norm': 0.13827987015247345, 'learning_rate': 1.2765957446808511e-05, 'epoch': 0.02}
  2%|▏         | 7/465 [11:27<11:50:54, 93.13s/it]  2%|▏         | 8/465 [13:08<12:08:56, 95.70s/it]                                                  {'loss': 1.6341, 'grad_norm': 0.12921155989170074, 'learning_rate': 1.4893617021276596e-05, 'epoch': 0.02}
  2%|▏         | 8/465 [13:08<12:08:56, 95.70s/it]  2%|▏         | 9/465 [14:58<12:40:56, 100.12s/it]                                                   {'loss': 1.715, 'grad_norm': 0.13031333684921265, 'learning_rate': 1.7021276595744682e-05, 'epoch': 0.02}
  2%|▏         | 9/465 [14:58<12:40:56, 100.12s/it]  2%|▏         | 10/465 [16:28<12:14:40, 96.88s/it]                                                   {'loss': 1.7027, 'grad_norm': 0.1435389518737793, 'learning_rate': 1.9148936170212766e-05, 'epoch': 0.02}
  2%|▏         | 10/465 [16:28<12:14:40, 96.88s/it]  2%|▏         | 11/465 [17:55<11:50:22, 93.88s/it]                                                   {'loss': 1.6892, 'grad_norm': 0.1080687865614891, 'learning_rate': 2.1276595744680852e-05, 'epoch': 0.02}
  2%|▏         | 11/465 [17:55<11:50:22, 93.88s/it]  3%|▎         | 12/465 [19:28<11:46:50, 93.62s/it]                                                   {'loss': 1.6799, 'grad_norm': 0.14710821211338043, 'learning_rate': 2.340425531914894e-05, 'epoch': 0.03}
  3%|▎         | 12/465 [19:28<11:46:50, 93.62s/it]  3%|▎         | 13/465 [21:18<12:22:54, 98.62s/it]                                                   {'loss': 1.7278, 'grad_norm': 0.09013041853904724, 'learning_rate': 2.5531914893617022e-05, 'epoch': 0.03}
  3%|▎         | 13/465 [21:18<12:22:54, 98.62s/it]  3%|▎         | 14/465 [22:48<12:00:54, 95.91s/it]                                                   {'loss': 1.6661, 'grad_norm': 0.09630319476127625, 'learning_rate': 2.765957446808511e-05, 'epoch': 0.03}
  3%|▎         | 14/465 [22:48<12:00:54, 95.91s/it]  3%|▎         | 15/465 [24:36<12:28:43, 99.83s/it]                                                   {'loss': 1.6905, 'grad_norm': 0.14180518686771393, 'learning_rate': 2.9787234042553192e-05, 'epoch': 0.03}
  3%|▎         | 15/465 [24:36<12:28:43, 99.83s/it]  3%|▎         | 16/465 [25:56<11:42:23, 93.86s/it]                                                   {'loss': 1.5744, 'grad_norm': 0.11698102951049805, 'learning_rate': 3.191489361702128e-05, 'epoch': 0.03}
  3%|▎         | 16/465 [25:56<11:42:23, 93.86s/it]  4%|▎         | 17/465 [27:26<11:32:06, 92.69s/it]                                                   {'loss': 1.6562, 'grad_norm': 0.12315864115953445, 'learning_rate': 3.4042553191489365e-05, 'epoch': 0.04}
  4%|▎         | 17/465 [27:26<11:32:06, 92.69s/it]  4%|▍         | 18/465 [29:03<11:39:25, 93.88s/it]                                                   {'loss': 1.6012, 'grad_norm': 0.15035349130630493, 'learning_rate': 3.617021276595745e-05, 'epoch': 0.04}
  4%|▍         | 18/465 [29:03<11:39:25, 93.88s/it]  4%|▍         | 19/465 [30:57<12:22:36, 99.90s/it]                                                   {'loss': 1.5904, 'grad_norm': 0.1349155157804489, 'learning_rate': 3.829787234042553e-05, 'epoch': 0.04}
  4%|▍         | 19/465 [30:57<12:22:36, 99.90s/it]  4%|▍         | 20/465 [32:39<12:24:58, 100.45s/it]                                                    {'loss': 1.5488, 'grad_norm': 0.11925691366195679, 'learning_rate': 4.0425531914893614e-05, 'epoch': 0.04}
  4%|▍         | 20/465 [32:39<12:24:58, 100.45s/it]  5%|▍         | 21/465 [34:18<12:20:27, 100.06s/it]                                                    {'loss': 1.4946, 'grad_norm': 0.09639134258031845, 'learning_rate': 4.2553191489361704e-05, 'epoch': 0.05}
  5%|▍         | 21/465 [34:18<12:20:27, 100.06s/it]  5%|▍         | 22/465 [35:50<12:00:40, 97.61s/it]                                                    {'loss': 1.4485, 'grad_norm': 0.12089133262634277, 'learning_rate': 4.468085106382979e-05, 'epoch': 0.05}
  5%|▍         | 22/465 [35:50<12:00:40, 97.61s/it]  5%|▍         | 23/465 [37:27<11:57:26, 97.39s/it]                                                   {'loss': 1.5153, 'grad_norm': 0.09950968623161316, 'learning_rate': 4.680851063829788e-05, 'epoch': 0.05}
  5%|▍         | 23/465 [37:27<11:57:26, 97.39s/it]  5%|▌         | 24/465 [38:57<11:39:27, 95.17s/it]                                                   {'loss': 1.4699, 'grad_norm': 0.11986855417490005, 'learning_rate': 4.893617021276596e-05, 'epoch': 0.05}
  5%|▌         | 24/465 [38:57<11:39:27, 95.17s/it]  5%|▌         | 25/465 [40:44<12:05:35, 98.94s/it]                                                   {'loss': 1.4242, 'grad_norm': 0.11567366123199463, 'learning_rate': 5.1063829787234044e-05, 'epoch': 0.05}
  5%|▌         | 25/465 [40:44<12:05:35, 98.94s/it]  6%|▌         | 26/465 [42:28<12:14:13, 100.35s/it]                                                    {'loss': 1.3892, 'grad_norm': 0.11103086173534393, 'learning_rate': 5.319148936170213e-05, 'epoch': 0.06}
  6%|▌         | 26/465 [42:28<12:14:13, 100.35s/it]  6%|▌         | 27/465 [43:51<11:34:12, 95.10s/it]                                                    {'loss': 1.3483, 'grad_norm': 0.11313125491142273, 'learning_rate': 5.531914893617022e-05, 'epoch': 0.06}
  6%|▌         | 27/465 [43:51<11:34:12, 95.10s/it]  6%|▌         | 28/465 [45:22<11:24:41, 94.01s/it]                                                   {'loss': 1.4013, 'grad_norm': 0.12148572504520416, 'learning_rate': 5.744680851063831e-05, 'epoch': 0.06}
  6%|▌         | 28/465 [45:22<11:24:41, 94.01s/it]  6%|▌         | 29/465 [47:06<11:43:39, 96.83s/it]                                                   {'loss': 1.3464, 'grad_norm': 0.11369835585355759, 'learning_rate': 5.9574468085106384e-05, 'epoch': 0.06}
  6%|▌         | 29/465 [47:06<11:43:39, 96.83s/it]  6%|▋         | 30/465 [48:50<11:57:42, 98.99s/it]                                                   {'loss': 1.299, 'grad_norm': 0.10793481767177582, 'learning_rate': 6.170212765957447e-05, 'epoch': 0.06}
  6%|▋         | 30/465 [48:50<11:57:42, 98.99s/it]  7%|▋         | 31/465 [50:41<12:22:25, 102.64s/it]                                                    {'loss': 1.2383, 'grad_norm': 0.10709214210510254, 'learning_rate': 6.382978723404256e-05, 'epoch': 0.07}
  7%|▋         | 31/465 [50:41<12:22:25, 102.64s/it]  7%|▋         | 32/465 [52:21<12:14:54, 101.84s/it]                                                    {'loss': 1.2591, 'grad_norm': 0.11143825948238373, 'learning_rate': 6.595744680851063e-05, 'epoch': 0.07}
  7%|▋         | 32/465 [52:21<12:14:54, 101.84s/it]  7%|▋         | 33/465 [53:52<11:50:22, 98.66s/it]                                                    {'loss': 1.3835, 'grad_norm': 0.11527981609106064, 'learning_rate': 6.808510638297873e-05, 'epoch': 0.07}
  7%|▋         | 33/465 [53:52<11:50:22, 98.66s/it]  7%|▋         | 34/465 [55:21<11:26:40, 95.59s/it]                                                   {'loss': 1.3284, 'grad_norm': 0.1309167891740799, 'learning_rate': 7.021276595744681e-05, 'epoch': 0.07}
  7%|▋         | 34/465 [55:21<11:26:40, 95.59s/it]  8%|▊         | 35/465 [56:52<11:15:33, 94.26s/it]                                                   {'loss': 1.2508, 'grad_norm': 0.09849771857261658, 'learning_rate': 7.23404255319149e-05, 'epoch': 0.08}
  8%|▊         | 35/465 [56:52<11:15:33, 94.26s/it]  8%|▊         | 36/465 [58:27<11:16:15, 94.58s/it]                                                   {'loss': 1.2722, 'grad_norm': 0.11817745864391327, 'learning_rate': 7.446808510638298e-05, 'epoch': 0.08}
  8%|▊         | 36/465 [58:27<11:16:15, 94.58s/it]  8%|▊         | 37/465 [1:00:09<11:29:46, 96.70s/it]                                                     {'loss': 1.2323, 'grad_norm': 0.10928753763437271, 'learning_rate': 7.659574468085106e-05, 'epoch': 0.08}
  8%|▊         | 37/465 [1:00:09<11:29:46, 96.70s/it]  8%|▊         | 38/465 [1:01:48<11:34:33, 97.60s/it]                                                     {'loss': 1.1332, 'grad_norm': 0.15253706276416779, 'learning_rate': 7.872340425531916e-05, 'epoch': 0.08}
  8%|▊         | 38/465 [1:01:48<11:34:33, 97.60s/it]  8%|▊         | 39/465 [1:03:30<11:42:29, 98.94s/it]                                                     {'loss': 1.2632, 'grad_norm': 0.13591238856315613, 'learning_rate': 8.085106382978723e-05, 'epoch': 0.08}
  8%|▊         | 39/465 [1:03:30<11:42:29, 98.94s/it]  9%|▊         | 40/465 [1:05:10<11:42:25, 99.16s/it]                                                     {'loss': 1.19, 'grad_norm': 0.12105491757392883, 'learning_rate': 8.297872340425533e-05, 'epoch': 0.09}
  9%|▊         | 40/465 [1:05:10<11:42:25, 99.16s/it]  9%|▉         | 41/465 [1:06:42<11:25:33, 97.01s/it]                                                     {'loss': 1.252, 'grad_norm': 0.12465889751911163, 'learning_rate': 8.510638297872341e-05, 'epoch': 0.09}
  9%|▉         | 41/465 [1:06:42<11:25:33, 97.01s/it]  9%|▉         | 42/465 [1:08:11<11:05:58, 94.47s/it]                                                     {'loss': 1.184, 'grad_norm': 0.1263621300458908, 'learning_rate': 8.723404255319149e-05, 'epoch': 0.09}
  9%|▉         | 42/465 [1:08:11<11:05:58, 94.47s/it]  9%|▉         | 43/465 [1:09:45<11:04:35, 94.49s/it]                                                     {'loss': 1.0919, 'grad_norm': 0.14129024744033813, 'learning_rate': 8.936170212765958e-05, 'epoch': 0.09}
  9%|▉         | 43/465 [1:09:45<11:04:35, 94.49s/it]  9%|▉         | 44/465 [1:11:36<11:36:17, 99.23s/it]                                                     {'loss': 1.0856, 'grad_norm': 0.13005980849266052, 'learning_rate': 9.148936170212766e-05, 'epoch': 0.09}
  9%|▉         | 44/465 [1:11:36<11:36:17, 99.23s/it] 10%|▉         | 45/465 [1:13:20<11:46:38, 100.95s/it]                                                      {'loss': 1.2387, 'grad_norm': 0.13321812450885773, 'learning_rate': 9.361702127659576e-05, 'epoch': 0.1}
 10%|▉         | 45/465 [1:13:20<11:46:38, 100.95s/it] 10%|▉         | 46/465 [1:14:59<11:38:52, 100.08s/it]                                                      {'loss': 1.0315, 'grad_norm': 0.12065809220075607, 'learning_rate': 9.574468085106384e-05, 'epoch': 0.1}
 10%|▉         | 46/465 [1:14:59<11:38:52, 100.08s/it] 10%|█         | 47/465 [1:16:42<11:44:06, 101.07s/it]                                                      {'loss': 1.0803, 'grad_norm': 0.10052234679460526, 'learning_rate': 9.787234042553192e-05, 'epoch': 0.1}
 10%|█         | 47/465 [1:16:42<11:44:06, 101.07s/it] 10%|█         | 48/465 [1:18:28<11:52:24, 102.50s/it]                                                      {'loss': 1.0914, 'grad_norm': 0.12183511257171631, 'learning_rate': 0.0001, 'epoch': 0.1}
 10%|█         | 48/465 [1:18:28<11:52:24, 102.50s/it] 11%|█         | 49/465 [1:19:56<11:20:34, 98.16s/it]                                                      {'loss': 1.0994, 'grad_norm': 0.11896929144859314, 'learning_rate': 9.999858783596664e-05, 'epoch': 0.11}
 11%|█         | 49/465 [1:19:56<11:20:34, 98.16s/it] 11%|█         | 50/465 [1:21:46<11:43:48, 101.75s/it]                                                      {'loss': 1.2321, 'grad_norm': 0.13747058808803558, 'learning_rate': 9.999435142363484e-05, 'epoch': 0.11}
 11%|█         | 50/465 [1:21:46<11:43:48, 101.75s/it] 11%|█         | 51/465 [1:23:46<12:19:56, 107.24s/it]                                                      {'loss': 1.1279, 'grad_norm': 0.12066569179296494, 'learning_rate': 9.998729100230497e-05, 'epoch': 0.11}
 11%|█         | 51/465 [1:23:46<12:19:56, 107.24s/it] 11%|█         | 52/465 [1:25:31<12:13:36, 106.58s/it]                                                      {'loss': 1.0232, 'grad_norm': 0.13841429352760315, 'learning_rate': 9.997740697079594e-05, 'epoch': 0.11}
 11%|█         | 52/465 [1:25:31<12:13:36, 106.58s/it] 11%|█▏        | 53/465 [1:27:06<11:48:14, 103.14s/it]                                                      {'loss': 1.0287, 'grad_norm': 0.12302405387163162, 'learning_rate': 9.99646998874227e-05, 'epoch': 0.11}
 11%|█▏        | 53/465 [1:27:06<11:48:14, 103.14s/it] 12%|█▏        | 54/465 [1:28:38<11:24:05, 99.87s/it]                                                      {'loss': 1.0635, 'grad_norm': 0.11814753711223602, 'learning_rate': 9.994917046996472e-05, 'epoch': 0.12}
 12%|█▏        | 54/465 [1:28:38<11:24:05, 99.87s/it] 12%|█▏        | 55/465 [1:30:01<10:47:16, 94.72s/it]                                                     {'loss': 1.0131, 'grad_norm': 0.1375712901353836, 'learning_rate': 9.993081959562538e-05, 'epoch': 0.12}
 12%|█▏        | 55/465 [1:30:01<10:47:16, 94.72s/it] 12%|█▏        | 56/465 [1:31:41<10:56:15, 96.27s/it]                                                     {'loss': 1.0479, 'grad_norm': 0.12544213235378265, 'learning_rate': 9.990964830098246e-05, 'epoch': 0.12}
 12%|█▏        | 56/465 [1:31:41<10:56:15, 96.27s/it] 12%|█▏        | 57/465 [1:33:35<11:31:13, 101.65s/it]                                                      {'loss': 1.059, 'grad_norm': 0.12425429373979568, 'learning_rate': 9.98856577819296e-05, 'epoch': 0.12}
 12%|█▏        | 57/465 [1:33:35<11:31:13, 101.65s/it] 12%|█▏        | 58/465 [1:35:21<11:37:34, 102.84s/it]                                                      {'loss': 1.1113, 'grad_norm': 0.1234331876039505, 'learning_rate': 9.985884939360872e-05, 'epoch': 0.12}
 12%|█▏        | 58/465 [1:35:21<11:37:34, 102.84s/it] 13%|█▎        | 59/465 [1:37:07<11:42:30, 103.82s/it]                                                      {'loss': 1.0291, 'grad_norm': 0.1090601459145546, 'learning_rate': 9.98292246503335e-05, 'epoch': 0.13}
 13%|█▎        | 59/465 [1:37:07<11:42:30, 103.82s/it] 13%|█▎        | 60/465 [1:38:41<11:20:30, 100.82s/it]                                                      {'loss': 1.1127, 'grad_norm': 0.12226150184869766, 'learning_rate': 9.979678522550382e-05, 'epoch': 0.13}
 13%|█▎        | 60/465 [1:38:41<11:20:30, 100.82s/it] 13%|█▎        | 61/465 [1:40:13<11:01:24, 98.23s/it]                                                      {'loss': 1.055, 'grad_norm': 0.11807958036661148, 'learning_rate': 9.976153295151123e-05, 'epoch': 0.13}
 13%|█▎        | 61/465 [1:40:13<11:01:24, 98.23s/it] 13%|█▎        | 62/465 [1:41:47<10:52:15, 97.11s/it]                                                     {'loss': 1.0617, 'grad_norm': 0.1321229338645935, 'learning_rate': 9.972346981963546e-05, 'epoch': 0.13}
 13%|█▎        | 62/465 [1:41:47<10:52:15, 97.11s/it] 14%|█▎        | 63/465 [1:43:25<10:51:37, 97.26s/it]                                                     {'loss': 0.9746, 'grad_norm': 0.15026617050170898, 'learning_rate': 9.968259797993197e-05, 'epoch': 0.14}
 14%|█▎        | 63/465 [1:43:25<10:51:37, 97.26s/it] 14%|█▍        | 64/465 [1:44:54<10:32:41, 94.67s/it]                                                     {'loss': 1.0205, 'grad_norm': 0.15451709926128387, 'learning_rate': 9.963891974111042e-05, 'epoch': 0.14}
 14%|█▍        | 64/465 [1:44:54<10:32:41, 94.67s/it] 14%|█▍        | 65/465 [1:46:36<10:46:51, 97.03s/it]                                                     {'loss': 1.0147, 'grad_norm': 0.1418372392654419, 'learning_rate': 9.959243757040433e-05, 'epoch': 0.14}
 14%|█▍        | 65/465 [1:46:36<10:46:51, 97.03s/it] 14%|█▍        | 66/465 [1:48:14<10:46:28, 97.21s/it]                                                     {'loss': 1.0642, 'grad_norm': 0.13248880207538605, 'learning_rate': 9.954315409343169e-05, 'epoch': 0.14}
 14%|█▍        | 66/465 [1:48:14<10:46:28, 97.21s/it] 14%|█▍        | 67/465 [1:49:46<10:34:26, 95.65s/it]                                                     {'loss': 0.985, 'grad_norm': 0.1313670575618744, 'learning_rate': 9.949107209404665e-05, 'epoch': 0.14}
 14%|█▍        | 67/465 [1:49:46<10:34:26, 95.65s/it] 15%|█▍        | 68/465 [1:51:26<10:42:04, 97.04s/it]                                                     {'loss': 0.9931, 'grad_norm': 0.13465169072151184, 'learning_rate': 9.943619451418224e-05, 'epoch': 0.15}
 15%|█▍        | 68/465 [1:51:26<10:42:04, 97.04s/it] 15%|█▍        | 69/465 [1:53:03<10:39:49, 96.94s/it]                                                     {'loss': 0.9408, 'grad_norm': 0.16481226682662964, 'learning_rate': 9.937852445368427e-05, 'epoch': 0.15}
 15%|█▍        | 69/465 [1:53:03<10:39:49, 96.94s/it] 15%|█▌        | 70/465 [1:54:26<10:11:43, 92.92s/it]                                                     {'loss': 1.0344, 'grad_norm': 0.11582261323928833, 'learning_rate': 9.931806517013612e-05, 'epoch': 0.15}
 15%|█▌        | 70/465 [1:54:26<10:11:43, 92.92s/it] 15%|█▌        | 71/465 [1:56:15<10:42:05, 97.78s/it]                                                     {'loss': 0.9592, 'grad_norm': 0.13190758228302002, 'learning_rate': 9.925482007867484e-05, 'epoch': 0.15}
 15%|█▌        | 71/465 [1:56:15<10:42:05, 97.78s/it] 15%|█▌        | 72/465 [1:57:40<10:14:34, 93.83s/it]                                                     {'loss': 0.9966, 'grad_norm': 0.10879170149564743, 'learning_rate': 9.918879275179818e-05, 'epoch': 0.15}
 15%|█▌        | 72/465 [1:57:40<10:14:34, 93.83s/it] 16%|█▌        | 73/465 [1:59:15<10:15:49, 94.26s/it]                                                     {'loss': 0.9733, 'grad_norm': 0.12909859418869019, 'learning_rate': 9.911998691916275e-05, 'epoch': 0.16}
 16%|█▌        | 73/465 [1:59:15<10:15:49, 94.26s/it] 16%|█▌        | 74/465 [2:00:48<10:10:44, 93.72s/it]                                                     {'loss': 0.9543, 'grad_norm': 0.14468923211097717, 'learning_rate': 9.904840646737346e-05, 'epoch': 0.16}
 16%|█▌        | 74/465 [2:00:48<10:10:44, 93.72s/it] 16%|█▌        | 75/465 [2:02:13<9:53:14, 91.27s/it]                                                     {'loss': 1.002, 'grad_norm': 0.12843036651611328, 'learning_rate': 9.89740554397639e-05, 'epoch': 0.16}
 16%|█▌        | 75/465 [2:02:13<9:53:14, 91.27s/it] 16%|█▋        | 76/465 [2:03:52<10:05:16, 93.36s/it]                                                     {'loss': 1.0158, 'grad_norm': 0.10390694439411163, 'learning_rate': 9.889693803616793e-05, 'epoch': 0.16}
 16%|█▋        | 76/465 [2:03:52<10:05:16, 93.36s/it] 17%|█▋        | 77/465 [2:05:30<10:12:51, 94.77s/it]                                                     {'loss': 0.9603, 'grad_norm': 0.13365206122398376, 'learning_rate': 9.881705861268251e-05, 'epoch': 0.17}
 17%|█▋        | 77/465 [2:05:30<10:12:51, 94.77s/it] 17%|█▋        | 78/465 [2:07:06<10:14:38, 95.29s/it]                                                     {'loss': 1.0225, 'grad_norm': 0.1581254005432129, 'learning_rate': 9.873442168142159e-05, 'epoch': 0.17}
 17%|█▋        | 78/465 [2:07:06<10:14:38, 95.29s/it] 17%|█▋        | 79/465 [2:08:44<10:18:28, 96.13s/it]                                                     {'loss': 0.9051, 'grad_norm': 0.15646125376224518, 'learning_rate': 9.864903191026124e-05, 'epoch': 0.17}
 17%|█▋        | 79/465 [2:08:44<10:18:28, 96.13s/it] 17%|█▋        | 80/465 [2:10:35<10:45:32, 100.60s/it]                                                      {'loss': 0.9796, 'grad_norm': 0.13823041319847107, 'learning_rate': 9.856089412257606e-05, 'epoch': 0.17}
 17%|█▋        | 80/465 [2:10:35<10:45:32, 100.60s/it] 17%|█▋        | 81/465 [2:12:12<10:37:26, 99.60s/it]                                                      {'loss': 0.9193, 'grad_norm': 0.13795703649520874, 'learning_rate': 9.847001329696653e-05, 'epoch': 0.17}
 17%|█▋        | 81/465 [2:12:13<10:37:26, 99.60s/it] 18%|█▊        | 82/465 [2:14:03<10:55:51, 102.75s/it]                                                      {'loss': 1.0262, 'grad_norm': 0.14890088140964508, 'learning_rate': 9.837639456697803e-05, 'epoch': 0.18}
 18%|█▊        | 82/465 [2:14:03<10:55:51, 102.75s/it] 18%|█▊        | 83/465 [2:15:44<10:51:12, 102.28s/it]                                                      {'loss': 0.9099, 'grad_norm': 0.12444636225700378, 'learning_rate': 9.828004322081065e-05, 'epoch': 0.18}
 18%|█▊        | 83/465 [2:15:44<10:51:12, 102.28s/it] 18%|█▊        | 84/465 [2:17:16<10:31:10, 99.40s/it]                                                      {'loss': 0.939, 'grad_norm': 0.1265580803155899, 'learning_rate': 9.818096470102067e-05, 'epoch': 0.18}
 18%|█▊        | 84/465 [2:17:16<10:31:10, 99.40s/it] 18%|█▊        | 85/465 [2:18:29<9:38:11, 91.29s/it]                                                     {'loss': 1.0239, 'grad_norm': 0.14613111317157745, 'learning_rate': 9.807916460421293e-05, 'epoch': 0.18}
 18%|█▊        | 85/465 [2:18:29<9:38:11, 91.29s/it] 18%|█▊        | 86/465 [2:20:21<10:16:08, 97.54s/it]                                                     {'loss': 0.9942, 'grad_norm': 0.12229347229003906, 'learning_rate': 9.797464868072488e-05, 'epoch': 0.19}
 18%|█▊        | 86/465 [2:20:21<10:16:08, 97.54s/it] 19%|█▊        | 87/465 [2:22:04<10:24:55, 99.19s/it]                                                     {'loss': 0.9182, 'grad_norm': 0.14592985808849335, 'learning_rate': 9.78674228343016e-05, 'epoch': 0.19}
 19%|█▊        | 87/465 [2:22:04<10:24:55, 99.19s/it] 19%|█▉        | 88/465 [2:23:43<10:23:33, 99.24s/it]                                                     {'loss': 0.922, 'grad_norm': 0.12457947432994843, 'learning_rate': 9.775749312176248e-05, 'epoch': 0.19}
 19%|█▉        | 88/465 [2:23:43<10:23:33, 99.24s/it] 19%|█▉        | 89/465 [2:25:09<9:57:04, 95.28s/it]                                                     {'loss': 0.9031, 'grad_norm': 0.1348893642425537, 'learning_rate': 9.764486575265894e-05, 'epoch': 0.19}
 19%|█▉        | 89/465 [2:25:09<9:57:04, 95.28s/it] 19%|█▉        | 90/465 [2:26:32<9:31:22, 91.42s/it]                                                    {'loss': 0.8432, 'grad_norm': 0.14342276751995087, 'learning_rate': 9.752954708892377e-05, 'epoch': 0.19}
 19%|█▉        | 90/465 [2:26:32<9:31:22, 91.42s/it] 20%|█▉        | 91/465 [2:28:05<9:33:20, 91.98s/it]                                                    {'loss': 0.9403, 'grad_norm': 0.13441318273544312, 'learning_rate': 9.741154364451178e-05, 'epoch': 0.2}
 20%|█▉        | 91/465 [2:28:05<9:33:20, 91.98s/it] 20%|█▉        | 92/465 [2:29:42<9:40:09, 93.32s/it]                                                    {'loss': 0.9332, 'grad_norm': 0.1369144171476364, 'learning_rate': 9.729086208503174e-05, 'epoch': 0.2}
 20%|█▉        | 92/465 [2:29:42<9:40:09, 93.32s/it] 20%|██        | 93/465 [2:31:38<10:22:01, 100.33s/it]                                                      {'loss': 0.9251, 'grad_norm': 0.13028225302696228, 'learning_rate': 9.716750922736997e-05, 'epoch': 0.2}
 20%|██        | 93/465 [2:31:38<10:22:01, 100.33s/it] 20%|██        | 94/465 [2:33:05<9:54:41, 96.18s/it]                                                      {'loss': 0.9196, 'grad_norm': 0.13032758235931396, 'learning_rate': 9.704149203930521e-05, 'epoch': 0.2}
 20%|██        | 94/465 [2:33:05<9:54:41, 96.18s/it] 20%|██        | 95/465 [2:34:49<10:07:58, 98.59s/it]                                                     {'loss': 0.8905, 'grad_norm': 0.12355832010507584, 'learning_rate': 9.691281763911513e-05, 'epoch': 0.2}
 20%|██        | 95/465 [2:34:49<10:07:58, 98.59s/it] 21%|██        | 96/465 [2:36:40<10:29:33, 102.37s/it]                                                      {'loss': 0.9206, 'grad_norm': 0.14071401953697205, 'learning_rate': 9.678149329517408e-05, 'epoch': 0.21}
 21%|██        | 96/465 [2:36:40<10:29:33, 102.37s/it] 21%|██        | 97/465 [2:38:09<10:02:54, 98.30s/it]                                                      {'loss': 0.8673, 'grad_norm': 0.1522558182477951, 'learning_rate': 9.664752642554271e-05, 'epoch': 0.21}
 21%|██        | 97/465 [2:38:09<10:02:54, 98.30s/it] 21%|██        | 98/465 [2:39:51<10:08:16, 99.45s/it]                                                     {'loss': 0.9107, 'grad_norm': 0.14342723786830902, 'learning_rate': 9.651092459754877e-05, 'epoch': 0.21}
 21%|██        | 98/465 [2:39:51<10:08:16, 99.45s/it] 21%|██▏       | 99/465 [2:41:50<10:42:53, 105.39s/it]                                                      {'loss': 0.8602, 'grad_norm': 0.17246152460575104, 'learning_rate': 9.637169552735986e-05, 'epoch': 0.21}
 21%|██▏       | 99/465 [2:41:50<10:42:53, 105.39s/it] 22%|██▏       | 100/465 [2:43:22<10:16:06, 101.28s/it]                                                       {'loss': 0.9099, 'grad_norm': 0.139085590839386, 'learning_rate': 9.622984707954732e-05, 'epoch': 0.22}
 22%|██▏       | 100/465 [2:43:22<10:16:06, 101.28s/it] 22%|██▏       | 101/465 [2:45:23<10:49:40, 107.09s/it]                                                       {'loss': 1.0106, 'grad_norm': 0.10753092914819717, 'learning_rate': 9.608538726664224e-05, 'epoch': 0.22}
 22%|██▏       | 101/465 [2:45:23<10:49:40, 107.09s/it] 22%|██▏       | 102/465 [2:47:06<10:41:04, 105.96s/it]                                                       {'loss': 0.8831, 'grad_norm': 0.1436910629272461, 'learning_rate': 9.59383242486827e-05, 'epoch': 0.22}
 22%|██▏       | 102/465 [2:47:06<10:41:04, 105.96s/it] 22%|██▏       | 103/465 [2:48:56<10:45:59, 107.07s/it]                                                       {'loss': 1.0154, 'grad_norm': 0.16437605023384094, 'learning_rate': 9.578866633275288e-05, 'epoch': 0.22}
 22%|██▏       | 103/465 [2:48:56<10:45:59, 107.07s/it] 22%|██▏       | 104/465 [2:50:27<10:15:21, 102.27s/it]                                                       {'loss': 0.962, 'grad_norm': 0.1293114274740219, 'learning_rate': 9.56364219725138e-05, 'epoch': 0.22}
 22%|██▏       | 104/465 [2:50:27<10:15:21, 102.27s/it] 23%|██▎       | 105/465 [2:51:58<9:54:29, 99.08s/it]                                                       {'loss': 0.8862, 'grad_norm': 0.12136611342430115, 'learning_rate': 9.548159976772592e-05, 'epoch': 0.23}
 23%|██▎       | 105/465 [2:51:58<9:54:29, 99.08s/it] 23%|██▎       | 106/465 [2:53:43<10:03:41, 100.90s/it]                                                       {'loss': 0.9709, 'grad_norm': 0.10978154838085175, 'learning_rate': 9.532420846376315e-05, 'epoch': 0.23}
 23%|██▎       | 106/465 [2:53:43<10:03:41, 100.90s/it] 23%|██▎       | 107/465 [2:55:13<9:41:11, 97.41s/it]                                                       {'loss': 0.9235, 'grad_norm': 0.14965899288654327, 'learning_rate': 9.516425695111906e-05, 'epoch': 0.23}
 23%|██▎       | 107/465 [2:55:13<9:41:11, 97.41s/it] 23%|██▎       | 108/465 [2:56:41<9:22:37, 94.56s/it]                                                     {'loss': 0.9355, 'grad_norm': 0.13601815700531006, 'learning_rate': 9.500175426490454e-05, 'epoch': 0.23}
 23%|██▎       | 108/465 [2:56:41<9:22:37, 94.56s/it] 23%|██▎       | 109/465 [2:58:10<9:11:12, 92.90s/it]                                                     {'loss': 0.9646, 'grad_norm': 0.1335771232843399, 'learning_rate': 9.483670958433759e-05, 'epoch': 0.23}
 23%|██▎       | 109/465 [2:58:10<9:11:12, 92.90s/it] 24%|██▎       | 110/465 [2:59:41<9:07:20, 92.51s/it]                                                     {'loss': 0.9618, 'grad_norm': 0.123247891664505, 'learning_rate': 9.466913223222467e-05, 'epoch': 0.24}
 24%|██▎       | 110/465 [2:59:41<9:07:20, 92.51s/it] 24%|██▍       | 111/465 [3:01:31<9:36:30, 97.71s/it]                                                     {'loss': 0.9494, 'grad_norm': 0.11364315450191498, 'learning_rate': 9.449903167443414e-05, 'epoch': 0.24}
 24%|██▍       | 111/465 [3:01:31<9:36:30, 97.71s/it] 24%|██▍       | 112/465 [3:03:06<9:30:05, 96.90s/it]                                                     {'loss': 0.9276, 'grad_norm': 0.14663945138454437, 'learning_rate': 9.432641751936162e-05, 'epoch': 0.24}
 24%|██▍       | 112/465 [3:03:06<9:30:05, 96.90s/it] 24%|██▍       | 113/465 [3:04:59<9:57:07, 101.78s/it]                                                      {'loss': 0.929, 'grad_norm': 0.14436087012290955, 'learning_rate': 9.415129951738713e-05, 'epoch': 0.24}
 24%|██▍       | 113/465 [3:04:59<9:57:07, 101.78s/it] 25%|██▍       | 114/465 [3:06:43<9:59:17, 102.44s/it]                                                      {'loss': 0.8932, 'grad_norm': 0.16216258704662323, 'learning_rate': 9.397368756032445e-05, 'epoch': 0.25}
 25%|██▍       | 114/465 [3:06:43<9:59:17, 102.44s/it] 25%|██▍       | 115/465 [3:08:27<9:59:00, 102.69s/it]                                                      {'loss': 0.9182, 'grad_norm': 0.1344306468963623, 'learning_rate': 9.37935916808623e-05, 'epoch': 0.25}
 25%|██▍       | 115/465 [3:08:27<9:59:00, 102.69s/it] 25%|██▍       | 116/465 [3:10:05<9:50:09, 101.46s/it]                                                      {'loss': 0.8946, 'grad_norm': 0.14261527359485626, 'learning_rate': 9.36110220519976e-05, 'epoch': 0.25}
 25%|██▍       | 116/465 [3:10:05<9:50:09, 101.46s/it] 25%|██▌       | 117/465 [3:11:56<10:04:00, 104.14s/it]                                                       {'loss': 0.9, 'grad_norm': 0.1415359079837799, 'learning_rate': 9.34259889864609e-05, 'epoch': 0.25}
 25%|██▌       | 117/465 [3:11:56<10:04:00, 104.14s/it] 25%|██▌       | 118/465 [3:13:36<9:56:36, 103.16s/it]                                                       {'loss': 0.927, 'grad_norm': 0.13524305820465088, 'learning_rate': 9.32385029361338e-05, 'epoch': 0.25}
 25%|██▌       | 118/465 [3:13:36<9:56:36, 103.16s/it] 26%|██▌       | 119/465 [3:15:22<9:58:15, 103.74s/it]                                                      {'loss': 0.9722, 'grad_norm': 0.1523279994726181, 'learning_rate': 9.304857449145859e-05, 'epoch': 0.26}
 26%|██▌       | 119/465 [3:15:22<9:58:15, 103.74s/it] 26%|██▌       | 120/465 [3:16:46<9:24:01, 98.09s/it]                                                      {'loss': 0.9532, 'grad_norm': 0.14855512976646423, 'learning_rate': 9.285621438083998e-05, 'epoch': 0.26}
 26%|██▌       | 120/465 [3:16:46<9:24:01, 98.09s/it] 26%|██▌       | 121/465 [3:18:38<9:46:15, 102.26s/it]                                                      {'loss': 0.9438, 'grad_norm': 0.13091951608657837, 'learning_rate': 9.26614334700392e-05, 'epoch': 0.26}
 26%|██▌       | 121/465 [3:18:38<9:46:15, 102.26s/it] 26%|██▌       | 122/465 [3:20:54<10:41:32, 112.22s/it]                                                       {'loss': 0.9173, 'grad_norm': 0.18057827651500702, 'learning_rate': 9.246424276156007e-05, 'epoch': 0.26}
 26%|██▌       | 122/465 [3:20:54<10:41:32, 112.22s/it] 26%|██▋       | 123/465 [3:22:16<9:48:16, 103.21s/it]                                                       {'loss': 0.9262, 'grad_norm': 0.17111659049987793, 'learning_rate': 9.226465339402768e-05, 'epoch': 0.26}
 26%|██▋       | 123/465 [3:22:16<9:48:16, 103.21s/it] 27%|██▋       | 124/465 [3:23:59<9:46:04, 103.12s/it]                                                      {'loss': 0.9002, 'grad_norm': 0.14881578087806702, 'learning_rate': 9.206267664155907e-05, 'epoch': 0.27}
 27%|██▋       | 124/465 [3:23:59<9:46:04, 103.12s/it] 27%|██▋       | 125/465 [3:25:26<9:16:41, 98.24s/it]                                                      {'loss': 0.8483, 'grad_norm': 0.1465662568807602, 'learning_rate': 9.185832391312644e-05, 'epoch': 0.27}
 27%|██▋       | 125/465 [3:25:26<9:16:41, 98.24s/it] 27%|██▋       | 126/465 [3:26:57<9:02:16, 95.98s/it]                                                     {'loss': 0.8999, 'grad_norm': 0.13528959453105927, 'learning_rate': 9.165160675191272e-05, 'epoch': 0.27}
 27%|██▋       | 126/465 [3:26:57<9:02:16, 95.98s/it] 27%|██▋       | 127/465 [3:28:34<9:03:55, 96.55s/it]                                                     {'loss': 0.9688, 'grad_norm': 0.17432819306850433, 'learning_rate': 9.144253683465953e-05, 'epoch': 0.27}
 27%|██▋       | 127/465 [3:28:34<9:03:55, 96.55s/it] 28%|██▊       | 128/465 [3:30:10<9:01:11, 96.35s/it]                                                     {'loss': 0.8823, 'grad_norm': 0.1406538337469101, 'learning_rate': 9.123112597100759e-05, 'epoch': 0.28}
 28%|██▊       | 128/465 [3:30:10<9:01:11, 96.35s/it] 28%|██▊       | 129/465 [3:32:08<9:34:54, 102.66s/it]                                                      {'loss': 0.8512, 'grad_norm': 0.1412602961063385, 'learning_rate': 9.101738610282956e-05, 'epoch': 0.28}
 28%|██▊       | 129/465 [3:32:08<9:34:54, 102.66s/it] 28%|██▊       | 130/465 [3:33:54<9:40:08, 103.91s/it]                                                      {'loss': 0.9289, 'grad_norm': 0.13116805255413055, 'learning_rate': 9.080132930355567e-05, 'epoch': 0.28}
 28%|██▊       | 130/465 [3:33:54<9:40:08, 103.91s/it] 28%|██▊       | 131/465 [3:35:28<9:21:29, 100.87s/it]                                                      {'loss': 0.9371, 'grad_norm': 0.1428663581609726, 'learning_rate': 9.058296777749154e-05, 'epoch': 0.28}
 28%|██▊       | 131/465 [3:35:28<9:21:29, 100.87s/it] 28%|██▊       | 132/465 [3:37:11<9:22:51, 101.42s/it]                                                      {'loss': 0.9477, 'grad_norm': 0.155864879488945, 'learning_rate': 9.03623138591289e-05, 'epoch': 0.28}
 28%|██▊       | 132/465 [3:37:11<9:22:51, 101.42s/it] 29%|██▊       | 133/465 [3:38:40<9:00:51, 97.75s/it]                                                      {'loss': 0.9033, 'grad_norm': 0.1247321218252182, 'learning_rate': 9.013938001244885e-05, 'epoch': 0.29}
 29%|██▊       | 133/465 [3:38:40<9:00:51, 97.75s/it] 29%|██▉       | 134/465 [3:40:12<8:49:57, 96.06s/it]                                                     {'loss': 0.9045, 'grad_norm': 0.14352329075336456, 'learning_rate': 8.99141788302178e-05, 'epoch': 0.29}
 29%|██▉       | 134/465 [3:40:12<8:49:57, 96.06s/it] 29%|██▉       | 135/465 [3:41:41<8:36:33, 93.92s/it]                                                     {'loss': 0.8554, 'grad_norm': 0.1415300965309143, 'learning_rate': 8.968672303327615e-05, 'epoch': 0.29}
 29%|██▉       | 135/465 [3:41:41<8:36:33, 93.92s/it] 29%|██▉       | 136/465 [3:43:29<8:57:19, 97.99s/it]                                                     {'loss': 0.8888, 'grad_norm': 0.16187922656536102, 'learning_rate': 8.945702546981969e-05, 'epoch': 0.29}
 29%|██▉       | 136/465 [3:43:29<8:57:19, 97.99s/it] 29%|██▉       | 137/465 [3:44:56<8:38:48, 94.90s/it]                                                     {'loss': 0.785, 'grad_norm': 0.14404848217964172, 'learning_rate': 8.922509911467395e-05, 'epoch': 0.29}
 29%|██▉       | 137/465 [3:44:56<8:38:48, 94.90s/it] 30%|██▉       | 138/465 [3:46:46<9:00:49, 99.23s/it]                                                     {'loss': 0.8238, 'grad_norm': 0.1256919950246811, 'learning_rate': 8.899095706856122e-05, 'epoch': 0.3}
 30%|██▉       | 138/465 [3:46:46<9:00:49, 99.23s/it] 30%|██▉       | 139/465 [3:48:20<8:50:24, 97.62s/it]                                                     {'loss': 0.8711, 'grad_norm': 0.1425367146730423, 'learning_rate': 8.875461255736055e-05, 'epoch': 0.3}
 30%|██▉       | 139/465 [3:48:20<8:50:24, 97.62s/it] 30%|███       | 140/465 [3:50:00<8:52:59, 98.40s/it]                                                     {'loss': 0.9493, 'grad_norm': 0.1212160512804985, 'learning_rate': 8.851607893136065e-05, 'epoch': 0.3}
 30%|███       | 140/465 [3:50:00<8:52:59, 98.40s/it] 30%|███       | 141/465 [3:51:31<8:39:56, 96.29s/it]                                                     {'loss': 0.8591, 'grad_norm': 0.1642446368932724, 'learning_rate': 8.827536966450584e-05, 'epoch': 0.3}
 30%|███       | 141/465 [3:51:31<8:39:56, 96.29s/it] 31%|███       | 142/465 [3:53:13<8:46:59, 97.89s/it]                                                     {'loss': 0.9195, 'grad_norm': 0.18784932792186737, 'learning_rate': 8.803249835363485e-05, 'epoch': 0.31}
 31%|███       | 142/465 [3:53:13<8:46:59, 97.89s/it] 31%|███       | 143/465 [3:54:46<8:37:31, 96.43s/it]                                                     {'loss': 0.9509, 'grad_norm': 0.16314731538295746, 'learning_rate': 8.778747871771292e-05, 'epoch': 0.31}
 31%|███       | 143/465 [3:54:46<8:37:31, 96.43s/it] 31%|███       | 144/465 [3:56:18<8:28:28, 95.04s/it]                                                     {'loss': 0.8602, 'grad_norm': 0.1552242487668991, 'learning_rate': 8.754032459705672e-05, 'epoch': 0.31}
 31%|███       | 144/465 [3:56:18<8:28:28, 95.04s/it] 31%|███       | 145/465 [3:57:57<8:33:26, 96.27s/it]                                                     {'loss': 0.8713, 'grad_norm': 0.1647910177707672, 'learning_rate': 8.729104995255263e-05, 'epoch': 0.31}
 31%|███       | 145/465 [3:57:57<8:33:26, 96.27s/it] 31%|███▏      | 146/465 [3:59:42<8:46:18, 98.99s/it]                                                     {'loss': 0.93, 'grad_norm': 0.14366790652275085, 'learning_rate': 8.703966886486819e-05, 'epoch': 0.31}
 31%|███▏      | 146/465 [3:59:42<8:46:18, 98.99s/it] 32%|███▏      | 147/465 [4:01:27<8:53:28, 100.66s/it]                                                      {'loss': 0.8333, 'grad_norm': 0.15002037584781647, 'learning_rate': 8.678619553365659e-05, 'epoch': 0.32}
 32%|███▏      | 147/465 [4:01:27<8:53:28, 100.66s/it] 32%|███▏      | 148/465 [4:02:50<8:25:03, 95.60s/it]                                                      {'loss': 0.8923, 'grad_norm': 0.16244393587112427, 'learning_rate': 8.653064427675471e-05, 'epoch': 0.32}
 32%|███▏      | 148/465 [4:02:50<8:25:03, 95.60s/it] 32%|███▏      | 149/465 [4:04:41<8:47:42, 100.20s/it]                                                      {'loss': 0.8882, 'grad_norm': 0.11924299597740173, 'learning_rate': 8.62730295293743e-05, 'epoch': 0.32}
 32%|███▏      | 149/465 [4:04:41<8:47:42, 100.20s/it] 32%|███▏      | 150/465 [4:06:18<8:40:58, 99.23s/it]                                                      {'loss': 0.8792, 'grad_norm': 0.11428327113389969, 'learning_rate': 8.601336584328659e-05, 'epoch': 0.32}
 32%|███▏      | 150/465 [4:06:18<8:40:58, 99.23s/it] 32%|███▏      | 151/465 [4:07:54<8:34:10, 98.25s/it]                                                     {'loss': 0.8789, 'grad_norm': 0.1339397132396698, 'learning_rate': 8.57516678860003e-05, 'epoch': 0.32}
 32%|███▏      | 151/465 [4:07:54<8:34:10, 98.25s/it] 33%|███▎      | 152/465 [4:09:22<8:16:27, 95.17s/it]                                                     {'loss': 0.8562, 'grad_norm': 0.15701065957546234, 'learning_rate': 8.548795043993317e-05, 'epoch': 0.33}
 33%|███▎      | 152/465 [4:09:22<8:16:27, 95.17s/it] 33%|███▎      | 153/465 [4:11:01<8:20:25, 96.24s/it]                                                     {'loss': 0.9642, 'grad_norm': 0.14530929923057556, 'learning_rate': 8.522222840157686e-05, 'epoch': 0.33}
 33%|███▎      | 153/465 [4:11:01<8:20:25, 96.24s/it] 33%|███▎      | 154/465 [4:12:40<8:23:01, 97.05s/it]                                                     {'loss': 0.8928, 'grad_norm': 0.1388304978609085, 'learning_rate': 8.495451678065562e-05, 'epoch': 0.33}
 33%|███▎      | 154/465 [4:12:40<8:23:01, 97.05s/it] 33%|███▎      | 155/465 [4:14:08<8:07:27, 94.35s/it]                                                     {'loss': 0.8887, 'grad_norm': 0.14002782106399536, 'learning_rate': 8.468483069927832e-05, 'epoch': 0.33}
 33%|███▎      | 155/465 [4:14:08<8:07:27, 94.35s/it] 34%|███▎      | 156/465 [4:15:49<8:16:32, 96.42s/it]                                                     {'loss': 0.9403, 'grad_norm': 0.13366802036762238, 'learning_rate': 8.441318539108434e-05, 'epoch': 0.34}
 34%|███▎      | 156/465 [4:15:49<8:16:32, 96.42s/it] 34%|███▍      | 157/465 [4:17:07<7:45:38, 90.71s/it]                                                     {'loss': 0.9471, 'grad_norm': 0.15361376106739044, 'learning_rate': 8.413959620038305e-05, 'epoch': 0.34}
 34%|███▍      | 157/465 [4:17:07<7:45:38, 90.71s/it] 34%|███▍      | 158/465 [4:18:48<8:00:51, 93.98s/it]                                                     {'loss': 0.8633, 'grad_norm': 0.14881622791290283, 'learning_rate': 8.386407858128706e-05, 'epoch': 0.34}
 34%|███▍      | 158/465 [4:18:48<8:00:51, 93.98s/it] 34%|███▍      | 159/465 [4:20:26<8:04:38, 95.03s/it]                                                     {'loss': 0.8077, 'grad_norm': 0.155231773853302, 'learning_rate': 8.358664809683925e-05, 'epoch': 0.34}
 34%|███▍      | 159/465 [4:20:26<8:04:38, 95.03s/it] 34%|███▍      | 160/465 [4:21:57<7:57:57, 94.03s/it]                                                     {'loss': 0.803, 'grad_norm': 0.14636605978012085, 'learning_rate': 8.330732041813367e-05, 'epoch': 0.34}
 34%|███▍      | 160/465 [4:21:57<7:57:57, 94.03s/it] 35%|███▍      | 161/465 [4:23:15<7:31:18, 89.07s/it]                                                     {'loss': 0.9377, 'grad_norm': 0.12898258864879608, 'learning_rate': 8.302611132343043e-05, 'epoch': 0.35}
 35%|███▍      | 161/465 [4:23:15<7:31:18, 89.07s/it] 35%|███▍      | 162/465 [4:24:42<7:27:13, 88.56s/it]                                                     {'loss': 0.8774, 'grad_norm': 0.15108364820480347, 'learning_rate': 8.274303669726426e-05, 'epoch': 0.35}
 35%|███▍      | 162/465 [4:24:42<7:27:13, 88.56s/it] 35%|███▌      | 163/465 [4:26:11<7:25:54, 88.59s/it]                                                     {'loss': 0.9027, 'grad_norm': 0.15289707481861115, 'learning_rate': 8.245811252954741e-05, 'epoch': 0.35}
 35%|███▌      | 163/465 [4:26:11<7:25:54, 88.59s/it] 35%|███▌      | 164/465 [4:27:51<7:41:00, 91.90s/it]                                                     {'loss': 0.8677, 'grad_norm': 0.1202683225274086, 'learning_rate': 8.217135491466636e-05, 'epoch': 0.35}
 35%|███▌      | 164/465 [4:27:51<7:41:00, 91.90s/it] 35%|███▌      | 165/465 [4:29:35<7:57:51, 95.57s/it]                                                     {'loss': 0.8642, 'grad_norm': 0.12937474250793457, 'learning_rate': 8.18827800505727e-05, 'epoch': 0.36}
 35%|███▌      | 165/465 [4:29:35<7:57:51, 95.57s/it] 36%|███▌      | 166/465 [4:31:33<8:29:35, 102.26s/it]                                                      {'loss': 0.9297, 'grad_norm': 0.16310615837574005, 'learning_rate': 8.159240423786821e-05, 'epoch': 0.36}
 36%|███▌      | 166/465 [4:31:33<8:29:35, 102.26s/it] 36%|███▌      | 167/465 [4:33:15<8:28:31, 102.39s/it]                                                      {'loss': 0.7994, 'grad_norm': 0.16867785155773163, 'learning_rate': 8.130024387888402e-05, 'epoch': 0.36}
 36%|███▌      | 167/465 [4:33:15<8:28:31, 102.39s/it] 36%|███▌      | 168/465 [4:34:37<7:55:43, 96.11s/it]                                                      {'loss': 0.837, 'grad_norm': 0.13920290768146515, 'learning_rate': 8.100631547675417e-05, 'epoch': 0.36}
 36%|███▌      | 168/465 [4:34:37<7:55:43, 96.11s/it] 36%|███▋      | 169/465 [4:35:59<7:34:18, 92.09s/it]                                                     {'loss': 0.9085, 'grad_norm': 0.13233527541160583, 'learning_rate': 8.07106356344834e-05, 'epoch': 0.36}
 36%|███▋      | 169/465 [4:35:59<7:34:18, 92.09s/it] 37%|███▋      | 170/465 [4:37:26<7:24:00, 90.31s/it]                                                     {'loss': 0.9044, 'grad_norm': 0.1586165726184845, 'learning_rate': 8.041322105400922e-05, 'epoch': 0.37}
 37%|███▋      | 170/465 [4:37:26<7:24:00, 90.31s/it] 37%|███▋      | 171/465 [4:39:09<7:41:12, 94.12s/it]                                                     {'loss': 0.8637, 'grad_norm': 0.1398227959871292, 'learning_rate': 8.011408853525862e-05, 'epoch': 0.37}
 37%|███▋      | 171/465 [4:39:09<7:41:12, 94.12s/it] 37%|███▋      | 172/465 [4:40:53<7:55:19, 97.33s/it]                                                     {'loss': 0.9136, 'grad_norm': 0.12503743171691895, 'learning_rate': 7.981325497519892e-05, 'epoch': 0.37}
 37%|███▋      | 172/465 [4:40:53<7:55:19, 97.33s/it] 37%|███▋      | 173/465 [4:42:22<7:41:18, 94.79s/it]                                                     {'loss': 0.8327, 'grad_norm': 0.1726609617471695, 'learning_rate': 7.951073736688347e-05, 'epoch': 0.37}
 37%|███▋      | 173/465 [4:42:22<7:41:18, 94.79s/it] 37%|███▋      | 174/465 [4:44:00<7:44:22, 95.75s/it]                                                     {'loss': 0.8816, 'grad_norm': 0.1589946299791336, 'learning_rate': 7.920655279849172e-05, 'epoch': 0.37}
 37%|███▋      | 174/465 [4:44:00<7:44:22, 95.75s/it] 38%|███▊      | 175/465 [4:45:28<7:30:36, 93.23s/it]                                                     {'loss': 0.9215, 'grad_norm': 0.16179592907428741, 'learning_rate': 7.890071845236395e-05, 'epoch': 0.38}
 38%|███▊      | 175/465 [4:45:28<7:30:36, 93.23s/it] 38%|███▊      | 176/465 [4:47:08<7:39:06, 95.32s/it]                                                     {'loss': 0.944, 'grad_norm': 0.15648683905601501, 'learning_rate': 7.859325160403071e-05, 'epoch': 0.38}
 38%|███▊      | 176/465 [4:47:08<7:39:06, 95.32s/it] 38%|███▊      | 177/465 [4:48:35<7:25:46, 92.87s/it]                                                     {'loss': 0.8226, 'grad_norm': 0.14198951423168182, 'learning_rate': 7.828416962123699e-05, 'epoch': 0.38}
 38%|███▊      | 177/465 [4:48:35<7:25:46, 92.87s/it] 38%|███▊      | 178/465 [4:50:01<7:14:18, 90.79s/it]                                                     {'loss': 0.9105, 'grad_norm': 0.1479557752609253, 'learning_rate': 7.797348996296116e-05, 'epoch': 0.38}
 38%|███▊      | 178/465 [4:50:01<7:14:18, 90.79s/it] 38%|███▊      | 179/465 [4:51:27<7:06:20, 89.44s/it]                                                     {'loss': 0.8737, 'grad_norm': 0.16714511811733246, 'learning_rate': 7.766123017842878e-05, 'epoch': 0.39}
 38%|███▊      | 179/465 [4:51:27<7:06:20, 89.44s/it] 39%|███▊      | 180/465 [4:53:04<7:15:59, 91.79s/it]                                                     {'loss': 0.9062, 'grad_norm': 0.16591404378414154, 'learning_rate': 7.734740790612136e-05, 'epoch': 0.39}
 39%|███▊      | 180/465 [4:53:04<7:15:59, 91.79s/it] 39%|███▉      | 181/465 [4:54:32<7:09:05, 90.65s/it]                                                     {'loss': 0.8638, 'grad_norm': 0.14788921177387238, 'learning_rate': 7.703204087277988e-05, 'epoch': 0.39}
 39%|███▉      | 181/465 [4:54:32<7:09:05, 90.65s/it] 39%|███▉      | 182/465 [4:56:08<7:14:58, 92.22s/it]                                                     {'loss': 0.9702, 'grad_norm': 0.14446093142032623, 'learning_rate': 7.671514689240365e-05, 'epoch': 0.39}
 39%|███▉      | 182/465 [4:56:08<7:14:58, 92.22s/it] 39%|███▉      | 183/465 [4:57:28<6:55:34, 88.42s/it]                                                     {'loss': 0.8028, 'grad_norm': 0.1386719048023224, 'learning_rate': 7.639674386524393e-05, 'epoch': 0.39}
 39%|███▉      | 183/465 [4:57:28<6:55:34, 88.42s/it] 40%|███▉      | 184/465 [4:58:58<6:55:52, 88.80s/it]                                                     {'loss': 0.861, 'grad_norm': 0.15632183849811554, 'learning_rate': 7.607684977679283e-05, 'epoch': 0.4}
 40%|███▉      | 184/465 [4:58:58<6:55:52, 88.80s/it] 40%|███▉      | 185/465 [5:00:26<6:54:29, 88.82s/it]                                                     {'loss': 0.896, 'grad_norm': 0.1266615241765976, 'learning_rate': 7.57554826967674e-05, 'epoch': 0.4}
 40%|███▉      | 185/465 [5:00:26<6:54:29, 88.82s/it] 40%|████      | 186/465 [5:01:59<6:58:28, 89.99s/it]                                                     {'loss': 0.7601, 'grad_norm': 0.14681176841259003, 'learning_rate': 7.543266077808892e-05, 'epoch': 0.4}
 40%|████      | 186/465 [5:01:59<6:58:28, 89.99s/it] 40%|████      | 187/465 [5:03:40<7:11:51, 93.21s/it]                                                     {'loss': 0.8276, 'grad_norm': 0.17437244951725006, 'learning_rate': 7.510840225585749e-05, 'epoch': 0.4}
 40%|████      | 187/465 [5:03:40<7:11:51, 93.21s/it] 40%|████      | 188/465 [5:05:10<7:06:10, 92.31s/it]                                                     {'loss': 0.8202, 'grad_norm': 0.13100272417068481, 'learning_rate': 7.478272544632204e-05, 'epoch': 0.4}
 40%|████      | 188/465 [5:05:10<7:06:10, 92.31s/it] 41%|████      | 189/465 [5:07:04<7:34:29, 98.80s/it]                                                     {'loss': 0.8679, 'grad_norm': 0.14233431220054626, 'learning_rate': 7.44556487458456e-05, 'epoch': 0.41}
 41%|████      | 189/465 [5:07:04<7:34:29, 98.80s/it] 41%|████      | 190/465 [5:08:36<7:23:53, 96.85s/it]                                                     {'loss': 0.8674, 'grad_norm': 0.16742989420890808, 'learning_rate': 7.412719062986632e-05, 'epoch': 0.41}
 41%|████      | 190/465 [5:08:36<7:23:53, 96.85s/it] 41%|████      | 191/465 [5:10:07<7:13:11, 94.86s/it]                                                     {'loss': 0.9659, 'grad_norm': 0.151905819773674, 'learning_rate': 7.379736965185368e-05, 'epoch': 0.41}
 41%|████      | 191/465 [5:10:07<7:13:11, 94.86s/it] 41%|████▏     | 192/465 [5:11:26<6:50:12, 90.15s/it]                                                     {'loss': 0.8391, 'grad_norm': 0.15735864639282227, 'learning_rate': 7.34662044422606e-05, 'epoch': 0.41}
 41%|████▏     | 192/465 [5:11:26<6:50:12, 90.15s/it] 42%|████▏     | 193/465 [5:13:10<7:08:08, 94.44s/it]                                                     {'loss': 0.838, 'grad_norm': 0.16747243702411652, 'learning_rate': 7.313371370747103e-05, 'epoch': 0.42}
 42%|████▏     | 193/465 [5:13:10<7:08:08, 94.44s/it] 42%|████▏     | 194/465 [5:14:38<6:58:12, 92.59s/it]                                                     {'loss': 0.8856, 'grad_norm': 0.13928073644638062, 'learning_rate': 7.279991622874319e-05, 'epoch': 0.42}
 42%|████▏     | 194/465 [5:14:38<6:58:12, 92.59s/it] 42%|████▏     | 195/465 [5:16:13<6:59:03, 93.12s/it]                                                     {'loss': 0.8484, 'grad_norm': 0.17696493864059448, 'learning_rate': 7.246483086114889e-05, 'epoch': 0.42}
 42%|████▏     | 195/465 [5:16:13<6:59:03, 93.12s/it] 42%|████▏     | 196/465 [5:17:50<7:03:33, 94.47s/it]                                                     {'loss': 0.8157, 'grad_norm': 0.15849685668945312, 'learning_rate': 7.212847653250829e-05, 'epoch': 0.42}
 42%|████▏     | 196/465 [5:17:50<7:03:33, 94.47s/it] 42%|████▏     | 197/465 [5:19:38<7:19:43, 98.44s/it]                                                     {'loss': 0.9489, 'grad_norm': 0.18988937139511108, 'learning_rate': 7.179087224232077e-05, 'epoch': 0.42}
 42%|████▏     | 197/465 [5:19:38<7:19:43, 98.44s/it] 43%|████▎     | 198/465 [5:21:25<7:29:33, 101.02s/it]                                                      {'loss': 0.8233, 'grad_norm': 0.16487658023834229, 'learning_rate': 7.145203706069183e-05, 'epoch': 0.43}
 43%|████▎     | 198/465 [5:21:25<7:29:33, 101.02s/it] 43%|████▎     | 199/465 [5:23:23<7:50:38, 106.16s/it]                                                      {'loss': 0.8503, 'grad_norm': 0.17065075039863586, 'learning_rate': 7.111199012725567e-05, 'epoch': 0.43}
 43%|████▎     | 199/465 [5:23:23<7:50:38, 106.16s/it] 43%|████▎     | 200/465 [5:25:00<7:36:16, 103.31s/it]                                                      {'loss': 0.9194, 'grad_norm': 0.14185944199562073, 'learning_rate': 7.077075065009433e-05, 'epoch': 0.43}
 43%|████▎     | 200/465 [5:25:00<7:36:16, 103.31s/it]
***** Running Evaluation *****

***** Running Evaluation *****

***** Running Evaluation *****
[INFO|trainer.py:4643] 2026-01-03 00:10:20,179 >> 
***** Running Evaluation *****
  Num examples = 3305
  Num examples = 3305
  Batch size = 1
  Batch size = 1
  Num examples = 3305
  Batch size = 1
[INFO|trainer.py:4645] 2026-01-03 00:10:20,179 >>   Num examples = 3305
[INFO|trainer.py:4648] 2026-01-03 00:10:20,179 >>   Batch size = 1

  0%|          | 0/827 [00:00<?, ?it/s][A
  0%|          | 2/827 [00:04<28:08,  2.05s/it][A
  0%|          | 3/827 [00:06<31:14,  2.27s/it][A
  0%|          | 4/827 [00:09<35:04,  2.56s/it][A
  1%|          | 5/827 [00:13<40:40,  2.97s/it][A
  1%|          | 6/827 [00:17<43:22,  3.17s/it][A
  1%|          | 7/827 [00:20<46:48,  3.42s/it][A
  1%|          | 8/827 [00:26<55:48,  4.09s/it][A
  1%|          | 9/827 [00:33<1:06:17,  4.86s/it][A
  1%|          | 10/827 [00:40<1:15:11,  5.52s/it][A
  1%|▏         | 11/827 [00:43<1:06:02,  4.86s/it][A
  1%|▏         | 12/827 [00:46<57:13,  4.21s/it]  [A
  2%|▏         | 13/827 [00:49<52:24,  3.86s/it][A
  2%|▏         | 14/827 [00:52<51:25,  3.80s/it][A
  2%|▏         | 15/827 [00:57<53:07,  3.93s/it][A
  2%|▏         | 16/827 [01:00<51:10,  3.79s/it][A
  2%|▏         | 17/827 [01:03<48:00,  3.56s/it][A
  2%|▏         | 18/827 [01:06<46:56,  3.48s/it][A
  2%|▏         | 19/827 [01:11<51:37,  3.83s/it][A
  2%|▏         | 20/827 [01:16<57:38,  4.29s/it][A
  3%|▎         | 21/827 [01:20<55:28,  4.13s/it][A
  3%|▎         | 22/827 [01:25<57:15,  4.27s/it][A
  3%|▎         | 23/827 [01:28<51:38,  3.85s/it][A
  3%|▎         | 24/827 [01:31<51:25,  3.84s/it][A
  3%|▎         | 25/827 [01:36<53:55,  4.03s/it][A
  3%|▎         | 26/827 [01:41<59:27,  4.45s/it][A
  3%|▎         | 27/827 [01:46<58:57,  4.42s/it][A
  3%|▎         | 28/827 [01:49<52:32,  3.95s/it][A
  4%|▎         | 29/827 [01:53<55:14,  4.15s/it][A
  4%|▎         | 30/827 [01:58<58:07,  4.38s/it][A
  4%|▎         | 31/827 [02:02<56:02,  4.22s/it][A
  4%|▍         | 32/827 [02:06<54:51,  4.14s/it][A
  4%|▍         | 33/827 [02:11<59:04,  4.46s/it][A
  4%|▍         | 34/827 [02:14<53:45,  4.07s/it][A
  4%|▍         | 35/827 [02:18<50:47,  3.85s/it][A
  4%|▍         | 36/827 [02:20<46:07,  3.50s/it][A
  4%|▍         | 37/827 [02:25<50:56,  3.87s/it][A
  5%|▍         | 38/827 [02:30<55:10,  4.20s/it][A
  5%|▍         | 39/827 [02:35<56:42,  4.32s/it][A
  5%|▍         | 40/827 [02:40<59:05,  4.50s/it][A
  5%|▍         | 41/827 [02:43<53:20,  4.07s/it][A
  5%|▌         | 42/827 [02:46<51:41,  3.95s/it][A
  5%|▌         | 43/827 [02:50<50:08,  3.84s/it][A
  5%|▌         | 44/827 [02:53<47:58,  3.68s/it][A
  5%|▌         | 45/827 [02:57<49:58,  3.84s/it][A
  6%|▌         | 46/827 [03:03<55:15,  4.24s/it][A
  6%|▌         | 47/827 [03:07<56:48,  4.37s/it][A
  6%|▌         | 48/827 [03:11<52:51,  4.07s/it][A
  6%|▌         | 49/827 [03:14<50:00,  3.86s/it][A
  6%|▌         | 50/827 [03:17<46:07,  3.56s/it][A
  6%|▌         | 51/827 [03:22<51:13,  3.96s/it][A
  6%|▋         | 52/827 [03:28<58:24,  4.52s/it][A
  6%|▋         | 53/827 [03:30<51:23,  3.98s/it][A
  7%|▋         | 54/827 [03:33<47:03,  3.65s/it][A
  7%|▋         | 55/827 [03:36<44:38,  3.47s/it][A
  7%|▋         | 56/827 [03:39<43:20,  3.37s/it][A
  7%|▋         | 57/827 [03:43<44:16,  3.45s/it][A
  7%|▋         | 58/827 [03:46<42:33,  3.32s/it][A
  7%|▋         | 59/827 [03:49<42:27,  3.32s/it][A
  7%|▋         | 60/827 [03:53<45:20,  3.55s/it][A
  7%|▋         | 61/827 [03:58<50:35,  3.96s/it][A
  7%|▋         | 62/827 [04:01<46:08,  3.62s/it][A
  8%|▊         | 63/827 [04:04<43:11,  3.39s/it][A
  8%|▊         | 64/827 [04:07<41:49,  3.29s/it][A
  8%|▊         | 65/827 [04:10<41:21,  3.26s/it][A
  8%|▊         | 66/827 [04:15<47:59,  3.78s/it][A
  8%|▊         | 67/827 [04:20<51:37,  4.08s/it][A
  8%|▊         | 68/827 [04:22<45:38,  3.61s/it][A
  8%|▊         | 69/827 [04:26<44:23,  3.51s/it][A
  8%|▊         | 70/827 [04:29<41:33,  3.29s/it][A
  9%|▊         | 71/827 [04:33<44:19,  3.52s/it][A
  9%|▊         | 72/827 [04:37<48:13,  3.83s/it][A
  9%|▉         | 73/827 [04:41<46:17,  3.68s/it][A
  9%|▉         | 74/827 [04:46<52:42,  4.20s/it][A
  9%|▉         | 75/827 [04:51<55:01,  4.39s/it][A
  9%|▉         | 76/827 [04:54<49:39,  3.97s/it][A
  9%|▉         | 77/827 [04:57<45:47,  3.66s/it][A
  9%|▉         | 78/827 [05:00<43:19,  3.47s/it][A
 10%|▉         | 79/827 [05:02<40:31,  3.25s/it][A
 10%|▉         | 80/827 [05:07<44:22,  3.56s/it][A
 10%|▉         | 81/827 [05:11<47:48,  3.84s/it][A
 10%|▉         | 82/827 [05:15<49:10,  3.96s/it][A
 10%|█         | 83/827 [05:19<49:22,  3.98s/it][A
 10%|█         | 84/827 [05:23<49:23,  3.99s/it][A
 10%|█         | 85/827 [05:28<49:36,  4.01s/it][A
 10%|█         | 86/827 [05:31<46:05,  3.73s/it][A
 11%|█         | 87/827 [05:37<54:35,  4.43s/it][A
 11%|█         | 88/827 [05:46<1:14:15,  6.03s/it][A
 11%|█         | 89/827 [05:54<1:18:48,  6.41s/it][A
 11%|█         | 90/827 [05:59<1:13:40,  6.00s/it][A
 11%|█         | 91/827 [06:02<1:04:16,  5.24s/it][A
 11%|█         | 92/827 [06:06<57:57,  4.73s/it]  [A
 11%|█         | 93/827 [06:09<50:54,  4.16s/it][A
 11%|█▏        | 94/827 [06:12<46:59,  3.85s/it][A
 11%|█▏        | 95/827 [06:15<43:51,  3.60s/it][A
 12%|█▏        | 96/827 [06:17<40:35,  3.33s/it][A
 12%|█▏        | 97/827 [06:20<37:02,  3.04s/it][A
 12%|█▏        | 98/827 [06:22<35:07,  2.89s/it][A
 12%|█▏        | 99/827 [06:25<33:55,  2.80s/it][A
 12%|█▏        | 100/827 [06:27<31:47,  2.62s/it][A
 12%|█▏        | 101/827 [06:31<34:56,  2.89s/it][A
 12%|█▏        | 102/827 [06:34<35:50,  2.97s/it][A
 12%|█▏        | 103/827 [06:37<35:21,  2.93s/it][A
 13%|█▎        | 104/827 [06:40<35:00,  2.91s/it][A
 13%|█▎        | 105/827 [06:43<36:02,  2.99s/it][A
 13%|█▎        | 106/827 [06:49<47:23,  3.94s/it][A
 13%|█▎        | 107/827 [06:55<55:47,  4.65s/it][A
 13%|█▎        | 108/827 [07:00<55:13,  4.61s/it][A
 13%|█▎        | 109/827 [07:03<51:35,  4.31s/it][A
 13%|█▎        | 110/827 [07:07<48:39,  4.07s/it][A
 13%|█▎        | 111/827 [07:12<51:33,  4.32s/it][A
 14%|█▎        | 112/827 [07:18<56:49,  4.77s/it][A
 14%|█▎        | 113/827 [07:21<53:39,  4.51s/it][A
 14%|█▍        | 114/827 [07:25<49:28,  4.16s/it][A
 14%|█▍        | 115/827 [07:28<46:17,  3.90s/it][A
 14%|█▍        | 116/827 [07:32<45:23,  3.83s/it][A
 14%|█▍        | 117/827 [07:36<47:19,  4.00s/it][A
 14%|█▍        | 118/827 [07:42<52:21,  4.43s/it][A
 14%|█▍        | 119/827 [07:47<54:55,  4.65s/it][A
 15%|█▍        | 120/827 [07:51<54:24,  4.62s/it][A
 15%|█▍        | 121/827 [07:55<49:30,  4.21s/it][A
 15%|█▍        | 122/827 [07:58<45:35,  3.88s/it][A
 15%|█▍        | 123/827 [08:00<41:15,  3.52s/it][A
 15%|█▍        | 124/827 [08:04<40:32,  3.46s/it][A
 15%|█▌        | 125/827 [08:08<41:51,  3.58s/it][A
 15%|█▌        | 126/827 [08:11<40:59,  3.51s/it][A
 15%|█▌        | 127/827 [08:14<39:34,  3.39s/it][A
 15%|█▌        | 128/827 [08:18<40:30,  3.48s/it][A
 16%|█▌        | 129/827 [08:20<37:46,  3.25s/it][A
 16%|█▌        | 130/827 [08:24<38:58,  3.36s/it][A
 16%|█▌        | 131/827 [08:28<40:44,  3.51s/it][A
 16%|█▌        | 132/827 [08:32<44:16,  3.82s/it][A
 16%|█▌        | 133/827 [08:37<46:20,  4.01s/it][A
 16%|█▌        | 134/827 [08:40<44:13,  3.83s/it][A
 16%|█▋        | 135/827 [08:45<47:00,  4.08s/it][A
 16%|█▋        | 136/827 [08:51<53:35,  4.65s/it][A
 17%|█▋        | 137/827 [08:54<48:19,  4.20s/it][A
 17%|█▋        | 138/827 [09:00<54:00,  4.70s/it][A
 17%|█▋        | 139/827 [09:07<1:00:34,  5.28s/it][A
 17%|█▋        | 140/827 [09:10<53:54,  4.71s/it]  [A
 17%|█▋        | 141/827 [09:14<50:50,  4.45s/it][A
 17%|█▋        | 142/827 [09:17<46:30,  4.07s/it][A
 17%|█▋        | 143/827 [09:20<44:13,  3.88s/it][A
 17%|█▋        | 144/827 [09:23<40:22,  3.55s/it][A
 18%|█▊        | 145/827 [09:27<40:50,  3.59s/it][A
 18%|█▊        | 146/827 [09:31<41:47,  3.68s/it][A
 18%|█▊        | 147/827 [09:34<40:19,  3.56s/it][A
 18%|█▊        | 148/827 [09:37<38:58,  3.44s/it][A
 18%|█▊        | 149/827 [09:40<37:57,  3.36s/it][A
 18%|█▊        | 150/827 [09:43<35:28,  3.14s/it][A
 18%|█▊        | 151/827 [09:46<33:26,  2.97s/it][A
 18%|█▊        | 152/827 [09:49<33:52,  3.01s/it][A
 19%|█▊        | 153/827 [09:52<34:06,  3.04s/it][A
 19%|█▊        | 154/827 [09:56<36:46,  3.28s/it][A
 19%|█▊        | 155/827 [10:01<42:16,  3.77s/it][A
 19%|█▉        | 156/827 [10:05<45:08,  4.04s/it][A
 19%|█▉        | 157/827 [10:08<42:26,  3.80s/it][A
 19%|█▉        | 158/827 [10:11<39:43,  3.56s/it][A
 19%|█▉        | 159/827 [10:16<42:13,  3.79s/it][A
 19%|█▉        | 160/827 [10:20<43:37,  3.93s/it][A
 19%|█▉        | 161/827 [10:23<39:20,  3.54s/it][A
 20%|█▉        | 162/827 [10:26<39:54,  3.60s/it][A
 20%|█▉        | 163/827 [10:30<40:45,  3.68s/it][A
 20%|█▉        | 164/827 [10:34<39:54,  3.61s/it][A
 20%|█▉        | 165/827 [10:37<37:57,  3.44s/it][A
 20%|██        | 166/827 [10:42<45:02,  4.09s/it][A
 20%|██        | 167/827 [10:51<1:00:19,  5.48s/it][A
 20%|██        | 168/827 [10:56<59:15,  5.40s/it]  [A
 20%|██        | 169/827 [10:59<51:50,  4.73s/it][A
 21%|██        | 170/827 [11:03<49:09,  4.49s/it][A
 21%|██        | 171/827 [11:07<46:51,  4.29s/it][A
 21%|██        | 172/827 [11:10<41:43,  3.82s/it][A
 21%|██        | 173/827 [11:13<37:58,  3.48s/it][A
 21%|██        | 174/827 [11:16<36:21,  3.34s/it][A
 21%|██        | 175/827 [11:19<37:28,  3.45s/it][A
 21%|██▏       | 176/827 [11:22<34:59,  3.23s/it][A
 21%|██▏       | 177/827 [11:25<33:21,  3.08s/it][A
 22%|██▏       | 178/827 [11:29<36:17,  3.36s/it][A
 22%|██▏       | 179/827 [11:34<42:51,  3.97s/it][A
 22%|██▏       | 180/827 [11:38<42:35,  3.95s/it][A
 22%|██▏       | 181/827 [11:42<41:30,  3.86s/it][A
 22%|██▏       | 182/827 [11:46<43:53,  4.08s/it][A
 22%|██▏       | 183/827 [11:49<40:30,  3.77s/it][A
 22%|██▏       | 184/827 [11:54<42:19,  3.95s/it][A
 22%|██▏       | 185/827 [11:59<44:57,  4.20s/it][A
 22%|██▏       | 186/827 [12:02<41:21,  3.87s/it][A
 23%|██▎       | 187/827 [12:04<37:48,  3.55s/it][A
 23%|██▎       | 188/827 [12:08<38:33,  3.62s/it][A
 23%|██▎       | 189/827 [12:13<42:03,  3.95s/it][A
 23%|██▎       | 190/827 [12:17<41:36,  3.92s/it][A
 23%|██▎       | 191/827 [12:21<41:27,  3.91s/it][A
 23%|██▎       | 192/827 [12:25<43:15,  4.09s/it][A
 23%|██▎       | 193/827 [12:29<42:32,  4.03s/it][A
 23%|██▎       | 194/827 [12:32<39:07,  3.71s/it][A
 24%|██▎       | 195/827 [12:35<36:57,  3.51s/it][A
 24%|██▎       | 196/827 [12:38<33:49,  3.22s/it][A
 24%|██▍       | 197/827 [12:40<32:16,  3.07s/it][A
 24%|██▍       | 198/827 [12:44<32:53,  3.14s/it][A
 24%|██▍       | 199/827 [12:47<33:40,  3.22s/it][A
 24%|██▍       | 200/827 [12:51<36:51,  3.53s/it][A
 24%|██▍       | 201/827 [12:56<41:01,  3.93s/it][A
 24%|██▍       | 202/827 [13:01<44:08,  4.24s/it][A
 25%|██▍       | 203/827 [13:06<45:37,  4.39s/it][A
 25%|██▍       | 204/827 [13:12<49:30,  4.77s/it][A
 25%|██▍       | 205/827 [13:15<44:58,  4.34s/it][A
 25%|██▍       | 206/827 [13:18<41:26,  4.00s/it][A
 25%|██▌       | 207/827 [13:21<39:06,  3.78s/it][A
 25%|██▌       | 208/827 [13:24<37:03,  3.59s/it][A
 25%|██▌       | 209/827 [13:28<35:38,  3.46s/it][A
 25%|██▌       | 210/827 [13:31<34:41,  3.37s/it][A
 26%|██▌       | 211/827 [13:36<38:47,  3.78s/it][A
 26%|██▌       | 212/827 [13:40<41:53,  4.09s/it][A
 26%|██▌       | 213/827 [13:46<45:33,  4.45s/it][A
 26%|██▌       | 214/827 [13:49<42:04,  4.12s/it][A
 26%|██▌       | 215/827 [13:52<39:00,  3.82s/it][A
 26%|██▌       | 216/827 [13:56<37:36,  3.69s/it][A
 26%|██▌       | 217/827 [13:59<38:12,  3.76s/it][A
 26%|██▋       | 218/827 [14:03<38:12,  3.77s/it][A
 26%|██▋       | 219/827 [14:06<35:38,  3.52s/it][A
 27%|██▋       | 220/827 [14:10<38:03,  3.76s/it][A
 27%|██▋       | 221/827 [14:16<43:22,  4.29s/it][A
 27%|██▋       | 222/827 [14:19<39:52,  3.95s/it][A
 27%|██▋       | 223/827 [14:23<37:57,  3.77s/it][A
 27%|██▋       | 224/827 [14:26<35:30,  3.53s/it][A
 27%|██▋       | 225/827 [14:29<34:13,  3.41s/it][A
 27%|██▋       | 226/827 [14:31<31:52,  3.18s/it][A
 27%|██▋       | 227/827 [14:36<35:22,  3.54s/it][A
 28%|██▊       | 228/827 [14:41<39:58,  4.00s/it][A
 28%|██▊       | 229/827 [14:44<37:32,  3.77s/it][A
 28%|██▊       | 230/827 [14:48<37:31,  3.77s/it][A
 28%|██▊       | 231/827 [14:51<36:07,  3.64s/it][A
 28%|██▊       | 232/827 [14:54<34:40,  3.50s/it][A
 28%|██▊       | 233/827 [14:59<37:45,  3.81s/it][A
 28%|██▊       | 234/827 [15:04<42:14,  4.27s/it][A
 28%|██▊       | 235/827 [15:09<44:04,  4.47s/it][A
 29%|██▊       | 236/827 [15:13<42:04,  4.27s/it][A
 29%|██▊       | 237/827 [15:18<45:53,  4.67s/it][A
 29%|██▉       | 238/827 [15:24<48:22,  4.93s/it][A
 29%|██▉       | 239/827 [15:29<47:46,  4.87s/it][A
 29%|██▉       | 240/827 [15:34<50:13,  5.13s/it][A
 29%|██▉       | 241/827 [15:40<51:41,  5.29s/it][A
 29%|██▉       | 242/827 [15:44<47:44,  4.90s/it][A
 29%|██▉       | 243/827 [15:48<45:20,  4.66s/it][A
 30%|██▉       | 244/827 [15:54<48:07,  4.95s/it][A
 30%|██▉       | 245/827 [15:57<43:41,  4.50s/it][A
 30%|██▉       | 246/827 [16:01<41:09,  4.25s/it][A
 30%|██▉       | 247/827 [16:05<41:09,  4.26s/it][A
 30%|██▉       | 248/827 [16:12<47:45,  4.95s/it][A
 30%|███       | 249/827 [16:16<46:16,  4.80s/it][A
 30%|███       | 250/827 [16:19<39:13,  4.08s/it][A
 30%|███       | 251/827 [16:22<38:10,  3.98s/it][A
 30%|███       | 252/827 [16:27<39:41,  4.14s/it][A
 31%|███       | 253/827 [16:30<36:00,  3.76s/it][A
 31%|███       | 254/827 [16:35<40:08,  4.20s/it][A
 31%|███       | 255/827 [16:42<47:51,  5.02s/it][A
 31%|███       | 256/827 [16:48<50:03,  5.26s/it][A
 31%|███       | 257/827 [16:52<47:23,  4.99s/it][A
 31%|███       | 258/827 [16:56<44:04,  4.65s/it][A
 31%|███▏      | 259/827 [17:02<47:31,  5.02s/it][A
 31%|███▏      | 260/827 [17:08<49:32,  5.24s/it][A
 32%|███▏      | 261/827 [17:11<45:29,  4.82s/it][A
 32%|███▏      | 262/827 [17:17<47:47,  5.08s/it][A
 32%|███▏      | 263/827 [17:23<50:45,  5.40s/it][A
 32%|███▏      | 264/827 [17:26<42:50,  4.57s/it][A
 32%|███▏      | 265/827 [17:29<37:23,  3.99s/it][A
 32%|███▏      | 266/827 [17:32<34:51,  3.73s/it][A
 32%|███▏      | 267/827 [17:35<33:00,  3.54s/it][A
 32%|███▏      | 268/827 [17:38<31:17,  3.36s/it][A
 33%|███▎      | 269/827 [17:42<33:37,  3.62s/it][A
 33%|███▎      | 270/827 [17:47<36:41,  3.95s/it][A
 33%|███▎      | 271/827 [17:50<34:10,  3.69s/it][A
 33%|███▎      | 272/827 [17:53<33:07,  3.58s/it][A
 33%|███▎      | 273/827 [17:59<38:42,  4.19s/it][A
 33%|███▎      | 274/827 [18:07<50:02,  5.43s/it][A
 33%|███▎      | 275/827 [18:12<48:12,  5.24s/it][A
 33%|███▎      | 276/827 [18:16<44:01,  4.79s/it][A
 33%|███▎      | 277/827 [18:19<40:52,  4.46s/it][A
 34%|███▎      | 278/827 [18:24<40:17,  4.40s/it][A
 34%|███▎      | 279/827 [18:27<36:53,  4.04s/it][A
 34%|███▍      | 280/827 [18:30<33:45,  3.70s/it][A
 34%|███▍      | 281/827 [18:33<33:59,  3.74s/it][A
 34%|███▍      | 282/827 [18:37<33:22,  3.67s/it][A
 34%|███▍      | 283/827 [18:40<32:11,  3.55s/it][A
 34%|███▍      | 284/827 [18:44<32:54,  3.64s/it][A
 34%|███▍      | 285/827 [18:48<34:38,  3.84s/it][A
 35%|███▍      | 286/827 [18:53<37:20,  4.14s/it][A
 35%|███▍      | 287/827 [18:56<32:42,  3.63s/it][A
 35%|███▍      | 288/827 [18:59<30:56,  3.44s/it][A
 35%|███▍      | 289/827 [19:02<29:54,  3.34s/it][A
 35%|███▌      | 290/827 [19:05<29:07,  3.25s/it][A
 35%|███▌      | 291/827 [19:08<27:35,  3.09s/it][A
 35%|███▌      | 292/827 [19:11<28:16,  3.17s/it][A
 35%|███▌      | 293/827 [19:14<27:19,  3.07s/it][A
 36%|███▌      | 294/827 [19:17<28:25,  3.20s/it][A
 36%|███▌      | 295/827 [19:21<29:04,  3.28s/it][A
 36%|███▌      | 296/827 [19:25<31:34,  3.57s/it][A
 36%|███▌      | 297/827 [19:29<32:01,  3.63s/it][A
 36%|███▌      | 298/827 [19:32<30:05,  3.41s/it][A
 36%|███▌      | 299/827 [19:35<29:21,  3.34s/it][A
 36%|███▋      | 300/827 [19:38<28:30,  3.25s/it][A
 36%|███▋      | 301/827 [19:41<27:50,  3.18s/it][A
 37%|███▋      | 302/827 [19:44<28:24,  3.25s/it][A
 37%|███▋      | 303/827 [19:47<25:50,  2.96s/it][A
 37%|███▋      | 304/827 [19:50<26:31,  3.04s/it][A
 37%|███▋      | 305/827 [19:54<29:37,  3.41s/it][A
 37%|███▋      | 306/827 [19:58<31:17,  3.60s/it][A
 37%|███▋      | 307/827 [20:01<30:08,  3.48s/it][A
 37%|███▋      | 308/827 [20:04<28:47,  3.33s/it][A
 37%|███▋      | 309/827 [20:08<28:55,  3.35s/it][A
 37%|███▋      | 310/827 [20:10<26:59,  3.13s/it][A
 38%|███▊      | 311/827 [20:13<25:37,  2.98s/it][A
 38%|███▊      | 312/827 [20:16<26:58,  3.14s/it][A
 38%|███▊      | 313/827 [20:21<29:41,  3.47s/it][A
 38%|███▊      | 314/827 [20:24<29:41,  3.47s/it][A
 38%|███▊      | 315/827 [20:29<32:09,  3.77s/it][A
 38%|███▊      | 316/827 [20:33<33:53,  3.98s/it][A
 38%|███▊      | 317/827 [20:38<36:27,  4.29s/it][A
 38%|███▊      | 318/827 [20:42<36:30,  4.30s/it][A
 39%|███▊      | 319/827 [20:46<35:27,  4.19s/it][A
 39%|███▊      | 320/827 [20:50<33:10,  3.93s/it][A
 39%|███▉      | 321/827 [20:54<34:32,  4.10s/it][A
 39%|███▉      | 322/827 [20:58<34:04,  4.05s/it][A
 39%|███▉      | 323/827 [21:02<33:39,  4.01s/it][A
 39%|███▉      | 324/827 [21:07<35:15,  4.21s/it][A
 39%|███▉      | 325/827 [21:10<32:20,  3.87s/it][A
 39%|███▉      | 326/827 [21:13<31:40,  3.79s/it][A
 40%|███▉      | 327/827 [21:18<33:46,  4.05s/it][A
 40%|███▉      | 328/827 [21:22<34:14,  4.12s/it][A
 40%|███▉      | 329/827 [21:28<39:07,  4.71s/it][A
 40%|███▉      | 330/827 [21:33<38:04,  4.60s/it][A
 40%|████      | 331/827 [21:37<38:14,  4.63s/it][A
 40%|████      | 332/827 [21:41<36:48,  4.46s/it][A
 40%|████      | 333/827 [21:46<38:08,  4.63s/it][A
 40%|████      | 334/827 [21:50<36:21,  4.42s/it][A
 41%|████      | 335/827 [21:54<33:57,  4.14s/it][A
 41%|████      | 336/827 [21:57<30:32,  3.73s/it][A
 41%|████      | 337/827 [22:00<29:08,  3.57s/it][A
 41%|████      | 338/827 [22:03<28:01,  3.44s/it][A
 41%|████      | 339/827 [22:06<26:59,  3.32s/it][A
 41%|████      | 340/827 [22:10<27:15,  3.36s/it][A
 41%|████      | 341/827 [22:13<28:32,  3.52s/it][A
 41%|████▏     | 342/827 [22:17<27:28,  3.40s/it][A
 41%|████▏     | 343/827 [22:20<26:46,  3.32s/it][A
 42%|████▏     | 344/827 [22:24<29:20,  3.65s/it][A
 42%|████▏     | 345/827 [22:29<32:39,  4.07s/it][A
 42%|████▏     | 346/827 [22:33<31:56,  3.98s/it][A
 42%|████▏     | 347/827 [22:37<30:59,  3.87s/it][A
 42%|████▏     | 348/827 [22:39<28:36,  3.58s/it][A
 42%|████▏     | 349/827 [22:44<30:37,  3.85s/it][A
 42%|████▏     | 350/827 [22:48<32:23,  4.08s/it][A
 42%|████▏     | 351/827 [22:52<30:38,  3.86s/it][A
 43%|████▎     | 352/827 [22:56<31:04,  3.92s/it][A
 43%|████▎     | 353/827 [22:59<29:35,  3.75s/it][A
 43%|████▎     | 354/827 [23:02<28:19,  3.59s/it][A
 43%|████▎     | 355/827 [23:07<30:26,  3.87s/it][A
 43%|████▎     | 356/827 [23:12<32:26,  4.13s/it][A
 43%|████▎     | 357/827 [23:15<31:03,  3.96s/it][A
 43%|████▎     | 358/827 [23:18<28:16,  3.62s/it][A
 43%|████▎     | 359/827 [23:22<27:47,  3.56s/it][A
 44%|████▎     | 360/827 [23:26<30:10,  3.88s/it][A
 44%|████▎     | 361/827 [23:30<30:10,  3.88s/it][A
 44%|████▍     | 362/827 [23:32<26:38,  3.44s/it][A
 44%|████▍     | 363/827 [23:37<28:14,  3.65s/it][A
 44%|████▍     | 364/827 [23:40<28:19,  3.67s/it][A
 44%|████▍     | 365/827 [23:44<27:12,  3.53s/it][A
 44%|████▍     | 366/827 [23:49<31:35,  4.11s/it][A
 44%|████▍     | 367/827 [23:53<32:18,  4.21s/it][A
 44%|████▍     | 368/827 [23:56<28:30,  3.73s/it][A
 45%|████▍     | 369/827 [24:00<27:56,  3.66s/it][A
 45%|████▍     | 370/827 [24:03<27:15,  3.58s/it][A
 45%|████▍     | 371/827 [24:07<28:47,  3.79s/it][A
 45%|████▍     | 372/827 [24:11<28:28,  3.75s/it][A
 45%|████▌     | 373/827 [24:15<29:59,  3.96s/it][A
 45%|████▌     | 374/827 [24:22<35:56,  4.76s/it][A
 45%|████▌     | 375/827 [24:30<43:49,  5.82s/it][A
 45%|████▌     | 376/827 [24:37<46:37,  6.20s/it][A
 46%|████▌     | 377/827 [24:42<42:53,  5.72s/it][A
 46%|████▌     | 378/827 [24:48<43:11,  5.77s/it][A
 46%|████▌     | 379/827 [24:57<49:54,  6.68s/it][A
 46%|████▌     | 380/827 [25:02<47:52,  6.43s/it][A
 46%|████▌     | 381/827 [25:06<41:32,  5.59s/it][A
 46%|████▌     | 382/827 [25:11<38:51,  5.24s/it][A
 46%|████▋     | 383/827 [25:15<36:16,  4.90s/it][A
 46%|████▋     | 384/827 [25:18<33:23,  4.52s/it][A
 47%|████▋     | 385/827 [25:21<30:21,  4.12s/it][A
 47%|████▋     | 386/827 [25:25<28:12,  3.84s/it][A
 47%|████▋     | 387/827 [25:28<27:52,  3.80s/it][A
 47%|████▋     | 388/827 [25:33<28:43,  3.93s/it][A
 47%|████▋     | 389/827 [25:36<28:04,  3.85s/it][A
 47%|████▋     | 390/827 [25:40<27:07,  3.72s/it][A
 47%|████▋     | 391/827 [25:43<26:02,  3.58s/it][A
 47%|████▋     | 392/827 [25:46<25:37,  3.53s/it][A
 48%|████▊     | 393/827 [25:51<27:01,  3.74s/it][A
 48%|████▊     | 394/827 [25:56<29:57,  4.15s/it][A
 48%|████▊     | 395/827 [26:00<30:38,  4.26s/it][A
 48%|████▊     | 396/827 [26:05<31:13,  4.35s/it][A
 48%|████▊     | 397/827 [26:08<28:00,  3.91s/it][A
 48%|████▊     | 398/827 [26:11<26:28,  3.70s/it][A
 48%|████▊     | 399/827 [26:14<25:29,  3.57s/it][A
 48%|████▊     | 400/827 [26:18<26:49,  3.77s/it][A
 48%|████▊     | 401/827 [26:23<29:09,  4.11s/it][A
 49%|████▊     | 402/827 [26:27<28:55,  4.08s/it][A
 49%|████▊     | 403/827 [26:32<30:15,  4.28s/it][A
 49%|████▉     | 404/827 [26:35<26:35,  3.77s/it][A
 49%|████▉     | 405/827 [26:38<25:03,  3.56s/it][A
 49%|████▉     | 406/827 [26:41<24:18,  3.47s/it][A
 49%|████▉     | 407/827 [26:44<22:46,  3.25s/it][A
 49%|████▉     | 408/827 [26:47<22:02,  3.16s/it][A
 49%|████▉     | 409/827 [26:50<21:41,  3.11s/it][A
 50%|████▉     | 410/827 [26:52<20:22,  2.93s/it][A
 50%|████▉     | 411/827 [26:55<20:27,  2.95s/it][A
 50%|████▉     | 412/827 [26:58<20:31,  2.97s/it][A
 50%|████▉     | 413/827 [27:02<23:12,  3.36s/it][A
 50%|█████     | 414/827 [27:07<26:23,  3.83s/it][A
 50%|█████     | 415/827 [27:10<24:07,  3.51s/it][A
 50%|█████     | 416/827 [27:14<25:23,  3.71s/it][A
 50%|█████     | 417/827 [27:19<27:31,  4.03s/it][A
 51%|█████     | 418/827 [27:25<30:27,  4.47s/it][A
 51%|█████     | 419/827 [27:28<29:13,  4.30s/it][A
 51%|█████     | 420/827 [27:33<28:48,  4.25s/it][A
 51%|█████     | 421/827 [27:36<27:22,  4.05s/it][A
 51%|█████     | 422/827 [27:39<25:15,  3.74s/it][A
 51%|█████     | 423/827 [27:42<23:42,  3.52s/it][A
 51%|█████▏    | 424/827 [27:46<24:17,  3.62s/it][A
 51%|█████▏    | 425/827 [27:50<25:54,  3.87s/it][A
 52%|█████▏    | 426/827 [27:55<27:48,  4.16s/it][A
 52%|█████▏    | 427/827 [28:00<28:49,  4.32s/it][A
 52%|█████▏    | 428/827 [28:05<29:35,  4.45s/it][A
 52%|█████▏    | 429/827 [28:08<27:51,  4.20s/it][A
 52%|█████▏    | 430/827 [28:12<27:02,  4.09s/it][A
 52%|█████▏    | 431/827 [28:16<26:46,  4.06s/it][A
 52%|█████▏    | 432/827 [28:21<28:48,  4.38s/it][A
 52%|█████▏    | 433/827 [28:28<32:31,  4.95s/it][A
 52%|█████▏    | 434/827 [28:33<33:11,  5.07s/it][A
 53%|█████▎    | 435/827 [28:36<28:58,  4.44s/it][A
 53%|█████▎    | 436/827 [28:40<28:08,  4.32s/it][A
 53%|█████▎    | 437/827 [28:44<28:14,  4.35s/it][A
 53%|█████▎    | 438/827 [28:48<26:12,  4.04s/it][A
 53%|█████▎    | 439/827 [28:51<25:21,  3.92s/it][A
 53%|█████▎    | 440/827 [28:55<25:15,  3.92s/it][A
 53%|█████▎    | 441/827 [28:58<23:06,  3.59s/it][A
 53%|█████▎    | 442/827 [29:01<21:37,  3.37s/it][A
 54%|█████▎    | 443/827 [29:04<20:55,  3.27s/it][A
 54%|█████▎    | 444/827 [29:08<21:47,  3.41s/it][A
 54%|█████▍    | 445/827 [29:13<25:25,  3.99s/it][A
 54%|█████▍    | 446/827 [29:16<23:48,  3.75s/it][A
 54%|█████▍    | 447/827 [29:21<25:02,  3.95s/it][A
 54%|█████▍    | 448/827 [29:26<26:46,  4.24s/it][A
 54%|█████▍    | 449/827 [29:33<32:38,  5.18s/it][A
 54%|█████▍    | 450/827 [29:38<32:44,  5.21s/it][A
 55%|█████▍    | 451/827 [29:42<29:36,  4.72s/it][A
 55%|█████▍    | 452/827 [29:47<30:04,  4.81s/it][A
 55%|█████▍    | 453/827 [29:52<31:10,  5.00s/it][A
 55%|█████▍    | 454/827 [29:56<28:01,  4.51s/it][A
 55%|█████▌    | 455/827 [30:00<26:56,  4.34s/it][A
 55%|█████▌    | 456/827 [30:05<29:11,  4.72s/it][A
 55%|█████▌    | 457/827 [30:10<28:29,  4.62s/it][A
 55%|█████▌    | 458/827 [30:13<26:50,  4.37s/it][A
 56%|█████▌    | 459/827 [30:18<27:14,  4.44s/it][A
 56%|█████▌    | 460/827 [30:24<29:11,  4.77s/it][A
 56%|█████▌    | 461/827 [30:31<33:42,  5.53s/it][A
 56%|█████▌    | 462/827 [30:35<31:39,  5.20s/it][A
 56%|█████▌    | 463/827 [30:38<27:31,  4.54s/it][A
 56%|█████▌    | 464/827 [30:43<27:33,  4.55s/it][A
 56%|█████▌    | 465/827 [30:47<26:51,  4.45s/it][A
 56%|█████▋    | 466/827 [30:50<24:54,  4.14s/it][A
 56%|█████▋    | 467/827 [30:54<24:27,  4.08s/it][A
 57%|█████▋    | 468/827 [30:59<24:43,  4.13s/it][A
 57%|█████▋    | 469/827 [31:03<25:45,  4.32s/it][A
 57%|█████▋    | 470/827 [31:07<23:34,  3.96s/it][A
 57%|█████▋    | 471/827 [31:11<23:42,  4.00s/it][A
 57%|█████▋    | 472/827 [31:17<27:44,  4.69s/it][A
 57%|█████▋    | 473/827 [31:21<26:58,  4.57s/it][A
 57%|█████▋    | 474/827 [31:25<25:43,  4.37s/it][A
 57%|█████▋    | 475/827 [31:28<23:09,  3.95s/it][A
 58%|█████▊    | 476/827 [31:30<20:18,  3.47s/it][A
 58%|█████▊    | 477/827 [31:33<18:24,  3.16s/it][A
 58%|█████▊    | 478/827 [31:37<19:19,  3.32s/it][A
 58%|█████▊    | 479/827 [31:41<21:54,  3.78s/it][A
 58%|█████▊    | 480/827 [31:46<22:39,  3.92s/it][A
 58%|█████▊    | 481/827 [31:50<23:53,  4.14s/it][A
 58%|█████▊    | 482/827 [31:53<21:37,  3.76s/it][A
 58%|█████▊    | 483/827 [31:55<18:50,  3.29s/it][A
 59%|█████▊    | 484/827 [31:58<18:30,  3.24s/it][A
 59%|█████▊    | 485/827 [32:01<17:44,  3.11s/it][A
 59%|█████▉    | 486/827 [32:05<17:59,  3.17s/it][A
 59%|█████▉    | 487/827 [32:08<18:24,  3.25s/it][A
 59%|█████▉    | 488/827 [32:13<21:00,  3.72s/it][A
 59%|█████▉    | 489/827 [32:18<22:30,  3.99s/it][A
 59%|█████▉    | 490/827 [32:22<23:13,  4.14s/it][A
 59%|█████▉    | 491/827 [32:25<21:33,  3.85s/it][A
 59%|█████▉    | 492/827 [32:29<20:50,  3.73s/it][A
 60%|█████▉    | 493/827 [32:32<20:13,  3.63s/it][A
 60%|█████▉    | 494/827 [32:35<19:53,  3.58s/it][A
 60%|█████▉    | 495/827 [32:39<19:48,  3.58s/it][A
 60%|█████▉    | 496/827 [32:43<20:02,  3.63s/it][A
 60%|██████    | 497/827 [32:45<18:21,  3.34s/it][A
 60%|██████    | 498/827 [32:49<17:55,  3.27s/it][A
 60%|██████    | 499/827 [32:53<19:40,  3.60s/it][A
 60%|██████    | 500/827 [32:56<18:52,  3.46s/it][A
 61%|██████    | 501/827 [32:59<17:41,  3.25s/it][A
 61%|██████    | 502/827 [33:03<19:41,  3.64s/it][A
 61%|██████    | 503/827 [33:08<21:15,  3.94s/it][A
 61%|██████    | 504/827 [33:11<20:26,  3.80s/it][A
 61%|██████    | 505/827 [33:15<20:09,  3.76s/it][A
 61%|██████    | 506/827 [33:19<19:53,  3.72s/it][A
 61%|██████▏   | 507/827 [33:24<21:45,  4.08s/it][A
 61%|██████▏   | 508/827 [33:30<25:08,  4.73s/it][A
 62%|██████▏   | 509/827 [33:33<21:42,  4.10s/it][A
 62%|██████▏   | 510/827 [33:36<20:51,  3.95s/it][A
 62%|██████▏   | 511/827 [33:39<19:27,  3.69s/it][A
 62%|██████▏   | 512/827 [33:42<17:56,  3.42s/it][A
 62%|██████▏   | 513/827 [33:46<18:15,  3.49s/it][A
 62%|██████▏   | 514/827 [33:49<18:17,  3.51s/it][A
 62%|██████▏   | 515/827 [33:52<17:45,  3.41s/it][A
 62%|██████▏   | 516/827 [33:56<18:20,  3.54s/it][A
 63%|██████▎   | 517/827 [34:00<17:51,  3.46s/it][A
 63%|██████▎   | 518/827 [34:02<16:45,  3.25s/it][A
 63%|██████▎   | 519/827 [34:05<16:16,  3.17s/it][A
 63%|██████▎   | 520/827 [34:08<16:16,  3.18s/it][A
 63%|██████▎   | 521/827 [34:13<17:40,  3.47s/it][A
 63%|██████▎   | 522/827 [34:18<20:16,  3.99s/it][A
 63%|██████▎   | 523/827 [34:21<18:27,  3.64s/it][A
 63%|██████▎   | 524/827 [34:25<18:46,  3.72s/it][A
 63%|██████▎   | 525/827 [34:30<21:30,  4.27s/it][A
 64%|██████▎   | 526/827 [34:36<23:24,  4.67s/it][A
 64%|██████▎   | 527/827 [34:40<22:20,  4.47s/it][A
 64%|██████▍   | 528/827 [34:44<22:28,  4.51s/it][A
 64%|██████▍   | 529/827 [34:49<22:51,  4.60s/it][A
 64%|██████▍   | 530/827 [34:54<22:36,  4.57s/it][A
 64%|██████▍   | 531/827 [34:57<20:10,  4.09s/it][A
 64%|██████▍   | 532/827 [35:00<19:08,  3.89s/it][A
 64%|██████▍   | 533/827 [35:05<20:39,  4.22s/it][A
 65%|██████▍   | 534/827 [35:10<22:01,  4.51s/it][A
 65%|██████▍   | 535/827 [35:15<22:39,  4.66s/it][A
 65%|██████▍   | 536/827 [35:19<20:52,  4.31s/it][A
 65%|██████▍   | 537/827 [35:21<18:04,  3.74s/it][A
 65%|██████▌   | 538/827 [35:24<16:58,  3.52s/it][A
 65%|██████▌   | 539/827 [35:27<16:11,  3.37s/it][A
 65%|██████▌   | 540/827 [35:31<16:25,  3.43s/it][A
 65%|██████▌   | 541/827 [35:35<16:56,  3.55s/it][A
 66%|██████▌   | 542/827 [35:38<17:06,  3.60s/it][A
 66%|██████▌   | 543/827 [35:43<18:15,  3.86s/it][A
 66%|██████▌   | 544/827 [35:47<19:26,  4.12s/it][A
 66%|██████▌   | 545/827 [35:52<19:44,  4.20s/it][A
 66%|██████▌   | 546/827 [35:56<19:35,  4.18s/it][A
 66%|██████▌   | 547/827 [36:00<19:32,  4.19s/it][A
 66%|██████▋   | 548/827 [36:03<17:27,  3.76s/it][A
 66%|██████▋   | 549/827 [36:06<16:08,  3.48s/it][A
 67%|██████▋   | 550/827 [36:09<15:56,  3.45s/it][A
 67%|██████▋   | 551/827 [36:13<16:38,  3.62s/it][A
 67%|██████▋   | 552/827 [36:17<16:27,  3.59s/it][A
 67%|██████▋   | 553/827 [36:20<16:16,  3.56s/it][A
 67%|██████▋   | 554/827 [36:23<14:45,  3.24s/it][A
 67%|██████▋   | 555/827 [36:26<14:26,  3.18s/it][A
 67%|██████▋   | 556/827 [36:29<14:23,  3.19s/it][A
 67%|██████▋   | 557/827 [36:32<13:38,  3.03s/it][A
 67%|██████▋   | 558/827 [36:35<14:40,  3.27s/it][A
 68%|██████▊   | 559/827 [36:39<15:20,  3.43s/it][A
 68%|██████▊   | 560/827 [36:42<14:56,  3.36s/it][A
 68%|██████▊   | 561/827 [36:46<14:47,  3.34s/it][A
 68%|██████▊   | 562/827 [36:49<15:11,  3.44s/it][A
 68%|██████▊   | 563/827 [36:53<15:07,  3.44s/it][A
 68%|██████▊   | 564/827 [36:56<14:59,  3.42s/it][A
 68%|██████▊   | 565/827 [37:01<16:18,  3.73s/it][A
 68%|██████▊   | 566/827 [37:05<17:16,  3.97s/it][A
 69%|██████▊   | 567/827 [37:08<15:51,  3.66s/it][A
 69%|██████▊   | 568/827 [37:12<15:42,  3.64s/it][A
 69%|██████▉   | 569/827 [37:15<15:00,  3.49s/it][A
 69%|██████▉   | 570/827 [37:18<15:05,  3.52s/it][A
 69%|██████▉   | 571/827 [37:24<17:01,  3.99s/it][A
 69%|██████▉   | 572/827 [37:29<18:32,  4.36s/it][A
 69%|██████▉   | 573/827 [37:34<19:39,  4.64s/it][A
 69%|██████▉   | 574/827 [37:38<18:38,  4.42s/it][A
 70%|██████▉   | 575/827 [37:43<19:36,  4.67s/it][A
 70%|██████▉   | 576/827 [37:47<18:36,  4.45s/it][A
 70%|██████▉   | 577/827 [37:50<16:57,  4.07s/it][A
 70%|██████▉   | 578/827 [37:54<16:28,  3.97s/it][A
 70%|███████   | 579/827 [37:57<15:27,  3.74s/it][A
 70%|███████   | 580/827 [38:02<16:32,  4.02s/it][A
 70%|███████   | 581/827 [38:09<19:47,  4.83s/it][A
 70%|███████   | 582/827 [38:13<19:08,  4.69s/it][A
 70%|███████   | 583/827 [38:16<17:11,  4.23s/it][A
 71%|███████   | 584/827 [38:19<15:44,  3.89s/it][A
 71%|███████   | 585/827 [38:22<14:30,  3.60s/it][A
 71%|███████   | 586/827 [38:26<14:52,  3.70s/it][A
 71%|███████   | 587/827 [38:33<18:44,  4.68s/it][A
 71%|███████   | 588/827 [38:40<21:29,  5.39s/it][A
 71%|███████   | 589/827 [38:45<20:28,  5.16s/it][A
 71%|███████▏  | 590/827 [38:51<21:14,  5.38s/it][A
 71%|███████▏  | 591/827 [38:57<22:42,  5.77s/it][A
 72%|███████▏  | 592/827 [39:01<19:40,  5.02s/it][A
 72%|███████▏  | 593/827 [39:04<18:04,  4.63s/it][A
 72%|███████▏  | 594/827 [39:07<16:05,  4.14s/it][A
 72%|███████▏  | 595/827 [39:11<14:57,  3.87s/it][A
 72%|███████▏  | 596/827 [39:17<18:06,  4.71s/it][A
 72%|███████▏  | 597/827 [39:23<19:34,  5.10s/it][A
 72%|███████▏  | 598/827 [39:26<17:03,  4.47s/it][A
 72%|███████▏  | 599/827 [39:30<15:40,  4.13s/it][A
 73%|███████▎  | 600/827 [39:34<15:46,  4.17s/it][A
 73%|███████▎  | 601/827 [39:38<15:22,  4.08s/it][A
 73%|███████▎  | 602/827 [39:42<15:00,  4.00s/it][A
 73%|███████▎  | 603/827 [39:45<13:53,  3.72s/it][A
 73%|███████▎  | 604/827 [39:48<13:53,  3.74s/it][A
 73%|███████▎  | 605/827 [39:52<13:18,  3.60s/it][A
 73%|███████▎  | 606/827 [39:55<13:03,  3.55s/it][A
 73%|███████▎  | 607/827 [39:59<13:29,  3.68s/it][A
 74%|███████▎  | 608/827 [40:02<12:57,  3.55s/it][A
 74%|███████▎  | 609/827 [40:08<14:42,  4.05s/it][A
 74%|███████▍  | 610/827 [40:14<17:08,  4.74s/it][A
 74%|███████▍  | 611/827 [40:18<15:48,  4.39s/it][A
 74%|███████▍  | 612/827 [40:22<15:26,  4.31s/it][A
 74%|███████▍  | 613/827 [40:26<15:34,  4.37s/it][A
 74%|███████▍  | 614/827 [40:30<15:09,  4.27s/it][A
 74%|███████▍  | 615/827 [40:34<14:18,  4.05s/it][A
 74%|███████▍  | 616/827 [40:37<13:29,  3.84s/it][A
 75%|███████▍  | 617/827 [40:42<14:19,  4.09s/it][A
 75%|███████▍  | 618/827 [40:48<16:46,  4.81s/it][A
 75%|███████▍  | 619/827 [40:51<14:38,  4.22s/it][A
 75%|███████▍  | 620/827 [40:55<13:56,  4.04s/it][A
 75%|███████▌  | 621/827 [40:58<12:36,  3.67s/it][A
 75%|███████▌  | 622/827 [41:00<11:45,  3.44s/it][A
 75%|███████▌  | 623/827 [41:04<11:28,  3.38s/it][A
 75%|███████▌  | 624/827 [41:08<12:38,  3.74s/it][A
 76%|███████▌  | 625/827 [41:12<12:13,  3.63s/it][A
 76%|███████▌  | 626/827 [41:15<11:54,  3.56s/it][A
 76%|███████▌  | 627/827 [41:18<11:45,  3.53s/it][A
 76%|███████▌  | 628/827 [41:21<11:02,  3.33s/it][A
 76%|███████▌  | 629/827 [41:24<10:43,  3.25s/it][A
 76%|███████▌  | 630/827 [41:27<10:29,  3.19s/it][A
 76%|███████▋  | 631/827 [41:32<12:12,  3.74s/it][A
 76%|███████▋  | 632/827 [41:37<13:23,  4.12s/it][A
 77%|███████▋  | 633/827 [41:42<14:01,  4.34s/it][A
 77%|███████▋  | 634/827 [41:49<15:58,  4.97s/it][A
 77%|███████▋  | 635/827 [41:52<13:55,  4.35s/it][A
 77%|███████▋  | 636/827 [41:56<13:23,  4.21s/it][A
 77%|███████▋  | 637/827 [42:00<13:46,  4.35s/it][A
 77%|███████▋  | 638/827 [42:05<13:54,  4.42s/it][A
 77%|███████▋  | 639/827 [42:08<12:39,  4.04s/it][A
 77%|███████▋  | 640/827 [42:11<11:31,  3.70s/it][A
 78%|███████▊  | 641/827 [42:15<11:41,  3.77s/it][A
 78%|███████▊  | 642/827 [42:18<11:31,  3.74s/it][A
 78%|███████▊  | 643/827 [42:23<12:08,  3.96s/it][A
 78%|███████▊  | 644/827 [42:27<12:33,  4.12s/it][A
 78%|███████▊  | 645/827 [42:31<11:56,  3.94s/it][A
 78%|███████▊  | 646/827 [42:34<10:56,  3.62s/it][A
 78%|███████▊  | 647/827 [42:37<10:36,  3.54s/it][A
 78%|███████▊  | 648/827 [42:41<10:47,  3.62s/it][A
 78%|███████▊  | 649/827 [42:45<10:43,  3.62s/it][A
 79%|███████▊  | 650/827 [42:48<10:05,  3.42s/it][A
 79%|███████▊  | 651/827 [42:51<09:51,  3.36s/it][A
 79%|███████▉  | 652/827 [42:54<09:43,  3.34s/it][A
 79%|███████▉  | 653/827 [42:58<09:56,  3.43s/it][A
 79%|███████▉  | 654/827 [43:03<11:23,  3.95s/it][A
 79%|███████▉  | 655/827 [43:08<12:37,  4.41s/it][A
 79%|███████▉  | 656/827 [43:11<11:01,  3.87s/it][A
 79%|███████▉  | 657/827 [43:14<10:11,  3.59s/it][A
 80%|███████▉  | 658/827 [43:18<10:46,  3.83s/it][A
 80%|███████▉  | 659/827 [43:26<13:56,  4.98s/it][A
 80%|███████▉  | 660/827 [43:31<13:56,  5.01s/it][A
 80%|███████▉  | 661/827 [43:36<14:11,  5.13s/it][A
 80%|████████  | 662/827 [43:40<13:05,  4.76s/it][A
 80%|████████  | 663/827 [43:44<12:19,  4.51s/it][A
 80%|████████  | 664/827 [43:49<12:38,  4.66s/it][A
 80%|████████  | 665/827 [43:53<11:45,  4.36s/it][A
 81%|████████  | 666/827 [43:57<11:28,  4.28s/it][A
 81%|████████  | 667/827 [44:00<10:32,  3.95s/it][A
 81%|████████  | 668/827 [44:04<10:21,  3.91s/it][A
 81%|████████  | 669/827 [44:07<09:23,  3.57s/it][A
 81%|████████  | 670/827 [44:10<08:55,  3.41s/it][A
 81%|████████  | 671/827 [44:13<09:02,  3.48s/it][A
 81%|████████▏ | 672/827 [44:17<08:56,  3.46s/it][A
 81%|████████▏ | 673/827 [44:20<08:33,  3.33s/it][A
 81%|████████▏ | 674/827 [44:23<08:27,  3.32s/it][A
 82%|████████▏ | 675/827 [44:27<08:36,  3.40s/it][A
 82%|████████▏ | 676/827 [44:31<09:19,  3.71s/it][A
 82%|████████▏ | 677/827 [44:36<10:18,  4.12s/it][A
 82%|████████▏ | 678/827 [44:41<10:36,  4.27s/it][A
 82%|████████▏ | 679/827 [44:45<10:44,  4.35s/it][A
 82%|████████▏ | 680/827 [44:48<09:34,  3.91s/it][A
 82%|████████▏ | 681/827 [44:53<10:04,  4.14s/it][A
 82%|████████▏ | 682/827 [44:58<10:37,  4.40s/it][A
 83%|████████▎ | 683/827 [45:01<09:28,  3.95s/it][A
 83%|████████▎ | 684/827 [45:04<08:41,  3.65s/it][A
 83%|████████▎ | 685/827 [45:06<07:56,  3.35s/it][A
 83%|████████▎ | 686/827 [45:12<09:19,  3.97s/it][A
 83%|████████▎ | 687/827 [45:19<11:30,  4.94s/it][A
 83%|████████▎ | 688/827 [45:24<11:26,  4.94s/it][A
 83%|████████▎ | 689/827 [45:29<11:14,  4.88s/it][A
 83%|████████▎ | 690/827 [45:32<09:57,  4.36s/it][A
 84%|████████▎ | 691/827 [45:35<09:03,  4.00s/it][A
 84%|████████▎ | 692/827 [45:39<08:59,  3.99s/it][A
 84%|████████▍ | 693/827 [45:42<08:26,  3.78s/it][A
 84%|████████▍ | 694/827 [45:46<07:59,  3.60s/it][A
 84%|████████▍ | 695/827 [45:50<08:12,  3.73s/it][A
 84%|████████▍ | 696/827 [45:54<08:46,  4.02s/it][A
 84%|████████▍ | 697/827 [45:58<08:18,  3.83s/it][A
 84%|████████▍ | 698/827 [46:00<07:25,  3.45s/it][A
 85%|████████▍ | 699/827 [46:03<07:01,  3.29s/it][A
 85%|████████▍ | 700/827 [46:09<08:49,  4.17s/it][A
 85%|████████▍ | 701/827 [46:16<10:06,  4.82s/it][A
 85%|████████▍ | 702/827 [46:20<09:47,  4.70s/it][A
 85%|████████▌ | 703/827 [46:24<09:15,  4.48s/it][A
 85%|████████▌ | 704/827 [46:27<08:19,  4.06s/it][A
 85%|████████▌ | 705/827 [46:30<07:25,  3.65s/it][A
 85%|████████▌ | 706/827 [46:33<06:47,  3.37s/it][A
 85%|████████▌ | 707/827 [46:36<07:02,  3.52s/it][A
 86%|████████▌ | 708/827 [46:40<06:50,  3.45s/it][A
 86%|████████▌ | 709/827 [46:44<06:59,  3.55s/it][A
 86%|████████▌ | 710/827 [46:47<06:54,  3.54s/it][A
 86%|████████▌ | 711/827 [46:51<06:52,  3.56s/it][A
 86%|████████▌ | 712/827 [46:54<06:30,  3.39s/it][A
 86%|████████▌ | 713/827 [46:57<06:23,  3.36s/it][A
 86%|████████▋ | 714/827 [47:01<06:33,  3.49s/it][A
 86%|████████▋ | 715/827 [47:05<06:59,  3.74s/it][A
 87%|████████▋ | 716/827 [47:09<06:49,  3.69s/it][A
 87%|████████▋ | 717/827 [47:14<07:31,  4.10s/it][A
 87%|████████▋ | 718/827 [47:19<07:55,  4.36s/it][A
 87%|████████▋ | 719/827 [47:22<07:12,  4.01s/it][A
 87%|████████▋ | 720/827 [47:25<06:51,  3.85s/it][A
 87%|████████▋ | 721/827 [47:29<06:32,  3.70s/it][A
 87%|████████▋ | 722/827 [47:32<06:26,  3.68s/it][A
 87%|████████▋ | 723/827 [47:36<06:11,  3.58s/it][A
 88%|████████▊ | 724/827 [47:38<05:37,  3.27s/it][A
 88%|████████▊ | 725/827 [47:43<06:11,  3.64s/it][A
 88%|████████▊ | 726/827 [47:48<07:00,  4.17s/it][A
 88%|████████▊ | 727/827 [47:52<06:51,  4.11s/it][A
 88%|████████▊ | 728/827 [47:55<06:16,  3.80s/it][A
 88%|████████▊ | 729/827 [47:58<05:34,  3.42s/it][A
 88%|████████▊ | 730/827 [48:01<05:39,  3.50s/it][A
 88%|████████▊ | 731/827 [48:06<06:11,  3.87s/it][A
 89%|████████▊ | 732/827 [48:10<06:22,  4.02s/it][A
 89%|████████▊ | 733/827 [48:15<06:24,  4.10s/it][A
 89%|████████▉ | 734/827 [48:17<05:36,  3.62s/it][A
 89%|████████▉ | 735/827 [48:21<05:37,  3.66s/it][A
 89%|████████▉ | 736/827 [48:26<06:17,  4.15s/it][A
 89%|████████▉ | 737/827 [48:31<06:31,  4.35s/it][A
 89%|████████▉ | 738/827 [48:34<05:51,  3.95s/it][A
 89%|████████▉ | 739/827 [48:39<06:09,  4.19s/it][A
 89%|████████▉ | 740/827 [48:44<06:40,  4.61s/it][A
 90%|████████▉ | 741/827 [48:48<06:14,  4.36s/it][A
 90%|████████▉ | 742/827 [48:52<05:54,  4.17s/it][A
 90%|████████▉ | 743/827 [48:57<06:18,  4.50s/it][A
 90%|████████▉ | 744/827 [49:01<05:54,  4.28s/it][A
 90%|█████████ | 745/827 [49:05<05:41,  4.17s/it][A
 90%|█████████ | 746/827 [49:10<06:02,  4.48s/it][A
 90%|█████████ | 747/827 [49:15<06:15,  4.70s/it][A
 90%|█████████ | 748/827 [49:18<05:34,  4.24s/it][A
 91%|█████████ | 749/827 [49:23<05:29,  4.23s/it][A
 91%|█████████ | 750/827 [49:27<05:27,  4.26s/it][A
 91%|█████████ | 751/827 [49:30<04:56,  3.91s/it][A
 91%|█████████ | 752/827 [49:33<04:31,  3.62s/it][A
 91%|█████████ | 753/827 [49:36<04:10,  3.39s/it][A
 91%|█████████ | 754/827 [49:39<03:56,  3.24s/it][A
 91%|█████████▏| 755/827 [49:43<04:15,  3.54s/it][A
 91%|█████████▏| 756/827 [49:48<04:40,  3.96s/it][A
 92%|█████████▏| 757/827 [49:52<04:36,  3.94s/it][A
 92%|█████████▏| 758/827 [49:56<04:37,  4.03s/it][A
 92%|█████████▏| 759/827 [49:59<04:11,  3.69s/it][A
 92%|█████████▏| 760/827 [50:04<04:39,  4.17s/it][A
 92%|█████████▏| 761/827 [50:11<05:25,  4.94s/it][A
 92%|█████████▏| 762/827 [50:17<05:35,  5.16s/it][A
 92%|█████████▏| 763/827 [50:23<05:44,  5.39s/it][A
 92%|█████████▏| 764/827 [50:26<04:59,  4.76s/it][A
 93%|█████████▎| 765/827 [50:29<04:18,  4.16s/it][A
 93%|█████████▎| 766/827 [50:31<03:49,  3.76s/it][A
 93%|█████████▎| 767/827 [50:35<03:39,  3.67s/it][A
 93%|█████████▎| 768/827 [50:39<03:37,  3.68s/it][A
 93%|█████████▎| 769/827 [50:43<03:50,  3.97s/it][A
 93%|█████████▎| 770/827 [50:47<03:35,  3.79s/it][A
 93%|█████████▎| 771/827 [50:50<03:31,  3.77s/it][A
 93%|█████████▎| 772/827 [50:54<03:24,  3.71s/it][A
 93%|█████████▎| 773/827 [50:58<03:24,  3.79s/it][A
 94%|█████████▎| 774/827 [51:01<03:09,  3.58s/it][A
 94%|█████████▎| 775/827 [51:05<03:10,  3.66s/it][A
 94%|█████████▍| 776/827 [51:08<03:02,  3.58s/it][A
 94%|█████████▍| 777/827 [51:12<02:53,  3.47s/it][A
 94%|█████████▍| 778/827 [51:15<02:50,  3.47s/it][A
 94%|█████████▍| 779/827 [51:18<02:38,  3.31s/it][A
 94%|█████████▍| 780/827 [51:21<02:31,  3.23s/it][A
 94%|█████████▍| 781/827 [51:24<02:24,  3.15s/it][A
 95%|█████████▍| 782/827 [51:27<02:16,  3.04s/it][A
 95%|█████████▍| 783/827 [51:30<02:18,  3.14s/it][A
 95%|█████████▍| 784/827 [51:33<02:16,  3.17s/it][A
 95%|█████████▍| 785/827 [51:37<02:19,  3.31s/it][A
 95%|█████████▌| 786/827 [51:41<02:24,  3.52s/it][A
 95%|█████████▌| 787/827 [51:46<02:42,  4.05s/it][A
 95%|█████████▌| 788/827 [51:50<02:35,  4.00s/it][A
 95%|█████████▌| 789/827 [51:54<02:35,  4.10s/it][A
 96%|█████████▌| 790/827 [51:58<02:26,  3.95s/it][A
 96%|█████████▌| 791/827 [52:01<02:11,  3.65s/it][A
 96%|█████████▌| 792/827 [52:04<02:00,  3.45s/it][A
 96%|█████████▌| 793/827 [52:08<02:07,  3.75s/it][A
 96%|█████████▌| 794/827 [52:14<02:18,  4.20s/it][A
 96%|█████████▌| 795/827 [52:19<02:29,  4.67s/it][A
 96%|█████████▋| 796/827 [52:24<02:27,  4.76s/it][A
 96%|█████████▋| 797/827 [52:27<02:05,  4.19s/it][A
 96%|█████████▋| 798/827 [52:30<01:49,  3.77s/it][A
 97%|█████████▋| 799/827 [52:33<01:34,  3.37s/it][A
 97%|█████████▋| 800/827 [52:39<01:54,  4.24s/it][A
 97%|█████████▋| 801/827 [52:46<02:11,  5.08s/it][A
 97%|█████████▋| 802/827 [52:50<02:02,  4.90s/it][A
 97%|█████████▋| 803/827 [52:56<01:59,  4.99s/it][A
 97%|█████████▋| 804/827 [53:00<01:54,  4.98s/it][A
 97%|█████████▋| 805/827 [53:04<01:36,  4.40s/it][A
 97%|█████████▋| 806/827 [53:06<01:21,  3.88s/it][A
 98%|█████████▊| 807/827 [53:09<01:13,  3.68s/it][A
 98%|█████████▊| 808/827 [53:13<01:09,  3.66s/it][A
 98%|█████████▊| 809/827 [53:16<01:03,  3.54s/it][A
 98%|█████████▊| 810/827 [53:20<00:59,  3.49s/it][A
 98%|█████████▊| 811/827 [53:23<00:56,  3.50s/it][A
 98%|█████████▊| 812/827 [53:27<00:53,  3.59s/it][A
 98%|█████████▊| 813/827 [53:32<00:56,  4.00s/it][A
 98%|█████████▊| 814/827 [53:37<00:57,  4.42s/it][A
 99%|█████████▊| 815/827 [53:41<00:50,  4.23s/it][A
 99%|█████████▊| 816/827 [53:45<00:45,  4.12s/it][A
 99%|█████████▉| 817/827 [53:48<00:38,  3.81s/it][A
 99%|█████████▉| 818/827 [53:53<00:37,  4.22s/it][A
 99%|█████████▉| 819/827 [53:58<00:35,  4.43s/it][A
 99%|█████████▉| 820/827 [54:03<00:31,  4.48s/it][A
 99%|█████████▉| 821/827 [54:06<00:25,  4.24s/it][A
 99%|█████████▉| 822/827 [54:10<00:20,  4.06s/it][A
100%|█████████▉| 823/827 [54:13<00:14,  3.64s/it][A
100%|█████████▉| 824/827 [54:16<00:10,  3.41s/it][A
100%|█████████▉| 825/827 [54:20<00:07,  3.77s/it][A
100%|█████████▉| 826/827 [54:26<00:04,  4.34s/it][A
100%|██████████| 827/827 [54:28<00:00,  3.77s/it][A                                                      
                                                 [A{'eval_loss': 0.8203322887420654, 'eval_runtime': 3273.8656, 'eval_samples_per_second': 1.01, 'eval_steps_per_second': 0.253, 'epoch': 0.43}
 43%|████▎     | 200/465 [6:19:34<7:36:16, 103.31s/it]
100%|██████████| 827/827 [54:28<00:00,  3.77s/it][A
                                                 [A[INFO|trainer.py:4309] 2026-01-03 01:04:56,689 >> Saving model checkpoint to saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-200
/opt/conda/lib/python3.11/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in Qwen/Qwen2.5-VL-7B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2421] 2026-01-03 01:04:56,773 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-200/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2026-01-03 01:04:56,775 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2026-01-03 01:04:56,777 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-200/special_tokens_map.json
[2026-01-03 01:04:57,390] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!
[2026-01-03 01:04:57,410] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-200/global_step200/mp_rank_00_model_states.pt
[2026-01-03 01:04:57,410] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-200/global_step200/mp_rank_00_model_states.pt...
[2026-01-03 01:04:57,478] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-200/global_step200/mp_rank_00_model_states.pt.
[2026-01-03 01:04:57,481] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2026-01-03 01:04:57,481] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-200/global_step200/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2026-01-03 01:04:57,481] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-200/global_step200/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2026-01-03 01:04:57,481] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-200/global_step200/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2026-01-03 01:04:57,570] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-200/global_step200/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2026-01-03 01:04:57,571] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-200/global_step200/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2026-01-03 01:04:57,571] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2026-01-03 01:04:57,585] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-200/global_step200/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2026-01-03 01:04:57,585] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-200/global_step200/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2026-01-03 01:04:57,585] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2026-01-03 01:04:57,600] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-200/global_step200/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2026-01-03 01:04:57,600] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-200/global_step200/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2026-01-03 01:04:57,600] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2026-01-03 01:04:57,643] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2026-01-03 01:04:57,656] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2026-01-03 01:04:57,656] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[INFO|image_processing_base.py:253] 2026-01-03 01:04:57,695 >> Image processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-200/preprocessor_config.json
[INFO|tokenization_utils_base.py:2421] 2026-01-03 01:04:57,703 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-200/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2026-01-03 01:04:57,706 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2026-01-03 01:04:57,708 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-200/special_tokens_map.json
[INFO|video_processing_utils.py:600] 2026-01-03 01:04:57,825 >> Video processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-200/video_preprocessor_config.json
[INFO|processing_utils.py:814] 2026-01-03 01:04:57,832 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-200/chat_template.jinja
 43%|████▎     | 201/465 [6:21:01<79:14:21, 1080.54s/it]                                                        {'loss': 0.7851, 'grad_norm': 0.18924659490585327, 'learning_rate': 7.04283379046524e-05, 'epoch': 0.43}
 43%|████▎     | 201/465 [6:21:01<79:14:21, 1080.54s/it] 43%|████▎     | 202/465 [6:22:32<57:15:40, 783.80s/it]                                                        {'loss': 0.7728, 'grad_norm': 0.15362882614135742, 'learning_rate': 7.008477123264848e-05, 'epoch': 0.43}
 43%|████▎     | 202/465 [6:22:32<57:15:40, 783.80s/it] 44%|████▎     | 203/465 [6:23:58<41:48:18, 574.42s/it]                                                       {'loss': 0.7846, 'grad_norm': 0.15872709453105927, 'learning_rate': 6.974007004098243e-05, 'epoch': 0.44}
 44%|████▎     | 203/465 [6:23:58<41:48:18, 574.42s/it] 44%|████▍     | 204/465 [6:25:43<31:25:35, 433.47s/it]                                                       {'loss': 0.8863, 'grad_norm': 0.1470559537410736, 'learning_rate': 6.939425380063924e-05, 'epoch': 0.44}
 44%|████▍     | 204/465 [6:25:43<31:25:35, 433.47s/it] 44%|████▍     | 205/465 [6:27:32<24:16:53, 336.20s/it]                                                       {'loss': 0.8339, 'grad_norm': 0.12996961176395416, 'learning_rate': 6.904734204558923e-05, 'epoch': 0.44}
 44%|████▍     | 205/465 [6:27:32<24:16:53, 336.20s/it] 44%|████▍     | 206/465 [6:29:11<19:04:00, 265.02s/it]                                                       {'loss': 0.8115, 'grad_norm': 0.16728830337524414, 'learning_rate': 6.869935437168449e-05, 'epoch': 0.44}
 44%|████▍     | 206/465 [6:29:11<19:04:00, 265.02s/it] 45%|████▍     | 207/465 [6:30:58<15:36:26, 217.78s/it]                                                       {'loss': 0.857, 'grad_norm': 0.1488219052553177, 'learning_rate': 6.835031043555211e-05, 'epoch': 0.45}
 45%|████▍     | 207/465 [6:30:58<15:36:26, 217.78s/it] 45%|████▍     | 208/465 [6:32:39<13:01:42, 182.50s/it]                                                       {'loss': 0.9324, 'grad_norm': 0.2339344620704651, 'learning_rate': 6.800022995348381e-05, 'epoch': 0.45}
 45%|████▍     | 208/465 [6:32:39<13:01:42, 182.50s/it] 45%|████▍     | 209/465 [6:34:12<11:04:19, 155.70s/it]                                                       {'loss': 0.8364, 'grad_norm': 0.14434847235679626, 'learning_rate': 6.76491327003222e-05, 'epoch': 0.45}
 45%|████▍     | 209/465 [6:34:12<11:04:19, 155.70s/it] 45%|████▌     | 210/465 [6:35:51<9:50:06, 138.85s/it]                                                       {'loss': 0.8005, 'grad_norm': 0.17496289312839508, 'learning_rate': 6.729703850834381e-05, 'epoch': 0.45}
 45%|████▌     | 210/465 [6:35:51<9:50:06, 138.85s/it] 45%|████▌     | 211/465 [6:37:30<8:57:26, 126.96s/it]                                                      {'loss': 0.8498, 'grad_norm': 0.1659567505121231, 'learning_rate': 6.694396726613883e-05, 'epoch': 0.45}
 45%|████▌     | 211/465 [6:37:30<8:57:26, 126.96s/it] 46%|████▌     | 212/465 [6:39:11<8:21:52, 119.02s/it]                                                      {'loss': 0.8022, 'grad_norm': 0.16816067695617676, 'learning_rate': 6.65899389174876e-05, 'epoch': 0.46}
 46%|████▌     | 212/465 [6:39:11<8:21:52, 119.02s/it] 46%|████▌     | 213/465 [6:40:37<7:38:12, 109.10s/it]                                                      {'loss': 0.841, 'grad_norm': 0.1410265862941742, 'learning_rate': 6.623497346023418e-05, 'epoch': 0.46}
 46%|████▌     | 213/465 [6:40:37<7:38:12, 109.10s/it] 46%|████▌     | 214/465 [6:41:58<7:00:53, 100.61s/it]                                                      {'loss': 0.8449, 'grad_norm': 0.16949188709259033, 'learning_rate': 6.587909094515664e-05, 'epoch': 0.46}
 46%|████▌     | 214/465 [6:41:58<7:00:53, 100.61s/it] 46%|████▌     | 215/465 [6:43:31<6:50:02, 98.41s/it]                                                      {'loss': 0.7832, 'grad_norm': 0.1386936604976654, 'learning_rate': 6.552231147483448e-05, 'epoch': 0.46}
 46%|████▌     | 215/465 [6:43:31<6:50:02, 98.41s/it] 46%|████▋     | 216/465 [6:45:20<7:01:14, 101.50s/it]                                                      {'loss': 0.8575, 'grad_norm': 0.1531885266304016, 'learning_rate': 6.516465520251314e-05, 'epoch': 0.46}
 46%|████▋     | 216/465 [6:45:20<7:01:14, 101.50s/it] 47%|████▋     | 217/465 [6:47:12<7:13:31, 104.88s/it]                                                      {'loss': 0.8526, 'grad_norm': 0.15911681950092316, 'learning_rate': 6.480614233096558e-05, 'epoch': 0.47}
 47%|████▋     | 217/465 [6:47:12<7:13:31, 104.88s/it] 47%|████▋     | 218/465 [6:48:45<6:56:15, 101.12s/it]                                                      {'loss': 0.8303, 'grad_norm': 0.18139980733394623, 'learning_rate': 6.444679311135112e-05, 'epoch': 0.47}
 47%|████▋     | 218/465 [6:48:45<6:56:15, 101.12s/it] 47%|████▋     | 219/465 [6:50:10<6:35:00, 96.34s/it]                                                      {'loss': 0.814, 'grad_norm': 0.13152527809143066, 'learning_rate': 6.408662784207149e-05, 'epoch': 0.47}
 47%|████▋     | 219/465 [6:50:10<6:35:00, 96.34s/it] 47%|████▋     | 220/465 [6:51:47<6:33:41, 96.42s/it]                                                     {'loss': 0.8643, 'grad_norm': 0.1698678582906723, 'learning_rate': 6.372566686762426e-05, 'epoch': 0.47}
 47%|████▋     | 220/465 [6:51:47<6:33:41, 96.42s/it] 48%|████▊     | 221/465 [6:53:34<6:45:55, 99.82s/it]                                                     {'loss': 0.8385, 'grad_norm': 0.1490190178155899, 'learning_rate': 6.336393057745365e-05, 'epoch': 0.48}
 48%|████▊     | 221/465 [6:53:34<6:45:55, 99.82s/it] 48%|████▊     | 222/465 [6:55:40<7:16:02, 107.66s/it]                                                      {'loss': 0.8544, 'grad_norm': 0.1677563488483429, 'learning_rate': 6.300143940479882e-05, 'epoch': 0.48}
 48%|████▊     | 222/465 [6:55:40<7:16:02, 107.66s/it] 48%|████▊     | 223/465 [6:57:09<6:50:42, 101.83s/it]                                                      {'loss': 0.8582, 'grad_norm': 0.14580167829990387, 'learning_rate': 6.26382138255396e-05, 'epoch': 0.48}
 48%|████▊     | 223/465 [6:57:09<6:50:42, 101.83s/it] 48%|████▊     | 224/465 [6:58:50<6:48:14, 101.64s/it]                                                      {'loss': 0.8413, 'grad_norm': 0.1666388064622879, 'learning_rate': 6.227427435703997e-05, 'epoch': 0.48}
 48%|████▊     | 224/465 [6:58:50<6:48:14, 101.64s/it] 48%|████▊     | 225/465 [7:00:21<6:33:45, 98.44s/it]                                                      {'loss': 0.8023, 'grad_norm': 0.16833661496639252, 'learning_rate': 6.190964155698903e-05, 'epoch': 0.48}
 48%|████▊     | 225/465 [7:00:21<6:33:45, 98.44s/it] 49%|████▊     | 226/465 [7:01:54<6:25:32, 96.79s/it]                                                     {'loss': 0.7969, 'grad_norm': 0.14808666706085205, 'learning_rate': 6.15443360222398e-05, 'epoch': 0.49}
 49%|████▊     | 226/465 [7:01:54<6:25:32, 96.79s/it] 49%|████▉     | 227/465 [7:03:46<6:42:53, 101.57s/it]                                                      {'loss': 0.9118, 'grad_norm': 0.153075709939003, 'learning_rate': 6.117837838764579e-05, 'epoch': 0.49}
 49%|████▉     | 227/465 [7:03:46<6:42:53, 101.57s/it] 49%|████▉     | 228/465 [7:05:26<6:39:06, 101.04s/it]                                                      {'loss': 0.8359, 'grad_norm': 0.15465426445007324, 'learning_rate': 6.081178932489536e-05, 'epoch': 0.49}
 49%|████▉     | 228/465 [7:05:26<6:39:06, 101.04s/it] 49%|████▉     | 229/465 [7:06:53<6:20:34, 96.76s/it]                                                      {'loss': 0.8362, 'grad_norm': 0.15371818840503693, 'learning_rate': 6.044458954134411e-05, 'epoch': 0.49}
 49%|████▉     | 229/465 [7:06:53<6:20:34, 96.76s/it] 49%|████▉     | 230/465 [7:08:12<5:58:26, 91.52s/it]                                                     {'loss': 0.861, 'grad_norm': 0.162680521607399, 'learning_rate': 6.0076799778845105e-05, 'epoch': 0.49}
 49%|████▉     | 230/465 [7:08:12<5:58:26, 91.52s/it] 50%|████▉     | 231/465 [7:09:27<5:37:14, 86.47s/it]                                                     {'loss': 0.8897, 'grad_norm': 0.13048064708709717, 'learning_rate': 5.9708440812577336e-05, 'epoch': 0.5}
 50%|████▉     | 231/465 [7:09:27<5:37:14, 86.47s/it] 50%|████▉     | 232/465 [7:10:59<5:42:35, 88.22s/it]                                                     {'loss': 0.7969, 'grad_norm': 0.20670920610427856, 'learning_rate': 5.933953344987214e-05, 'epoch': 0.5}
 50%|████▉     | 232/465 [7:10:59<5:42:35, 88.22s/it] 50%|█████     | 233/465 [7:12:47<6:03:51, 94.10s/it]                                                     {'loss': 0.8009, 'grad_norm': 0.19025148451328278, 'learning_rate': 5.897009852903792e-05, 'epoch': 0.5}
 50%|█████     | 233/465 [7:12:47<6:03:51, 94.10s/it] 50%|█████     | 234/465 [7:14:30<6:12:07, 96.65s/it]                                                     {'loss': 0.7954, 'grad_norm': 0.1487751305103302, 'learning_rate': 5.8600156918182923e-05, 'epoch': 0.5}
 50%|█████     | 234/465 [7:14:30<6:12:07, 96.65s/it] 51%|█████     | 235/465 [7:16:11<6:16:09, 98.13s/it]                                                     {'loss': 0.8028, 'grad_norm': 0.18348456919193268, 'learning_rate': 5.8229729514036705e-05, 'epoch': 0.51}
 51%|█████     | 235/465 [7:16:11<6:16:09, 98.13s/it] 51%|█████     | 236/465 [7:17:57<6:23:06, 100.38s/it]                                                      {'loss': 0.8589, 'grad_norm': 0.1512431502342224, 'learning_rate': 5.7858837240769505e-05, 'epoch': 0.51}
 51%|█████     | 236/465 [7:17:57<6:23:06, 100.38s/it] 51%|█████     | 237/465 [7:19:41<6:26:05, 101.60s/it]                                                      {'loss': 0.8632, 'grad_norm': 0.18174052238464355, 'learning_rate': 5.748750104881051e-05, 'epoch': 0.51}
 51%|█████     | 237/465 [7:19:41<6:26:05, 101.60s/it] 51%|█████     | 238/465 [7:21:14<6:14:38, 99.02s/it]                                                      {'loss': 0.9099, 'grad_norm': 0.19047801196575165, 'learning_rate': 5.7115741913664264e-05, 'epoch': 0.51}
 51%|█████     | 238/465 [7:21:14<6:14:38, 99.02s/it] 51%|█████▏    | 239/465 [7:22:46<6:05:02, 96.92s/it]                                                     {'loss': 0.8525, 'grad_norm': 0.12966029345989227, 'learning_rate': 5.6743580834725974e-05, 'epoch': 0.51}
 51%|█████▏    | 239/465 [7:22:46<6:05:02, 96.92s/it] 52%|█████▏    | 240/465 [7:24:43<6:26:13, 102.99s/it]                                                      {'loss': 0.9698, 'grad_norm': 0.14243903756141663, 'learning_rate': 5.637103883409525e-05, 'epoch': 0.52}
 52%|█████▏    | 240/465 [7:24:43<6:26:13, 102.99s/it] 52%|█████▏    | 241/465 [7:26:23<6:20:40, 101.97s/it]                                                      {'loss': 0.7931, 'grad_norm': 0.202545166015625, 'learning_rate': 5.599813695538866e-05, 'epoch': 0.52}
 52%|█████▏    | 241/465 [7:26:23<6:20:40, 101.97s/it] 52%|█████▏    | 242/465 [7:27:53<6:05:09, 98.25s/it]                                                      {'loss': 0.866, 'grad_norm': 0.16247425973415375, 'learning_rate': 5.562489626255104e-05, 'epoch': 0.52}
 52%|█████▏    | 242/465 [7:27:53<6:05:09, 98.25s/it] 52%|█████▏    | 243/465 [7:29:48<6:22:43, 103.44s/it]                                                      {'loss': 0.8844, 'grad_norm': 0.16936862468719482, 'learning_rate': 5.52513378386657e-05, 'epoch': 0.52}
 52%|█████▏    | 243/465 [7:29:48<6:22:43, 103.44s/it] 52%|█████▏    | 244/465 [7:31:29<6:18:37, 102.80s/it]                                                      {'loss': 0.8361, 'grad_norm': 0.15424107015132904, 'learning_rate': 5.487748278476341e-05, 'epoch': 0.53}
 52%|█████▏    | 244/465 [7:31:29<6:18:37, 102.80s/it] 53%|█████▎    | 245/465 [7:33:13<6:17:17, 102.90s/it]                                                      {'loss': 0.8228, 'grad_norm': 0.1696554720401764, 'learning_rate': 5.4503352218630675e-05, 'epoch': 0.53}
 53%|█████▎    | 245/465 [7:33:13<6:17:17, 102.90s/it] 53%|█████▎    | 246/465 [7:34:55<6:15:24, 102.85s/it]                                                      {'loss': 0.8475, 'grad_norm': 0.1640557199716568, 'learning_rate': 5.4128967273616625e-05, 'epoch': 0.53}
 53%|█████▎    | 246/465 [7:34:55<6:15:24, 102.85s/it] 53%|█████▎    | 247/465 [7:36:24<5:58:25, 98.65s/it]                                                      {'loss': 0.9014, 'grad_norm': 0.16343429684638977, 'learning_rate': 5.375434909743941e-05, 'epoch': 0.53}
 53%|█████▎    | 247/465 [7:36:24<5:58:25, 98.65s/it] 53%|█████▎    | 248/465 [7:37:58<5:52:03, 97.35s/it]                                                     {'loss': 0.8306, 'grad_norm': 0.1635279506444931, 'learning_rate': 5.337951885099166e-05, 'epoch': 0.53}
 53%|█████▎    | 248/465 [7:37:58<5:52:03, 97.35s/it] 54%|█████▎    | 249/465 [7:39:49<6:04:51, 101.35s/it]                                                      {'loss': 0.8906, 'grad_norm': 0.16879568994045258, 'learning_rate': 5.300449770714502e-05, 'epoch': 0.54}
 54%|█████▎    | 249/465 [7:39:49<6:04:51, 101.35s/it] 54%|█████▍    | 250/465 [7:41:31<6:03:49, 101.53s/it]                                                      {'loss': 0.8437, 'grad_norm': 0.14345857501029968, 'learning_rate': 5.2629306849554386e-05, 'epoch': 0.54}
 54%|█████▍    | 250/465 [7:41:31<6:03:49, 101.53s/it] 54%|█████▍    | 251/465 [7:43:12<6:01:25, 101.34s/it]                                                      {'loss': 0.7898, 'grad_norm': 0.15412352979183197, 'learning_rate': 5.225396747146112e-05, 'epoch': 0.54}
 54%|█████▍    | 251/465 [7:43:12<6:01:25, 101.34s/it] 54%|█████▍    | 252/465 [7:44:37<5:42:43, 96.54s/it]                                                      {'loss': 0.7885, 'grad_norm': 0.17083263397216797, 'learning_rate': 5.187850077449604e-05, 'epoch': 0.54}
 54%|█████▍    | 252/465 [7:44:37<5:42:43, 96.54s/it] 54%|█████▍    | 253/465 [7:46:02<5:28:48, 93.06s/it]                                                     {'loss': 0.8363, 'grad_norm': 0.14885784685611725, 'learning_rate': 5.150292796748174e-05, 'epoch': 0.54}
 54%|█████▍    | 253/465 [7:46:02<5:28:48, 93.06s/it] 55%|█████▍    | 254/465 [7:47:35<5:27:15, 93.06s/it]                                                     {'loss': 0.7866, 'grad_norm': 0.1787872016429901, 'learning_rate': 5.1127270265234604e-05, 'epoch': 0.55}
 55%|█████▍    | 254/465 [7:47:35<5:27:15, 93.06s/it] 55%|█████▍    | 255/465 [7:49:01<5:18:12, 90.92s/it]                                                     {'loss': 0.8396, 'grad_norm': 0.2083553522825241, 'learning_rate': 5.075154888736653e-05, 'epoch': 0.55}
 55%|█████▍    | 255/465 [7:49:01<5:18:12, 90.92s/it] 55%|█████▌    | 256/465 [7:50:21<5:04:45, 87.49s/it]                                                     {'loss': 0.8175, 'grad_norm': 0.17726080119609833, 'learning_rate': 5.03757850570861e-05, 'epoch': 0.55}
 55%|█████▌    | 256/465 [7:50:21<5:04:45, 87.49s/it] 55%|█████▌    | 257/465 [7:52:01<5:16:43, 91.36s/it]                                                     {'loss': 0.8912, 'grad_norm': 0.15760189294815063, 'learning_rate': 5e-05, 'epoch': 0.55}
 55%|█████▌    | 257/465 [7:52:01<5:16:43, 91.36s/it] 55%|█████▌    | 258/465 [7:53:56<5:39:54, 98.53s/it]                                                     {'loss': 0.8712, 'grad_norm': 0.14984934031963348, 'learning_rate': 4.962421494291391e-05, 'epoch': 0.56}
 55%|█████▌    | 258/465 [7:53:56<5:39:54, 98.53s/it] 56%|█████▌    | 259/465 [7:55:33<5:36:07, 97.90s/it]                                                     {'loss': 0.7912, 'grad_norm': 0.1701132208108902, 'learning_rate': 4.924845111263349e-05, 'epoch': 0.56}
 56%|█████▌    | 259/465 [7:55:33<5:36:07, 97.90s/it] 56%|█████▌    | 260/465 [7:57:18<5:41:57, 100.09s/it]                                                      {'loss': 0.861, 'grad_norm': 0.15055285394191742, 'learning_rate': 4.88727297347654e-05, 'epoch': 0.56}
 56%|█████▌    | 260/465 [7:57:18<5:41:57, 100.09s/it] 56%|█████▌    | 261/465 [7:59:04<5:46:43, 101.98s/it]                                                      {'loss': 0.8546, 'grad_norm': 0.13798674941062927, 'learning_rate': 4.849707203251827e-05, 'epoch': 0.56}
 56%|█████▌    | 261/465 [7:59:04<5:46:43, 101.98s/it] 56%|█████▋    | 262/465 [8:00:39<5:37:52, 99.86s/it]                                                      {'loss': 0.7712, 'grad_norm': 0.170024111866951, 'learning_rate': 4.8121499225503976e-05, 'epoch': 0.56}
 56%|█████▋    | 262/465 [8:00:39<5:37:52, 99.86s/it] 57%|█████▋    | 263/465 [8:02:20<5:37:27, 100.23s/it]                                                      {'loss': 0.9083, 'grad_norm': 0.1851540505886078, 'learning_rate': 4.774603252853888e-05, 'epoch': 0.57}
 57%|█████▋    | 263/465 [8:02:20<5:37:27, 100.23s/it] 57%|█████▋    | 264/465 [8:03:36<5:10:43, 92.76s/it]                                                      {'loss': 0.8476, 'grad_norm': 0.18077996373176575, 'learning_rate': 4.737069315044562e-05, 'epoch': 0.57}
 57%|█████▋    | 264/465 [8:03:36<5:10:43, 92.76s/it] 57%|█████▋    | 265/465 [8:05:09<5:09:51, 92.96s/it]                                                     {'loss': 0.8599, 'grad_norm': 0.15786145627498627, 'learning_rate': 4.699550229285499e-05, 'epoch': 0.57}
 57%|█████▋    | 265/465 [8:05:09<5:09:51, 92.96s/it] 57%|█████▋    | 266/465 [8:06:53<5:19:24, 96.31s/it]                                                     {'loss': 0.8169, 'grad_norm': 0.15499088168144226, 'learning_rate': 4.662048114900837e-05, 'epoch': 0.57}
 57%|█████▋    | 266/465 [8:06:53<5:19:24, 96.31s/it] 57%|█████▋    | 267/465 [8:08:30<5:17:50, 96.32s/it]                                                     {'loss': 0.7805, 'grad_norm': 0.17904849350452423, 'learning_rate': 4.624565090256059e-05, 'epoch': 0.57}
 57%|█████▋    | 267/465 [8:08:30<5:17:50, 96.32s/it] 58%|█████▊    | 268/465 [8:10:10<5:20:11, 97.52s/it]                                                     {'loss': 0.8874, 'grad_norm': 0.1286681890487671, 'learning_rate': 4.5871032726383386e-05, 'epoch': 0.58}
 58%|█████▊    | 268/465 [8:10:10<5:20:11, 97.52s/it] 58%|█████▊    | 269/465 [8:11:43<5:13:56, 96.10s/it]                                                     {'loss': 0.8703, 'grad_norm': 0.18322473764419556, 'learning_rate': 4.549664778136933e-05, 'epoch': 0.58}
 58%|█████▊    | 269/465 [8:11:43<5:13:56, 96.10s/it] 58%|█████▊    | 270/465 [8:13:09<5:03:09, 93.28s/it]                                                     {'loss': 0.8057, 'grad_norm': 0.170892596244812, 'learning_rate': 4.5122517215236595e-05, 'epoch': 0.58}
 58%|█████▊    | 270/465 [8:13:09<5:03:09, 93.28s/it] 58%|█████▊    | 271/465 [8:14:59<5:16:57, 98.03s/it]                                                     {'loss': 0.8575, 'grad_norm': 0.13863034546375275, 'learning_rate': 4.474866216133433e-05, 'epoch': 0.58}
 58%|█████▊    | 271/465 [8:14:59<5:16:57, 98.03s/it] 58%|█████▊    | 272/465 [8:16:53<5:31:08, 102.94s/it]                                                      {'loss': 0.8207, 'grad_norm': 0.16479593515396118, 'learning_rate': 4.437510373744897e-05, 'epoch': 0.59}
 58%|█████▊    | 272/465 [8:16:53<5:31:08, 102.94s/it] 59%|█████▊    | 273/465 [8:18:24<5:17:39, 99.27s/it]                                                      {'loss': 0.8031, 'grad_norm': 0.17437468469142914, 'learning_rate': 4.400186304461136e-05, 'epoch': 0.59}
 59%|█████▊    | 273/465 [8:18:24<5:17:39, 99.27s/it] 59%|█████▉    | 274/465 [8:20:09<5:21:21, 100.95s/it]                                                      {'loss': 0.8668, 'grad_norm': 0.16804294288158417, 'learning_rate': 4.362896116590475e-05, 'epoch': 0.59}
 59%|█████▉    | 274/465 [8:20:09<5:21:21, 100.95s/it] 59%|█████▉    | 275/465 [8:21:27<4:58:14, 94.18s/it]                                                      {'loss': 0.8414, 'grad_norm': 0.17151427268981934, 'learning_rate': 4.3256419165274045e-05, 'epoch': 0.59}
 59%|█████▉    | 275/465 [8:21:27<4:58:14, 94.18s/it] 59%|█████▉    | 276/465 [8:22:46<4:42:17, 89.62s/it]                                                     {'loss': 0.8925, 'grad_norm': 0.15461492538452148, 'learning_rate': 4.288425808633575e-05, 'epoch': 0.59}
 59%|█████▉    | 276/465 [8:22:46<4:42:17, 89.62s/it] 60%|█████▉    | 277/465 [8:24:12<4:37:49, 88.67s/it]                                                     {'loss': 0.8216, 'grad_norm': 0.18296632170677185, 'learning_rate': 4.25124989511895e-05, 'epoch': 0.6}
 60%|█████▉    | 277/465 [8:24:12<4:37:49, 88.67s/it] 60%|█████▉    | 278/465 [8:26:02<4:55:47, 94.90s/it]                                                     {'loss': 0.8401, 'grad_norm': 0.15237101912498474, 'learning_rate': 4.2141162759230506e-05, 'epoch': 0.6}
 60%|█████▉    | 278/465 [8:26:02<4:55:47, 94.90s/it] 60%|██████    | 279/465 [8:27:32<4:50:16, 93.64s/it]                                                     {'loss': 0.777, 'grad_norm': 0.14984996616840363, 'learning_rate': 4.17702704859633e-05, 'epoch': 0.6}
 60%|██████    | 279/465 [8:27:32<4:50:16, 93.64s/it] 60%|██████    | 280/465 [8:28:54<4:37:11, 89.90s/it]                                                     {'loss': 0.7844, 'grad_norm': 0.17175684869289398, 'learning_rate': 4.139984308181709e-05, 'epoch': 0.6}
 60%|██████    | 280/465 [8:28:54<4:37:11, 89.90s/it] 60%|██████    | 281/465 [8:30:37<4:47:39, 93.80s/it]                                                     {'loss': 0.9109, 'grad_norm': 0.14158065617084503, 'learning_rate': 4.1029901470962104e-05, 'epoch': 0.6}
 60%|██████    | 281/465 [8:30:37<4:47:39, 93.80s/it] 61%|██████    | 282/465 [8:32:25<4:59:34, 98.22s/it]                                                     {'loss': 0.8427, 'grad_norm': 0.1914331465959549, 'learning_rate': 4.066046655012786e-05, 'epoch': 0.61}
 61%|██████    | 282/465 [8:32:25<4:59:34, 98.22s/it] 61%|██████    | 283/465 [8:34:03<4:57:45, 98.16s/it]                                                     {'loss': 0.8195, 'grad_norm': 0.16764232516288757, 'learning_rate': 4.0291559187422675e-05, 'epoch': 0.61}
 61%|██████    | 283/465 [8:34:03<4:57:45, 98.16s/it] 61%|██████    | 284/465 [8:35:54<5:07:24, 101.90s/it]                                                      {'loss': 0.7877, 'grad_norm': 0.14690932631492615, 'learning_rate': 3.992320022115491e-05, 'epoch': 0.61}
 61%|██████    | 284/465 [8:35:54<5:07:24, 101.90s/it] 61%|██████▏   | 285/465 [8:37:43<5:12:31, 104.17s/it]                                                      {'loss': 0.8071, 'grad_norm': 0.1606157273054123, 'learning_rate': 3.9555410458655905e-05, 'epoch': 0.61}
 61%|██████▏   | 285/465 [8:37:43<5:12:31, 104.17s/it] 62%|██████▏   | 286/465 [8:39:06<4:51:31, 97.72s/it]                                                      {'loss': 0.7667, 'grad_norm': 0.19417637586593628, 'learning_rate': 3.9188210675104645e-05, 'epoch': 0.62}
 62%|██████▏   | 286/465 [8:39:06<4:51:31, 97.72s/it] 62%|██████▏   | 287/465 [8:40:43<4:49:40, 97.64s/it]                                                     {'loss': 0.8314, 'grad_norm': 0.17465847730636597, 'learning_rate': 3.882162161235421e-05, 'epoch': 0.62}
 62%|██████▏   | 287/465 [8:40:43<4:49:40, 97.64s/it] 62%|██████▏   | 288/465 [8:42:31<4:57:07, 100.72s/it]                                                      {'loss': 0.8238, 'grad_norm': 0.148899108171463, 'learning_rate': 3.8455663977760215e-05, 'epoch': 0.62}
 62%|██████▏   | 288/465 [8:42:31<4:57:07, 100.72s/it] 62%|██████▏   | 289/465 [8:44:01<4:46:07, 97.54s/it]                                                      {'loss': 0.8075, 'grad_norm': 0.1752234250307083, 'learning_rate': 3.809035844301099e-05, 'epoch': 0.62}
 62%|██████▏   | 289/465 [8:44:01<4:46:07, 97.54s/it] 62%|██████▏   | 290/465 [8:45:41<4:46:33, 98.25s/it]                                                     {'loss': 0.8158, 'grad_norm': 0.17505741119384766, 'learning_rate': 3.772572564296005e-05, 'epoch': 0.62}
 62%|██████▏   | 290/465 [8:45:41<4:46:33, 98.25s/it] 63%|██████▎   | 291/465 [8:46:57<4:25:20, 91.50s/it]                                                     {'loss': 0.7962, 'grad_norm': 0.16911514103412628, 'learning_rate': 3.736178617446041e-05, 'epoch': 0.63}
 63%|██████▎   | 291/465 [8:46:57<4:25:20, 91.50s/it] 63%|██████▎   | 292/465 [8:48:43<4:35:57, 95.71s/it]                                                     {'loss': 0.8334, 'grad_norm': 0.1583256721496582, 'learning_rate': 3.699856059520119e-05, 'epoch': 0.63}
 63%|██████▎   | 292/465 [8:48:43<4:35:57, 95.71s/it] 63%|██████▎   | 293/465 [8:50:25<4:40:04, 97.70s/it]                                                     {'loss': 0.8346, 'grad_norm': 0.16851308941841125, 'learning_rate': 3.663606942254636e-05, 'epoch': 0.63}
 63%|██████▎   | 293/465 [8:50:25<4:40:04, 97.70s/it] 63%|██████▎   | 294/465 [8:52:07<4:42:28, 99.11s/it]                                                     {'loss': 0.7881, 'grad_norm': 0.1368909776210785, 'learning_rate': 3.627433313237576e-05, 'epoch': 0.63}
 63%|██████▎   | 294/465 [8:52:07<4:42:28, 99.11s/it] 63%|██████▎   | 295/465 [8:53:41<4:36:29, 97.59s/it]                                                     {'loss': 0.8514, 'grad_norm': 0.15480497479438782, 'learning_rate': 3.591337215792852e-05, 'epoch': 0.63}
 63%|██████▎   | 295/465 [8:53:41<4:36:29, 97.59s/it] 64%|██████▎   | 296/465 [8:55:25<4:40:19, 99.52s/it]                                                     {'loss': 0.7707, 'grad_norm': 0.16434478759765625, 'learning_rate': 3.555320688864889e-05, 'epoch': 0.64}
 64%|██████▎   | 296/465 [8:55:25<4:40:19, 99.52s/it] 64%|██████▍   | 297/465 [8:57:20<4:51:15, 104.02s/it]                                                      {'loss': 0.8304, 'grad_norm': 0.15231837332248688, 'learning_rate': 3.519385766903442e-05, 'epoch': 0.64}
 64%|██████▍   | 297/465 [8:57:20<4:51:15, 104.02s/it] 64%|██████▍   | 298/465 [8:58:48<4:36:21, 99.29s/it]                                                      {'loss': 0.7808, 'grad_norm': 0.17874163389205933, 'learning_rate': 3.483534479748688e-05, 'epoch': 0.64}
 64%|██████▍   | 298/465 [8:58:48<4:36:21, 99.29s/it] 64%|██████▍   | 299/465 [9:00:26<4:33:34, 98.89s/it]                                                     {'loss': 0.8037, 'grad_norm': 0.17318156361579895, 'learning_rate': 3.447768852516554e-05, 'epoch': 0.64}
 64%|██████▍   | 299/465 [9:00:26<4:33:34, 98.89s/it] 65%|██████▍   | 300/465 [9:02:08<4:34:22, 99.77s/it]                                                     {'loss': 0.7773, 'grad_norm': 0.17412829399108887, 'learning_rate': 3.4120909054843376e-05, 'epoch': 0.65}
 65%|██████▍   | 300/465 [9:02:08<4:34:22, 99.77s/it] 65%|██████▍   | 301/465 [9:03:36<4:23:30, 96.40s/it]                                                     {'loss': 0.8312, 'grad_norm': 0.1842799037694931, 'learning_rate': 3.3765026539765834e-05, 'epoch': 0.65}
 65%|██████▍   | 301/465 [9:03:36<4:23:30, 96.40s/it] 65%|██████▍   | 302/465 [9:05:11<4:20:26, 95.87s/it]                                                     {'loss': 0.864, 'grad_norm': 0.1641799956560135, 'learning_rate': 3.341006108251242e-05, 'epoch': 0.65}
 65%|██████▍   | 302/465 [9:05:11<4:20:26, 95.87s/it] 65%|██████▌   | 303/465 [9:07:00<4:29:03, 99.65s/it]                                                     {'loss': 0.9192, 'grad_norm': 0.14753352105617523, 'learning_rate': 3.3056032733861184e-05, 'epoch': 0.65}
 65%|██████▌   | 303/465 [9:07:00<4:29:03, 99.65s/it] 65%|██████▌   | 304/465 [9:08:33<4:22:34, 97.85s/it]                                                     {'loss': 0.8539, 'grad_norm': 0.16246280074119568, 'learning_rate': 3.27029614916562e-05, 'epoch': 0.65}
 65%|██████▌   | 304/465 [9:08:33<4:22:34, 97.85s/it] 66%|██████▌   | 305/465 [9:10:32<4:37:21, 104.01s/it]                                                      {'loss': 0.7955, 'grad_norm': 0.18782635033130646, 'learning_rate': 3.23508672996778e-05, 'epoch': 0.66}
 66%|██████▌   | 305/465 [9:10:32<4:37:21, 104.01s/it] 66%|██████▌   | 306/465 [9:12:14<4:34:35, 103.62s/it]                                                      {'loss': 0.7683, 'grad_norm': 0.16871538758277893, 'learning_rate': 3.19997700465162e-05, 'epoch': 0.66}
 66%|██████▌   | 306/465 [9:12:14<4:34:35, 103.62s/it] 66%|██████▌   | 307/465 [9:13:36<4:15:41, 97.10s/it]                                                      {'loss': 0.8517, 'grad_norm': 0.17625869810581207, 'learning_rate': 3.164968956444791e-05, 'epoch': 0.66}
 66%|██████▌   | 307/465 [9:13:36<4:15:41, 97.10s/it] 66%|██████▌   | 308/465 [9:15:12<4:13:02, 96.70s/it]                                                     {'loss': 0.8143, 'grad_norm': 0.15586303174495697, 'learning_rate': 3.130064562831553e-05, 'epoch': 0.66}
 66%|██████▌   | 308/465 [9:15:12<4:13:02, 96.70s/it] 66%|██████▋   | 309/465 [9:16:53<4:14:33, 97.91s/it]                                                     {'loss': 0.8396, 'grad_norm': 0.150865837931633, 'learning_rate': 3.0952657954410794e-05, 'epoch': 0.66}
 66%|██████▋   | 309/465 [9:16:53<4:14:33, 97.91s/it] 67%|██████▋   | 310/465 [9:18:48<4:26:34, 103.19s/it]                                                      {'loss': 0.8015, 'grad_norm': 0.14502035081386566, 'learning_rate': 3.0605746199360754e-05, 'epoch': 0.67}
 67%|██████▋   | 310/465 [9:18:48<4:26:34, 103.19s/it] 67%|██████▋   | 311/465 [9:20:43<4:33:47, 106.67s/it]                                                      {'loss': 0.8213, 'grad_norm': 0.17169514298439026, 'learning_rate': 3.0259929959017585e-05, 'epoch': 0.67}
 67%|██████▋   | 311/465 [9:20:43<4:33:47, 106.67s/it] 67%|██████▋   | 312/465 [9:22:29<4:31:25, 106.44s/it]                                                      {'loss': 0.8337, 'grad_norm': 0.167897030711174, 'learning_rate': 2.991522876735154e-05, 'epoch': 0.67}
 67%|██████▋   | 312/465 [9:22:29<4:31:25, 106.44s/it] 67%|██████▋   | 313/465 [9:24:11<4:26:37, 105.25s/it]                                                      {'loss': 0.8689, 'grad_norm': 0.1842924803495407, 'learning_rate': 2.9571662095347598e-05, 'epoch': 0.67}
 67%|██████▋   | 313/465 [9:24:11<4:26:37, 105.25s/it] 68%|██████▊   | 314/465 [9:25:58<4:25:57, 105.68s/it]                                                      {'loss': 0.8346, 'grad_norm': 0.15298523008823395, 'learning_rate': 2.9229249349905684e-05, 'epoch': 0.68}
 68%|██████▊   | 314/465 [9:25:58<4:25:57, 105.68s/it] 68%|██████▊   | 315/465 [9:27:35<4:17:39, 103.07s/it]                                                      {'loss': 0.8558, 'grad_norm': 0.14725425839424133, 'learning_rate': 2.888800987274433e-05, 'epoch': 0.68}
 68%|██████▊   | 315/465 [9:27:35<4:17:39, 103.07s/it] 68%|██████▊   | 316/465 [9:29:16<4:14:35, 102.52s/it]                                                      {'loss': 0.7607, 'grad_norm': 0.1694299578666687, 'learning_rate': 2.8547962939308186e-05, 'epoch': 0.68}
 68%|██████▊   | 316/465 [9:29:16<4:14:35, 102.52s/it] 68%|██████▊   | 317/465 [9:30:44<4:01:42, 97.99s/it]                                                      {'loss': 0.7958, 'grad_norm': 0.18690398335456848, 'learning_rate': 2.8209127757679243e-05, 'epoch': 0.68}
 68%|██████▊   | 317/465 [9:30:44<4:01:42, 97.99s/it] 68%|██████▊   | 318/465 [9:32:18<3:57:35, 96.98s/it]                                                     {'loss': 0.7524, 'grad_norm': 0.18088920414447784, 'learning_rate': 2.787152346749173e-05, 'epoch': 0.68}
 68%|██████▊   | 318/465 [9:32:18<3:57:35, 96.98s/it] 69%|██████▊   | 319/465 [9:34:04<4:02:04, 99.48s/it]                                                     {'loss': 0.7708, 'grad_norm': 0.15656079351902008, 'learning_rate': 2.7535169138851125e-05, 'epoch': 0.69}
 69%|██████▊   | 319/465 [9:34:04<4:02:04, 99.48s/it] 69%|██████▉   | 320/465 [9:35:41<3:58:53, 98.85s/it]                                                     {'loss': 0.7945, 'grad_norm': 0.23437464237213135, 'learning_rate': 2.7200083771256818e-05, 'epoch': 0.69}
 69%|██████▉   | 320/465 [9:35:41<3:58:53, 98.85s/it] 69%|██████▉   | 321/465 [9:37:12<3:51:16, 96.37s/it]                                                     {'loss': 0.8319, 'grad_norm': 0.14614337682724, 'learning_rate': 2.6866286292528987e-05, 'epoch': 0.69}
 69%|██████▉   | 321/465 [9:37:12<3:51:16, 96.37s/it] 69%|██████▉   | 322/465 [9:38:54<3:53:42, 98.06s/it]                                                     {'loss': 0.7641, 'grad_norm': 0.16180825233459473, 'learning_rate': 2.6533795557739405e-05, 'epoch': 0.69}
 69%|██████▉   | 322/465 [9:38:54<3:53:42, 98.06s/it] 69%|██████▉   | 323/465 [9:40:26<3:48:00, 96.34s/it]                                                     {'loss': 0.7944, 'grad_norm': 0.18773704767227173, 'learning_rate': 2.6202630348146324e-05, 'epoch': 0.69}
 69%|██████▉   | 323/465 [9:40:26<3:48:00, 96.34s/it] 70%|██████▉   | 324/465 [9:41:55<3:41:26, 94.23s/it]                                                     {'loss': 0.8093, 'grad_norm': 0.16550666093826294, 'learning_rate': 2.5872809370133705e-05, 'epoch': 0.7}
 70%|██████▉   | 324/465 [9:41:55<3:41:26, 94.23s/it] 70%|██████▉   | 325/465 [9:43:34<3:43:07, 95.62s/it]                                                     {'loss': 0.8821, 'grad_norm': 0.21187888085842133, 'learning_rate': 2.554435125415441e-05, 'epoch': 0.7}
 70%|██████▉   | 325/465 [9:43:34<3:43:07, 95.62s/it] 70%|███████   | 326/465 [9:45:13<3:43:30, 96.48s/it]                                                     {'loss': 0.8093, 'grad_norm': 0.17680171132087708, 'learning_rate': 2.5217274553677972e-05, 'epoch': 0.7}
 70%|███████   | 326/465 [9:45:13<3:43:30, 96.48s/it] 70%|███████   | 327/465 [9:46:50<3:42:31, 96.75s/it]                                                     {'loss': 0.8871, 'grad_norm': 0.15111997723579407, 'learning_rate': 2.489159774414252e-05, 'epoch': 0.7}
 70%|███████   | 327/465 [9:46:50<3:42:31, 96.75s/it] 71%|███████   | 328/465 [9:48:28<3:41:54, 97.19s/it]                                                     {'loss': 0.7952, 'grad_norm': 0.1505058854818344, 'learning_rate': 2.4567339221911083e-05, 'epoch': 0.71}
 71%|███████   | 328/465 [9:48:28<3:41:54, 97.19s/it] 71%|███████   | 329/465 [9:50:05<3:39:56, 97.03s/it]                                                     {'loss': 0.7351, 'grad_norm': 0.16055378317832947, 'learning_rate': 2.4244517303232613e-05, 'epoch': 0.71}
 71%|███████   | 329/465 [9:50:05<3:39:56, 97.03s/it] 71%|███████   | 330/465 [9:51:39<3:36:20, 96.15s/it]                                                     {'loss': 0.8081, 'grad_norm': 0.15569132566452026, 'learning_rate': 2.3923150223207176e-05, 'epoch': 0.71}
 71%|███████   | 330/465 [9:51:39<3:36:20, 96.15s/it] 71%|███████   | 331/465 [9:53:17<3:36:06, 96.77s/it]                                                     {'loss': 0.8555, 'grad_norm': 0.1793905347585678, 'learning_rate': 2.3603256134756062e-05, 'epoch': 0.71}
 71%|███████   | 331/465 [9:53:17<3:36:06, 96.77s/it] 71%|███████▏  | 332/465 [9:55:02<3:39:52, 99.19s/it]                                                     {'loss': 0.8517, 'grad_norm': 0.16717945039272308, 'learning_rate': 2.328485310759635e-05, 'epoch': 0.71}
 71%|███████▏  | 332/465 [9:55:02<3:39:52, 99.19s/it] 72%|███████▏  | 333/465 [9:56:36<3:35:07, 97.78s/it]                                                     {'loss': 0.7743, 'grad_norm': 0.1456334888935089, 'learning_rate': 2.296795912722014e-05, 'epoch': 0.72}
 72%|███████▏  | 333/465 [9:56:36<3:35:07, 97.78s/it] 72%|███████▏  | 334/465 [9:58:08<3:29:33, 95.98s/it]                                                     {'loss': 0.7934, 'grad_norm': 0.16502420604228973, 'learning_rate': 2.2652592093878666e-05, 'epoch': 0.72}
 72%|███████▏  | 334/465 [9:58:08<3:29:33, 95.98s/it] 72%|███████▏  | 335/465 [9:59:31<3:19:03, 91.87s/it]                                                     {'loss': 0.8197, 'grad_norm': 0.14753200113773346, 'learning_rate': 2.2338769821571227e-05, 'epoch': 0.72}
 72%|███████▏  | 335/465 [9:59:31<3:19:03, 91.87s/it] 72%|███████▏  | 336/465 [10:00:56<3:13:06, 89.82s/it]                                                      {'loss': 0.8157, 'grad_norm': 0.15044058859348297, 'learning_rate': 2.202651003703885e-05, 'epoch': 0.72}
 72%|███████▏  | 336/465 [10:00:56<3:13:06, 89.82s/it] 72%|███████▏  | 337/465 [10:02:47<3:25:19, 96.24s/it]                                                      {'loss': 0.8512, 'grad_norm': 0.13010267913341522, 'learning_rate': 2.1715830378763025e-05, 'epoch': 0.73}
 72%|███████▏  | 337/465 [10:02:47<3:25:19, 96.24s/it] 73%|███████▎  | 338/465 [10:04:21<3:22:30, 95.67s/it]                                                      {'loss': 0.8384, 'grad_norm': 0.16804136335849762, 'learning_rate': 2.1406748395969306e-05, 'epoch': 0.73}
 73%|███████▎  | 338/465 [10:04:21<3:22:30, 95.67s/it] 73%|███████▎  | 339/465 [10:05:55<3:19:37, 95.06s/it]                                                      {'loss': 0.8956, 'grad_norm': 0.16821561753749847, 'learning_rate': 2.1099281547636058e-05, 'epoch': 0.73}
 73%|███████▎  | 339/465 [10:05:55<3:19:37, 95.06s/it] 73%|███████▎  | 340/465 [10:07:29<3:17:21, 94.73s/it]                                                      {'loss': 0.8253, 'grad_norm': 0.14661191403865814, 'learning_rate': 2.0793447201508286e-05, 'epoch': 0.73}
 73%|███████▎  | 340/465 [10:07:29<3:17:21, 94.73s/it] 73%|███████▎  | 341/465 [10:09:08<3:18:41, 96.14s/it]                                                      {'loss': 0.8036, 'grad_norm': 0.18028698861598969, 'learning_rate': 2.0489262633116535e-05, 'epoch': 0.73}
 73%|███████▎  | 341/465 [10:09:08<3:18:41, 96.14s/it] 74%|███████▎  | 342/465 [10:10:48<3:19:25, 97.28s/it]                                                      {'loss': 0.901, 'grad_norm': 0.1242961436510086, 'learning_rate': 2.0186745024801102e-05, 'epoch': 0.74}
 74%|███████▎  | 342/465 [10:10:48<3:19:25, 97.28s/it] 74%|███████▍  | 343/465 [10:12:20<3:14:27, 95.64s/it]                                                      {'loss': 0.734, 'grad_norm': 0.13182981312274933, 'learning_rate': 1.9885911464741414e-05, 'epoch': 0.74}
 74%|███████▍  | 343/465 [10:12:20<3:14:27, 95.64s/it] 74%|███████▍  | 344/465 [10:14:04<3:18:10, 98.27s/it]                                                      {'loss': 0.8811, 'grad_norm': 0.1696578711271286, 'learning_rate': 1.9586778945990787e-05, 'epoch': 0.74}
 74%|███████▍  | 344/465 [10:14:04<3:18:10, 98.27s/it] 74%|███████▍  | 345/465 [10:15:41<3:15:42, 97.86s/it]                                                      {'loss': 0.7983, 'grad_norm': 0.16768300533294678, 'learning_rate': 1.928936436551661e-05, 'epoch': 0.74}
 74%|███████▍  | 345/465 [10:15:41<3:15:42, 97.86s/it] 74%|███████▍  | 346/465 [10:17:17<3:12:58, 97.30s/it]                                                      {'loss': 0.77, 'grad_norm': 0.19685332477092743, 'learning_rate': 1.8993684523245842e-05, 'epoch': 0.74}
 74%|███████▍  | 346/465 [10:17:17<3:12:58, 97.30s/it] 75%|███████▍  | 347/465 [10:18:51<3:09:17, 96.25s/it]                                                      {'loss': 0.8081, 'grad_norm': 0.16268479824066162, 'learning_rate': 1.8699756121115997e-05, 'epoch': 0.75}
 75%|███████▍  | 347/465 [10:18:51<3:09:17, 96.25s/it] 75%|███████▍  | 348/465 [10:20:35<3:12:29, 98.71s/it]                                                      {'loss': 0.7789, 'grad_norm': 0.16702120006084442, 'learning_rate': 1.8407595762131812e-05, 'epoch': 0.75}
 75%|███████▍  | 348/465 [10:20:35<3:12:29, 98.71s/it] 75%|███████▌  | 349/465 [10:22:02<3:03:51, 95.10s/it]                                                      {'loss': 0.7933, 'grad_norm': 0.16157396137714386, 'learning_rate': 1.811721994942731e-05, 'epoch': 0.75}
 75%|███████▌  | 349/465 [10:22:02<3:03:51, 95.10s/it] 75%|███████▌  | 350/465 [10:23:37<3:02:09, 95.04s/it]                                                      {'loss': 0.7577, 'grad_norm': 0.19322197139263153, 'learning_rate': 1.7828645085333644e-05, 'epoch': 0.75}
 75%|███████▌  | 350/465 [10:23:37<3:02:09, 95.04s/it] 75%|███████▌  | 351/465 [10:25:26<3:08:29, 99.21s/it]                                                      {'loss': 0.8858, 'grad_norm': 0.1591736376285553, 'learning_rate': 1.7541887470452605e-05, 'epoch': 0.76}
 75%|███████▌  | 351/465 [10:25:26<3:08:29, 99.21s/it] 76%|███████▌  | 352/465 [10:26:51<2:58:43, 94.90s/it]                                                      {'loss': 0.8279, 'grad_norm': 0.17541660368442535, 'learning_rate': 1.725696330273575e-05, 'epoch': 0.76}
 76%|███████▌  | 352/465 [10:26:51<2:58:43, 94.90s/it] 76%|███████▌  | 353/465 [10:28:32<3:00:47, 96.86s/it]                                                      {'loss': 0.798, 'grad_norm': 0.17688237130641937, 'learning_rate': 1.6973888676569593e-05, 'epoch': 0.76}
 76%|███████▌  | 353/465 [10:28:32<3:00:47, 96.86s/it] 76%|███████▌  | 354/465 [10:30:04<2:56:32, 95.43s/it]                                                      {'loss': 0.7981, 'grad_norm': 0.15111123025417328, 'learning_rate': 1.669267958186633e-05, 'epoch': 0.76}
 76%|███████▌  | 354/465 [10:30:04<2:56:32, 95.43s/it] 76%|███████▋  | 355/465 [10:31:44<2:57:15, 96.69s/it]                                                      {'loss': 0.8199, 'grad_norm': 0.17409057915210724, 'learning_rate': 1.6413351903160763e-05, 'epoch': 0.76}
 76%|███████▋  | 355/465 [10:31:44<2:57:15, 96.69s/it] 77%|███████▋  | 356/465 [10:33:31<3:01:30, 99.92s/it]                                                      {'loss': 0.78, 'grad_norm': 0.24460121989250183, 'learning_rate': 1.6135921418712956e-05, 'epoch': 0.77}
 77%|███████▋  | 356/465 [10:33:31<3:01:30, 99.92s/it] 77%|███████▋  | 357/465 [10:35:18<3:03:23, 101.89s/it]                                                       {'loss': 0.8243, 'grad_norm': 0.1905689537525177, 'learning_rate': 1.586040379961695e-05, 'epoch': 0.77}
 77%|███████▋  | 357/465 [10:35:18<3:03:23, 101.89s/it] 77%|███████▋  | 358/465 [10:36:36<2:49:11, 94.87s/it]                                                       {'loss': 0.8219, 'grad_norm': 0.17913389205932617, 'learning_rate': 1.558681460891567e-05, 'epoch': 0.77}
 77%|███████▋  | 358/465 [10:36:36<2:49:11, 94.87s/it] 77%|███████▋  | 359/465 [10:38:02<2:42:42, 92.10s/it]                                                      {'loss': 0.7339, 'grad_norm': 0.17610593140125275, 'learning_rate': 1.5315169300721694e-05, 'epoch': 0.77}
 77%|███████▋  | 359/465 [10:38:02<2:42:42, 92.10s/it] 77%|███████▋  | 360/465 [10:39:39<2:43:37, 93.50s/it]                                                      {'loss': 0.7945, 'grad_norm': 0.19587284326553345, 'learning_rate': 1.5045483219344386e-05, 'epoch': 0.77}
 77%|███████▋  | 360/465 [10:39:39<2:43:37, 93.50s/it] 78%|███████▊  | 361/465 [10:41:18<2:45:12, 95.31s/it]                                                      {'loss': 0.8837, 'grad_norm': 0.15222099423408508, 'learning_rate': 1.4777771598423146e-05, 'epoch': 0.78}
 78%|███████▊  | 361/465 [10:41:18<2:45:12, 95.31s/it] 78%|███████▊  | 362/465 [10:42:49<2:41:21, 93.99s/it]                                                      {'loss': 0.8514, 'grad_norm': 0.16889089345932007, 'learning_rate': 1.4512049560066836e-05, 'epoch': 0.78}
 78%|███████▊  | 362/465 [10:42:49<2:41:21, 93.99s/it] 78%|███████▊  | 363/465 [10:44:07<2:31:18, 89.00s/it]                                                      {'loss': 0.8692, 'grad_norm': 0.1577160805463791, 'learning_rate': 1.4248332113999708e-05, 'epoch': 0.78}
 78%|███████▊  | 363/465 [10:44:07<2:31:18, 89.00s/it] 78%|███████▊  | 364/465 [10:45:30<2:26:57, 87.30s/it]                                                      {'loss': 0.8198, 'grad_norm': 0.1742713451385498, 'learning_rate': 1.3986634156713418e-05, 'epoch': 0.78}
 78%|███████▊  | 364/465 [10:45:30<2:26:57, 87.30s/it] 78%|███████▊  | 365/465 [10:47:05<2:29:31, 89.72s/it]                                                      {'loss': 0.8061, 'grad_norm': 0.2070607990026474, 'learning_rate': 1.3726970470625706e-05, 'epoch': 0.79}
 78%|███████▊  | 365/465 [10:47:05<2:29:31, 89.72s/it] 79%|███████▊  | 366/465 [10:48:48<2:34:35, 93.69s/it]                                                      {'loss': 0.887, 'grad_norm': 0.20774130523204803, 'learning_rate': 1.3469355723245303e-05, 'epoch': 0.79}
 79%|███████▊  | 366/465 [10:48:48<2:34:35, 93.69s/it] 79%|███████▉  | 367/465 [10:50:29<2:36:18, 95.70s/it]                                                      {'loss': 0.7539, 'grad_norm': 0.16976703703403473, 'learning_rate': 1.3213804466343421e-05, 'epoch': 0.79}
 79%|███████▉  | 367/465 [10:50:29<2:36:18, 95.70s/it] 79%|███████▉  | 368/465 [10:52:01<2:33:04, 94.69s/it]                                                      {'loss': 0.7836, 'grad_norm': 0.15887334942817688, 'learning_rate': 1.2960331135131826e-05, 'epoch': 0.79}
 79%|███████▉  | 368/465 [10:52:01<2:33:04, 94.69s/it] 79%|███████▉  | 369/465 [10:53:26<2:26:52, 91.79s/it]                                                      {'loss': 0.8126, 'grad_norm': 0.16997359693050385, 'learning_rate': 1.2708950047447372e-05, 'epoch': 0.79}
 79%|███████▉  | 369/465 [10:53:26<2:26:52, 91.79s/it] 80%|███████▉  | 370/465 [10:55:06<2:29:00, 94.11s/it]                                                      {'loss': 0.8026, 'grad_norm': 0.18049566447734833, 'learning_rate': 1.2459675402943289e-05, 'epoch': 0.8}
 80%|███████▉  | 370/465 [10:55:06<2:29:00, 94.11s/it] 80%|███████▉  | 371/465 [10:56:43<2:28:52, 95.02s/it]                                                      {'loss': 0.8334, 'grad_norm': 0.15317772328853607, 'learning_rate': 1.2212521282287092e-05, 'epoch': 0.8}
 80%|███████▉  | 371/465 [10:56:43<2:28:52, 95.02s/it] 80%|████████  | 372/465 [10:58:13<2:25:08, 93.64s/it]                                                      {'loss': 0.8671, 'grad_norm': 0.16611787676811218, 'learning_rate': 1.1967501646365148e-05, 'epoch': 0.8}
 80%|████████  | 372/465 [10:58:13<2:25:08, 93.64s/it] 80%|████████  | 373/465 [10:59:43<2:22:02, 92.63s/it]                                                      {'loss': 0.7403, 'grad_norm': 0.1569908857345581, 'learning_rate': 1.172463033549418e-05, 'epoch': 0.8}
 80%|████████  | 373/465 [10:59:43<2:22:02, 92.63s/it] 80%|████████  | 374/465 [11:01:32<2:27:37, 97.33s/it]                                                      {'loss': 0.7805, 'grad_norm': 0.17170177400112152, 'learning_rate': 1.1483921068639353e-05, 'epoch': 0.8}
 80%|████████  | 374/465 [11:01:32<2:27:37, 97.33s/it] 81%|████████  | 375/465 [11:03:13<2:27:53, 98.60s/it]                                                      {'loss': 0.8853, 'grad_norm': 0.19092769920825958, 'learning_rate': 1.1245387442639454e-05, 'epoch': 0.81}
 81%|████████  | 375/465 [11:03:13<2:27:53, 98.60s/it] 81%|████████  | 376/465 [11:04:53<2:26:40, 98.88s/it]                                                      {'loss': 0.8344, 'grad_norm': 0.15662860870361328, 'learning_rate': 1.1009042931438785e-05, 'epoch': 0.81}
 81%|████████  | 376/465 [11:04:53<2:26:40, 98.88s/it] 81%|████████  | 377/465 [11:06:31<2:24:45, 98.70s/it]                                                      {'loss': 0.8163, 'grad_norm': 0.15359440445899963, 'learning_rate': 1.077490088532605e-05, 'epoch': 0.81}
 81%|████████  | 377/465 [11:06:31<2:24:45, 98.70s/it] 81%|████████▏ | 378/465 [11:08:04<2:20:47, 97.09s/it]                                                      {'loss': 0.8157, 'grad_norm': 0.17940819263458252, 'learning_rate': 1.0542974530180327e-05, 'epoch': 0.81}
 81%|████████▏ | 378/465 [11:08:04<2:20:47, 97.09s/it] 82%|████████▏ | 379/465 [11:09:38<2:17:47, 96.13s/it]                                                      {'loss': 0.8067, 'grad_norm': 0.17654114961624146, 'learning_rate': 1.0313276966723867e-05, 'epoch': 0.82}
 82%|████████▏ | 379/465 [11:09:38<2:17:47, 96.13s/it] 82%|████████▏ | 380/465 [11:11:06<2:12:42, 93.68s/it]                                                      {'loss': 0.772, 'grad_norm': 0.17453870177268982, 'learning_rate': 1.00858211697822e-05, 'epoch': 0.82}
 82%|████████▏ | 380/465 [11:11:06<2:12:42, 93.68s/it] 82%|████████▏ | 381/465 [11:12:41<2:11:37, 94.02s/it]                                                      {'loss': 0.817, 'grad_norm': 0.1412034034729004, 'learning_rate': 9.860619987551157e-06, 'epoch': 0.82}
 82%|████████▏ | 381/465 [11:12:41<2:11:37, 94.02s/it] 82%|████████▏ | 382/465 [11:14:33<2:17:21, 99.30s/it]                                                      {'loss': 0.7497, 'grad_norm': 0.19428758323192596, 'learning_rate': 9.637686140871121e-06, 'epoch': 0.82}
 82%|████████▏ | 382/465 [11:14:33<2:17:21, 99.30s/it] 82%|████████▏ | 383/465 [11:16:14<2:16:20, 99.77s/it]                                                      {'loss': 0.8109, 'grad_norm': 0.15655417740345, 'learning_rate': 9.417032222508476e-06, 'epoch': 0.82}
 82%|████████▏ | 383/465 [11:16:14<2:16:20, 99.77s/it] 83%|████████▎ | 384/465 [11:17:47<2:12:11, 97.92s/it]                                                      {'loss': 0.7406, 'grad_norm': 0.19888140261173248, 'learning_rate': 9.19867069644434e-06, 'epoch': 0.83}
 83%|████████▎ | 384/465 [11:17:47<2:12:11, 97.92s/it] 83%|████████▎ | 385/465 [11:19:29<2:12:04, 99.06s/it]                                                      {'loss': 0.8062, 'grad_norm': 0.15711919963359833, 'learning_rate': 8.982613897170438e-06, 'epoch': 0.83}
 83%|████████▎ | 385/465 [11:19:29<2:12:04, 99.06s/it] 83%|████████▎ | 386/465 [11:21:01<2:07:46, 97.04s/it]                                                      {'loss': 0.8133, 'grad_norm': 0.1706402599811554, 'learning_rate': 8.76887402899243e-06, 'epoch': 0.83}
 83%|████████▎ | 386/465 [11:21:01<2:07:46, 97.04s/it] 83%|████████▎ | 387/465 [11:22:26<2:01:27, 93.44s/it]                                                      {'loss': 0.7551, 'grad_norm': 0.15620183944702148, 'learning_rate': 8.557463165340478e-06, 'epoch': 0.83}
 83%|████████▎ | 387/465 [11:22:26<2:01:27, 93.44s/it] 83%|████████▎ | 388/465 [11:24:15<2:05:52, 98.09s/it]                                                      {'loss': 0.8228, 'grad_norm': 0.21975940465927124, 'learning_rate': 8.348393248087289e-06, 'epoch': 0.83}
 83%|████████▎ | 388/465 [11:24:15<2:05:52, 98.09s/it] 84%|████████▎ | 389/465 [11:25:51<2:03:26, 97.45s/it]                                                      {'loss': 0.7982, 'grad_norm': 0.16053169965744019, 'learning_rate': 8.141676086873572e-06, 'epoch': 0.84}
 84%|████████▎ | 389/465 [11:25:51<2:03:26, 97.45s/it] 84%|████████▍ | 390/465 [11:27:28<2:01:25, 97.14s/it]                                                      {'loss': 0.7493, 'grad_norm': 0.15952329337596893, 'learning_rate': 7.937323358440935e-06, 'epoch': 0.84}
 84%|████████▍ | 390/465 [11:27:28<2:01:25, 97.14s/it] 84%|████████▍ | 391/465 [11:29:23<2:06:29, 102.56s/it]                                                       {'loss': 0.7798, 'grad_norm': 0.15711842477321625, 'learning_rate': 7.735346605972322e-06, 'epoch': 0.84}
 84%|████████▍ | 391/465 [11:29:23<2:06:29, 102.56s/it] 84%|████████▍ | 392/465 [11:30:55<2:00:52, 99.35s/it]                                                       {'loss': 0.737, 'grad_norm': 0.18054606020450592, 'learning_rate': 7.535757238439939e-06, 'epoch': 0.84}
 84%|████████▍ | 392/465 [11:30:55<2:00:52, 99.35s/it] 85%|████████▍ | 393/465 [11:32:38<2:00:46, 100.65s/it]                                                       {'loss': 0.7716, 'grad_norm': 0.18451017141342163, 'learning_rate': 7.338566529960816e-06, 'epoch': 0.85}
 85%|████████▍ | 393/465 [11:32:38<2:00:46, 100.65s/it] 85%|████████▍ | 394/465 [11:34:16<1:58:05, 99.80s/it]                                                       {'loss': 0.8159, 'grad_norm': 0.15926183760166168, 'learning_rate': 7.143785619160026e-06, 'epoch': 0.85}
 85%|████████▍ | 394/465 [11:34:16<1:58:05, 99.80s/it] 85%|████████▍ | 395/465 [11:35:58<1:57:17, 100.54s/it]                                                       {'loss': 0.8048, 'grad_norm': 0.21847321093082428, 'learning_rate': 6.951425508541432e-06, 'epoch': 0.85}
 85%|████████▍ | 395/465 [11:35:58<1:57:17, 100.54s/it] 85%|████████▌ | 396/465 [11:37:35<1:54:21, 99.44s/it]                                                       {'loss': 0.8768, 'grad_norm': 0.17779649794101715, 'learning_rate': 6.761497063866207e-06, 'epoch': 0.85}
 85%|████████▌ | 396/465 [11:37:35<1:54:21, 99.44s/it] 85%|████████▌ | 397/465 [11:39:23<1:55:33, 101.96s/it]                                                       {'loss': 0.8447, 'grad_norm': 0.1595103144645691, 'learning_rate': 6.574011013539111e-06, 'epoch': 0.85}
 85%|████████▌ | 397/465 [11:39:23<1:55:33, 101.96s/it] 86%|████████▌ | 398/465 [11:40:57<1:51:00, 99.41s/it]                                                       {'loss': 0.8184, 'grad_norm': 0.14282506704330444, 'learning_rate': 6.388977948002406e-06, 'epoch': 0.86}
 86%|████████▌ | 398/465 [11:40:57<1:51:00, 99.41s/it] 86%|████████▌ | 399/465 [11:42:49<1:53:49, 103.47s/it]                                                       {'loss': 0.7937, 'grad_norm': 0.17363643646240234, 'learning_rate': 6.206408319137702e-06, 'epoch': 0.86}
 86%|████████▌ | 399/465 [11:42:49<1:53:49, 103.47s/it] 86%|████████▌ | 400/465 [11:44:20<1:47:44, 99.45s/it]                                                       {'loss': 0.7736, 'grad_norm': 0.1730382889509201, 'learning_rate': 6.026312439675552e-06, 'epoch': 0.86}
 86%|████████▌ | 400/465 [11:44:20<1:47:44, 99.45s/it]
***** Running Evaluation *****

***** Running Evaluation *****

***** Running Evaluation *****
  Num examples = 3305
  Num examples = 3305
  Batch size = 1
  Batch size = 1
  Num examples = 3305
  Batch size = 1
[INFO|trainer.py:4643] 2026-01-03 06:29:39,714 >> 
***** Running Evaluation *****
[INFO|trainer.py:4645] 2026-01-03 06:29:39,714 >>   Num examples = 3305
[INFO|trainer.py:4648] 2026-01-03 06:29:39,714 >>   Batch size = 1

  0%|          | 0/827 [00:00<?, ?it/s][A
  0%|          | 2/827 [00:04<28:13,  2.05s/it][A
  0%|          | 3/827 [00:06<31:13,  2.27s/it][A
  0%|          | 4/827 [00:09<34:45,  2.53s/it][A
  1%|          | 5/827 [00:13<40:18,  2.94s/it][A
  1%|          | 6/827 [00:16<43:05,  3.15s/it][A
  1%|          | 7/827 [00:20<46:41,  3.42s/it][A
  1%|          | 8/827 [00:26<55:50,  4.09s/it][A
  1%|          | 9/827 [00:33<1:06:25,  4.87s/it][A
  1%|          | 10/827 [00:39<1:14:49,  5.50s/it][A
  1%|▏         | 11/827 [00:43<1:05:48,  4.84s/it][A
  1%|▏         | 12/827 [00:46<57:06,  4.20s/it]  [A
  2%|▏         | 13/827 [00:49<52:19,  3.86s/it][A
  2%|▏         | 14/827 [00:52<51:28,  3.80s/it][A
  2%|▏         | 15/827 [00:57<53:11,  3.93s/it][A
  2%|▏         | 16/827 [01:00<51:21,  3.80s/it][A
  2%|▏         | 17/827 [01:03<48:09,  3.57s/it][A
  2%|▏         | 18/827 [01:06<47:10,  3.50s/it][A
  2%|▏         | 19/827 [01:11<51:44,  3.84s/it][A
  2%|▏         | 20/827 [01:16<57:46,  4.30s/it][A
  3%|▎         | 21/827 [01:20<55:21,  4.12s/it][A
  3%|▎         | 22/827 [01:25<57:10,  4.26s/it][A
  3%|▎         | 23/827 [01:28<51:34,  3.85s/it][A
  3%|▎         | 24/827 [01:31<51:15,  3.83s/it][A
  3%|▎         | 25/827 [01:36<53:37,  4.01s/it][A
  3%|▎         | 26/827 [01:41<58:11,  4.36s/it][A
  3%|▎         | 27/827 [01:45<57:55,  4.34s/it][A
  3%|▎         | 28/827 [01:48<51:53,  3.90s/it][A
  4%|▎         | 29/827 [01:53<54:48,  4.12s/it][A
  4%|▎         | 30/827 [01:58<57:49,  4.35s/it][A
  4%|▎         | 31/827 [02:02<55:40,  4.20s/it][A
  4%|▍         | 32/827 [02:05<54:45,  4.13s/it][A
  4%|▍         | 33/827 [02:11<59:02,  4.46s/it][A
  4%|▍         | 34/827 [02:14<53:46,  4.07s/it][A
  4%|▍         | 35/827 [02:17<50:50,  3.85s/it][A
  4%|▍         | 36/827 [02:20<46:07,  3.50s/it][A
  4%|▍         | 37/827 [02:25<50:52,  3.86s/it][A
  5%|▍         | 38/827 [02:30<55:05,  4.19s/it][A
  5%|▍         | 39/827 [02:34<56:27,  4.30s/it][A
  5%|▍         | 40/827 [02:39<58:50,  4.49s/it][A
  5%|▍         | 41/827 [02:42<53:01,  4.05s/it][A
  5%|▌         | 42/827 [02:46<51:24,  3.93s/it][A
  5%|▌         | 43/827 [02:49<50:04,  3.83s/it][A
  5%|▌         | 44/827 [02:53<47:59,  3.68s/it][A
  5%|▌         | 45/827 [02:57<50:01,  3.84s/it][A
  6%|▌         | 46/827 [03:02<55:17,  4.25s/it][A
  6%|▌         | 47/827 [03:07<56:47,  4.37s/it][A
  6%|▌         | 48/827 [03:10<52:52,  4.07s/it][A
  6%|▌         | 49/827 [03:13<50:02,  3.86s/it][A
  6%|▌         | 50/827 [03:16<46:04,  3.56s/it][A
  6%|▌         | 51/827 [03:21<51:04,  3.95s/it][A
  6%|▋         | 52/827 [03:27<58:08,  4.50s/it][A
  6%|▋         | 53/827 [03:30<51:11,  3.97s/it][A
  7%|▋         | 54/827 [03:33<46:48,  3.63s/it][A
  7%|▋         | 55/827 [03:36<44:23,  3.45s/it][A
  7%|▋         | 56/827 [03:39<43:10,  3.36s/it][A
  7%|▋         | 57/827 [03:42<44:07,  3.44s/it][A
  7%|▋         | 58/827 [03:45<42:25,  3.31s/it][A
  7%|▋         | 59/827 [03:49<42:19,  3.31s/it][A
  7%|▋         | 60/827 [03:53<45:13,  3.54s/it][A
  7%|▋         | 61/827 [03:58<50:32,  3.96s/it][A
  7%|▋         | 62/827 [04:00<45:55,  3.60s/it][A
  8%|▊         | 63/827 [04:03<43:07,  3.39s/it][A
  8%|▊         | 64/827 [04:06<41:42,  3.28s/it][A
  8%|▊         | 65/827 [04:09<41:02,  3.23s/it][A
  8%|▊         | 66/827 [04:14<47:48,  3.77s/it][A
  8%|▊         | 67/827 [04:19<51:33,  4.07s/it][A
  8%|▊         | 68/827 [04:22<45:37,  3.61s/it][A
  8%|▊         | 69/827 [04:25<44:21,  3.51s/it][A
  8%|▊         | 70/827 [04:28<41:29,  3.29s/it][A
  9%|▊         | 71/827 [04:32<44:17,  3.52s/it][A
  9%|▊         | 72/827 [04:36<48:12,  3.83s/it][A
  9%|▉         | 73/827 [04:40<46:04,  3.67s/it][A
  9%|▉         | 74/827 [04:45<52:28,  4.18s/it][A
  9%|▉         | 75/827 [04:50<54:55,  4.38s/it][A
  9%|▉         | 76/827 [04:53<49:35,  3.96s/it][A
  9%|▉         | 77/827 [04:56<45:40,  3.65s/it][A
  9%|▉         | 78/827 [04:59<43:14,  3.46s/it][A
 10%|▉         | 79/827 [05:02<40:27,  3.25s/it][A
 10%|▉         | 80/827 [05:06<44:18,  3.56s/it][A
 10%|▉         | 81/827 [05:10<47:39,  3.83s/it][A
 10%|▉         | 82/827 [05:15<48:59,  3.95s/it][A
 10%|█         | 83/827 [05:19<49:12,  3.97s/it][A
 10%|█         | 84/827 [05:23<49:25,  3.99s/it][A
 10%|█         | 85/827 [05:27<49:41,  4.02s/it][A
 10%|█         | 86/827 [05:30<46:05,  3.73s/it][A
 11%|█         | 87/827 [05:36<54:35,  4.43s/it][A
 11%|█         | 88/827 [05:46<1:13:54,  6.00s/it][A
 11%|█         | 89/827 [05:53<1:18:28,  6.38s/it][A
 11%|█         | 90/827 [05:58<1:13:28,  5.98s/it][A
 11%|█         | 91/827 [06:01<1:04:02,  5.22s/it][A
 11%|█         | 92/827 [06:05<57:51,  4.72s/it]  [A
 11%|█         | 93/827 [06:08<50:41,  4.14s/it][A
 11%|█▏        | 94/827 [06:11<46:48,  3.83s/it][A
 11%|█▏        | 95/827 [06:14<43:49,  3.59s/it][A
 12%|█▏        | 96/827 [06:16<40:31,  3.33s/it][A
 12%|█▏        | 97/827 [06:19<36:50,  3.03s/it][A
 12%|█▏        | 98/827 [06:21<34:53,  2.87s/it][A
 12%|█▏        | 99/827 [06:24<33:51,  2.79s/it][A
 12%|█▏        | 100/827 [06:26<31:37,  2.61s/it][A
 12%|█▏        | 101/827 [06:30<34:43,  2.87s/it][A
 12%|█▏        | 102/827 [06:33<35:33,  2.94s/it][A
 12%|█▏        | 103/827 [06:36<35:06,  2.91s/it][A
 13%|█▎        | 104/827 [06:38<34:44,  2.88s/it][A
 13%|█▎        | 105/827 [06:42<35:41,  2.97s/it][A
 13%|█▎        | 106/827 [06:48<47:02,  3.91s/it][A
 13%|█▎        | 107/827 [06:54<55:17,  4.61s/it][A
 13%|█▎        | 108/827 [06:58<54:54,  4.58s/it][A
 13%|█▎        | 109/827 [07:02<51:21,  4.29s/it][A
 13%|█▎        | 110/827 [07:06<48:26,  4.05s/it][A
 13%|█▎        | 111/827 [07:10<51:12,  4.29s/it][A
 14%|█▎        | 112/827 [07:16<56:26,  4.74s/it][A
 14%|█▎        | 113/827 [07:20<53:15,  4.48s/it][A
 14%|█▍        | 114/827 [07:23<49:08,  4.14s/it][A
 14%|█▍        | 115/827 [07:27<45:47,  3.86s/it][A
 14%|█▍        | 116/827 [07:30<44:55,  3.79s/it][A
 14%|█▍        | 117/827 [07:35<46:49,  3.96s/it][A
 14%|█▍        | 118/827 [07:40<52:03,  4.41s/it][A
 14%|█▍        | 119/827 [07:45<54:46,  4.64s/it][A
 15%|█▍        | 120/827 [07:50<54:22,  4.61s/it][A
 15%|█▍        | 121/827 [07:53<49:29,  4.21s/it][A
 15%|█▍        | 122/827 [07:56<45:37,  3.88s/it][A
 15%|█▍        | 123/827 [07:59<41:19,  3.52s/it][A
 15%|█▍        | 124/827 [08:02<40:36,  3.47s/it][A
 15%|█▌        | 125/827 [08:06<41:52,  3.58s/it][A
 15%|█▌        | 126/827 [08:09<40:53,  3.50s/it][A
 15%|█▌        | 127/827 [08:12<39:38,  3.40s/it][A
 15%|█▌        | 128/827 [08:16<40:39,  3.49s/it][A
 16%|█▌        | 129/827 [08:19<37:55,  3.26s/it][A
 16%|█▌        | 130/827 [08:22<39:08,  3.37s/it][A
 16%|█▌        | 131/827 [08:26<40:54,  3.53s/it][A
 16%|█▌        | 132/827 [08:31<44:18,  3.83s/it][A
 16%|█▌        | 133/827 [08:35<46:16,  4.00s/it][A
 16%|█▌        | 134/827 [08:39<44:06,  3.82s/it][A
 16%|█▋        | 135/827 [08:43<46:49,  4.06s/it][A
 16%|█▋        | 136/827 [08:49<53:23,  4.64s/it][A
 17%|█▋        | 137/827 [08:52<48:08,  4.19s/it][A
 17%|█▋        | 138/827 [08:58<53:55,  4.70s/it][A
 17%|█▋        | 139/827 [09:05<1:00:39,  5.29s/it][A
 17%|█▋        | 140/827 [09:08<53:58,  4.71s/it]  [A
 17%|█▋        | 141/827 [09:12<50:49,  4.45s/it][A
 17%|█▋        | 142/827 [09:15<46:32,  4.08s/it][A
 17%|█▋        | 143/827 [09:19<44:13,  3.88s/it][A
 17%|█▋        | 144/827 [09:22<40:20,  3.54s/it][A
 18%|█▊        | 145/827 [09:25<40:44,  3.58s/it][A
 18%|█▊        | 146/827 [09:29<41:36,  3.67s/it][A
 18%|█▊        | 147/827 [09:32<40:04,  3.54s/it][A
 18%|█▊        | 148/827 [09:36<38:50,  3.43s/it][A
 18%|█▊        | 149/827 [09:39<37:55,  3.36s/it][A
 18%|█▊        | 150/827 [09:41<35:23,  3.14s/it][A
 18%|█▊        | 151/827 [09:44<33:16,  2.95s/it][A
 18%|█▊        | 152/827 [09:47<33:44,  3.00s/it][A
 19%|█▊        | 153/827 [09:50<33:59,  3.03s/it][A
 19%|█▊        | 154/827 [09:54<36:45,  3.28s/it][A
 19%|█▊        | 155/827 [09:59<42:08,  3.76s/it][A
 19%|█▉        | 156/827 [10:03<45:02,  4.03s/it][A
 19%|█▉        | 157/827 [10:07<42:18,  3.79s/it][A
 19%|█▉        | 158/827 [10:10<39:38,  3.55s/it][A
 19%|█▉        | 159/827 [10:14<42:09,  3.79s/it][A
 19%|█▉        | 160/827 [10:18<43:36,  3.92s/it][A
 19%|█▉        | 161/827 [10:21<39:16,  3.54s/it][A
 20%|█▉        | 162/827 [10:25<39:46,  3.59s/it][A
 20%|█▉        | 163/827 [10:29<40:39,  3.67s/it][A
 20%|█▉        | 164/827 [10:32<39:40,  3.59s/it][A
 20%|█▉        | 165/827 [10:35<37:47,  3.43s/it][A
 20%|██        | 166/827 [10:41<44:56,  4.08s/it][A
 20%|██        | 167/827 [10:49<1:00:13,  5.47s/it][A
 20%|██        | 168/827 [10:54<59:11,  5.39s/it]  [A
 20%|██        | 169/827 [10:58<51:45,  4.72s/it][A
 21%|██        | 170/827 [11:02<49:05,  4.48s/it][A
 21%|██        | 171/827 [11:05<46:48,  4.28s/it][A
 21%|██        | 172/827 [11:08<41:36,  3.81s/it][A
 21%|██        | 173/827 [11:11<37:53,  3.48s/it][A
 21%|██        | 174/827 [11:14<36:17,  3.34s/it][A
 21%|██        | 175/827 [11:17<37:18,  3.43s/it][A
 21%|██▏       | 176/827 [11:20<34:39,  3.19s/it][A
 21%|██▏       | 177/827 [11:23<33:04,  3.05s/it][A
 22%|██▏       | 178/827 [11:27<36:01,  3.33s/it][A
 22%|██▏       | 179/827 [11:32<42:26,  3.93s/it][A
 22%|██▏       | 180/827 [11:36<42:23,  3.93s/it][A
 22%|██▏       | 181/827 [11:40<41:17,  3.83s/it][A
 22%|██▏       | 182/827 [11:44<43:37,  4.06s/it][A
 22%|██▏       | 183/827 [11:47<40:19,  3.76s/it][A
 22%|██▏       | 184/827 [11:52<42:12,  3.94s/it][A
 22%|██▏       | 185/827 [11:56<44:56,  4.20s/it][A
 22%|██▏       | 186/827 [12:00<41:19,  3.87s/it][A
 23%|██▎       | 187/827 [12:02<37:42,  3.53s/it][A
 23%|██▎       | 188/827 [12:06<38:25,  3.61s/it][A
 23%|██▎       | 189/827 [12:11<42:02,  3.95s/it][A
 23%|██▎       | 190/827 [12:15<41:30,  3.91s/it][A
 23%|██▎       | 191/827 [12:19<41:26,  3.91s/it][A
 23%|██▎       | 192/827 [12:23<43:17,  4.09s/it][A
 23%|██▎       | 193/827 [12:27<42:36,  4.03s/it][A
 23%|██▎       | 194/827 [12:30<39:05,  3.71s/it][A
 24%|██▎       | 195/827 [12:33<36:54,  3.50s/it][A
 24%|██▎       | 196/827 [12:35<33:41,  3.20s/it][A
 24%|██▍       | 197/827 [12:38<32:02,  3.05s/it][A
 24%|██▍       | 198/827 [12:41<32:46,  3.13s/it][A
 24%|██▍       | 199/827 [12:45<33:35,  3.21s/it][A
 24%|██▍       | 200/827 [12:49<36:44,  3.52s/it][A
 24%|██▍       | 201/827 [12:54<40:54,  3.92s/it][A
 24%|██▍       | 202/827 [12:59<44:03,  4.23s/it][A
 25%|██▍       | 203/827 [13:04<45:30,  4.38s/it][A
 25%|██▍       | 204/827 [13:09<49:22,  4.75s/it][A
 25%|██▍       | 205/827 [13:13<44:47,  4.32s/it][A
 25%|██▍       | 206/827 [13:16<41:12,  3.98s/it][A
 25%|██▌       | 207/827 [13:19<38:55,  3.77s/it][A
 25%|██▌       | 208/827 [13:22<36:56,  3.58s/it][A
 25%|██▌       | 209/827 [13:25<35:30,  3.45s/it][A
 25%|██▌       | 210/827 [13:28<34:36,  3.37s/it][A
 26%|██▌       | 211/827 [13:33<38:36,  3.76s/it][A
 26%|██▌       | 212/827 [13:38<41:37,  4.06s/it][A
 26%|██▌       | 213/827 [13:43<45:23,  4.44s/it][A
 26%|██▌       | 214/827 [13:47<41:57,  4.11s/it][A
 26%|██▌       | 215/827 [13:50<38:57,  3.82s/it][A
 26%|██▌       | 216/827 [13:53<37:35,  3.69s/it][A
 26%|██▌       | 217/827 [13:57<38:11,  3.76s/it][A
 26%|██▋       | 218/827 [14:01<38:03,  3.75s/it][A
 26%|██▋       | 219/827 [14:04<35:31,  3.51s/it][A
 27%|██▋       | 220/827 [14:08<37:45,  3.73s/it][A
 27%|██▋       | 221/827 [14:13<43:01,  4.26s/it][A
 27%|██▋       | 222/827 [14:17<39:41,  3.94s/it][A
 27%|██▋       | 223/827 [14:20<37:53,  3.76s/it][A
 27%|██▋       | 224/827 [14:23<35:29,  3.53s/it][A
 27%|██▋       | 225/827 [14:26<34:12,  3.41s/it][A
 27%|██▋       | 226/827 [14:29<31:56,  3.19s/it][A
 27%|██▋       | 227/827 [14:33<35:21,  3.54s/it][A
 28%|██▊       | 228/827 [14:38<39:58,  4.00s/it][A
 28%|██▊       | 229/827 [14:41<37:31,  3.77s/it][A
 28%|██▊       | 230/827 [14:45<37:33,  3.78s/it][A
 28%|██▊       | 231/827 [14:49<36:13,  3.65s/it][A
 28%|██▊       | 232/827 [14:52<34:49,  3.51s/it][A
 28%|██▊       | 233/827 [14:56<37:41,  3.81s/it][A
 28%|██▊       | 234/827 [15:02<42:03,  4.25s/it][A
 28%|██▊       | 235/827 [15:06<43:56,  4.45s/it][A
 29%|██▊       | 236/827 [15:10<42:01,  4.27s/it][A
 29%|██▊       | 237/827 [15:16<45:47,  4.66s/it][A
 29%|██▉       | 238/827 [15:21<48:20,  4.92s/it][A
 29%|██▉       | 239/827 [15:26<47:49,  4.88s/it][A
 29%|██▉       | 240/827 [15:32<50:16,  5.14s/it][A
 29%|██▉       | 241/827 [15:38<51:38,  5.29s/it][A
 29%|██▉       | 242/827 [15:42<47:39,  4.89s/it][A
 29%|██▉       | 243/827 [15:46<45:15,  4.65s/it][A
 30%|██▉       | 244/827 [15:51<48:08,  4.95s/it][A
 30%|██▉       | 245/827 [15:55<43:37,  4.50s/it][A
 30%|██▉       | 246/827 [15:58<41:06,  4.24s/it][A
 30%|██▉       | 247/827 [16:03<41:06,  4.25s/it][A
 30%|██▉       | 248/827 [16:09<47:41,  4.94s/it][A
 30%|███       | 249/827 [16:14<46:11,  4.79s/it][A
 30%|███       | 250/827 [16:16<39:10,  4.07s/it][A
 30%|███       | 251/827 [16:20<37:59,  3.96s/it][A
 30%|███       | 252/827 [16:24<39:31,  4.13s/it][A
 31%|███       | 253/827 [16:27<35:52,  3.75s/it][A
 31%|███       | 254/827 [16:32<40:08,  4.20s/it][A
 31%|███       | 255/827 [16:39<46:21,  4.86s/it][A
 31%|███       | 256/827 [16:45<48:57,  5.14s/it][A
 31%|███       | 257/827 [16:49<46:39,  4.91s/it][A
 31%|███       | 258/827 [16:53<43:33,  4.59s/it][A
 31%|███▏      | 259/827 [16:59<47:12,  4.99s/it][A
 31%|███▏      | 260/827 [17:05<49:22,  5.22s/it][A
 32%|███▏      | 261/827 [17:08<45:23,  4.81s/it][A
 32%|███▏      | 262/827 [17:14<47:43,  5.07s/it][A
 32%|███▏      | 263/827 [17:20<50:42,  5.40s/it][A
 32%|███▏      | 264/827 [17:23<42:50,  4.57s/it][A
 32%|███▏      | 265/827 [17:25<37:23,  3.99s/it][A
 32%|███▏      | 266/827 [17:29<34:56,  3.74s/it][A
 32%|███▏      | 267/827 [17:32<32:59,  3.53s/it][A
 32%|███▏      | 268/827 [17:35<31:20,  3.36s/it][A
 33%|███▎      | 269/827 [17:39<33:47,  3.63s/it][A
 33%|███▎      | 270/827 [17:44<36:51,  3.97s/it][A
 33%|███▎      | 271/827 [17:47<34:16,  3.70s/it][A
 33%|███▎      | 272/827 [17:50<33:08,  3.58s/it][A
 33%|███▎      | 273/827 [17:56<38:38,  4.19s/it][A
 33%|███▎      | 274/827 [18:04<49:51,  5.41s/it][A
 33%|███▎      | 275/827 [18:09<48:04,  5.23s/it][A
 33%|███▎      | 276/827 [18:12<43:51,  4.78s/it][A
 33%|███▎      | 277/827 [18:16<40:47,  4.45s/it][A
 34%|███▎      | 278/827 [18:20<40:07,  4.38s/it][A
 34%|███▎      | 279/827 [18:24<36:50,  4.03s/it][A
 34%|███▍      | 280/827 [18:26<33:43,  3.70s/it][A
 34%|███▍      | 281/827 [18:30<33:56,  3.73s/it][A
 34%|███▍      | 282/827 [18:34<33:16,  3.66s/it][A
 34%|███▍      | 283/827 [18:37<32:11,  3.55s/it][A
 34%|███▍      | 284/827 [18:41<32:52,  3.63s/it][A
 34%|███▍      | 285/827 [18:45<34:31,  3.82s/it][A
 35%|███▍      | 286/827 [18:50<37:09,  4.12s/it][A
 35%|███▍      | 287/827 [18:52<32:34,  3.62s/it][A
 35%|███▍      | 288/827 [18:55<30:49,  3.43s/it][A
 35%|███▍      | 289/827 [18:58<29:48,  3.33s/it][A
 35%|███▌      | 290/827 [19:02<29:04,  3.25s/it][A
 35%|███▌      | 291/827 [19:04<27:24,  3.07s/it][A
 35%|███▌      | 292/827 [19:08<28:13,  3.17s/it][A
 35%|███▌      | 293/827 [19:10<27:20,  3.07s/it][A
 36%|███▌      | 294/827 [19:14<28:25,  3.20s/it][A
 36%|███▌      | 295/827 [19:17<29:01,  3.27s/it][A
 36%|███▌      | 296/827 [19:22<31:37,  3.57s/it][A
 36%|███▌      | 297/827 [19:25<32:06,  3.63s/it][A
 36%|███▌      | 298/827 [19:28<30:04,  3.41s/it][A
 36%|███▌      | 299/827 [19:32<29:25,  3.34s/it][A
 36%|███▋      | 300/827 [19:35<28:31,  3.25s/it][A
 36%|███▋      | 301/827 [19:38<27:47,  3.17s/it][A
 37%|███▋      | 302/827 [19:41<28:13,  3.23s/it][A
 37%|███▋      | 303/827 [19:43<25:43,  2.95s/it][A
 37%|███▋      | 304/827 [19:46<26:28,  3.04s/it][A
 37%|███▋      | 305/827 [19:51<29:40,  3.41s/it][A
 37%|███▋      | 306/827 [19:55<31:22,  3.61s/it][A
 37%|███▋      | 307/827 [19:58<30:16,  3.49s/it][A
 37%|███▋      | 308/827 [20:01<28:52,  3.34s/it][A
 37%|███▋      | 309/827 [20:04<28:57,  3.35s/it][A
 37%|███▋      | 310/827 [20:07<27:01,  3.14s/it][A
 38%|███▊      | 311/827 [20:10<25:38,  2.98s/it][A
 38%|███▊      | 312/827 [20:13<26:50,  3.13s/it][A
 38%|███▊      | 313/827 [20:17<29:31,  3.45s/it][A
 38%|███▊      | 314/827 [20:21<29:34,  3.46s/it][A
 38%|███▊      | 315/827 [20:25<32:04,  3.76s/it][A
 38%|███▊      | 316/827 [20:30<33:55,  3.98s/it][A
 38%|███▊      | 317/827 [20:35<36:29,  4.29s/it][A
 38%|███▊      | 318/827 [20:39<36:26,  4.30s/it][A
 39%|███▊      | 319/827 [20:43<35:23,  4.18s/it][A
 39%|███▊      | 320/827 [20:46<33:12,  3.93s/it][A
 39%|███▉      | 321/827 [20:51<34:39,  4.11s/it][A
 39%|███▉      | 322/827 [20:55<34:12,  4.06s/it][A
 39%|███▉      | 323/827 [20:59<33:36,  4.00s/it][A
 39%|███▉      | 324/827 [21:03<35:15,  4.21s/it][A
 39%|███▉      | 325/827 [21:06<32:22,  3.87s/it][A
 39%|███▉      | 326/827 [21:10<31:42,  3.80s/it][A
 40%|███▉      | 327/827 [21:15<33:48,  4.06s/it][A
 40%|███▉      | 328/827 [21:19<34:13,  4.12s/it][A
 40%|███▉      | 329/827 [21:25<39:02,  4.70s/it][A
 40%|███▉      | 330/827 [21:29<37:57,  4.58s/it][A
 40%|████      | 331/827 [21:34<38:08,  4.61s/it][A
 40%|████      | 332/827 [21:38<36:41,  4.45s/it][A
 40%|████      | 333/827 [21:43<38:08,  4.63s/it][A
 40%|████      | 334/827 [21:47<36:20,  4.42s/it][A
 41%|████      | 335/827 [21:51<33:55,  4.14s/it][A
 41%|████      | 336/827 [21:53<30:34,  3.74s/it][A
 41%|████      | 337/827 [21:57<29:11,  3.57s/it][A
 41%|████      | 338/827 [22:00<28:05,  3.45s/it][A
 41%|████      | 339/827 [22:03<27:02,  3.32s/it][A
 41%|████      | 340/827 [22:06<27:16,  3.36s/it][A
 41%|████      | 341/827 [22:10<28:27,  3.51s/it][A
 41%|████▏     | 342/827 [22:13<27:24,  3.39s/it][A
 41%|████▏     | 343/827 [22:16<26:49,  3.32s/it][A
 42%|████▏     | 344/827 [22:21<29:19,  3.64s/it][A
 42%|████▏     | 345/827 [22:26<32:27,  4.04s/it][A
 42%|████▏     | 346/827 [22:29<31:43,  3.96s/it][A
 42%|████▏     | 347/827 [22:33<30:53,  3.86s/it][A
 42%|████▏     | 348/827 [22:36<28:31,  3.57s/it][A
 42%|████▏     | 349/827 [22:40<30:33,  3.83s/it][A
 42%|████▏     | 350/827 [22:45<32:16,  4.06s/it][A
 42%|████▏     | 351/827 [22:48<30:35,  3.86s/it][A
 43%|████▎     | 352/827 [22:52<31:02,  3.92s/it][A
 43%|████▎     | 353/827 [22:56<29:35,  3.74s/it][A
 43%|████▎     | 354/827 [22:59<28:08,  3.57s/it][A
 43%|████▎     | 355/827 [23:04<30:23,  3.86s/it][A
 43%|████▎     | 356/827 [23:08<32:27,  4.13s/it][A
 43%|████▎     | 357/827 [23:12<31:04,  3.97s/it][A
 43%|████▎     | 358/827 [23:15<28:16,  3.62s/it][A
 43%|████▎     | 359/827 [23:18<27:41,  3.55s/it][A
 44%|████▎     | 360/827 [23:23<29:58,  3.85s/it][A
 44%|████▎     | 361/827 [23:26<29:59,  3.86s/it][A
 44%|████▍     | 362/827 [23:29<26:35,  3.43s/it][A
 44%|████▍     | 363/827 [23:33<28:15,  3.65s/it][A
 44%|████▍     | 364/827 [23:37<28:20,  3.67s/it][A
 44%|████▍     | 365/827 [23:40<27:03,  3.51s/it][A
 44%|████▍     | 366/827 [23:45<31:29,  4.10s/it][A
 44%|████▍     | 367/827 [23:50<32:13,  4.20s/it][A
 44%|████▍     | 368/827 [23:52<28:31,  3.73s/it][A
 45%|████▍     | 369/827 [23:56<27:55,  3.66s/it][A
 45%|████▍     | 370/827 [23:59<27:16,  3.58s/it][A
 45%|████▍     | 371/827 [24:04<28:50,  3.80s/it][A
 45%|████▍     | 372/827 [24:07<28:33,  3.76s/it][A
 45%|████▌     | 373/827 [24:12<29:56,  3.96s/it][A
 45%|████▌     | 374/827 [24:18<35:52,  4.75s/it][A
 45%|████▌     | 375/827 [24:27<43:44,  5.81s/it][A
 45%|████▌     | 376/827 [24:34<46:32,  6.19s/it][A
 46%|████▌     | 377/827 [24:38<42:50,  5.71s/it][A
 46%|████▌     | 378/827 [24:44<43:09,  5.77s/it][A
 46%|████▌     | 379/827 [24:53<49:54,  6.69s/it][A
 46%|████▌     | 380/827 [24:59<47:53,  6.43s/it][A
 46%|████▌     | 381/827 [25:02<41:28,  5.58s/it][A
 46%|████▌     | 382/827 [25:07<38:53,  5.24s/it][A
 46%|████▋     | 383/827 [25:11<36:19,  4.91s/it][A
 46%|████▋     | 384/827 [25:15<33:16,  4.51s/it][A
 47%|████▋     | 385/827 [25:18<30:08,  4.09s/it][A
 47%|████▋     | 386/827 [25:21<27:58,  3.81s/it][A
 47%|████▋     | 387/827 [25:25<27:38,  3.77s/it][A
 47%|████▋     | 388/827 [25:29<28:29,  3.90s/it][A
 47%|████▋     | 389/827 [25:32<27:50,  3.81s/it][A
 47%|████▋     | 390/827 [25:36<26:56,  3.70s/it][A
 47%|████▋     | 391/827 [25:39<25:50,  3.56s/it][A
 47%|████▋     | 392/827 [25:42<25:31,  3.52s/it][A
 48%|████▊     | 393/827 [25:47<26:59,  3.73s/it][A
 48%|████▊     | 394/827 [25:52<29:54,  4.15s/it][A
 48%|████▊     | 395/827 [25:56<30:38,  4.26s/it][A
 48%|████▊     | 396/827 [26:01<31:11,  4.34s/it][A
 48%|████▊     | 397/827 [26:04<27:58,  3.90s/it][A
 48%|████▊     | 398/827 [26:07<26:27,  3.70s/it][A
 48%|████▊     | 399/827 [26:10<25:31,  3.58s/it][A
 48%|████▊     | 400/827 [26:14<26:45,  3.76s/it][A
 48%|████▊     | 401/827 [26:19<29:08,  4.10s/it][A
 49%|████▊     | 402/827 [26:23<28:56,  4.09s/it][A
 49%|████▊     | 403/827 [26:28<30:16,  4.28s/it][A
 49%|████▉     | 404/827 [26:31<26:35,  3.77s/it][A
 49%|████▉     | 405/827 [26:34<25:06,  3.57s/it][A
 49%|████▉     | 406/827 [26:37<24:20,  3.47s/it][A
 49%|████▉     | 407/827 [26:40<22:46,  3.25s/it][A
 49%|████▉     | 408/827 [26:43<22:04,  3.16s/it][A
 49%|████▉     | 409/827 [26:46<21:43,  3.12s/it][A
 50%|████▉     | 410/827 [26:48<20:19,  2.92s/it][A
 50%|████▉     | 411/827 [26:51<20:26,  2.95s/it][A
 50%|████▉     | 412/827 [26:54<20:31,  2.97s/it][A
 50%|████▉     | 413/827 [26:59<23:11,  3.36s/it][A
 50%|█████     | 414/827 [27:03<26:22,  3.83s/it][A
 50%|█████     | 415/827 [27:06<24:06,  3.51s/it][A
 50%|█████     | 416/827 [27:10<25:23,  3.71s/it][A
 50%|█████     | 417/827 [27:15<27:30,  4.03s/it][A
 51%|█████     | 418/827 [27:21<30:25,  4.46s/it][A
 51%|█████     | 419/827 [27:25<29:13,  4.30s/it][A
 51%|█████     | 420/827 [27:29<28:49,  4.25s/it][A
 51%|█████     | 421/827 [27:32<27:21,  4.04s/it][A
 51%|█████     | 422/827 [27:35<25:15,  3.74s/it][A
 51%|█████     | 423/827 [27:38<23:44,  3.52s/it][A
 51%|█████▏    | 424/827 [27:42<24:19,  3.62s/it][A
 51%|█████▏    | 425/827 [27:47<25:54,  3.87s/it][A
 52%|█████▏    | 426/827 [27:51<27:50,  4.16s/it][A
 52%|█████▏    | 427/827 [27:56<28:52,  4.33s/it][A
 52%|█████▏    | 428/827 [28:01<29:34,  4.45s/it][A
 52%|█████▏    | 429/827 [28:05<27:50,  4.20s/it][A
 52%|█████▏    | 430/827 [28:08<26:58,  4.08s/it][A
 52%|█████▏    | 431/827 [28:12<26:44,  4.05s/it][A
 52%|█████▏    | 432/827 [28:17<28:46,  4.37s/it][A
 52%|█████▏    | 433/827 [28:24<32:28,  4.95s/it][A
 52%|█████▏    | 434/827 [28:29<33:13,  5.07s/it][A
 53%|█████▎    | 435/827 [28:32<28:57,  4.43s/it][A
 53%|█████▎    | 436/827 [28:36<28:04,  4.31s/it][A
 53%|█████▎    | 437/827 [28:40<28:09,  4.33s/it][A
 53%|█████▎    | 438/827 [28:44<26:09,  4.04s/it][A
 53%|█████▎    | 439/827 [28:47<25:14,  3.90s/it][A
 53%|█████▎    | 440/827 [28:51<25:07,  3.90s/it][A
 53%|█████▎    | 441/827 [28:54<23:01,  3.58s/it][A
 53%|█████▎    | 442/827 [28:57<21:35,  3.36s/it][A
 54%|█████▎    | 443/827 [29:00<20:54,  3.27s/it][A
 54%|█████▎    | 444/827 [29:04<21:47,  3.41s/it][A
 54%|█████▍    | 445/827 [29:09<25:26,  4.00s/it][A
 54%|█████▍    | 446/827 [29:12<23:45,  3.74s/it][A
 54%|█████▍    | 447/827 [29:17<25:00,  3.95s/it][A
 54%|█████▍    | 448/827 [29:22<26:38,  4.22s/it][A
 54%|█████▍    | 449/827 [29:29<32:31,  5.16s/it][A
 54%|█████▍    | 450/827 [29:34<32:36,  5.19s/it][A
 55%|█████▍    | 451/827 [29:38<29:27,  4.70s/it][A
 55%|█████▍    | 452/827 [29:43<29:59,  4.80s/it][A
 55%|█████▍    | 453/827 [29:48<31:01,  4.98s/it][A
 55%|█████▍    | 454/827 [29:52<27:56,  4.49s/it][A
 55%|█████▌    | 455/827 [29:55<26:47,  4.32s/it][A
 55%|█████▌    | 456/827 [30:01<29:07,  4.71s/it][A
 55%|█████▌    | 457/827 [30:05<28:22,  4.60s/it][A
 55%|█████▌    | 458/827 [30:09<26:47,  4.36s/it][A
 56%|█████▌    | 459/827 [30:14<27:09,  4.43s/it][A
 56%|█████▌    | 460/827 [30:19<29:03,  4.75s/it][A
 56%|█████▌    | 461/827 [30:27<33:33,  5.50s/it][A
 56%|█████▌    | 462/827 [30:31<31:30,  5.18s/it][A
 56%|█████▌    | 463/827 [30:34<27:37,  4.55s/it][A
 56%|█████▌    | 464/827 [30:39<27:37,  4.56s/it][A
 56%|█████▌    | 465/827 [30:43<26:54,  4.46s/it][A
 56%|█████▋    | 466/827 [30:46<24:54,  4.14s/it][A
 56%|█████▋    | 467/827 [30:50<24:21,  4.06s/it][A
 57%|█████▋    | 468/827 [30:54<24:39,  4.12s/it][A
 57%|█████▋    | 469/827 [30:59<25:44,  4.31s/it][A
 57%|█████▋    | 470/827 [31:02<23:28,  3.95s/it][A
 57%|█████▋    | 471/827 [31:06<23:40,  3.99s/it][A
 57%|█████▋    | 472/827 [31:13<27:45,  4.69s/it][A
 57%|█████▋    | 473/827 [31:17<26:59,  4.57s/it][A
 57%|█████▋    | 474/827 [31:21<25:43,  4.37s/it][A
 57%|█████▋    | 475/827 [31:24<23:05,  3.94s/it][A
 58%|█████▊    | 476/827 [31:26<20:17,  3.47s/it][A
 58%|█████▊    | 477/827 [31:29<18:22,  3.15s/it][A
 58%|█████▊    | 478/827 [31:32<19:18,  3.32s/it][A
 58%|█████▊    | 479/827 [31:37<21:57,  3.79s/it][A
 58%|█████▊    | 480/827 [31:41<22:44,  3.93s/it][A
 58%|█████▊    | 481/827 [31:46<24:01,  4.16s/it][A
 58%|█████▊    | 482/827 [31:49<21:41,  3.77s/it][A
 58%|█████▊    | 483/827 [31:51<18:53,  3.29s/it][A
 59%|█████▊    | 484/827 [31:54<18:25,  3.22s/it][A
 59%|█████▊    | 485/827 [31:57<17:40,  3.10s/it][A
 59%|█████▉    | 486/827 [32:00<17:55,  3.15s/it][A
 59%|█████▉    | 487/827 [32:04<18:23,  3.25s/it][A
 59%|█████▉    | 488/827 [32:09<20:57,  3.71s/it][A
 59%|█████▉    | 489/827 [32:13<22:22,  3.97s/it][A
 59%|█████▉    | 490/827 [32:18<23:08,  4.12s/it][A
 59%|█████▉    | 491/827 [32:21<21:30,  3.84s/it][A
 59%|█████▉    | 492/827 [32:24<20:48,  3.73s/it][A
 60%|█████▉    | 493/827 [32:28<20:13,  3.63s/it][A
 60%|█████▉    | 494/827 [32:31<19:53,  3.58s/it][A
 60%|█████▉    | 495/827 [32:35<19:49,  3.58s/it][A
 60%|█████▉    | 496/827 [32:39<20:04,  3.64s/it][A
 60%|██████    | 497/827 [32:41<18:20,  3.34s/it][A
 60%|██████    | 498/827 [32:44<17:56,  3.27s/it][A
 60%|██████    | 499/827 [32:49<19:43,  3.61s/it][A
 60%|██████    | 500/827 [32:52<18:53,  3.47s/it][A
 61%|██████    | 501/827 [32:55<17:40,  3.25s/it][A
 61%|██████    | 502/827 [32:59<19:39,  3.63s/it][A
 61%|██████    | 503/827 [33:04<21:14,  3.93s/it][A
 61%|██████    | 504/827 [33:07<20:18,  3.77s/it][A
 61%|██████    | 505/827 [33:11<20:01,  3.73s/it][A
 61%|██████    | 506/827 [33:14<19:44,  3.69s/it][A
 61%|██████▏   | 507/827 [33:19<21:39,  4.06s/it][A
 61%|██████▏   | 508/827 [33:25<25:01,  4.71s/it][A
 62%|██████▏   | 509/827 [33:28<21:40,  4.09s/it][A
 62%|██████▏   | 510/827 [33:32<20:51,  3.95s/it][A
 62%|██████▏   | 511/827 [33:35<19:23,  3.68s/it][A
 62%|██████▏   | 512/827 [33:38<17:56,  3.42s/it][A
 62%|██████▏   | 513/827 [33:41<18:10,  3.47s/it][A
 62%|██████▏   | 514/827 [33:45<18:09,  3.48s/it][A
 62%|██████▏   | 515/827 [33:48<17:41,  3.40s/it][A
 62%|██████▏   | 516/827 [33:52<18:19,  3.54s/it][A
 63%|██████▎   | 517/827 [33:55<17:48,  3.45s/it][A
 63%|██████▎   | 518/827 [33:58<16:43,  3.25s/it][A
 63%|██████▎   | 519/827 [34:01<16:16,  3.17s/it][A
 63%|██████▎   | 520/827 [34:04<16:09,  3.16s/it][A
 63%|██████▎   | 521/827 [34:08<17:28,  3.43s/it][A
 63%|██████▎   | 522/827 [34:13<20:08,  3.96s/it][A
 63%|██████▎   | 523/827 [34:16<18:22,  3.63s/it][A
 63%|██████▎   | 524/827 [34:20<18:40,  3.70s/it][A
 63%|██████▎   | 525/827 [34:25<21:29,  4.27s/it][A
 64%|██████▎   | 526/827 [34:31<23:27,  4.68s/it][A
 64%|██████▎   | 527/827 [34:35<22:21,  4.47s/it][A
 64%|██████▍   | 528/827 [34:40<22:27,  4.51s/it][A
 64%|██████▍   | 529/827 [34:44<22:48,  4.59s/it][A
 64%|██████▍   | 530/827 [34:49<22:30,  4.55s/it][A
 64%|██████▍   | 531/827 [34:52<20:05,  4.07s/it][A
 64%|██████▍   | 532/827 [34:55<19:04,  3.88s/it][A
 64%|██████▍   | 533/827 [35:00<20:34,  4.20s/it][A
 65%|██████▍   | 534/827 [35:05<21:57,  4.50s/it][A
 65%|██████▍   | 535/827 [35:10<22:37,  4.65s/it][A
 65%|██████▍   | 536/827 [35:14<20:53,  4.31s/it][A
 65%|██████▍   | 537/827 [35:16<18:04,  3.74s/it][A
 65%|██████▌   | 538/827 [35:19<16:56,  3.52s/it][A
 65%|██████▌   | 539/827 [35:22<16:06,  3.36s/it][A
 65%|██████▌   | 540/827 [35:26<16:17,  3.41s/it][A
 65%|██████▌   | 541/827 [35:30<16:46,  3.52s/it][A
 66%|██████▌   | 542/827 [35:33<16:57,  3.57s/it][A
 66%|██████▌   | 543/827 [35:38<18:04,  3.82s/it][A
 66%|██████▌   | 544/827 [35:42<19:17,  4.09s/it][A
 66%|██████▌   | 545/827 [35:47<19:35,  4.17s/it][A
 66%|██████▌   | 546/827 [35:51<19:30,  4.16s/it][A
 66%|██████▌   | 547/827 [35:55<19:30,  4.18s/it][A
 66%|██████▋   | 548/827 [35:58<17:24,  3.74s/it][A
 66%|██████▋   | 549/827 [36:01<16:02,  3.46s/it][A
 67%|██████▋   | 550/827 [36:04<15:51,  3.43s/it][A
 67%|██████▋   | 551/827 [36:08<16:35,  3.61s/it][A
 67%|██████▋   | 552/827 [36:12<16:26,  3.59s/it][A
 67%|██████▋   | 553/827 [36:15<16:14,  3.55s/it][A
 67%|██████▋   | 554/827 [36:18<14:42,  3.23s/it][A
 67%|██████▋   | 555/827 [36:21<14:21,  3.17s/it][A
 67%|██████▋   | 556/827 [36:24<14:18,  3.17s/it][A
 67%|██████▋   | 557/827 [36:26<13:31,  3.01s/it][A
 67%|██████▋   | 558/827 [36:30<14:32,  3.24s/it][A
 68%|██████▊   | 559/827 [36:34<15:13,  3.41s/it][A
 68%|██████▊   | 560/827 [36:37<14:51,  3.34s/it][A
 68%|██████▊   | 561/827 [36:40<14:43,  3.32s/it][A
 68%|██████▊   | 562/827 [36:44<15:09,  3.43s/it][A
 68%|██████▊   | 563/827 [36:48<14:59,  3.41s/it][A
 68%|██████▊   | 564/827 [36:51<14:49,  3.38s/it][A
 68%|██████▊   | 565/827 [36:55<16:06,  3.69s/it][A
 68%|██████▊   | 566/827 [37:00<17:04,  3.93s/it][A
 69%|██████▊   | 567/827 [37:03<15:40,  3.62s/it][A
 69%|██████▊   | 568/827 [37:06<15:32,  3.60s/it][A
 69%|██████▉   | 569/827 [37:09<14:51,  3.46s/it][A
 69%|██████▉   | 570/827 [37:13<14:59,  3.50s/it][A
 69%|██████▉   | 571/827 [37:18<16:54,  3.96s/it][A
 69%|██████▉   | 572/827 [37:23<18:22,  4.32s/it][A
 69%|██████▉   | 573/827 [37:28<19:28,  4.60s/it][A
 69%|██████▉   | 574/827 [37:32<18:29,  4.39s/it][A
 70%|██████▉   | 575/827 [37:37<19:25,  4.63s/it][A
 70%|██████▉   | 576/827 [37:41<18:25,  4.41s/it][A
 70%|██████▉   | 577/827 [37:45<16:51,  4.05s/it][A
 70%|██████▉   | 578/827 [37:48<16:25,  3.96s/it][A
 70%|███████   | 579/827 [37:51<15:23,  3.72s/it][A
 70%|███████   | 580/827 [37:56<16:26,  4.00s/it][A
 70%|███████   | 581/827 [38:03<19:40,  4.80s/it][A
 70%|███████   | 582/827 [38:07<18:57,  4.64s/it][A
 70%|███████   | 583/827 [38:10<17:03,  4.19s/it][A
 71%|███████   | 584/827 [38:13<15:36,  3.85s/it][A
 71%|███████   | 585/827 [38:16<14:25,  3.57s/it][A
 71%|███████   | 586/827 [38:20<14:44,  3.67s/it][A
 71%|███████   | 587/827 [38:27<18:36,  4.65s/it][A
 71%|███████   | 588/827 [38:34<21:22,  5.37s/it][A
 71%|███████   | 589/827 [38:39<20:20,  5.13s/it][A
 71%|███████▏  | 590/827 [38:45<21:13,  5.37s/it][A
 71%|███████▏  | 591/827 [38:51<22:42,  5.77s/it][A
 72%|███████▏  | 592/827 [38:55<19:41,  5.03s/it][A
 72%|███████▏  | 593/827 [38:58<18:06,  4.64s/it][A
 72%|███████▏  | 594/827 [39:01<16:06,  4.15s/it][A
 72%|███████▏  | 595/827 [39:05<15:00,  3.88s/it][A
 72%|███████▏  | 596/827 [39:11<18:09,  4.72s/it][A
 72%|███████▏  | 597/827 [39:17<19:37,  5.12s/it][A
 72%|███████▏  | 598/827 [39:20<17:06,  4.48s/it][A
 72%|███████▏  | 599/827 [39:24<15:40,  4.13s/it][A
 73%|███████▎  | 600/827 [39:28<15:40,  4.14s/it][A
 73%|███████▎  | 601/827 [39:32<15:20,  4.07s/it][A
 73%|███████▎  | 602/827 [39:35<14:59,  4.00s/it][A
 73%|███████▎  | 603/827 [39:39<13:53,  3.72s/it][A
 73%|███████▎  | 604/827 [39:42<13:54,  3.74s/it][A
 73%|███████▎  | 605/827 [39:46<13:19,  3.60s/it][A
 73%|███████▎  | 606/827 [39:49<13:06,  3.56s/it][A
 73%|███████▎  | 607/827 [39:53<13:31,  3.69s/it][A
 74%|███████▎  | 608/827 [39:56<13:00,  3.56s/it][A
 74%|███████▎  | 609/827 [40:02<14:43,  4.05s/it][A
 74%|███████▍  | 610/827 [40:08<17:06,  4.73s/it][A
 74%|███████▍  | 611/827 [40:11<15:48,  4.39s/it][A
 74%|███████▍  | 612/827 [40:16<15:22,  4.29s/it][A
 74%|███████▍  | 613/827 [40:20<15:26,  4.33s/it][A
 74%|███████▍  | 614/827 [40:24<15:00,  4.23s/it][A
 74%|███████▍  | 615/827 [40:27<14:12,  4.02s/it][A
 74%|███████▍  | 616/827 [40:31<13:23,  3.81s/it][A
 75%|███████▍  | 617/827 [40:35<14:16,  4.08s/it][A
 75%|███████▍  | 618/827 [40:42<16:44,  4.81s/it][A
 75%|███████▍  | 619/827 [40:45<14:36,  4.22s/it][A
 75%|███████▍  | 620/827 [40:48<13:55,  4.04s/it][A
 75%|███████▌  | 621/827 [40:51<12:32,  3.65s/it][A
 75%|███████▌  | 622/827 [40:54<11:40,  3.42s/it][A
 75%|███████▌  | 623/827 [40:57<11:25,  3.36s/it][A
 75%|███████▌  | 624/827 [41:02<12:39,  3.74s/it][A
 76%|███████▌  | 625/827 [41:05<12:14,  3.64s/it][A
 76%|███████▌  | 626/827 [41:09<11:55,  3.56s/it][A
 76%|███████▌  | 627/827 [41:12<11:42,  3.51s/it][A
 76%|███████▌  | 628/827 [41:15<11:00,  3.32s/it][A
 76%|███████▌  | 629/827 [41:18<10:40,  3.24s/it][A
 76%|███████▌  | 630/827 [41:21<10:26,  3.18s/it][A
 76%|███████▋  | 631/827 [41:26<12:10,  3.73s/it][A
 76%|███████▋  | 632/827 [41:31<13:19,  4.10s/it][A
 77%|███████▋  | 633/827 [41:36<14:00,  4.33s/it][A
 77%|███████▋  | 634/827 [41:42<15:59,  4.97s/it][A
 77%|███████▋  | 635/827 [41:45<13:55,  4.35s/it][A
 77%|███████▋  | 636/827 [41:49<13:24,  4.21s/it][A
 77%|███████▋  | 637/827 [41:54<13:45,  4.35s/it][A
 77%|███████▋  | 638/827 [41:58<13:52,  4.41s/it][A
 77%|███████▋  | 639/827 [42:02<12:38,  4.04s/it][A
 77%|███████▋  | 640/827 [42:04<11:29,  3.69s/it][A
 78%|███████▊  | 641/827 [42:08<11:39,  3.76s/it][A
 78%|███████▊  | 642/827 [42:12<11:28,  3.72s/it][A
 78%|███████▊  | 643/827 [42:16<12:06,  3.95s/it][A
 78%|███████▊  | 644/827 [42:21<12:32,  4.11s/it][A
 78%|███████▊  | 645/827 [42:24<11:56,  3.93s/it][A
 78%|███████▊  | 646/827 [42:27<10:54,  3.62s/it][A
 78%|███████▊  | 647/827 [42:31<10:34,  3.52s/it][A
 78%|███████▊  | 648/827 [42:34<10:46,  3.61s/it][A
 78%|███████▊  | 649/827 [42:38<10:43,  3.61s/it][A
 79%|███████▊  | 650/827 [42:41<10:04,  3.42s/it][A
 79%|███████▊  | 651/827 [42:44<09:48,  3.34s/it][A
 79%|███████▉  | 652/827 [42:47<09:42,  3.33s/it][A
 79%|███████▉  | 653/827 [42:51<09:52,  3.41s/it][A
 79%|███████▉  | 654/827 [42:56<11:19,  3.93s/it][A
 79%|███████▉  | 655/827 [43:02<12:34,  4.39s/it][A
 79%|███████▉  | 656/827 [43:04<10:58,  3.85s/it][A
 79%|███████▉  | 657/827 [43:07<10:06,  3.57s/it][A
 80%|███████▉  | 658/827 [43:11<10:39,  3.78s/it][A
 80%|███████▉  | 659/827 [43:19<13:47,  4.93s/it][A
 80%|███████▉  | 660/827 [43:24<13:48,  4.96s/it][A
 80%|███████▉  | 661/827 [43:30<14:06,  5.10s/it][A
 80%|████████  | 662/827 [43:33<13:03,  4.75s/it][A
 80%|████████  | 663/827 [43:37<12:16,  4.49s/it][A
 80%|████████  | 664/827 [43:42<12:35,  4.64s/it][A
 80%|████████  | 665/827 [43:46<11:45,  4.35s/it][A
 81%|████████  | 666/827 [43:50<11:29,  4.28s/it][A
 81%|████████  | 667/827 [43:53<10:31,  3.95s/it][A
 81%|████████  | 668/827 [43:57<10:20,  3.90s/it][A
 81%|████████  | 669/827 [44:00<09:22,  3.56s/it][A
 81%|████████  | 670/827 [44:03<08:59,  3.43s/it][A
 81%|████████  | 671/827 [44:07<09:01,  3.47s/it][A
 81%|████████▏ | 672/827 [44:10<08:55,  3.45s/it][A
 81%|████████▏ | 673/827 [44:13<08:31,  3.32s/it][A
 81%|████████▏ | 674/827 [44:16<08:23,  3.29s/it][A
 82%|████████▏ | 675/827 [44:20<08:35,  3.39s/it][A
 82%|████████▏ | 676/827 [44:24<09:16,  3.68s/it][A
 82%|████████▏ | 677/827 [44:29<10:13,  4.09s/it][A
 82%|████████▏ | 678/827 [44:34<10:31,  4.24s/it][A
 82%|████████▏ | 679/827 [44:38<10:39,  4.32s/it][A
 82%|████████▏ | 680/827 [44:41<09:30,  3.88s/it][A
 82%|████████▏ | 681/827 [44:46<10:01,  4.12s/it][A
 82%|████████▏ | 682/827 [44:51<10:35,  4.38s/it][A
 83%|████████▎ | 683/827 [44:54<09:27,  3.94s/it][A
 83%|████████▎ | 684/827 [44:57<08:40,  3.64s/it][A
 83%|████████▎ | 685/827 [44:59<07:52,  3.33s/it][A
 83%|████████▎ | 686/827 [45:05<09:19,  3.97s/it][A
 83%|████████▎ | 687/827 [45:12<11:31,  4.94s/it][A
 83%|████████▎ | 688/827 [45:17<11:27,  4.95s/it][A
 83%|████████▎ | 689/827 [45:22<11:15,  4.89s/it][A
 83%|████████▎ | 690/827 [45:25<09:56,  4.35s/it][A
 84%|████████▎ | 691/827 [45:28<09:01,  3.98s/it][A
 84%|████████▎ | 692/827 [45:32<08:55,  3.97s/it][A
 84%|████████▍ | 693/827 [45:35<08:23,  3.75s/it][A
 84%|████████▍ | 694/827 [45:38<07:54,  3.57s/it][A
 84%|████████▍ | 695/827 [45:42<08:08,  3.70s/it][A
 84%|████████▍ | 696/827 [45:47<08:42,  3.99s/it][A
 84%|████████▍ | 697/827 [45:50<08:16,  3.82s/it][A
 84%|████████▍ | 698/827 [45:53<07:24,  3.44s/it][A
 85%|████████▍ | 699/827 [45:56<06:59,  3.27s/it][A
 85%|████████▍ | 700/827 [46:02<08:45,  4.13s/it][A
 85%|████████▍ | 701/827 [46:08<10:02,  4.78s/it][A
 85%|████████▍ | 702/827 [46:13<09:43,  4.67s/it][A
 85%|████████▌ | 703/827 [46:17<09:11,  4.45s/it][A
 85%|████████▌ | 704/827 [46:20<08:16,  4.03s/it][A
 85%|████████▌ | 705/827 [46:22<07:21,  3.62s/it][A
 85%|████████▌ | 706/827 [46:25<06:45,  3.35s/it][A
 85%|████████▌ | 707/827 [46:29<07:01,  3.51s/it][A
 86%|████████▌ | 708/827 [46:32<06:49,  3.44s/it][A
 86%|████████▌ | 709/827 [46:36<06:58,  3.55s/it][A
 86%|████████▌ | 710/827 [46:39<06:54,  3.54s/it][A
 86%|████████▌ | 711/827 [46:43<06:53,  3.56s/it][A
 86%|████████▌ | 712/827 [46:46<06:30,  3.39s/it][A
 86%|████████▌ | 713/827 [46:49<06:22,  3.36s/it][A
 86%|████████▋ | 714/827 [46:53<06:33,  3.48s/it][A
 86%|████████▋ | 715/827 [46:57<06:58,  3.74s/it][A
 87%|████████▋ | 716/827 [47:01<06:46,  3.66s/it][A
 87%|████████▋ | 717/827 [47:06<07:26,  4.06s/it][A
 87%|████████▋ | 718/827 [47:11<07:52,  4.33s/it][A
 87%|████████▋ | 719/827 [47:14<07:11,  3.99s/it][A
 87%|████████▋ | 720/827 [47:18<06:51,  3.84s/it][A
 87%|████████▋ | 721/827 [47:21<06:31,  3.70s/it][A
 87%|████████▋ | 722/827 [47:25<06:25,  3.68s/it][A
 87%|████████▋ | 723/827 [47:28<06:12,  3.58s/it][A
 88%|████████▊ | 724/827 [47:31<05:37,  3.27s/it][A
 88%|████████▊ | 725/827 [47:35<06:11,  3.65s/it][A
 88%|████████▊ | 726/827 [47:40<06:59,  4.15s/it][A
 88%|████████▊ | 727/827 [47:44<06:50,  4.11s/it][A
 88%|████████▊ | 728/827 [47:47<06:16,  3.80s/it][A
 88%|████████▊ | 729/827 [47:50<05:35,  3.42s/it][A
 88%|████████▊ | 730/827 [47:54<05:39,  3.50s/it][A
 88%|████████▊ | 731/827 [47:58<06:09,  3.85s/it][A
 89%|████████▊ | 732/827 [48:03<06:21,  4.01s/it][A
 89%|████████▊ | 733/827 [48:07<06:24,  4.09s/it][A
 89%|████████▉ | 734/827 [48:09<05:36,  3.61s/it][A
 89%|████████▉ | 735/827 [48:13<05:36,  3.66s/it][A
 89%|████████▉ | 736/827 [48:19<06:17,  4.15s/it][A
 89%|████████▉ | 737/827 [48:23<06:31,  4.35s/it][A
 89%|████████▉ | 738/827 [48:26<05:52,  3.96s/it][A
 89%|████████▉ | 739/827 [48:31<06:08,  4.19s/it][A
 89%|████████▉ | 740/827 [48:37<06:40,  4.60s/it][A
 90%|████████▉ | 741/827 [48:40<06:12,  4.33s/it][A
 90%|████████▉ | 742/827 [48:44<05:52,  4.15s/it][A
 90%|████████▉ | 743/827 [48:49<06:17,  4.49s/it][A
 90%|████████▉ | 744/827 [48:53<05:54,  4.27s/it][A
 90%|█████████ | 745/827 [48:57<05:41,  4.16s/it][A
 90%|█████████ | 746/827 [49:02<06:01,  4.46s/it][A
 90%|█████████ | 747/827 [49:07<06:13,  4.67s/it][A
 90%|█████████ | 748/827 [49:11<05:33,  4.22s/it][A
 91%|█████████ | 749/827 [49:15<05:27,  4.20s/it][A
 91%|█████████ | 750/827 [49:19<05:25,  4.23s/it][A
 91%|█████████ | 751/827 [49:22<04:55,  3.89s/it][A
 91%|█████████ | 752/827 [49:25<04:30,  3.61s/it][A
 91%|█████████ | 753/827 [49:28<04:09,  3.38s/it][A
 91%|█████████ | 754/827 [49:31<03:55,  3.23s/it][A
 91%|█████████▏| 755/827 [49:35<04:14,  3.53s/it][A
 91%|█████████▏| 756/827 [49:40<04:39,  3.94s/it][A
 92%|█████████▏| 757/827 [49:44<04:36,  3.95s/it][A
 92%|█████████▏| 758/827 [49:48<04:38,  4.03s/it][A
 92%|█████████▏| 759/827 [49:51<04:10,  3.69s/it][A
 92%|█████████▏| 760/827 [49:56<04:39,  4.18s/it][A
 92%|█████████▏| 761/827 [50:03<05:26,  4.95s/it][A
 92%|█████████▏| 762/827 [50:09<05:36,  5.18s/it][A
 92%|█████████▏| 763/827 [50:15<05:45,  5.40s/it][A
 92%|█████████▏| 764/827 [50:18<05:00,  4.77s/it][A
 93%|█████████▎| 765/827 [50:21<04:18,  4.16s/it][A
 93%|█████████▎| 766/827 [50:24<03:49,  3.76s/it][A
 93%|█████████▎| 767/827 [50:27<03:40,  3.67s/it][A
 93%|█████████▎| 768/827 [50:31<03:36,  3.67s/it][A
 93%|█████████▎| 769/827 [50:35<03:49,  3.95s/it][A
 93%|█████████▎| 770/827 [50:39<03:34,  3.77s/it][A
 93%|█████████▎| 771/827 [50:42<03:30,  3.76s/it][A
 93%|█████████▎| 772/827 [50:46<03:23,  3.70s/it][A
 93%|█████████▎| 773/827 [50:50<03:24,  3.79s/it][A
 94%|█████████▎| 774/827 [50:53<03:09,  3.58s/it][A
 94%|█████████▎| 775/827 [50:57<03:10,  3.66s/it][A
 94%|█████████▍| 776/827 [51:00<03:01,  3.57s/it][A
 94%|█████████▍| 777/827 [51:03<02:52,  3.45s/it][A
 94%|█████████▍| 778/827 [51:07<02:48,  3.45s/it][A
 94%|█████████▍| 779/827 [51:10<02:37,  3.28s/it][A
 94%|█████████▍| 780/827 [51:13<02:30,  3.21s/it][A
 94%|█████████▍| 781/827 [51:16<02:24,  3.13s/it][A
 95%|█████████▍| 782/827 [51:19<02:16,  3.03s/it][A
 95%|█████████▍| 783/827 [51:22<02:17,  3.14s/it][A
 95%|█████████▍| 784/827 [51:25<02:16,  3.17s/it][A
 95%|█████████▍| 785/827 [51:29<02:19,  3.31s/it][A
 95%|█████████▌| 786/827 [51:33<02:24,  3.52s/it][A
 95%|█████████▌| 787/827 [51:38<02:41,  4.04s/it][A
 95%|█████████▌| 788/827 [51:42<02:35,  3.98s/it][A
 95%|█████████▌| 789/827 [51:46<02:35,  4.08s/it][A
 96%|█████████▌| 790/827 [51:50<02:25,  3.94s/it][A
 96%|█████████▌| 791/827 [51:53<02:11,  3.64s/it][A
 96%|█████████▌| 792/827 [51:56<02:00,  3.44s/it][A
 96%|█████████▌| 793/827 [52:00<02:07,  3.74s/it][A
 96%|█████████▌| 794/827 [52:05<02:18,  4.19s/it][A
 96%|█████████▌| 795/827 [52:11<02:29,  4.67s/it][A
 96%|█████████▋| 796/827 [52:16<02:27,  4.76s/it][A
 96%|█████████▋| 797/827 [52:19<02:05,  4.19s/it][A
 96%|█████████▋| 798/827 [52:22<01:49,  3.77s/it][A
 97%|█████████▋| 799/827 [52:24<01:34,  3.37s/it][A
 97%|█████████▋| 800/827 [52:31<01:54,  4.24s/it][A
 97%|█████████▋| 801/827 [52:38<02:11,  5.08s/it][A
 97%|█████████▋| 802/827 [52:42<02:02,  4.89s/it][A
 97%|█████████▋| 803/827 [52:47<01:59,  4.98s/it][A
 97%|█████████▋| 804/827 [52:52<01:54,  4.96s/it][A
 97%|█████████▋| 805/827 [52:55<01:36,  4.39s/it][A
 97%|█████████▋| 806/827 [52:58<01:21,  3.88s/it][A
 98%|█████████▊| 807/827 [53:01<01:13,  3.68s/it][A
 98%|█████████▊| 808/827 [53:05<01:09,  3.65s/it][A
 98%|█████████▊| 809/827 [53:08<01:03,  3.51s/it][A
 98%|█████████▊| 810/827 [53:11<00:59,  3.48s/it][A
 98%|█████████▊| 811/827 [53:15<00:55,  3.47s/it][A
 98%|█████████▊| 812/827 [53:19<00:53,  3.57s/it][A
 98%|█████████▊| 813/827 [53:23<00:55,  3.98s/it][A
 98%|█████████▊| 814/827 [53:29<00:57,  4.41s/it][A
 99%|█████████▊| 815/827 [53:33<00:50,  4.21s/it][A
 99%|█████████▊| 816/827 [53:36<00:45,  4.10s/it][A
 99%|█████████▉| 817/827 [53:39<00:37,  3.76s/it][A
 99%|█████████▉| 818/827 [53:45<00:37,  4.17s/it][A
 99%|█████████▉| 819/827 [53:49<00:35,  4.38s/it][A
 99%|█████████▉| 820/827 [53:54<00:31,  4.44s/it][A
 99%|█████████▉| 821/827 [53:58<00:25,  4.22s/it][A
 99%|█████████▉| 822/827 [54:01<00:20,  4.05s/it][A
100%|█████████▉| 823/827 [54:04<00:14,  3.63s/it][A
100%|█████████▉| 824/827 [54:07<00:10,  3.41s/it][A
100%|█████████▉| 825/827 [54:11<00:07,  3.76s/it][A
100%|█████████▉| 826/827 [54:17<00:04,  4.33s/it][A
100%|██████████| 827/827 [54:20<00:00,  3.76s/it][A                                                      
                                                 [A{'eval_loss': 0.7711528539657593, 'eval_runtime': 3265.1605, 'eval_samples_per_second': 1.012, 'eval_steps_per_second': 0.253, 'epoch': 0.86}
 86%|████████▌ | 400/465 [12:38:45<1:47:44, 99.45s/it]
100%|██████████| 827/827 [54:20<00:00,  3.76s/it][A
                                                 [A[INFO|trainer.py:4309] 2026-01-03 07:24:07,406 >> Saving model checkpoint to saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-400
/opt/conda/lib/python3.11/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in Qwen/Qwen2.5-VL-7B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2421] 2026-01-03 07:24:07,485 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-400/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2026-01-03 07:24:07,488 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2026-01-03 07:24:07,491 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-400/special_tokens_map.json
[2026-01-03 07:24:08,195] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step400 is about to be saved!
[2026-01-03 07:24:08,214] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-400/global_step400/mp_rank_00_model_states.pt
[2026-01-03 07:24:08,214] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-400/global_step400/mp_rank_00_model_states.pt...
[2026-01-03 07:24:08,276] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-400/global_step400/mp_rank_00_model_states.pt.
[2026-01-03 07:24:08,279] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2026-01-03 07:24:08,279] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-400/global_step400/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2026-01-03 07:24:08,279] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-400/global_step400/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2026-01-03 07:24:08,279] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-400/global_step400/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2026-01-03 07:24:08,357] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-400/global_step400/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2026-01-03 07:24:08,358] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-400/global_step400/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2026-01-03 07:24:08,358] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2026-01-03 07:24:08,359] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-400/global_step400/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2026-01-03 07:24:08,359] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-400/global_step400/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2026-01-03 07:24:08,359] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2026-01-03 07:24:08,371] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-400/global_step400/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2026-01-03 07:24:08,371] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-400/global_step400/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2026-01-03 07:24:08,371] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2026-01-03 07:24:08,377] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2026-01-03 07:24:08,381] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2026-01-03 07:24:08,381] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[INFO|image_processing_base.py:253] 2026-01-03 07:24:08,404 >> Image processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-400/preprocessor_config.json
[INFO|tokenization_utils_base.py:2421] 2026-01-03 07:24:08,407 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-400/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2026-01-03 07:24:08,409 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2026-01-03 07:24:08,412 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-400/special_tokens_map.json
[INFO|video_processing_utils.py:600] 2026-01-03 07:24:08,525 >> Video processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-400/video_preprocessor_config.json
[INFO|processing_utils.py:814] 2026-01-03 07:24:08,528 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-400/chat_template.jinja
 86%|████████▌ | 401/465 [12:40:23<19:10:30, 1078.60s/it]                                                         {'loss': 0.7713, 'grad_norm': 0.18673080205917358, 'learning_rate': 5.8487004826128735e-06, 'epoch': 0.86}
 86%|████████▌ | 401/465 [12:40:23<19:10:30, 1078.60s/it] 86%|████████▋ | 402/465 [12:42:09<13:46:20, 786.99s/it]                                                         {'loss': 0.8423, 'grad_norm': 0.1417696475982666, 'learning_rate': 5.673582480638395e-06, 'epoch': 0.86}
 86%|████████▋ | 402/465 [12:42:09<13:46:20, 786.99s/it] 87%|████████▋ | 403/465 [12:43:42<9:58:03, 578.76s/it]                                                        {'loss': 0.8077, 'grad_norm': 0.1672695130109787, 'learning_rate': 5.500968325565858e-06, 'epoch': 0.87}
 87%|████████▋ | 403/465 [12:43:42<9:58:03, 578.76s/it] 87%|████████▋ | 404/465 [12:45:12<7:19:17, 432.09s/it]                                                       {'loss': 0.742, 'grad_norm': 0.18735772371292114, 'learning_rate': 5.330867767775333e-06, 'epoch': 0.87}
 87%|████████▋ | 404/465 [12:45:12<7:19:17, 432.09s/it] 87%|████████▋ | 405/465 [12:46:41<5:29:06, 329.11s/it]                                                       {'loss': 0.7807, 'grad_norm': 0.15820662677288055, 'learning_rate': 5.163290415662408e-06, 'epoch': 0.87}
 87%|████████▋ | 405/465 [12:46:41<5:29:06, 329.11s/it] 87%|████████▋ | 406/465 [12:47:59<4:09:42, 253.93s/it]                                                       {'loss': 0.7526, 'grad_norm': 0.16573281586170197, 'learning_rate': 4.998245735095458e-06, 'epoch': 0.87}
 87%|████████▋ | 406/465 [12:47:59<4:09:42, 253.93s/it] 88%|████████▊ | 407/465 [12:49:47<3:23:02, 210.05s/it]                                                       {'loss': 0.7923, 'grad_norm': 0.1680140495300293, 'learning_rate': 4.835743048880959e-06, 'epoch': 0.88}
 88%|████████▊ | 407/465 [12:49:47<3:23:02, 210.05s/it] 88%|████████▊ | 408/465 [12:51:13<2:44:01, 172.65s/it]                                                       {'loss': 0.8098, 'grad_norm': 0.18301822245121002, 'learning_rate': 4.675791536236857e-06, 'epoch': 0.88}
 88%|████████▊ | 408/465 [12:51:13<2:44:01, 172.65s/it] 88%|████████▊ | 409/465 [12:52:49<2:19:56, 149.93s/it]                                                       {'loss': 0.826, 'grad_norm': 0.1389734148979187, 'learning_rate': 4.5184002322740785e-06, 'epoch': 0.88}
 88%|████████▊ | 409/465 [12:52:49<2:19:56, 149.93s/it] 88%|████████▊ | 410/465 [12:54:22<2:01:33, 132.60s/it]                                                       {'loss': 0.8009, 'grad_norm': 0.17818821966648102, 'learning_rate': 4.363578027486187e-06, 'epoch': 0.88}
 88%|████████▊ | 410/465 [12:54:22<2:01:33, 132.60s/it] 88%|████████▊ | 411/465 [12:56:22<1:56:01, 128.92s/it]                                                       {'loss': 0.8052, 'grad_norm': 0.14295083284378052, 'learning_rate': 4.2113336672471245e-06, 'epoch': 0.88}
 88%|████████▊ | 411/465 [12:56:22<1:56:01, 128.92s/it] 89%|████████▊ | 412/465 [12:57:54<1:44:03, 117.80s/it]                                                       {'loss': 0.799, 'grad_norm': 0.1573028564453125, 'learning_rate': 4.061675751317312e-06, 'epoch': 0.89}
 89%|████████▊ | 412/465 [12:57:54<1:44:03, 117.80s/it] 89%|████████▉ | 413/465 [12:59:48<1:41:13, 116.79s/it]                                                       {'loss': 0.8695, 'grad_norm': 0.1658574640750885, 'learning_rate': 3.914612733357775e-06, 'epoch': 0.89}
 89%|████████▉ | 413/465 [12:59:48<1:41:13, 116.79s/it] 89%|████████▉ | 414/465 [13:01:35<1:36:44, 113.81s/it]                                                       {'loss': 0.7825, 'grad_norm': 0.15989480912685394, 'learning_rate': 3.770152920452685e-06, 'epoch': 0.89}
 89%|████████▉ | 414/465 [13:01:35<1:36:44, 113.81s/it] 89%|████████▉ | 415/465 [13:03:25<1:33:56, 112.73s/it]                                                       {'loss': 0.8141, 'grad_norm': 0.1912926286458969, 'learning_rate': 3.6283044726401593e-06, 'epoch': 0.89}
 89%|████████▉ | 415/465 [13:03:25<1:33:56, 112.73s/it] 89%|████████▉ | 416/465 [13:05:05<1:28:52, 108.83s/it]                                                       {'loss': 0.8104, 'grad_norm': 0.18171602487564087, 'learning_rate': 3.489075402451225e-06, 'epoch': 0.9}
 89%|████████▉ | 416/465 [13:05:05<1:28:52, 108.83s/it] 90%|████████▉ | 417/465 [13:06:53<1:26:48, 108.52s/it]                                                       {'loss': 0.7835, 'grad_norm': 0.18824531137943268, 'learning_rate': 3.3524735744573043e-06, 'epoch': 0.9}
 90%|████████▉ | 417/465 [13:06:53<1:26:48, 108.52s/it] 90%|████████▉ | 418/465 [13:08:21<1:20:17, 102.51s/it]                                                       {'loss': 0.8665, 'grad_norm': 0.16324491798877716, 'learning_rate': 3.2185067048259244e-06, 'epoch': 0.9}
 90%|████████▉ | 418/465 [13:08:21<1:20:17, 102.51s/it] 90%|█████████ | 419/465 [13:10:06<1:19:08, 103.22s/it]                                                       {'loss': 0.7053, 'grad_norm': 0.18539638817310333, 'learning_rate': 3.0871823608848714e-06, 'epoch': 0.9}
 90%|█████████ | 419/465 [13:10:06<1:19:08, 103.22s/it] 90%|█████████ | 420/465 [13:11:43<1:16:04, 101.42s/it]                                                       {'loss': 0.9226, 'grad_norm': 0.12741193175315857, 'learning_rate': 2.958507960694784e-06, 'epoch': 0.9}
 90%|█████████ | 420/465 [13:11:43<1:16:04, 101.42s/it] 91%|█████████ | 421/465 [13:13:11<1:11:14, 97.14s/it]                                                       {'loss': 0.8862, 'grad_norm': 0.14176613092422485, 'learning_rate': 2.8324907726300366e-06, 'epoch': 0.91}
 91%|█████████ | 421/465 [13:13:11<1:11:14, 97.14s/it] 91%|█████████ | 422/465 [13:14:53<1:10:51, 98.87s/it]                                                      {'loss': 0.8185, 'grad_norm': 0.16957563161849976, 'learning_rate': 2.7091379149682685e-06, 'epoch': 0.91}
 91%|█████████ | 422/465 [13:14:53<1:10:51, 98.87s/it] 91%|█████████ | 423/465 [13:16:26<1:07:52, 96.97s/it]                                                      {'loss': 0.8664, 'grad_norm': 0.15086811780929565, 'learning_rate': 2.5884563554882336e-06, 'epoch': 0.91}
 91%|█████████ | 423/465 [13:16:26<1:07:52, 96.97s/it] 91%|█████████ | 424/465 [13:18:16<1:08:52, 100.80s/it]                                                       {'loss': 0.7857, 'grad_norm': 0.13941898941993713, 'learning_rate': 2.4704529110762265e-06, 'epoch': 0.91}
 91%|█████████ | 424/465 [13:18:16<1:08:52, 100.80s/it] 91%|█████████▏| 425/465 [13:19:56<1:07:03, 100.58s/it]                                                       {'loss': 0.7401, 'grad_norm': 0.1917508840560913, 'learning_rate': 2.355134247341073e-06, 'epoch': 0.91}
 91%|█████████▏| 425/465 [13:19:56<1:07:03, 100.58s/it] 92%|█████████▏| 426/465 [13:21:36<1:05:18, 100.48s/it]                                                       {'loss': 0.8365, 'grad_norm': 0.14430932700634003, 'learning_rate': 2.242506878237538e-06, 'epoch': 0.92}
 92%|█████████▏| 426/465 [13:21:36<1:05:18, 100.48s/it] 92%|█████████▏| 427/465 [13:23:06<1:01:36, 97.27s/it]                                                       {'loss': 0.8274, 'grad_norm': 0.15644143521785736, 'learning_rate': 2.1325771656984072e-06, 'epoch': 0.92}
 92%|█████████▏| 427/465 [13:23:06<1:01:36, 97.27s/it] 92%|█████████▏| 428/465 [13:24:47<1:00:36, 98.29s/it]                                                      {'loss': 0.7883, 'grad_norm': 0.16379447281360626, 'learning_rate': 2.0253513192751373e-06, 'epoch': 0.92}
 92%|█████████▏| 428/465 [13:24:47<1:00:36, 98.29s/it] 92%|█████████▏| 429/465 [13:26:40<1:01:43, 102.87s/it]                                                       {'loss': 0.8251, 'grad_norm': 0.16713114082813263, 'learning_rate': 1.9208353957870686e-06, 'epoch': 0.92}
 92%|█████████▏| 429/465 [13:26:40<1:01:43, 102.87s/it] 92%|█████████▏| 430/465 [13:28:12<58:10, 99.72s/it]                                                       {'loss': 0.81, 'grad_norm': 0.1865633726119995, 'learning_rate': 1.8190352989793325e-06, 'epoch': 0.93}
 92%|█████████▏| 430/465 [13:28:12<58:10, 99.72s/it] 93%|█████████▎| 431/465 [13:29:54<56:48, 100.24s/it]                                                     {'loss': 0.8043, 'grad_norm': 0.16547396779060364, 'learning_rate': 1.7199567791893522e-06, 'epoch': 0.93}
 93%|█████████▎| 431/465 [13:29:54<56:48, 100.24s/it] 93%|█████████▎| 432/465 [13:31:33<54:52, 99.77s/it]                                                     {'loss': 0.9318, 'grad_norm': 0.12168657779693604, 'learning_rate': 1.6236054330219853e-06, 'epoch': 0.93}
 93%|█████████▎| 432/465 [13:31:33<54:52, 99.77s/it] 93%|█████████▎| 433/465 [13:33:00<51:12, 96.01s/it]                                                    {'loss': 0.7792, 'grad_norm': 0.18871457874774933, 'learning_rate': 1.5299867030334814e-06, 'epoch': 0.93}
 93%|█████████▎| 433/465 [13:33:00<51:12, 96.01s/it] 93%|█████████▎| 434/465 [13:34:52<52:04, 100.79s/it]                                                     {'loss': 0.7995, 'grad_norm': 0.1262262761592865, 'learning_rate': 1.439105877423963e-06, 'epoch': 0.93}
 93%|█████████▎| 434/465 [13:34:52<52:04, 100.79s/it] 94%|█████████▎| 435/465 [13:36:47<52:30, 105.01s/it]                                                     {'loss': 0.7852, 'grad_norm': 0.16684944927692413, 'learning_rate': 1.3509680897387578e-06, 'epoch': 0.94}
 94%|█████████▎| 435/465 [13:36:47<52:30, 105.01s/it] 94%|█████████▍| 436/465 [13:38:07<47:13, 97.70s/it]                                                     {'loss': 0.8019, 'grad_norm': 0.17041835188865662, 'learning_rate': 1.2655783185784254e-06, 'epoch': 0.94}
 94%|█████████▍| 436/465 [13:38:07<47:13, 97.70s/it] 94%|█████████▍| 437/465 [13:39:46<45:44, 98.01s/it]                                                    {'loss': 0.8165, 'grad_norm': 0.17546139657497406, 'learning_rate': 1.182941387317499e-06, 'epoch': 0.94}
 94%|█████████▍| 437/465 [13:39:46<45:44, 98.01s/it] 94%|█████████▍| 438/465 [13:41:20<43:36, 96.90s/it]                                                    {'loss': 0.8761, 'grad_norm': 0.171559140086174, 'learning_rate': 1.1030619638320805e-06, 'epoch': 0.94}
 94%|█████████▍| 438/465 [13:41:20<43:36, 96.90s/it] 94%|█████████▍| 439/465 [13:42:56<41:53, 96.68s/it]                                                    {'loss': 0.7961, 'grad_norm': 0.1366448998451233, 'learning_rate': 1.0259445602361084e-06, 'epoch': 0.94}
 94%|█████████▍| 439/465 [13:42:56<41:53, 96.68s/it] 95%|█████████▍| 440/465 [13:44:49<42:13, 101.32s/it]                                                     {'loss': 0.8659, 'grad_norm': 0.14921854436397552, 'learning_rate': 9.51593532626538e-07, 'epoch': 0.95}
 95%|█████████▍| 440/465 [13:44:49<42:13, 101.32s/it] 95%|█████████▍| 441/465 [13:46:26<40:01, 100.06s/it]                                                     {'loss': 0.8584, 'grad_norm': 0.16862142086029053, 'learning_rate': 8.800130808372553e-07, 'epoch': 0.95}
 95%|█████████▍| 441/465 [13:46:26<40:01, 100.06s/it] 95%|█████████▌| 442/465 [13:48:01<37:49, 98.65s/it]                                                     {'loss': 0.7177, 'grad_norm': 0.1536954939365387, 'learning_rate': 8.112072482018341e-07, 'epoch': 0.95}
 95%|█████████▌| 442/465 [13:48:01<37:49, 98.65s/it] 95%|█████████▌| 443/465 [13:49:33<35:27, 96.69s/it]                                                    {'loss': 0.876, 'grad_norm': 0.1622801423072815, 'learning_rate': 7.451799213251621e-07, 'epoch': 0.95}
 95%|█████████▌| 443/465 [13:49:33<35:27, 96.69s/it] 95%|█████████▌| 444/465 [13:51:08<33:41, 96.26s/it]                                                    {'loss': 0.758, 'grad_norm': 0.16035096347332, 'learning_rate': 6.819348298638839e-07, 'epoch': 0.96}
 95%|█████████▌| 444/465 [13:51:08<33:41, 96.26s/it] 96%|█████████▌| 445/465 [13:52:43<31:55, 95.78s/it]                                                    {'loss': 0.7282, 'grad_norm': 0.1734892576932907, 'learning_rate': 6.214755463157418e-07, 'epoch': 0.96}
 96%|█████████▌| 445/465 [13:52:43<31:55, 95.78s/it] 96%|█████████▌| 446/465 [13:54:25<30:55, 97.67s/it]                                                    {'loss': 0.7974, 'grad_norm': 0.16820360720157623, 'learning_rate': 5.638054858177644e-07, 'epoch': 0.96}
 96%|█████████▌| 446/465 [13:54:25<30:55, 97.67s/it] 96%|█████████▌| 447/465 [13:55:59<28:58, 96.57s/it]                                                    {'loss': 0.7952, 'grad_norm': 0.1635526865720749, 'learning_rate': 5.089279059533658e-07, 'epoch': 0.96}
 96%|█████████▌| 447/465 [13:55:59<28:58, 96.57s/it] 96%|█████████▋| 448/465 [13:57:40<27:44, 97.91s/it]                                                    {'loss': 0.7928, 'grad_norm': 0.1506500244140625, 'learning_rate': 4.568459065683206e-07, 'epoch': 0.96}
 96%|█████████▋| 448/465 [13:57:40<27:44, 97.91s/it] 97%|█████████▋| 449/465 [13:59:04<25:00, 93.80s/it]                                                    {'loss': 0.7756, 'grad_norm': 0.18585827946662903, 'learning_rate': 4.075624295956759e-07, 'epoch': 0.97}
 97%|█████████▋| 449/465 [13:59:04<25:00, 93.80s/it] 97%|█████████▋| 450/465 [14:00:25<22:26, 89.77s/it]                                                    {'loss': 0.7185, 'grad_norm': 0.16429950296878815, 'learning_rate': 3.6108025888958453e-07, 'epoch': 0.97}
 97%|█████████▋| 450/465 [14:00:25<22:26, 89.77s/it] 97%|█████████▋| 451/465 [14:01:59<21:17, 91.24s/it]                                                    {'loss': 0.7973, 'grad_norm': 0.20511974394321442, 'learning_rate': 3.1740202006804167e-07, 'epoch': 0.97}
 97%|█████████▋| 451/465 [14:01:59<21:17, 91.24s/it] 97%|█████████▋| 452/465 [14:03:31<19:48, 91.41s/it]                                                    {'loss': 0.782, 'grad_norm': 0.1532817780971527, 'learning_rate': 2.7653018036454257e-07, 'epoch': 0.97}
 97%|█████████▋| 452/465 [14:03:31<19:48, 91.41s/it] 97%|█████████▋| 453/465 [14:05:12<18:50, 94.23s/it]                                                    {'loss': 0.7796, 'grad_norm': 0.19144146144390106, 'learning_rate': 2.38467048488783e-07, 'epoch': 0.97}
 97%|█████████▋| 453/465 [14:05:12<18:50, 94.23s/it] 98%|█████████▊| 454/465 [14:07:13<18:44, 102.20s/it]                                                     {'loss': 0.874, 'grad_norm': 0.13379932940006256, 'learning_rate': 2.0321477449619098e-07, 'epoch': 0.98}
 98%|█████████▊| 454/465 [14:07:13<18:44, 102.20s/it] 98%|█████████▊| 455/465 [14:08:39<16:14, 97.40s/it]                                                     {'loss': 0.8643, 'grad_norm': 0.13815559446811676, 'learning_rate': 1.7077534966650766e-07, 'epoch': 0.98}
 98%|█████████▊| 455/465 [14:08:39<16:14, 97.40s/it] 98%|█████████▊| 456/465 [14:10:21<14:48, 98.75s/it]                                                    {'loss': 0.7871, 'grad_norm': 0.1512797474861145, 'learning_rate': 1.4115060639128818e-07, 'epoch': 0.98}
 98%|█████████▊| 456/465 [14:10:21<14:48, 98.75s/it] 98%|█████████▊| 457/465 [14:11:50<12:46, 95.87s/it]                                                    {'loss': 0.7702, 'grad_norm': 0.17520219087600708, 'learning_rate': 1.1434221807041234e-07, 'epoch': 0.98}
 98%|█████████▊| 457/465 [14:11:50<12:46, 95.87s/it] 98%|█████████▊| 458/465 [14:13:27<11:12, 96.12s/it]                                                    {'loss': 0.7814, 'grad_norm': 0.16507044434547424, 'learning_rate': 9.035169901754903e-08, 'epoch': 0.99}
 98%|█████████▊| 458/465 [14:13:27<11:12, 96.12s/it] 99%|█████████▊| 459/465 [14:14:50<09:13, 92.18s/it]                                                    {'loss': 0.863, 'grad_norm': 0.11935660243034363, 'learning_rate': 6.918040437463024e-08, 'epoch': 0.99}
 99%|█████████▊| 459/465 [14:14:50<09:13, 92.18s/it] 99%|█████████▉| 460/465 [14:16:42<08:10, 98.10s/it]                                                    {'loss': 0.8617, 'grad_norm': 0.18571841716766357, 'learning_rate': 5.082953003528457e-08, 'epoch': 0.99}
 99%|█████████▉| 460/465 [14:16:42<08:10, 98.10s/it] 99%|█████████▉| 461/465 [14:18:12<06:22, 95.65s/it]                                                    {'loss': 0.8191, 'grad_norm': 0.16670161485671997, 'learning_rate': 3.5300112577302255e-08, 'epoch': 0.99}
 99%|█████████▉| 461/465 [14:18:12<06:22, 95.65s/it] 99%|█████████▉| 462/465 [14:20:02<05:00, 100.05s/it]                                                     {'loss': 0.7664, 'grad_norm': 0.13852477073669434, 'learning_rate': 2.2593029204076576e-08, 'epoch': 0.99}
 99%|█████████▉| 462/465 [14:20:02<05:00, 100.05s/it]100%|█████████▉| 463/465 [14:21:44<03:21, 100.64s/it]                                                     {'loss': 0.7601, 'grad_norm': 0.1887453943490982, 'learning_rate': 1.2708997695043412e-08, 'epoch': 1.0}
100%|█████████▉| 463/465 [14:21:44<03:21, 100.64s/it]100%|█████████▉| 464/465 [14:23:32<01:42, 102.75s/it]                                                     {'loss': 0.8138, 'grad_norm': 0.18412789702415466, 'learning_rate': 5.648576365169245e-09, 'epoch': 1.0}
100%|█████████▉| 464/465 [14:23:32<01:42, 102.75s/it]100%|██████████| 465/465 [14:24:27<00:00, 88.48s/it]                                                     {'loss': 0.7953, 'grad_norm': 0.1840483397245407, 'learning_rate': 1.412164033365304e-09, 'epoch': 1.0}
100%|██████████| 465/465 [14:24:27<00:00, 88.48s/it][INFO|trainer.py:4309] 2026-01-03 09:09:49,553 >> Saving model checkpoint to saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-465
/opt/conda/lib/python3.11/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in Qwen/Qwen2.5-VL-7B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2421] 2026-01-03 09:09:49,636 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-465/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2026-01-03 09:09:49,638 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-465/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2026-01-03 09:09:49,641 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-465/special_tokens_map.json
[2026-01-03 09:09:50,315] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step465 is about to be saved!
[2026-01-03 09:09:50,334] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-465/global_step465/mp_rank_00_model_states.pt
[2026-01-03 09:09:50,334] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-465/global_step465/mp_rank_00_model_states.pt...
[2026-01-03 09:09:50,398] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-465/global_step465/mp_rank_00_model_states.pt.
[2026-01-03 09:09:50,401] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-465/global_step465/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2026-01-03 09:09:50,401] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-465/global_step465/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2026-01-03 09:09:50,401] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-465/global_step465/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2026-01-03 09:09:50,401] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-465/global_step465/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2026-01-03 09:09:50,475] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-465/global_step465/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2026-01-03 09:09:50,475] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-465/global_step465/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2026-01-03 09:09:50,475] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step465 is ready now!
[2026-01-03 09:09:50,478] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-465/global_step465/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2026-01-03 09:09:50,479] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-465/global_step465/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2026-01-03 09:09:50,479] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step465 is ready now!
[2026-01-03 09:09:50,479] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-465/global_step465/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2026-01-03 09:09:50,480] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-465/global_step465/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2026-01-03 09:09:50,480] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-465/global_step465/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2026-01-03 09:09:50,480] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step465 is ready now!
[2026-01-03 09:09:50,482] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-465/global_step465/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2026-01-03 09:09:50,482] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step465 is ready now!


Training completed. Do not forget to share your model on huggingface.co/models =)




Training completed. Do not forget to share your model on huggingface.co/models =)




Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|image_processing_base.py:253] 2026-01-03 09:09:50,503 >> Image processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-465/preprocessor_config.json
[INFO|tokenization_utils_base.py:2421] 2026-01-03 09:09:50,506 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-465/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2026-01-03 09:09:50,509 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-465/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2026-01-03 09:09:50,511 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-465/special_tokens_map.json
[INFO|video_processing_utils.py:600] 2026-01-03 09:09:50,626 >> Video processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-465/video_preprocessor_config.json
[INFO|processing_utils.py:814] 2026-01-03 09:09:50,629 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/checkpoint-465/chat_template.jinja
[INFO|trainer.py:2810] 2026-01-03 09:09:50,883 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                    {'train_runtime': 51873.1205, 'train_samples_per_second': 0.573, 'train_steps_per_second': 0.009, 'train_loss': 0.918163150741208, 'epoch': 1.0}
100%|██████████| 465/465 [14:24:31<00:00, 88.48s/it]100%|██████████| 465/465 [14:24:31<00:00, 111.55s/it]
[INFO|image_processing_base.py:253] 2026-01-03 09:09:50,890 >> Image processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/preprocessor_config.json
[INFO|tokenization_utils_base.py:2421] 2026-01-03 09:09:50,893 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2026-01-03 09:09:50,896 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2026-01-03 09:09:50,898 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/special_tokens_map.json
[INFO|video_processing_utils.py:600] 2026-01-03 09:09:51,015 >> Video processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/video_preprocessor_config.json
[INFO|processing_utils.py:814] 2026-01-03 09:09:51,018 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/chat_template.jinja

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1
[INFO|trainer.py:4309] 2026-01-03 09:09:53,709 >> Saving model checkpoint to saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected
/opt/conda/lib/python3.11/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in Qwen/Qwen2.5-VL-7B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2421] 2026-01-03 09:09:53,780 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2026-01-03 09:09:53,783 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2026-01-03 09:09:53,785 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/special_tokens_map.json
***** train metrics *****
  epoch                    =           1.0
  total_flos               = 12198965358GF
  train_loss               =        0.9182
  train_runtime            =   14:24:33.12
  train_samples_per_second =         0.573
  train_steps_per_second   =         0.009
Figure saved at: saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/training_loss.png
Figure saved at: saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R0C1F0X0_traineval_eval200_corrected/training_eval_loss.png
[WARNING|2026-01-03 09:09:54] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.
[INFO|trainer.py:4643] 2026-01-03 09:09:54,149 >> 
***** Running Evaluation *****
[INFO|trainer.py:4645] 2026-01-03 09:09:54,149 >>   Num examples = 3305
[INFO|trainer.py:4648] 2026-01-03 09:09:54,149 >>   Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1
  0%|          | 0/827 [00:00<?, ?it/s]  0%|          | 2/827 [00:04<28:15,  2.06s/it]  0%|          | 3/827 [00:06<31:07,  2.27s/it]  0%|          | 4/827 [00:09<34:24,  2.51s/it]  1%|          | 5/827 [00:13<39:59,  2.92s/it]  1%|          | 6/827 [00:16<42:51,  3.13s/it]  1%|          | 7/827 [00:20<46:30,  3.40s/it]  1%|          | 8/827 [00:26<55:49,  4.09s/it]  1%|          | 9/827 [00:33<1:06:33,  4.88s/it]  1%|          | 10/827 [00:39<1:14:50,  5.50s/it]  1%|▏         | 11/827 [00:43<1:05:45,  4.83s/it]  1%|▏         | 12/827 [00:45<56:58,  4.19s/it]    2%|▏         | 13/827 [00:49<52:12,  3.85s/it]  2%|▏         | 14/827 [00:52<51:17,  3.79s/it]  2%|▏         | 15/827 [00:56<52:58,  3.91s/it]  2%|▏         | 16/827 [01:00<51:02,  3.78s/it]  2%|▏         | 17/827 [01:03<47:59,  3.55s/it]  2%|▏         | 18/827 [01:06<46:58,  3.48s/it]  2%|▏         | 19/827 [01:11<51:29,  3.82s/it]  2%|▏         | 20/827 [01:16<57:28,  4.27s/it]  3%|▎         | 21/827 [01:20<55:07,  4.10s/it]  3%|▎         | 22/827 [01:24<57:00,  4.25s/it]  3%|▎         | 23/827 [01:27<51:23,  3.84s/it]  3%|▎         | 24/827 [01:31<50:58,  3.81s/it]  3%|▎         | 25/827 [01:35<53:23,  3.99s/it]  3%|▎         | 26/827 [01:41<58:02,  4.35s/it]  3%|▎         | 27/827 [01:45<57:47,  4.33s/it]  3%|▎         | 28/827 [01:48<51:39,  3.88s/it]  4%|▎         | 29/827 [01:52<54:30,  4.10s/it]  4%|▎         | 30/827 [01:57<57:31,  4.33s/it]  4%|▎         | 31/827 [02:01<55:28,  4.18s/it]  4%|▍         | 32/827 [02:05<54:36,  4.12s/it]  4%|▍         | 33/827 [02:10<58:48,  4.44s/it]  4%|▍         | 34/827 [02:13<53:30,  4.05s/it]  4%|▍         | 35/827 [02:17<50:38,  3.84s/it]  4%|▍         | 36/827 [02:19<45:51,  3.48s/it]  4%|▍         | 37/827 [02:24<50:39,  3.85s/it]  5%|▍         | 38/827 [02:29<54:56,  4.18s/it]  5%|▍         | 39/827 [02:34<56:14,  4.28s/it]  5%|▍         | 40/827 [02:38<58:42,  4.48s/it]  5%|▍         | 41/827 [02:42<52:56,  4.04s/it]  5%|▌         | 42/827 [02:45<51:18,  3.92s/it]  5%|▌         | 43/827 [02:49<49:59,  3.83s/it]  5%|▌         | 44/827 [02:52<47:47,  3.66s/it]  5%|▌         | 45/827 [02:56<49:46,  3.82s/it]  6%|▌         | 46/827 [03:01<55:00,  4.23s/it]  6%|▌         | 47/827 [03:06<56:32,  4.35s/it]  6%|▌         | 48/827 [03:09<52:39,  4.06s/it]  6%|▌         | 49/827 [03:13<49:52,  3.85s/it]  6%|▌         | 50/827 [03:16<45:52,  3.54s/it]  6%|▌         | 51/827 [03:20<50:43,  3.92s/it]  6%|▋         | 52/827 [03:26<57:40,  4.46s/it]  6%|▋         | 53/827 [03:29<50:50,  3.94s/it]  7%|▋         | 54/827 [03:32<46:28,  3.61s/it]  7%|▋         | 55/827 [03:35<44:08,  3.43s/it]  7%|▋         | 56/827 [03:38<42:43,  3.32s/it]  7%|▋         | 57/827 [03:41<43:43,  3.41s/it]  7%|▋         | 58/827 [03:44<42:04,  3.28s/it]  7%|▋         | 59/827 [03:48<42:05,  3.29s/it]  7%|▋         | 60/827 [03:52<45:02,  3.52s/it]  7%|▋         | 61/827 [03:57<50:20,  3.94s/it]  7%|▋         | 62/827 [03:59<45:46,  3.59s/it]  8%|▊         | 63/827 [04:02<42:56,  3.37s/it]  8%|▊         | 64/827 [04:05<41:31,  3.27s/it]  8%|▊         | 65/827 [04:08<40:52,  3.22s/it]  8%|▊         | 66/827 [04:13<47:32,  3.75s/it]  8%|▊         | 67/827 [04:18<51:17,  4.05s/it]  8%|▊         | 68/827 [04:21<45:22,  3.59s/it]  8%|▊         | 69/827 [04:24<44:08,  3.49s/it]  8%|▊         | 70/827 [04:27<41:18,  3.27s/it]  9%|▊         | 71/827 [04:31<44:02,  3.50s/it]  9%|▊         | 72/827 [04:35<48:00,  3.82s/it]  9%|▉         | 73/827 [04:39<45:56,  3.66s/it]  9%|▉         | 74/827 [04:44<52:23,  4.17s/it]  9%|▉         | 75/827 [04:49<54:46,  4.37s/it]  9%|▉         | 76/827 [04:52<49:15,  3.94s/it]  9%|▉         | 77/827 [04:55<45:45,  3.66s/it]  9%|▉         | 78/827 [04:58<43:14,  3.46s/it] 10%|▉         | 79/827 [05:00<40:25,  3.24s/it] 10%|▉         | 80/827 [05:05<44:13,  3.55s/it] 10%|▉         | 81/827 [05:09<47:38,  3.83s/it] 10%|▉         | 82/827 [05:13<48:49,  3.93s/it] 10%|█         | 83/827 [05:17<48:59,  3.95s/it] 10%|█         | 84/827 [05:21<49:13,  3.97s/it] 10%|█         | 85/827 [05:25<49:33,  4.01s/it] 10%|█         | 86/827 [05:29<45:54,  3.72s/it] 11%|█         | 87/827 [05:34<54:11,  4.39s/it] 11%|█         | 88/827 [05:44<1:13:17,  5.95s/it] 11%|█         | 89/827 [05:51<1:17:54,  6.33s/it] 11%|█         | 90/827 [05:56<1:12:58,  5.94s/it] 11%|█         | 91/827 [06:00<1:03:34,  5.18s/it] 11%|█         | 92/827 [06:03<57:18,  4.68s/it]   11%|█         | 93/827 [06:06<50:18,  4.11s/it] 11%|█▏        | 94/827 [06:09<46:32,  3.81s/it] 11%|█▏        | 95/827 [06:12<43:32,  3.57s/it] 12%|█▏        | 96/827 [06:15<40:19,  3.31s/it] 12%|█▏        | 97/827 [06:17<36:40,  3.01s/it] 12%|█▏        | 98/827 [06:20<34:50,  2.87s/it] 12%|█▏        | 99/827 [06:22<33:49,  2.79s/it] 12%|█▏        | 100/827 [06:24<31:38,  2.61s/it] 12%|█▏        | 101/827 [06:28<34:36,  2.86s/it] 12%|█▏        | 102/827 [06:31<35:27,  2.94s/it] 12%|█▏        | 103/827 [06:34<35:01,  2.90s/it] 13%|█▎        | 104/827 [06:37<34:35,  2.87s/it] 13%|█▎        | 105/827 [06:40<35:33,  2.95s/it] 13%|█▎        | 106/827 [06:46<46:55,  3.91s/it] 13%|█▎        | 107/827 [06:52<55:14,  4.60s/it] 13%|█▎        | 108/827 [06:57<54:45,  4.57s/it] 13%|█▎        | 109/827 [07:00<51:13,  4.28s/it] 13%|█▎        | 110/827 [07:04<48:27,  4.06s/it] 13%|█▎        | 111/827 [07:09<51:13,  4.29s/it] 14%|█▎        | 112/827 [07:14<56:25,  4.73s/it] 14%|█▎        | 113/827 [07:18<53:13,  4.47s/it] 14%|█▍        | 114/827 [07:22<49:05,  4.13s/it] 14%|█▍        | 115/827 [07:25<45:44,  3.85s/it] 14%|█▍        | 116/827 [07:28<44:49,  3.78s/it] 14%|█▍        | 117/827 [07:33<46:39,  3.94s/it] 14%|█▍        | 118/827 [07:38<51:50,  4.39s/it] 14%|█▍        | 119/827 [07:43<54:33,  4.62s/it] 15%|█▍        | 120/827 [07:48<54:07,  4.59s/it] 15%|█▍        | 121/827 [07:51<49:19,  4.19s/it] 15%|█▍        | 122/827 [07:54<45:25,  3.87s/it] 15%|█▍        | 123/827 [07:57<41:04,  3.50s/it] 15%|█▍        | 124/827 [08:00<40:22,  3.45s/it] 15%|█▌        | 125/827 [08:04<41:39,  3.56s/it] 15%|█▌        | 126/827 [08:07<40:43,  3.49s/it] 15%|█▌        | 127/827 [08:10<39:24,  3.38s/it] 15%|█▌        | 128/827 [08:14<40:23,  3.47s/it] 16%|█▌        | 129/827 [08:17<37:41,  3.24s/it] 16%|█▌        | 130/827 [08:20<38:55,  3.35s/it] 16%|█▌        | 131/827 [08:24<40:39,  3.50s/it] 16%|█▌        | 132/827 [08:29<43:55,  3.79s/it] 16%|█▌        | 133/827 [08:33<45:55,  3.97s/it] 16%|█▌        | 134/827 [08:37<43:52,  3.80s/it] 16%|█▋        | 135/827 [08:41<46:41,  4.05s/it] 16%|█▋        | 136/827 [08:47<53:11,  4.62s/it] 17%|█▋        | 137/827 [08:50<47:56,  4.17s/it] 17%|█▋        | 138/827 [08:56<53:43,  4.68s/it] 17%|█▋        | 139/827 [09:03<1:00:23,  5.27s/it] 17%|█▋        | 140/827 [09:06<53:48,  4.70s/it]   17%|█▋        | 141/827 [09:10<50:43,  4.44s/it] 17%|█▋        | 142/827 [09:13<46:28,  4.07s/it] 17%|█▋        | 143/827 [09:17<44:19,  3.89s/it] 17%|█▋        | 144/827 [09:19<40:25,  3.55s/it] 18%|█▊        | 145/827 [09:23<40:43,  3.58s/it] 18%|█▊        | 146/827 [09:27<41:36,  3.67s/it] 18%|█▊        | 147/827 [09:30<40:05,  3.54s/it] 18%|█▊        | 148/827 [09:33<38:44,  3.42s/it] 18%|█▊        | 149/827 [09:36<37:47,  3.34s/it] 18%|█▊        | 150/827 [09:39<35:17,  3.13s/it] 18%|█▊        | 151/827 [09:42<33:08,  2.94s/it] 18%|█▊        | 152/827 [09:45<33:36,  2.99s/it] 19%|█▊        | 153/827 [09:48<33:53,  3.02s/it] 19%|█▊        | 154/827 [09:52<36:38,  3.27s/it] 19%|█▊        | 155/827 [09:57<42:09,  3.76s/it] 19%|█▉        | 156/827 [10:01<45:03,  4.03s/it] 19%|█▉        | 157/827 [10:04<42:21,  3.79s/it] 19%|█▉        | 158/827 [10:07<39:37,  3.55s/it] 19%|█▉        | 159/827 [10:12<42:01,  3.77s/it] 19%|█▉        | 160/827 [10:16<43:26,  3.91s/it] 19%|█▉        | 161/827 [10:19<39:06,  3.52s/it] 20%|█▉        | 162/827 [10:22<39:38,  3.58s/it] 20%|█▉        | 163/827 [10:26<40:35,  3.67s/it] 20%|█▉        | 164/827 [10:30<39:34,  3.58s/it] 20%|█▉        | 165/827 [10:33<37:58,  3.44s/it] 20%|██        | 166/827 [10:38<44:47,  4.07s/it] 20%|██        | 167/827 [10:47<1:00:00,  5.46s/it] 20%|██        | 168/827 [10:52<59:00,  5.37s/it]   20%|██        | 169/827 [10:55<51:36,  4.71s/it] 21%|██        | 170/827 [10:59<48:53,  4.47s/it] 21%|██        | 171/827 [11:03<46:37,  4.26s/it] 21%|██        | 172/827 [11:06<41:27,  3.80s/it] 21%|██        | 173/827 [11:08<37:48,  3.47s/it] 21%|██        | 174/827 [11:11<36:08,  3.32s/it] 21%|██        | 175/827 [11:15<37:09,  3.42s/it] 21%|██▏       | 176/827 [11:18<34:30,  3.18s/it] 21%|██▏       | 177/827 [11:20<32:59,  3.05s/it] 22%|██▏       | 178/827 [11:24<35:49,  3.31s/it] 22%|██▏       | 179/827 [11:30<42:14,  3.91s/it] 22%|██▏       | 180/827 [11:33<42:12,  3.91s/it] 22%|██▏       | 181/827 [11:37<41:09,  3.82s/it] 22%|██▏       | 182/827 [11:42<43:31,  4.05s/it] 22%|██▏       | 183/827 [11:45<40:13,  3.75s/it] 22%|██▏       | 184/827 [11:49<42:02,  3.92s/it] 22%|██▏       | 185/827 [11:54<44:45,  4.18s/it] 22%|██▏       | 186/827 [11:57<41:07,  3.85s/it] 23%|██▎       | 187/827 [12:00<37:34,  3.52s/it] 23%|██▎       | 188/827 [12:03<38:22,  3.60s/it] 23%|██▎       | 189/827 [12:08<41:59,  3.95s/it] 23%|██▎       | 190/827 [12:12<41:26,  3.90s/it] 23%|██▎       | 191/827 [12:16<41:19,  3.90s/it] 23%|██▎       | 192/827 [12:20<43:05,  4.07s/it] 23%|██▎       | 193/827 [12:24<42:24,  4.01s/it] 23%|██▎       | 194/827 [12:27<38:58,  3.69s/it] 24%|██▎       | 195/827 [12:30<36:49,  3.50s/it] 24%|██▎       | 196/827 [12:33<33:30,  3.19s/it] 24%|██▍       | 197/827 [12:35<31:55,  3.04s/it] 24%|██▍       | 198/827 [12:39<32:35,  3.11s/it] 24%|██▍       | 199/827 [12:42<33:24,  3.19s/it] 24%|██▍       | 200/827 [12:46<36:35,  3.50s/it] 24%|██▍       | 201/827 [12:51<40:49,  3.91s/it] 24%|██▍       | 202/827 [12:56<43:59,  4.22s/it] 25%|██▍       | 203/827 [13:01<45:25,  4.37s/it] 25%|██▍       | 204/827 [13:06<49:19,  4.75s/it] 25%|██▍       | 205/827 [13:10<44:45,  4.32s/it] 25%|██▍       | 206/827 [13:13<41:09,  3.98s/it] 25%|██▌       | 207/827 [13:16<38:49,  3.76s/it] 25%|██▌       | 208/827 [13:19<36:37,  3.55s/it] 25%|██▌       | 209/827 [13:22<35:11,  3.42s/it] 25%|██▌       | 210/827 [13:26<34:21,  3.34s/it] 26%|██▌       | 211/827 [13:30<38:15,  3.73s/it] 26%|██▌       | 212/827 [13:35<41:18,  4.03s/it] 26%|██▌       | 213/827 [13:40<45:05,  4.41s/it] 26%|██▌       | 214/827 [13:44<41:48,  4.09s/it] 26%|██▌       | 215/827 [13:47<38:49,  3.81s/it] 26%|██▌       | 216/827 [13:50<37:25,  3.67s/it] 26%|██▌       | 217/827 [13:54<37:58,  3.74s/it] 26%|██▋       | 218/827 [13:58<37:57,  3.74s/it] 26%|██▋       | 219/827 [14:01<35:23,  3.49s/it] 27%|██▋       | 220/827 [14:05<37:40,  3.72s/it] 27%|██▋       | 221/827 [14:10<42:55,  4.25s/it] 27%|██▋       | 222/827 [14:13<39:33,  3.92s/it] 27%|██▋       | 223/827 [14:17<37:45,  3.75s/it] 27%|██▋       | 224/827 [14:20<35:24,  3.52s/it] 27%|██▋       | 225/827 [14:23<34:10,  3.41s/it] 27%|██▋       | 226/827 [14:26<31:50,  3.18s/it] 27%|██▋       | 227/827 [14:30<35:17,  3.53s/it] 28%|██▊       | 228/827 [14:35<39:48,  3.99s/it] 28%|██▊       | 229/827 [14:38<37:26,  3.76s/it] 28%|██▊       | 230/827 [14:42<37:29,  3.77s/it] 28%|██▊       | 231/827 [14:45<36:07,  3.64s/it] 28%|██▊       | 232/827 [14:49<34:43,  3.50s/it] 28%|██▊       | 233/827 [14:53<37:37,  3.80s/it] 28%|██▊       | 234/827 [14:58<41:56,  4.24s/it] 28%|██▊       | 235/827 [15:03<43:49,  4.44s/it] 29%|██▊       | 236/827 [15:07<41:55,  4.26s/it] 29%|██▊       | 237/827 [15:13<45:52,  4.67s/it] 29%|██▉       | 238/827 [15:18<48:25,  4.93s/it] 29%|██▉       | 239/827 [15:23<47:48,  4.88s/it] 29%|██▉       | 240/827 [15:29<50:11,  5.13s/it] 29%|██▉       | 241/827 [15:34<51:33,  5.28s/it] 29%|██▉       | 242/827 [15:38<47:30,  4.87s/it] 29%|██▉       | 243/827 [15:42<45:08,  4.64s/it] 30%|██▉       | 244/827 [15:48<48:01,  4.94s/it] 30%|██▉       | 245/827 [15:51<43:32,  4.49s/it] 30%|██▉       | 246/827 [15:55<41:03,  4.24s/it] 30%|██▉       | 247/827 [15:59<41:06,  4.25s/it] 30%|██▉       | 248/827 [16:06<47:40,  4.94s/it] 30%|███       | 249/827 [16:10<46:12,  4.80s/it] 30%|███       | 250/827 [16:13<39:06,  4.07s/it] 30%|███       | 251/827 [16:16<37:53,  3.95s/it] 30%|███       | 252/827 [16:21<39:26,  4.12s/it] 31%|███       | 253/827 [16:24<35:51,  3.75s/it] 31%|███       | 254/827 [16:29<39:59,  4.19s/it] 31%|███       | 255/827 [16:35<46:14,  4.85s/it] 31%|███       | 256/827 [16:41<48:55,  5.14s/it] 31%|███       | 257/827 [16:46<46:37,  4.91s/it] 31%|███       | 258/827 [16:49<43:36,  4.60s/it] 31%|███▏      | 259/827 [16:55<47:13,  4.99s/it] 31%|███▏      | 260/827 [17:01<49:20,  5.22s/it] 32%|███▏      | 261/827 [17:05<45:22,  4.81s/it] 32%|███▏      | 262/827 [17:11<47:41,  5.06s/it] 32%|███▏      | 263/827 [17:17<50:39,  5.39s/it] 32%|███▏      | 264/827 [17:19<42:44,  4.55s/it] 32%|███▏      | 265/827 [17:22<37:16,  3.98s/it] 32%|███▏      | 266/827 [17:25<34:47,  3.72s/it] 32%|███▏      | 267/827 [17:28<32:56,  3.53s/it] 32%|███▏      | 268/827 [17:31<31:14,  3.35s/it] 33%|███▎      | 269/827 [17:35<33:36,  3.61s/it] 33%|███▎      | 270/827 [17:40<36:43,  3.96s/it] 33%|███▎      | 271/827 [17:43<34:14,  3.69s/it] 33%|███▎      | 272/827 [17:47<33:08,  3.58s/it] 33%|███▎      | 273/827 [17:52<38:30,  4.17s/it] 33%|███▎      | 274/827 [18:00<49:37,  5.38s/it] 33%|███▎      | 275/827 [18:05<47:56,  5.21s/it] 33%|███▎      | 276/827 [18:09<43:44,  4.76s/it] 33%|███▎      | 277/827 [18:12<40:40,  4.44s/it] 34%|███▎      | 278/827 [18:17<39:56,  4.37s/it] 34%|███▎      | 279/827 [18:20<36:48,  4.03s/it] 34%|███▍      | 280/827 [18:23<33:38,  3.69s/it] 34%|███▍      | 281/827 [18:27<33:51,  3.72s/it] 34%|███▍      | 282/827 [18:30<33:12,  3.66s/it] 34%|███▍      | 283/827 [18:33<32:03,  3.53s/it] 34%|███▍      | 284/827 [18:37<32:42,  3.61s/it] 34%|███▍      | 285/827 [18:41<34:17,  3.80s/it] 35%|███▍      | 286/827 [18:46<37:02,  4.11s/it] 35%|███▍      | 287/827 [18:49<32:30,  3.61s/it] 35%|███▍      | 288/827 [18:52<30:44,  3.42s/it] 35%|███▍      | 289/827 [18:55<29:41,  3.31s/it] 35%|███▌      | 290/827 [18:58<28:56,  3.23s/it] 35%|███▌      | 291/827 [19:00<27:17,  3.05s/it] 35%|███▌      | 292/827 [19:04<28:04,  3.15s/it] 35%|███▌      | 293/827 [19:07<27:10,  3.05s/it] 36%|███▌      | 294/827 [19:10<28:20,  3.19s/it] 36%|███▌      | 295/827 [19:14<28:57,  3.27s/it] 36%|███▌      | 296/827 [19:18<31:28,  3.56s/it] 36%|███▌      | 297/827 [19:22<31:56,  3.62s/it] 36%|███▌      | 298/827 [19:24<29:56,  3.40s/it] 36%|███▌      | 299/827 [19:28<29:14,  3.32s/it] 36%|███▋      | 300/827 [19:31<28:23,  3.23s/it] 36%|███▋      | 301/827 [19:34<27:36,  3.15s/it] 37%|███▋      | 302/827 [19:37<28:04,  3.21s/it] 37%|███▋      | 303/827 [19:39<25:36,  2.93s/it] 37%|███▋      | 304/827 [19:42<26:21,  3.02s/it] 37%|███▋      | 305/827 [19:47<29:31,  3.39s/it] 37%|███▋      | 306/827 [19:51<31:12,  3.59s/it] 37%|███▋      | 307/827 [19:54<30:04,  3.47s/it] 37%|███▋      | 308/827 [19:57<28:43,  3.32s/it] 37%|███▋      | 309/827 [20:00<28:49,  3.34s/it] 37%|███▋      | 310/827 [20:03<26:55,  3.12s/it] 38%|███▊      | 311/827 [20:06<25:39,  2.98s/it] 38%|███▊      | 312/827 [20:09<26:45,  3.12s/it] 38%|███▊      | 313/827 [20:13<29:24,  3.43s/it] 38%|███▊      | 314/827 [20:17<29:32,  3.45s/it] 38%|███▊      | 315/827 [20:21<31:57,  3.74s/it] 38%|███▊      | 316/827 [20:26<33:45,  3.96s/it] 38%|███▊      | 317/827 [20:31<36:19,  4.27s/it] 38%|███▊      | 318/827 [20:35<36:19,  4.28s/it] 39%|███▊      | 319/827 [20:39<35:18,  4.17s/it] 39%|███▊      | 320/827 [20:42<32:59,  3.90s/it] 39%|███▉      | 321/827 [20:47<34:24,  4.08s/it] 39%|███▉      | 322/827 [20:51<33:59,  4.04s/it] 39%|███▉      | 323/827 [20:54<33:28,  3.98s/it] 39%|███▉      | 324/827 [20:59<35:08,  4.19s/it] 39%|███▉      | 325/827 [21:02<32:14,  3.85s/it] 39%|███▉      | 326/827 [21:06<31:31,  3.78s/it] 40%|███▉      | 327/827 [21:10<33:36,  4.03s/it] 40%|███▉      | 328/827 [21:15<34:01,  4.09s/it] 40%|███▉      | 329/827 [21:21<38:54,  4.69s/it] 40%|███▉      | 330/827 [21:25<37:52,  4.57s/it] 40%|████      | 331/827 [21:30<38:02,  4.60s/it] 40%|████      | 332/827 [21:34<36:36,  4.44s/it] 40%|████      | 333/827 [21:39<38:01,  4.62s/it] 40%|████      | 334/827 [21:43<36:12,  4.41s/it] 41%|████      | 335/827 [21:46<33:51,  4.13s/it] 41%|████      | 336/827 [21:49<30:24,  3.72s/it] 41%|████      | 337/827 [21:52<29:04,  3.56s/it] 41%|████      | 338/827 [21:55<27:56,  3.43s/it] 41%|████      | 339/827 [21:58<26:55,  3.31s/it] 41%|████      | 340/827 [22:02<27:05,  3.34s/it] 41%|████      | 341/827 [22:05<28:16,  3.49s/it] 41%|████▏     | 342/827 [22:09<27:17,  3.38s/it] 41%|████▏     | 343/827 [22:12<26:37,  3.30s/it] 42%|████▏     | 344/827 [22:16<29:11,  3.63s/it] 42%|████▏     | 345/827 [22:21<32:20,  4.03s/it] 42%|████▏     | 346/827 [22:25<31:34,  3.94s/it] 42%|████▏     | 347/827 [22:28<30:40,  3.83s/it] 42%|████▏     | 348/827 [22:31<28:21,  3.55s/it] 42%|████▏     | 349/827 [22:36<30:16,  3.80s/it] 42%|████▏     | 350/827 [22:40<32:02,  4.03s/it] 42%|████▏     | 351/827 [22:44<30:24,  3.83s/it] 43%|████▎     | 352/827 [22:48<30:52,  3.90s/it] 43%|████▎     | 353/827 [22:51<29:29,  3.73s/it] 43%|████▎     | 354/827 [22:54<28:05,  3.56s/it] 43%|████▎     | 355/827 [22:59<30:20,  3.86s/it] 43%|████▎     | 356/827 [23:03<32:20,  4.12s/it] 43%|████▎     | 357/827 [23:07<30:58,  3.95s/it] 43%|████▎     | 358/827 [23:10<28:12,  3.61s/it] 43%|████▎     | 359/827 [23:13<27:33,  3.53s/it] 44%|████▎     | 360/827 [23:18<29:52,  3.84s/it] 44%|████▎     | 361/827 [23:22<29:55,  3.85s/it] 44%|████▍     | 362/827 [23:24<26:28,  3.42s/it] 44%|████▍     | 363/827 [23:28<28:06,  3.63s/it] 44%|████▍     | 364/827 [23:32<28:14,  3.66s/it] 44%|████▍     | 365/827 [23:35<26:53,  3.49s/it] 44%|████▍     | 366/827 [23:40<31:15,  4.07s/it] 44%|████▍     | 367/827 [23:45<32:01,  4.18s/it] 44%|████▍     | 368/827 [23:47<28:20,  3.70s/it] 45%|████▍     | 369/827 [23:51<27:47,  3.64s/it] 45%|████▍     | 370/827 [23:54<27:07,  3.56s/it] 45%|████▍     | 371/827 [23:59<28:41,  3.77s/it] 45%|████▍     | 372/827 [24:02<28:23,  3.74s/it] 45%|████▌     | 373/827 [24:07<29:46,  3.93s/it] 45%|████▌     | 374/827 [24:13<35:38,  4.72s/it] 45%|████▌     | 375/827 [24:21<43:28,  5.77s/it] 45%|████▌     | 376/827 [24:28<46:20,  6.17s/it] 46%|████▌     | 377/827 [24:33<42:39,  5.69s/it] 46%|████▌     | 378/827 [24:39<42:57,  5.74s/it] 46%|████▌     | 379/827 [24:48<49:43,  6.66s/it] 46%|████▌     | 380/827 [24:53<47:42,  6.40s/it] 46%|████▌     | 381/827 [24:57<41:21,  5.56s/it] 46%|████▌     | 382/827 [25:02<38:43,  5.22s/it] 46%|████▋     | 383/827 [25:06<36:11,  4.89s/it] 46%|████▋     | 384/827 [25:09<33:09,  4.49s/it] 47%|████▋     | 385/827 [25:12<30:07,  4.09s/it] 47%|████▋     | 386/827 [25:15<27:58,  3.81s/it] 47%|████▋     | 387/827 [25:19<27:35,  3.76s/it] 47%|████▋     | 388/827 [25:23<28:19,  3.87s/it] 47%|████▋     | 389/827 [25:27<27:45,  3.80s/it] 47%|████▋     | 390/827 [25:30<26:51,  3.69s/it] 47%|████▋     | 391/827 [25:34<25:52,  3.56s/it] 47%|████▋     | 392/827 [25:37<25:29,  3.52s/it] 48%|████▊     | 393/827 [25:41<26:53,  3.72s/it] 48%|████▊     | 394/827 [25:46<29:46,  4.13s/it] 48%|████▊     | 395/827 [25:51<30:29,  4.24s/it] 48%|████▊     | 396/827 [25:55<31:03,  4.32s/it] 48%|████▊     | 397/827 [25:58<27:52,  3.89s/it] 48%|████▊     | 398/827 [26:01<26:22,  3.69s/it] 48%|████▊     | 399/827 [26:05<25:26,  3.57s/it] 48%|████▊     | 400/827 [26:09<26:44,  3.76s/it] 48%|████▊     | 401/827 [26:14<29:05,  4.10s/it] 49%|████▊     | 402/827 [26:18<28:51,  4.07s/it] 49%|████▊     | 403/827 [26:23<30:10,  4.27s/it] 49%|████▉     | 404/827 [26:25<26:30,  3.76s/it] 49%|████▉     | 405/827 [26:28<25:00,  3.55s/it] 49%|████▉     | 406/827 [26:31<24:15,  3.46s/it] 49%|████▉     | 407/827 [26:34<22:42,  3.24s/it] 49%|████▉     | 408/827 [26:37<21:56,  3.14s/it] 49%|████▉     | 409/827 [26:40<21:36,  3.10s/it] 50%|████▉     | 410/827 [26:42<20:12,  2.91s/it] 50%|████▉     | 411/827 [26:45<20:18,  2.93s/it] 50%|████▉     | 412/827 [26:48<20:24,  2.95s/it] 50%|████▉     | 413/827 [26:53<23:06,  3.35s/it] 50%|█████     | 414/827 [26:58<26:20,  3.83s/it] 50%|█████     | 415/827 [27:00<24:03,  3.50s/it] 50%|█████     | 416/827 [27:05<25:18,  3.69s/it] 50%|█████     | 417/827 [27:09<27:27,  4.02s/it] 51%|█████     | 418/827 [27:15<30:22,  4.46s/it] 51%|█████     | 419/827 [27:19<29:09,  4.29s/it] 51%|█████     | 420/827 [27:23<28:44,  4.24s/it] 51%|█████     | 421/827 [27:26<27:24,  4.05s/it] 51%|█████     | 422/827 [27:29<25:15,  3.74s/it] 51%|█████     | 423/827 [27:32<23:40,  3.52s/it] 51%|█████▏    | 424/827 [27:36<24:17,  3.62s/it] 51%|█████▏    | 425/827 [27:41<25:49,  3.85s/it] 52%|█████▏    | 426/827 [27:46<27:45,  4.15s/it] 52%|█████▏    | 427/827 [27:50<28:41,  4.30s/it] 52%|█████▏    | 428/827 [27:55<29:25,  4.43s/it] 52%|█████▏    | 429/827 [27:59<27:42,  4.18s/it] 52%|█████▏    | 430/827 [28:02<26:53,  4.06s/it] 52%|█████▏    | 431/827 [28:06<26:40,  4.04s/it] 52%|█████▏    | 432/827 [28:11<28:41,  4.36s/it] 52%|█████▏    | 433/827 [28:18<32:26,  4.94s/it] 52%|█████▏    | 434/827 [28:23<33:09,  5.06s/it] 53%|█████▎    | 435/827 [28:26<28:55,  4.43s/it] 53%|█████▎    | 436/827 [28:30<28:03,  4.30s/it] 53%|█████▎    | 437/827 [28:34<28:06,  4.33s/it] 53%|█████▎    | 438/827 [28:38<26:06,  4.03s/it] 53%|█████▎    | 439/827 [28:41<25:05,  3.88s/it] 53%|█████▎    | 440/827 [28:45<25:00,  3.88s/it] 53%|█████▎    | 441/827 [28:48<22:54,  3.56s/it] 53%|█████▎    | 442/827 [28:51<21:26,  3.34s/it] 54%|█████▎    | 443/827 [28:54<20:46,  3.25s/it] 54%|█████▎    | 444/827 [28:58<21:36,  3.39s/it] 54%|█████▍    | 445/827 [29:03<25:15,  3.97s/it] 54%|█████▍    | 446/827 [29:06<23:38,  3.72s/it] 54%|█████▍    | 447/827 [29:10<24:53,  3.93s/it] 54%|█████▍    | 448/827 [29:15<26:34,  4.21s/it] 54%|█████▍    | 449/827 [29:23<32:28,  5.15s/it] 54%|█████▍    | 450/827 [29:28<32:32,  5.18s/it] 55%|█████▍    | 451/827 [29:31<29:24,  4.69s/it] 55%|█████▍    | 452/827 [29:36<29:57,  4.79s/it] 55%|█████▍    | 453/827 [29:42<31:01,  4.98s/it] 55%|█████▍    | 454/827 [29:45<27:53,  4.49s/it] 55%|█████▌    | 455/827 [29:49<26:44,  4.31s/it] 55%|█████▌    | 456/827 [29:55<29:02,  4.70s/it] 55%|█████▌    | 457/827 [29:59<28:17,  4.59s/it] 55%|█████▌    | 458/827 [30:03<26:41,  4.34s/it] 56%|█████▌    | 459/827 [30:07<27:01,  4.41s/it] 56%|█████▌    | 460/827 [30:13<28:49,  4.71s/it] 56%|█████▌    | 461/827 [30:20<33:16,  5.46s/it] 56%|█████▌    | 462/827 [30:24<31:18,  5.15s/it] 56%|█████▌    | 463/827 [30:27<27:18,  4.50s/it] 56%|█████▌    | 464/827 [30:32<27:21,  4.52s/it] 56%|█████▌    | 465/827 [30:36<26:43,  4.43s/it] 56%|█████▋    | 466/827 [30:40<24:45,  4.11s/it] 56%|█████▋    | 467/827 [30:43<24:14,  4.04s/it] 57%|█████▋    | 468/827 [30:48<24:31,  4.10s/it] 57%|█████▋    | 469/827 [30:52<25:39,  4.30s/it] 57%|█████▋    | 470/827 [30:56<23:22,  3.93s/it] 57%|█████▋    | 471/827 [31:00<23:33,  3.97s/it] 57%|█████▋    | 472/827 [31:06<27:37,  4.67s/it] 57%|█████▋    | 473/827 [31:10<26:52,  4.55s/it] 57%|█████▋    | 474/827 [31:14<25:38,  4.36s/it] 57%|█████▋    | 475/827 [31:17<23:01,  3.92s/it] 58%|█████▊    | 476/827 [31:19<20:12,  3.45s/it] 58%|█████▊    | 477/827 [31:22<18:16,  3.13s/it] 58%|█████▊    | 478/827 [31:25<19:13,  3.30s/it] 58%|█████▊    | 479/827 [31:30<21:52,  3.77s/it] 58%|█████▊    | 480/827 [31:35<22:39,  3.92s/it] 58%|█████▊    | 481/827 [31:39<23:54,  4.15s/it] 58%|█████▊    | 482/827 [31:42<21:37,  3.76s/it] 58%|█████▊    | 483/827 [31:44<18:54,  3.30s/it] 59%|█████▊    | 484/827 [31:47<18:21,  3.21s/it] 59%|█████▊    | 485/827 [31:50<17:35,  3.09s/it] 59%|█████▉    | 486/827 [31:53<17:50,  3.14s/it] 59%|█████▉    | 487/827 [31:57<18:17,  3.23s/it] 59%|█████▉    | 488/827 [32:02<20:49,  3.69s/it] 59%|█████▉    | 489/827 [32:06<22:14,  3.95s/it] 59%|█████▉    | 490/827 [32:11<23:01,  4.10s/it] 59%|█████▉    | 491/827 [32:14<21:23,  3.82s/it] 59%|█████▉    | 492/827 [32:17<20:42,  3.71s/it] 60%|█████▉    | 493/827 [32:21<20:06,  3.61s/it] 60%|█████▉    | 494/827 [32:24<19:47,  3.57s/it] 60%|█████▉    | 495/827 [32:28<19:43,  3.56s/it] 60%|█████▉    | 496/827 [32:31<19:57,  3.62s/it] 60%|██████    | 497/827 [32:34<18:19,  3.33s/it] 60%|██████    | 498/827 [32:37<17:53,  3.26s/it] 60%|██████    | 499/827 [32:41<19:37,  3.59s/it] 60%|██████    | 500/827 [32:45<18:49,  3.45s/it] 61%|██████    | 501/827 [32:47<17:36,  3.24s/it] 61%|██████    | 502/827 [32:52<19:35,  3.62s/it] 61%|██████    | 503/827 [32:56<21:11,  3.92s/it] 61%|██████    | 504/827 [33:00<20:13,  3.76s/it] 61%|██████    | 505/827 [33:03<19:58,  3.72s/it] 61%|██████    | 506/827 [33:07<19:43,  3.69s/it] 61%|██████▏   | 507/827 [33:12<21:31,  4.03s/it] 61%|██████▏   | 508/827 [33:18<24:56,  4.69s/it] 62%|██████▏   | 509/827 [33:21<21:32,  4.07s/it] 62%|██████▏   | 510/827 [33:24<20:43,  3.92s/it] 62%|██████▏   | 511/827 [33:27<19:16,  3.66s/it] 62%|██████▏   | 512/827 [33:30<17:50,  3.40s/it] 62%|██████▏   | 513/827 [33:34<18:02,  3.45s/it] 62%|██████▏   | 514/827 [33:37<18:03,  3.46s/it] 62%|██████▏   | 515/827 [33:40<17:36,  3.39s/it] 62%|██████▏   | 516/827 [33:44<18:13,  3.52s/it] 63%|██████▎   | 517/827 [33:48<17:42,  3.43s/it] 63%|██████▎   | 518/827 [33:50<16:38,  3.23s/it] 63%|██████▎   | 519/827 [33:53<16:13,  3.16s/it] 63%|██████▎   | 520/827 [33:56<16:04,  3.14s/it] 63%|██████▎   | 521/827 [34:00<17:23,  3.41s/it] 63%|██████▎   | 522/827 [34:06<20:03,  3.94s/it] 63%|██████▎   | 523/827 [34:08<18:18,  3.61s/it] 63%|██████▎   | 524/827 [34:12<18:36,  3.69s/it] 63%|██████▎   | 525/827 [34:18<21:22,  4.25s/it] 64%|██████▎   | 526/827 [34:23<23:20,  4.65s/it] 64%|██████▎   | 527/827 [34:27<22:15,  4.45s/it] 64%|██████▍   | 528/827 [34:32<22:21,  4.49s/it] 64%|██████▍   | 529/827 [34:37<22:38,  4.56s/it] 64%|██████▍   | 530/827 [34:41<22:24,  4.53s/it] 64%|██████▍   | 531/827 [34:44<19:59,  4.05s/it] 64%|██████▍   | 532/827 [34:48<18:59,  3.86s/it] 64%|██████▍   | 533/827 [34:52<20:31,  4.19s/it] 65%|██████▍   | 534/827 [34:58<21:54,  4.49s/it] 65%|██████▍   | 535/827 [35:03<22:33,  4.64s/it] 65%|██████▍   | 536/827 [35:06<20:48,  4.29s/it] 65%|██████▍   | 537/827 [35:09<18:01,  3.73s/it] 65%|██████▌   | 538/827 [35:12<16:54,  3.51s/it] 65%|██████▌   | 539/827 [35:15<16:04,  3.35s/it] 65%|██████▌   | 540/827 [35:18<16:16,  3.40s/it] 65%|██████▌   | 541/827 [35:22<16:45,  3.52s/it] 66%|██████▌   | 542/827 [35:25<16:53,  3.56s/it] 66%|██████▌   | 543/827 [35:30<17:59,  3.80s/it] 66%|██████▌   | 544/827 [35:35<19:10,  4.06s/it] 66%|██████▌   | 545/827 [35:39<19:29,  4.15s/it] 66%|██████▌   | 546/827 [35:43<19:25,  4.15s/it] 66%|██████▌   | 547/827 [35:47<19:24,  4.16s/it] 66%|██████▋   | 548/827 [35:50<17:19,  3.73s/it] 66%|██████▋   | 549/827 [35:53<15:56,  3.44s/it] 67%|██████▋   | 550/827 [35:56<15:46,  3.42s/it] 67%|██████▋   | 551/827 [36:00<16:30,  3.59s/it] 67%|██████▋   | 552/827 [36:04<16:22,  3.57s/it] 67%|██████▋   | 553/827 [36:07<16:09,  3.54s/it] 67%|██████▋   | 554/827 [36:10<14:39,  3.22s/it] 67%|██████▋   | 555/827 [36:13<14:17,  3.15s/it] 67%|██████▋   | 556/827 [36:16<14:15,  3.16s/it] 67%|██████▋   | 557/827 [36:18<13:29,  3.00s/it] 67%|██████▋   | 558/827 [36:22<14:31,  3.24s/it] 68%|██████▊   | 559/827 [36:26<15:12,  3.40s/it] 68%|██████▊   | 560/827 [36:29<14:48,  3.33s/it] 68%|██████▊   | 561/827 [36:32<14:39,  3.30s/it] 68%|██████▊   | 562/827 [36:36<15:02,  3.41s/it] 68%|██████▊   | 563/827 [36:39<14:53,  3.39s/it] 68%|██████▊   | 564/827 [36:43<14:44,  3.36s/it] 68%|██████▊   | 565/827 [36:47<16:02,  3.67s/it] 68%|██████▊   | 566/827 [36:51<17:02,  3.92s/it] 69%|██████▊   | 567/827 [36:54<15:38,  3.61s/it] 69%|██████▊   | 568/827 [36:58<15:32,  3.60s/it] 69%|██████▉   | 569/827 [37:01<14:51,  3.46s/it] 69%|██████▉   | 570/827 [37:05<14:55,  3.48s/it] 69%|██████▉   | 571/827 [37:10<16:50,  3.95s/it] 69%|██████▉   | 572/827 [37:15<18:15,  4.30s/it] 69%|██████▉   | 573/827 [37:20<19:21,  4.57s/it] 69%|██████▉   | 574/827 [37:24<18:25,  4.37s/it] 70%|██████▉   | 575/827 [37:29<19:22,  4.61s/it] 70%|██████▉   | 576/827 [37:33<18:23,  4.40s/it] 70%|██████▉   | 577/827 [37:36<16:47,  4.03s/it] 70%|██████▉   | 578/827 [37:40<16:20,  3.94s/it] 70%|███████   | 579/827 [37:43<15:20,  3.71s/it] 70%|███████   | 580/827 [37:48<16:21,  3.98s/it] 70%|███████   | 581/827 [37:54<19:36,  4.78s/it] 70%|███████   | 582/827 [37:59<18:53,  4.63s/it] 70%|███████   | 583/827 [38:02<17:00,  4.18s/it] 71%|███████   | 584/827 [38:05<15:35,  3.85s/it] 71%|███████   | 585/827 [38:08<14:22,  3.56s/it] 71%|███████   | 586/827 [38:12<14:41,  3.66s/it] 71%|███████   | 587/827 [38:18<18:32,  4.63s/it] 71%|███████   | 588/827 [38:25<21:19,  5.35s/it] 71%|███████   | 589/827 [38:30<20:18,  5.12s/it] 71%|███████▏  | 590/827 [38:36<21:09,  5.36s/it] 71%|███████▏  | 591/827 [38:43<22:38,  5.76s/it] 72%|███████▏  | 592/827 [38:46<19:37,  5.01s/it] 72%|███████▏  | 593/827 [38:50<18:02,  4.63s/it] 72%|███████▏  | 594/827 [38:53<16:04,  4.14s/it] 72%|███████▏  | 595/827 [38:56<14:55,  3.86s/it] 72%|███████▏  | 596/827 [39:03<18:04,  4.70s/it] 72%|███████▏  | 597/827 [39:09<19:31,  5.09s/it] 72%|███████▏  | 598/827 [39:12<17:01,  4.46s/it] 72%|███████▏  | 599/827 [39:15<15:32,  4.09s/it] 73%|███████▎  | 600/827 [39:19<15:33,  4.11s/it] 73%|███████▎  | 601/827 [39:23<15:12,  4.04s/it] 73%|███████▎  | 602/827 [39:27<14:53,  3.97s/it] 73%|███████▎  | 603/827 [39:30<13:47,  3.70s/it] 73%|███████▎  | 604/827 [39:33<13:48,  3.72s/it] 73%|███████▎  | 605/827 [39:37<13:15,  3.58s/it] 73%|███████▎  | 606/827 [39:40<13:00,  3.53s/it] 73%|███████▎  | 607/827 [39:44<13:25,  3.66s/it] 74%|███████▎  | 608/827 [39:47<12:53,  3.53s/it] 74%|███████▎  | 609/827 [39:52<14:38,  4.03s/it] 74%|███████▍  | 610/827 [39:59<17:03,  4.72s/it] 74%|███████▍  | 611/827 [40:02<15:42,  4.36s/it] 74%|███████▍  | 612/827 [40:06<15:15,  4.26s/it] 74%|███████▍  | 613/827 [40:11<15:19,  4.30s/it] 74%|███████▍  | 614/827 [40:15<14:57,  4.21s/it] 74%|███████▍  | 615/827 [40:18<14:08,  4.00s/it] 74%|███████▍  | 616/827 [40:22<13:21,  3.80s/it] 75%|███████▍  | 617/827 [40:26<14:12,  4.06s/it] 75%|███████▍  | 618/827 [40:33<16:39,  4.78s/it] 75%|███████▍  | 619/827 [40:36<14:33,  4.20s/it] 75%|███████▍  | 620/827 [40:39<13:53,  4.03s/it] 75%|███████▌  | 621/827 [40:42<12:28,  3.63s/it] 75%|███████▌  | 622/827 [40:45<11:38,  3.40s/it] 75%|███████▌  | 623/827 [40:48<11:23,  3.35s/it] 75%|███████▌  | 624/827 [40:53<12:36,  3.73s/it] 76%|███████▌  | 625/827 [40:56<12:11,  3.62s/it] 76%|███████▌  | 626/827 [40:59<11:52,  3.54s/it] 76%|███████▌  | 627/827 [41:03<11:40,  3.50s/it] 76%|███████▌  | 628/827 [41:06<10:57,  3.30s/it] 76%|███████▌  | 629/827 [41:09<10:37,  3.22s/it] 76%|███████▌  | 630/827 [41:12<10:22,  3.16s/it] 76%|███████▋  | 631/827 [41:17<12:06,  3.71s/it] 76%|███████▋  | 632/827 [41:22<13:16,  4.09s/it] 77%|███████▋  | 633/827 [41:26<13:57,  4.32s/it] 77%|███████▋  | 634/827 [41:33<15:55,  4.95s/it] 77%|███████▋  | 635/827 [41:36<13:53,  4.34s/it] 77%|███████▋  | 636/827 [41:40<13:20,  4.19s/it] 77%|███████▋  | 637/827 [41:44<13:41,  4.32s/it] 77%|███████▋  | 638/827 [41:49<13:49,  4.39s/it] 77%|███████▋  | 639/827 [41:52<12:34,  4.02s/it] 77%|███████▋  | 640/827 [41:55<11:24,  3.66s/it] 78%|███████▊  | 641/827 [41:59<11:35,  3.74s/it] 78%|███████▊  | 642/827 [42:02<11:25,  3.71s/it] 78%|███████▊  | 643/827 [42:07<12:04,  3.94s/it] 78%|███████▊  | 644/827 [42:11<12:31,  4.10s/it] 78%|███████▊  | 645/827 [42:15<11:54,  3.93s/it] 78%|███████▊  | 646/827 [42:18<10:53,  3.61s/it] 78%|███████▊  | 647/827 [42:21<10:34,  3.52s/it] 78%|███████▊  | 648/827 [42:25<10:45,  3.61s/it] 78%|███████▊  | 649/827 [42:28<10:41,  3.60s/it] 79%|███████▊  | 650/827 [42:31<10:03,  3.41s/it] 79%|███████▊  | 651/827 [42:35<09:47,  3.34s/it] 79%|███████▉  | 652/827 [42:38<09:40,  3.32s/it] 79%|███████▉  | 653/827 [42:41<09:49,  3.39s/it] 79%|███████▉  | 654/827 [42:46<11:16,  3.91s/it] 79%|███████▉  | 655/827 [42:52<12:32,  4.38s/it] 79%|███████▉  | 656/827 [42:55<10:56,  3.84s/it] 79%|███████▉  | 657/827 [42:57<10:04,  3.56s/it] 80%|███████▉  | 658/827 [43:02<10:38,  3.78s/it] 80%|███████▉  | 659/827 [43:09<13:46,  4.92s/it] 80%|███████▉  | 660/827 [43:14<13:48,  4.96s/it] 80%|███████▉  | 661/827 [43:20<14:06,  5.10s/it] 80%|████████  | 662/827 [43:24<13:02,  4.74s/it] 80%|████████  | 663/827 [43:28<12:14,  4.48s/it] 80%|████████  | 664/827 [43:33<12:33,  4.62s/it] 80%|████████  | 665/827 [43:36<11:42,  4.34s/it] 81%|████████  | 666/827 [43:40<11:26,  4.27s/it] 81%|████████  | 667/827 [43:43<10:30,  3.94s/it] 81%|████████  | 668/827 [43:47<10:20,  3.90s/it] 81%|████████  | 669/827 [43:50<09:23,  3.56s/it] 81%|████████  | 670/827 [43:53<08:54,  3.41s/it] 81%|████████  | 671/827 [43:57<08:58,  3.45s/it] 81%|████████▏ | 672/827 [44:00<08:53,  3.44s/it] 81%|████████▏ | 673/827 [44:03<08:29,  3.31s/it] 81%|████████▏ | 674/827 [44:06<08:22,  3.29s/it] 82%|████████▏ | 675/827 [44:10<08:33,  3.38s/it] 82%|████████▏ | 676/827 [44:14<09:13,  3.67s/it] 82%|████████▏ | 677/827 [44:19<10:10,  4.07s/it] 82%|████████▏ | 678/827 [44:24<10:28,  4.22s/it] 82%|████████▏ | 679/827 [44:28<10:37,  4.31s/it] 82%|████████▏ | 680/827 [44:31<09:29,  3.87s/it] 82%|████████▏ | 681/827 [44:36<10:00,  4.11s/it] 82%|████████▏ | 682/827 [44:41<10:34,  4.37s/it] 83%|████████▎ | 683/827 [44:44<09:26,  3.93s/it] 83%|████████▎ | 684/827 [44:47<08:39,  3.63s/it] 83%|████████▎ | 685/827 [44:49<07:52,  3.33s/it] 83%|████████▎ | 686/827 [44:55<09:15,  3.94s/it] 83%|████████▎ | 687/827 [45:02<11:27,  4.91s/it] 83%|████████▎ | 688/827 [45:07<11:23,  4.92s/it] 83%|████████▎ | 689/827 [45:12<11:13,  4.88s/it] 83%|████████▎ | 690/827 [45:15<09:55,  4.35s/it] 84%|████████▎ | 691/827 [45:18<08:59,  3.97s/it] 84%|████████▎ | 692/827 [45:22<08:53,  3.95s/it] 84%|████████▍ | 693/827 [45:25<08:20,  3.73s/it] 84%|████████▍ | 694/827 [45:28<07:52,  3.55s/it] 84%|████████▍ | 695/827 [45:32<08:07,  3.69s/it] 84%|████████▍ | 696/827 [45:37<08:42,  3.99s/it] 84%|████████▍ | 697/827 [45:40<08:15,  3.81s/it] 84%|████████▍ | 698/827 [45:43<07:23,  3.44s/it] 85%|████████▍ | 699/827 [45:46<06:57,  3.26s/it] 85%|████████▍ | 700/827 [45:52<08:44,  4.13s/it] 85%|████████▍ | 701/827 [45:58<10:02,  4.78s/it] 85%|████████▍ | 702/827 [46:02<09:44,  4.67s/it] 85%|████████▌ | 703/827 [46:06<09:11,  4.45s/it] 85%|████████▌ | 704/827 [46:09<08:15,  4.03s/it] 85%|████████▌ | 705/827 [46:12<07:21,  3.62s/it] 85%|████████▌ | 706/827 [46:15<06:44,  3.34s/it] 85%|████████▌ | 707/827 [46:19<06:59,  3.50s/it] 86%|████████▌ | 708/827 [46:22<06:47,  3.43s/it] 86%|████████▌ | 709/827 [46:26<06:56,  3.53s/it] 86%|████████▌ | 710/827 [46:29<06:53,  3.54s/it] 86%|████████▌ | 711/827 [46:33<06:52,  3.56s/it] 86%|████████▌ | 712/827 [46:36<06:29,  3.39s/it] 86%|████████▌ | 713/827 [46:39<06:21,  3.35s/it] 86%|████████▋ | 714/827 [46:43<06:32,  3.47s/it] 86%|████████▋ | 715/827 [46:47<06:57,  3.73s/it] 87%|████████▋ | 716/827 [46:51<06:46,  3.66s/it] 87%|████████▋ | 717/827 [46:56<07:25,  4.05s/it] 87%|████████▋ | 718/827 [47:01<07:50,  4.32s/it] 87%|████████▋ | 719/827 [47:04<07:09,  3.98s/it] 87%|████████▋ | 720/827 [47:07<06:49,  3.82s/it] 87%|████████▋ | 721/827 [47:11<06:29,  3.68s/it] 87%|████████▋ | 722/827 [47:14<06:24,  3.67s/it] 87%|████████▋ | 723/827 [47:17<06:10,  3.56s/it] 88%|████████▊ | 724/827 [47:20<05:36,  3.27s/it] 88%|████████▊ | 725/827 [47:25<06:10,  3.64s/it] 88%|████████▊ | 726/827 [47:30<06:59,  4.15s/it] 88%|████████▊ | 727/827 [47:34<06:49,  4.10s/it] 88%|████████▊ | 728/827 [47:37<06:14,  3.79s/it] 88%|████████▊ | 729/827 [47:39<05:33,  3.41s/it] 88%|████████▊ | 730/827 [47:43<05:38,  3.49s/it] 88%|████████▊ | 731/827 [47:48<06:08,  3.84s/it] 89%|████████▊ | 732/827 [47:52<06:20,  4.00s/it] 89%|████████▊ | 733/827 [47:56<06:23,  4.08s/it] 89%|████████▉ | 734/827 [47:59<05:35,  3.61s/it] 89%|████████▉ | 735/827 [48:03<05:36,  3.66s/it] 89%|████████▉ | 736/827 [48:08<06:17,  4.14s/it] 89%|████████▉ | 737/827 [48:13<06:30,  4.34s/it] 89%|████████▉ | 738/827 [48:16<05:51,  3.94s/it] 89%|████████▉ | 739/827 [48:21<06:08,  4.18s/it] 89%|████████▉ | 740/827 [48:26<06:39,  4.59s/it] 90%|████████▉ | 741/827 [48:30<06:11,  4.32s/it] 90%|████████▉ | 742/827 [48:34<05:51,  4.14s/it] 90%|████████▉ | 743/827 [48:39<06:16,  4.48s/it] 90%|████████▉ | 744/827 [48:43<05:54,  4.27s/it] 90%|█████████ | 745/827 [48:46<05:40,  4.16s/it] 90%|█████████ | 746/827 [48:52<05:59,  4.44s/it] 90%|█████████ | 747/827 [48:57<06:12,  4.66s/it] 90%|█████████ | 748/827 [49:00<05:32,  4.21s/it] 91%|█████████ | 749/827 [49:04<05:26,  4.19s/it] 91%|█████████ | 750/827 [49:08<05:25,  4.22s/it] 91%|█████████ | 751/827 [49:11<04:54,  3.88s/it] 91%|█████████ | 752/827 [49:14<04:29,  3.60s/it] 91%|█████████ | 753/827 [49:17<04:09,  3.37s/it] 91%|█████████ | 754/827 [49:20<03:55,  3.22s/it] 91%|█████████▏| 755/827 [49:24<04:12,  3.51s/it] 91%|█████████▏| 756/827 [49:29<04:38,  3.93s/it] 92%|█████████▏| 757/827 [49:33<04:34,  3.92s/it] 92%|█████████▏| 758/827 [49:37<04:36,  4.01s/it] 92%|█████████▏| 759/827 [49:40<04:09,  3.67s/it] 92%|█████████▏| 760/827 [49:45<04:38,  4.16s/it] 92%|█████████▏| 761/827 [49:52<05:25,  4.94s/it] 92%|█████████▏| 762/827 [49:58<05:35,  5.16s/it] 92%|█████████▏| 763/827 [50:04<05:44,  5.38s/it] 92%|█████████▏| 764/827 [50:07<04:58,  4.75s/it] 93%|█████████▎| 765/827 [50:10<04:17,  4.15s/it] 93%|█████████▎| 766/827 [50:13<03:48,  3.75s/it] 93%|█████████▎| 767/827 [50:16<03:39,  3.66s/it] 93%|█████████▎| 768/827 [50:20<03:36,  3.67s/it] 93%|█████████▎| 769/827 [50:24<03:48,  3.95s/it] 93%|█████████▎| 770/827 [50:28<03:34,  3.77s/it] 93%|█████████▎| 771/827 [50:31<03:30,  3.76s/it] 93%|█████████▎| 772/827 [50:35<03:23,  3.70s/it] 93%|█████████▎| 773/827 [50:39<03:24,  3.78s/it] 94%|█████████▎| 774/827 [50:42<03:09,  3.58s/it] 94%|█████████▎| 775/827 [50:46<03:09,  3.65s/it] 94%|█████████▍| 776/827 [50:49<03:01,  3.56s/it] 94%|█████████▍| 777/827 [50:52<02:52,  3.45s/it] 94%|█████████▍| 778/827 [50:56<02:48,  3.44s/it] 94%|█████████▍| 779/827 [50:59<02:37,  3.28s/it] 94%|█████████▍| 780/827 [51:02<02:30,  3.20s/it] 94%|█████████▍| 781/827 [51:05<02:23,  3.13s/it] 95%|█████████▍| 782/827 [51:07<02:15,  3.02s/it] 95%|█████████▍| 783/827 [51:11<02:17,  3.13s/it] 95%|█████████▍| 784/827 [51:14<02:15,  3.16s/it] 95%|█████████▍| 785/827 [51:18<02:18,  3.30s/it] 95%|█████████▌| 786/827 [51:22<02:23,  3.50s/it] 95%|█████████▌| 787/827 [51:27<02:41,  4.03s/it] 95%|█████████▌| 788/827 [51:31<02:35,  3.98s/it] 95%|█████████▌| 789/827 [51:35<02:34,  4.07s/it] 96%|█████████▌| 790/827 [51:39<02:25,  3.93s/it] 96%|█████████▌| 791/827 [51:42<02:10,  3.64s/it] 96%|█████████▌| 792/827 [51:45<02:00,  3.44s/it] 96%|█████████▌| 793/827 [51:49<02:06,  3.71s/it] 96%|█████████▌| 794/827 [51:54<02:17,  4.17s/it] 96%|█████████▌| 795/827 [52:00<02:28,  4.65s/it] 96%|█████████▋| 796/827 [52:05<02:26,  4.73s/it] 96%|█████████▋| 797/827 [52:08<02:05,  4.18s/it] 96%|█████████▋| 798/827 [52:11<01:49,  3.76s/it] 97%|█████████▋| 799/827 [52:13<01:33,  3.36s/it] 97%|█████████▋| 800/827 [52:19<01:53,  4.21s/it] 97%|█████████▋| 801/827 [52:26<02:11,  5.05s/it] 97%|█████████▋| 802/827 [52:31<02:01,  4.88s/it] 97%|█████████▋| 803/827 [52:36<01:59,  4.97s/it] 97%|█████████▋| 804/827 [52:41<01:53,  4.95s/it] 97%|█████████▋| 805/827 [52:44<01:36,  4.38s/it] 97%|█████████▋| 806/827 [52:46<01:21,  3.87s/it] 98%|█████████▊| 807/827 [52:50<01:13,  3.67s/it] 98%|█████████▊| 808/827 [52:53<01:09,  3.64s/it] 98%|█████████▊| 809/827 [52:56<01:02,  3.50s/it] 98%|█████████▊| 810/827 [53:00<00:58,  3.46s/it] 98%|█████████▊| 811/827 [53:03<00:55,  3.45s/it] 98%|█████████▊| 812/827 [53:07<00:53,  3.56s/it] 98%|█████████▊| 813/827 [53:12<00:55,  3.97s/it] 98%|█████████▊| 814/827 [53:17<00:57,  4.39s/it] 99%|█████████▊| 815/827 [53:21<00:50,  4.20s/it] 99%|█████████▊| 816/827 [53:25<00:45,  4.09s/it] 99%|█████████▉| 817/827 [53:28<00:37,  3.75s/it] 99%|█████████▉| 818/827 [53:33<00:37,  4.16s/it] 99%|█████████▉| 819/827 [53:38<00:34,  4.37s/it] 99%|█████████▉| 820/827 [53:42<00:31,  4.43s/it] 99%|█████████▉| 821/827 [53:46<00:25,  4.21s/it] 99%|█████████▉| 822/827 [53:50<00:20,  4.03s/it]100%|█████████▉| 823/827 [53:52<00:14,  3.62s/it]100%|█████████▉| 824/827 [53:55<00:10,  3.39s/it]100%|█████████▉| 825/827 [54:00<00:07,  3.74s/it]100%|█████████▉| 826/827 [54:05<00:04,  4.32s/it]100%|██████████| 827/827 [54:08<00:00,  3.75s/it]100%|██████████| 827/827 [54:08<00:00,  3.93s/it]
***** eval metrics *****
  epoch                   =        1.0
  eval_loss               =     0.7701
  eval_runtime            = 0:54:13.11
  eval_samples_per_second =      1.016
  eval_steps_per_second   =      0.254
[INFO|modelcard.py:456] 2026-01-03 10:04:07,270 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /scratch/indrisch/LLaMA-Factory/wandb/wandb/offline-run-20260102_184517-63x2u7jd[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/wandb/offline-run-20260102_184517-63x2u7jd/logs[0m

scontrol show job 177487
JobId=177487 JobName=slurm_qwen2_5vl_lora_sft_SQA3Devery24_R0C1F0X0_traineval.sh
   UserId=indrisch(3122343) GroupId=indrisch(3122343) MCS_label=N/A
   Priority=109074 Nice=0 Account=def-wangcs QOS=normal
   JobState=COMPLETING Reason=None Dependency=(null)
   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0
   RunTime=15:38:41 TimeLimit=17:00:00 TimeMin=N/A
   SubmitTime=2025-12-31T22:53:39 EligibleTime=2025-12-31T22:53:39
   AccrueTime=2025-12-31T22:53:39
   StartTime=2026-01-02T18:25:32 EndTime=2026-01-03T10:04:13 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2026-01-02T18:25:32 Scheduler=Backfill
   Partition=compute_full_node AllocNode:Sid=trig-login01:3155452
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=trig0061
   BatchHost=trig0061
   NumNodes=1 NumCPUs=96 NumTasks=1 CPUs/Task=96 ReqB:S:C:T=0:0:*:*
   ReqTRES=cpu=96,mem=770000M,node=1,billing=4,gres/gpu=4
   AllocTRES=cpu=96,mem=770000M,node=1,billing=4,gres/gpu=4
   Socks/Node=* NtasksPerN:B:S:C=1:0:*:* CoreSpec=*
   MinCPUsNode=96 MinMemoryNode=770000M MinTmpDiskNode=0
   Features=(null) DelayBoot=00:00:00
   OverSubscribe=NO Contiguous=0 Licenses=(null) Network=(null)
   Command=./slurm_qwen2_5vl_lora_sft_SQA3Devery24_R0C1F0X0_traineval.sh
   WorkDir=/scratch/indrisch/LLaMA-Factory/models/qwen2_5vl_lora_sft_SQA3D
   Comment=/opt/slurm/bin/sbatch --export=NONE --get-user-env=L ./slurm_qwen2_5vl_lora_sft_SQA3Devery24_R0C1F0X0_traineval.sh 
   StdErr=/scratch/indrisch/LLaMA-Factory/models/qwen2_5vl_lora_sft_SQA3D/out/R0C1F0X0/%N-qwen2_5vl_lora_sft_SQA3Devery24_R0C1F0X0_traineval-177487.out
   StdIn=/dev/null
   StdOut=/scratch/indrisch/LLaMA-Factory/models/qwen2_5vl_lora_sft_SQA3D/out/R0C1F0X0/%N-qwen2_5vl_lora_sft_SQA3Devery24_R0C1F0X0_traineval-177487.out
   TresPerNode=gres/gpu:h100:4
   TresPerTask=cpu=96
   

sacct -j 177487
JobID           JobName    Account    Elapsed  MaxVMSize     MaxRSS  SystemCPU    UserCPU ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- ---------- ---------- -------- 
177487       slurm_qwe+ def-wangcs   15:38:41                         00:00:00   00:00:00      0:0 
177487.batch      batch def-wangcs   15:38:41                         00:00:00   00:00:00      0:0 
177487.exte+     extern def-wangcs   15:38:41                         00:00:00   00:00:00      0:0 

