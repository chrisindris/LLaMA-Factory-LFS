
==========
== CUDA ==
==========

CUDA Version 12.4.1

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

[INFO|2025-12-05 04:51:12] llamafactory.launcher:143 >> Initializing 8 distributed tasks at: 127.0.0.1:51713
W1205 04:51:13.784000 1055587 site-packages/torch/distributed/run.py:792] 
W1205 04:51:13.784000 1055587 site-packages/torch/distributed/run.py:792] *****************************************
W1205 04:51:13.784000 1055587 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1205 04:51:13.784000 1055587 site-packages/torch/distributed/run.py:792] *****************************************
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode

⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
[2025-12-05 04:51:28,145] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-05 04:51:28,145] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-05 04:51:28,145] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-05 04:51:28,145] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-05 04:51:28,145] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-05 04:51:28,145] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-05 04:51:28,145] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-05 04:51:28,145] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2025-12-05 04:51:35,333] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-05 04:51:35,333] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-12-05 04:51:35,337] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-05 04:51:35,380] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-05 04:51:35,382] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-05 04:51:35,515] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-05 04:51:35,563] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-05 04:51:35,575] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-05 04:51:35,655] [INFO] [comm.py:669:init_distributed] cdb=None
[INFO|2025-12-05 04:51:37] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.
[INFO|2025-12-05 04:51:37] llamafactory.hparams.parser:423 >> Process rank: 0, world size: 8, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|hub.py:421] 2025-12-05 04:51:37,250 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2025-12-05 04:51:37,277 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2025-12-05 04:51:37,287 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-12-05 04:51:37,287 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-12-05 04:51:37,287 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-12-05 04:51:37,287 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-05 04:51:37,287 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-05 04:51:37,288 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-12-05 04:51:37,288 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-12-05 04:51:37,682 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|hub.py:421] 2025-12-05 04:51:37,683 >> Offline mode: forcing local_files_only=True
[INFO|hub.py:421] 2025-12-05 04:51:37,684 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:316] 2025-12-05 04:51:37,685 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2025-12-05 04:51:37,688 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:316] 2025-12-05 04:51:37,693 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2025-12-05 04:51:37,694 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:428] 2025-12-05 04:51:37,705 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:1949] 2025-12-05 04:51:37,705 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2025-12-05 04:51:37,709 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-12-05 04:51:37,709 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-12-05 04:51:37,709 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-12-05 04:51:37,709 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-05 04:51:37,709 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-05 04:51:37,709 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-12-05 04:51:37,709 >> loading file chat_template.jinja from cache at None
[INFO|2025-12-05 04:51:37] llamafactory.hparams.parser:423 >> Process rank: 7, world size: 8, device: cuda:7, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-12-05 04:51:37] llamafactory.hparams.parser:423 >> Process rank: 2, world size: 8, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-12-05 04:51:37] llamafactory.hparams.parser:423 >> Process rank: 3, world size: 8, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-12-05 04:51:37] llamafactory.hparams.parser:423 >> Process rank: 1, world size: 8, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-12-05 04:51:37] llamafactory.hparams.parser:423 >> Process rank: 4, world size: 8, device: cuda:4, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-12-05 04:51:37] llamafactory.hparams.parser:423 >> Process rank: 6, world size: 8, device: cuda:6, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-12-05 04:51:37] llamafactory.hparams.parser:423 >> Process rank: 5, world size: 8, device: cuda:5, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2364] 2025-12-05 04:51:37,911 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:660] 2025-12-05 04:51:37,914 >> Offline mode: forcing local_files_only=True
[INFO|video_processing_utils.py:726] 2025-12-05 04:51:37,917 >> loading configuration file video_preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|video_processing_utils.py:770] 2025-12-05 04:51:37,922 >> Video processor Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:928] 2025-12-05 04:51:37,922 >> Offline mode: forcing local_files_only=True
[INFO|processing_utils.py:1116] 2025-12-05 04:51:37,927 >> loading configuration file processor_config.json from cache at None
[INFO|processing_utils.py:1199] 2025-12-05 04:51:38,274 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2_5_VLProcessor"
}

[INFO|2025-12-05 04:51:38] llamafactory.data.loader:143 >> Loading dataset /scratch/indrisch/huggingface/hub/datasets--cvis-tmu--llamafactory-sqa3d-traces-multiimage-vqa/snapshots/ce5c54adc1608d1726730c9ff334e65b6dd70e46/data/...
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
[rank7]:[W1205 04:51:38.817787651 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 7]  using GPU 7 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W1205 04:51:38.833291250 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank4]:[W1205 04:51:38.871488694 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 4]  using GPU 4 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W1205 04:51:38.873619154 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W1205 04:51:38.878979823 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank6]:[W1205 04:51:38.936448922 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 6]  using GPU 6 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank5]:[W1205 04:51:39.970433686 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 5]  using GPU 5 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank0]:[W1205 04:51:39.240745230 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
training example:
input_ids:
[151644, 8948, 271, 262, 1446, 525, 264, 16585, 11129, 4142, 11528, 17847, 369, 27979, 647, 47522, 389, 264, 3175, 29519, 6109, 6839, 3941, 5248, 5335, 382, 262, 26149, 510, 262, 68252, 1283, 419, 1943, 11, 498, 686, 5258, 451, 6109, 5335, 438, 366, 1805, 29, 9492, 13, 18877, 1105, 438, 2155, 6194, 315, 279, 83490, 6109, 13, 28634, 311, 5335, 553, 220, 16, 5980, 1922, 304, 1973, 315, 11094, 25, 508, 16, 1125, 508, 17, 1125, 4593, 11, 508, 45, 29562, 262, 3596, 35304, 1718, 8575, 38439, 3298, 14017, 510, 262, 7288, 7854, 264, 12966, 220, 18, 35, 10502, 1614, 3941, 6194, 25, 3754, 6171, 3941, 5335, 11, 23583, 8674, 6249, 17040, 11, 323, 990, 5452, 14, 509, 8957, 14688, 21060, 369, 7990, 55916, 624, 262, 7288, 72077, 409, 849, 292, 3793, 26087, 983, 847, 1290, 97089, 17996, 4065, 14, 29898, 484, 32511, 56111, 7612, 1825, 35978, 28455, 448, 5091, 311, 279, 64171, 14, 8092, 13, 1416, 279, 58385, 374, 54761, 11, 1638, 311, 279, 1429, 38219, 8622, 1651, 323, 1977, 773, 624, 262, 7288, 5443, 1172, 9434, 5904, 26, 653, 537, 17023, 63133, 3565, 476, 17188, 389, 9250, 6540, 7797, 6770, 1633, 43898, 3793, 624, 262, 7288, 3197, 5248, 11178, 3000, 11, 8830, 553, 279, 1850, 2432, 311, 27979, 17133, 488, 4274, 1835, 3941, 6194, 13, 1416, 2058, 54761, 1283, 13295, 678, 5335, 11, 1584, 429, 9355, 382, 262, 30990, 320, 327, 32739, 1378, 10010, 11, 304, 419, 1973, 982, 262, 366, 26865, 397, 262, 14822, 14319, 29208, 32711, 304, 220, 18, 4142, 16, 15, 2805, 7354, 13, 356, 632, 5904, 448, 2168, 14937, 320, 68, 1302, 2572, 65750, 18, 5669, 3100, 7579, 18010, 1290, 315, 8718, 26, 508, 16, 5669, 1852, 1894, 11, 42396, 64212, 2823, 63594, 714, 4583, 320, 2640, 37294, 17, 20, 15, 11211, 7241, 5871, 568, 7036, 894, 17796, 8957, 14, 2969, 26745, 487, 323, 1246, 498, 19673, 432, 624, 262, 690, 26865, 397, 262, 366, 9217, 397, 262, 362, 3175, 63594, 4226, 1172, 11, 892, 1231, 387, 510, 262, 7288, 1416, 14016, 25, 3776, 37328, 4226, 11, 4504, 320, 68, 1302, 2572, 1036, 17, 32511, 476, 7414, 14, 2753, 320, 68, 1302, 2572, 1036, 9693, 854, 4292, 262, 7288, 1416, 537, 14016, 25, 362, 11652, 476, 1378, 624, 262, 1416, 4226, 537, 9434, 11, 421, 1052, 374, 902, 2797, 5904, 11, 476, 421, 279, 4226, 374, 35197, 54761, 25, 1036, 33260, 8253, 854, 624, 262, 690, 9217, 1339, 262, 62947, 609, 43797, 50, 510, 262, 7288, 3155, 4183, 13153, 279, 3405, 476, 1140, 5335, 26, 653, 4183, 2550, 4718, 476, 4960, 14158, 624, 262, 7288, 84468, 4285, 1894, 5036, 448, 518, 1429, 825, 22739, 26087, 4145, 3446, 838, 4322, 1574, 58689, 7256, 854, 4292, 262, 7288, 2823, 12966, 3941, 6194, 26, 421, 6194, 28295, 11, 62552, 279, 11299, 15432, 5904, 323, 5185, 432, 304, 366, 26865, 29816, 257, 151645, 198, 151644, 872, 198, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 30, 220, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151645, 198, 151644, 77091, 198, 198, 32462, 432, 11, 1077, 594, 1430, 311, 7071, 700, 1128, 594, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 62410, 13, 5512, 11, 1077, 594, 50087, 279, 14852, 6505, 13, 576, 21921, 374, 4363, 264, 4158, 11, 1378, 1737, 95112, 4982, 13, 2411, 220, 16, 16, 297, 62410, 320, 3481, 7950, 3082, 701, 582, 1184, 311, 1779, 429, 7702, 13, 21131, 518, 279, 5335, 11, 5310, 279, 825, 9027, 279, 21921, 3082, 11, 1052, 594, 264, 17846, 3403, 279, 19309, 11, 714, 3783, 11, 902, 2293, 11489, 11, 279, 3405, 374, 911, 279, 21921, 13, 13824, 11, 7196, 279, 330, 66, 75512, 1, 1588, 374, 279, 5819, 4982, 13, 13824, 11, 902, 11, 279, 19309, 702, 264, 21921, 3685, 30, 13824, 11, 902, 11, 279, 1887, 21921, 374, 279, 4158, 61607, 13, 13824, 11, 304, 279, 2168, 1380, 279, 6249, 4933, 279, 19309, 323, 21921, 11, 279, 1909, 315, 279, 21921, 2293, 11489, 11, 902, 11, 7196, 279, 17846, 30, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 2727, 330, 65, 77832, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 3263, 2055, 220, 16, 16, 297, 62410, 1035, 387, 279, 8416, 2115, 3082, 8674, 311, 279, 19309, 13, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 279, 19309, 3082, 13, 2160, 429, 949, 315, 279, 21921, 6505, 30, 13824, 11, 902, 11, 7196, 279, 21921, 374, 279, 4158, 61607, 1212, 279, 19309, 13, 13824, 11, 902, 11, 279, 1909, 315, 429, 21921, 2293, 11489, 11, 902, 11, 304, 1045, 5335, 11, 1052, 594, 264, 17846, 25911, 476, 21810, 13, 13824, 11, 902, 11, 1077, 594, 1779, 279, 1156, 2421, 5335, 13, 576, 17196, 45766, 374, 33866, 11, 1221, 279, 19309, 3082, 13, 13824, 11, 279, 4396, 4226, 25, 3330, 518, 279, 5335, 11, 279, 1633, 389, 1909, 315, 279, 14852, 21921, 320, 1782, 4158, 26482, 4982, 8, 518, 220, 16, 16, 297, 62410, 374, 264, 17846, 13, 13824, 11, 902, 11, 3783, 21677, 17846, 374, 389, 279, 7002, 3403, 279, 19309, 13, 13824, 11, 7196, 358, 5786, 878, 13, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 594, 3405, 25, 330, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 7521, 2055, 1156, 11, 10542, 279, 21921, 13, 576, 4158, 26482, 4982, 3143, 220, 16, 16, 297, 62410, 320, 3481, 7950, 568, 3555, 594, 389, 1909, 30, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 429, 3082, 13, 13824, 11, 9834, 0, 758, 279, 21844, 476, 21810, 11, 1052, 594, 264, 17846, 13, 2055, 279, 4226, 374, 264, 17846, 4192, 1249, 8253, 1128, 374, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 527, 20666, 11, 23643, 279, 27979, 6789, 323, 5424, 304, 279, 5335, 25, 18611, 16, 13, 95210, 279, 1036, 65, 77832, 21921, 854, 304, 3405, 25, 362, 4158, 11, 1378, 481, 26482, 5819, 4982, 374, 9434, 3143, 279, 1909, 481, 2115, 320, 16, 16, 297, 527, 20666, 8, 3082, 8674, 311, 1008, 37664, 1075, 279, 19309, 323, 17196, 13, 2303, 17, 13, 1374, 19991, 6171, 3403, 419, 21921, 25, 758, 279, 5335, 11, 264, 3070, 73225, 334, 374, 21810, 389, 279, 7002, 3403, 419, 21921, 3082, 13, 79226, 54216, 151645, 198]
inputs:
<|im_start|>system

    You are a careful vision–language assistant for spatial VQA on a single indoor scene shown across multiple images.

    INPUT:
    Immediately after this message, you will receive N scene images as <image> tags. Treat them as different views of the SAME scene. Refer to images by 1-based index in order of appearance: [1], [2], …, [N].

    REASONING PRINCIPLES:
    • Build a consistent 3D mental model across views: track objects across images, infer relative camera pose, and use scale/occlusion/shadows for depth cues.
    • Interpret deictic terms (“to my right/left/in front/behind”) EGOCENTRICALLY with respect to the narrator/agent. If the viewpoint is ambiguous, default to the most informative central view and say so.
    • Use only visible evidence; do not invent unseen details or rely on external knowledge beyond basic object/color terms.
    • When multiple candidates exist, resolve by the best match to spatial phrase + salience across views. If still ambiguous after checking all images, state that clearly.

    OUTPUT (exactly two blocks, in this order):
    <think>
    Step-by-step reasoning in 3–10 short steps. Cite evidence with image indices (e.g., “[3]: light wood desk right of monitor; [1]: same color, confirms”). Be concise but complete (aim ≤250 tokens unless necessary). Note any occlusion/ambiguity and how you resolved it.
    </think>
    <answer>
    A single concise answer only, which may be:
    • If sufficient: One-word answer, Count (e.g., “2”) or Yes/No (e.g., “yes”).
    • If not sufficient: A sentence or two.
    If answer not visible, if there is no clear evidence, or if the answer is genuinely ambiguous: “cannot determine”.
    </answer>

    STYLE & RULES:
    • Do NOT repeat the question or list images; do NOT output JSON or extra sections.
    • Prefer simple color names with at most one modifier (“light/dark/pale/beige”).
    • Be consistent across views; if views disagree, prioritize the clearest evidence and note it in <think>.
    <|im_end|>
<|im_start|>user
What is on top of the bathroom cabinet that is on my 11 o'clock? <|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|im_end|>
<|im_start|>assistant

Got it, let's try to figure out what's on top of the bathroom cabinet at 11 o'clock. First, let's visualize the bathroom setup. The cabinet is likely a white, two-drawer unit. At 11 o'clock (top-left area), we need to check that spot. Looking at the images, especially the one showing the cabinet area, there's a mirror above the sink, but wait, no—wait, the question is about the cabinet. Wait, maybe the "cabinet" here is the storage unit. Wait, no, the sink has a cabinet below? Wait, no, the main cabinet is the white drawers. Wait, in the image where the camera shows the sink and cabinet, the top of the cabinet—wait, no, maybe the mirror? Wait, no, let's check again. Wait, the user says "bathroom cabinet that is on my 11 o'clock". So 11 o'clock would be the upper left area relative to the sink. Looking at the images, there's a mirror mounted on the wall above the sink area. Is that part of the cabinet setup? Wait, no, maybe the cabinet is the white drawers under the sink. Wait, no, the top of that cabinet—wait, no, in some images, there's a mirror reflected or mounted. Wait, no, let's check the first few images. The shower curtain is colorful, then the sink area. Wait, the correct answer: looking at the images, the object on top of the bathroom cabinet (the white drawer unit) at 11 o'clock is a mirror. Wait, no, wait—the mirror is on the wall above the sink. Wait, maybe I misread. Wait, no, let's check again. Wait, the user's question: "What is on top of the bathroom cabinet that is on my 11 o'clock?" So first, identify the cabinet. The white drawer unit near 11 o'clock (top-left). What's on top? Looking at the images, there's a mirror mounted on the wall above that area. Wait, yes! In the reflection or mounted, there's a mirror. So the answer is a mirror.


To determine what is on top of the bathroom cabinet at 11 o’clock, analyze the spatial layout and elements in the images:  

1. Locate the “bathroom cabinet” in question: A white, two - drawer storage unit is visible near the top - left (11 o’clock) area relative to other fixtures like the sink and shower.  
2. Examine objects above this cabinet: In the images, a **mirror** is mounted on the wall above this cabinet area.  


Mirror<|im_end|>

label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 198, 32462, 432, 11, 1077, 594, 1430, 311, 7071, 700, 1128, 594, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 62410, 13, 5512, 11, 1077, 594, 50087, 279, 14852, 6505, 13, 576, 21921, 374, 4363, 264, 4158, 11, 1378, 1737, 95112, 4982, 13, 2411, 220, 16, 16, 297, 62410, 320, 3481, 7950, 3082, 701, 582, 1184, 311, 1779, 429, 7702, 13, 21131, 518, 279, 5335, 11, 5310, 279, 825, 9027, 279, 21921, 3082, 11, 1052, 594, 264, 17846, 3403, 279, 19309, 11, 714, 3783, 11, 902, 2293, 11489, 11, 279, 3405, 374, 911, 279, 21921, 13, 13824, 11, 7196, 279, 330, 66, 75512, 1, 1588, 374, 279, 5819, 4982, 13, 13824, 11, 902, 11, 279, 19309, 702, 264, 21921, 3685, 30, 13824, 11, 902, 11, 279, 1887, 21921, 374, 279, 4158, 61607, 13, 13824, 11, 304, 279, 2168, 1380, 279, 6249, 4933, 279, 19309, 323, 21921, 11, 279, 1909, 315, 279, 21921, 2293, 11489, 11, 902, 11, 7196, 279, 17846, 30, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 2727, 330, 65, 77832, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 3263, 2055, 220, 16, 16, 297, 62410, 1035, 387, 279, 8416, 2115, 3082, 8674, 311, 279, 19309, 13, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 279, 19309, 3082, 13, 2160, 429, 949, 315, 279, 21921, 6505, 30, 13824, 11, 902, 11, 7196, 279, 21921, 374, 279, 4158, 61607, 1212, 279, 19309, 13, 13824, 11, 902, 11, 279, 1909, 315, 429, 21921, 2293, 11489, 11, 902, 11, 304, 1045, 5335, 11, 1052, 594, 264, 17846, 25911, 476, 21810, 13, 13824, 11, 902, 11, 1077, 594, 1779, 279, 1156, 2421, 5335, 13, 576, 17196, 45766, 374, 33866, 11, 1221, 279, 19309, 3082, 13, 13824, 11, 279, 4396, 4226, 25, 3330, 518, 279, 5335, 11, 279, 1633, 389, 1909, 315, 279, 14852, 21921, 320, 1782, 4158, 26482, 4982, 8, 518, 220, 16, 16, 297, 62410, 374, 264, 17846, 13, 13824, 11, 902, 11, 3783, 21677, 17846, 374, 389, 279, 7002, 3403, 279, 19309, 13, 13824, 11, 7196, 358, 5786, 878, 13, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 594, 3405, 25, 330, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 7521, 2055, 1156, 11, 10542, 279, 21921, 13, 576, 4158, 26482, 4982, 3143, 220, 16, 16, 297, 62410, 320, 3481, 7950, 568, 3555, 594, 389, 1909, 30, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 429, 3082, 13, 13824, 11, 9834, 0, 758, 279, 21844, 476, 21810, 11, 1052, 594, 264, 17846, 13, 2055, 279, 4226, 374, 264, 17846, 4192, 1249, 8253, 1128, 374, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 527, 20666, 11, 23643, 279, 27979, 6789, 323, 5424, 304, 279, 5335, 25, 18611, 16, 13, 95210, 279, 1036, 65, 77832, 21921, 854, 304, 3405, 25, 362, 4158, 11, 1378, 481, 26482, 5819, 4982, 374, 9434, 3143, 279, 1909, 481, 2115, 320, 16, 16, 297, 527, 20666, 8, 3082, 8674, 311, 1008, 37664, 1075, 279, 19309, 323, 17196, 13, 2303, 17, 13, 1374, 19991, 6171, 3403, 419, 21921, 25, 758, 279, 5335, 11, 264, 3070, 73225, 334, 374, 21810, 389, 279, 7002, 3403, 419, 21921, 3082, 13, 79226, 54216, 151645, 198]
labels:

Got it, let's try to figure out what's on top of the bathroom cabinet at 11 o'clock. First, let's visualize the bathroom setup. The cabinet is likely a white, two-drawer unit. At 11 o'clock (top-left area), we need to check that spot. Looking at the images, especially the one showing the cabinet area, there's a mirror above the sink, but wait, no—wait, the question is about the cabinet. Wait, maybe the "cabinet" here is the storage unit. Wait, no, the sink has a cabinet below? Wait, no, the main cabinet is the white drawers. Wait, in the image where the camera shows the sink and cabinet, the top of the cabinet—wait, no, maybe the mirror? Wait, no, let's check again. Wait, the user says "bathroom cabinet that is on my 11 o'clock". So 11 o'clock would be the upper left area relative to the sink. Looking at the images, there's a mirror mounted on the wall above the sink area. Is that part of the cabinet setup? Wait, no, maybe the cabinet is the white drawers under the sink. Wait, no, the top of that cabinet—wait, no, in some images, there's a mirror reflected or mounted. Wait, no, let's check the first few images. The shower curtain is colorful, then the sink area. Wait, the correct answer: looking at the images, the object on top of the bathroom cabinet (the white drawer unit) at 11 o'clock is a mirror. Wait, no, wait—the mirror is on the wall above the sink. Wait, maybe I misread. Wait, no, let's check again. Wait, the user's question: "What is on top of the bathroom cabinet that is on my 11 o'clock?" So first, identify the cabinet. The white drawer unit near 11 o'clock (top-left). What's on top? Looking at the images, there's a mirror mounted on the wall above that area. Wait, yes! In the reflection or mounted, there's a mirror. So the answer is a mirror.


To determine what is on top of the bathroom cabinet at 11 o’clock, analyze the spatial layout and elements in the images:  

1. Locate the “bathroom cabinet” in question: A white, two - drawer storage unit is visible near the top - left (11 o’clock) area relative to other fixtures like the sink and shower.  
2. Examine objects above this cabinet: In the images, a **mirror** is mounted on the wall above this cabinet area.  


Mirror<|im_end|>

[INFO|hub.py:421] 2025-12-05 04:51:44,357 >> Offline mode: forcing local_files_only=True
[INFO|configuration_utils.py:765] 2025-12-05 04:51:44,359 >> loading configuration file config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
[INFO|configuration_utils.py:839] 2025-12-05 04:51:44,362 >> Model config Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 128000,
  "max_window_layers": 28,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "_name_or_path": "Qwen/Qwen2.5-VL-7B-Instruct",
    "architectures": [
      "Qwen2_5_VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "dtype": "bfloat16",
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "layer_types": [
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention"
    ],
    "max_position_embeddings": 128000,
    "max_window_layers": 28,
    "model_type": "qwen2_5_vl_text",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "use_cache": true,
    "use_sliding_window": false,
    "vision_token_id": 151654,
    "vocab_size": 152064
  },
  "tie_word_embeddings": false,
  "transformers_version": "4.57.0",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 3584,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

[INFO|2025-12-05 04:51:44] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
[INFO|2025-12-05 04:51:44] llamafactory.model.model_utils.liger_kernel:143 >> Liger kernel has been applied to the model.
[INFO|hub.py:421] 2025-12-05 04:51:44,830 >> Offline mode: forcing local_files_only=True
[INFO|hub.py:421] 2025-12-05 04:51:44,832 >> Offline mode: forcing local_files_only=True
[INFO|modeling_utils.py:4840] 2025-12-05 04:51:44,834 >> Offline mode: forcing local_files_only=True
[INFO|modeling_utils.py:1176] 2025-12-05 04:51:44,837 >> loading weights file model.safetensors from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/model.safetensors.index.json
[INFO|modeling_utils.py:4377] 2025-12-05 04:51:44,841 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[2025-12-05 04:51:44,841] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
[WARNING|logging.py:328] 2025-12-05 04:51:44,864 >> Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLForConditionalGeneration is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[INFO|configuration_utils.py:986] 2025-12-05 04:51:44,875 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

[WARNING|logging.py:328] 2025-12-05 04:51:44,884 >> Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[INFO|modeling_utils.py:2345] 2025-12-05 04:51:44,887 >> Instantiating Qwen2_5_VisionTransformerPretrainedModel model under default dtype torch.float32.
[2025-12-05 04:51:44,893] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
[WARNING|logging.py:328] 2025-12-05 04:51:44,894 >> Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VisionTransformerPretrainedModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[2025-12-05 04:51:44,896] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
[2025-12-05 04:51:44,897] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
[2025-12-05 04:51:44,899] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
[2025-12-05 04:51:44,900] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
[2025-12-05 04:51:44,901] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
[2025-12-05 04:51:44,902] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLForConditionalGeneration is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLForConditionalGeneration is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLForConditionalGeneration is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLForConditionalGeneration is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLForConditionalGeneration is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLForConditionalGeneration is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLForConditionalGeneration is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VisionTransformerPretrainedModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VisionTransformerPretrainedModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VisionTransformerPretrainedModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VisionTransformerPretrainedModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VisionTransformerPretrainedModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VisionTransformerPretrainedModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VisionTransformerPretrainedModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[INFO|modeling_utils.py:2345] 2025-12-05 04:51:46,068 >> Instantiating Qwen2_5_VLTextModel model under default dtype torch.float32.
[WARNING|logging.py:328] 2025-12-05 04:51:46,071 >> Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLTextModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLTextModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLTextModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLTextModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLTextModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLTextModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLTextModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2_5_VLTextModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", dtype=torch.float16)`
[2025-12-05 04:51:47,992] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 729, num_elems = 8.29B
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.13it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.13it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.13it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.13it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.14it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.13it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.14it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:01<00:04,  1.07s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.18it/s]Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.18it/s]Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.18it/s]Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.19it/s]Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.18it/s]Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.18it/s]Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.19it/s]Loading checkpoint shards:  40%|████      | 2/5 [00:01<00:02,  1.12it/s]Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.27it/s]Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.27it/s]Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.27it/s]Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.27it/s]Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.27it/s]Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.27it/s]Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.27it/s]Loading checkpoint shards:  60%|██████    | 3/5 [00:02<00:01,  1.27it/s]Loading checkpoint shards:  80%|████████  | 4/5 [00:03<00:00,  1.36it/s]Loading checkpoint shards:  80%|████████  | 4/5 [00:03<00:00,  1.36it/s]Loading checkpoint shards:  80%|████████  | 4/5 [00:03<00:00,  1.36it/s]Loading checkpoint shards:  80%|████████  | 4/5 [00:03<00:00,  1.35it/s]Loading checkpoint shards:  80%|████████  | 4/5 [00:03<00:00,  1.36it/s]Loading checkpoint shards:  80%|████████  | 4/5 [00:03<00:00,  1.36it/s]Loading checkpoint shards:  80%|████████  | 4/5 [00:03<00:00,  1.36it/s]Loading checkpoint shards:  80%|████████  | 4/5 [00:03<00:00,  1.35it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.82it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.82it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.52it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.52it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.82it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.83it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.52it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.82it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.52it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.52it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.82it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.82it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.52it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.52it/s]
Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.87it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.50it/s]
[INFO|configuration_utils.py:941] 2025-12-05 04:51:51,365 >> loading configuration file generation_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/generation_config.json
[INFO|configuration_utils.py:986] 2025-12-05 04:51:51,365 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 1e-06
}

[INFO|dynamic_module_utils.py:423] 2025-12-05 04:51:51,367 >> Could not locate the custom_generate/generate.py inside Qwen/Qwen2.5-VL-7B-Instruct.
[INFO|2025-12-05 04:51:51] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2025-12-05 04:51:51] llamafactory.model.model_utils.attention:143 >> Using FlashAttention-2 for faster training and inference.
[INFO|2025-12-05 04:51:51] llamafactory.model.adapter:143 >> DeepSpeed ZeRO3 detected, remaining trainable params in float32.
[INFO|2025-12-05 04:51:51] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA
[INFO|2025-12-05 04:51:51] llamafactory.model.model_utils.misc:143 >> Found linear modules: q_proj,up_proj,down_proj,v_proj,o_proj,gate_proj,k_proj
[INFO|2025-12-05 04:51:51] llamafactory.model.model_utils.visual:143 >> Set vision model not trainable: ['visual.patch_embed', 'visual.blocks'].
[INFO|2025-12-05 04:51:51] llamafactory.model.model_utils.visual:143 >> Set multi model projector not trainable: visual.merger.
[INFO|2025-12-05 04:51:51] llamafactory.model.loader:143 >> trainable params: 20,185,088 || all params: 8,312,351,744 || trainable%: 0.2428
name: base_model.model.model.visual.patch_embed.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.ln_q.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.0.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.0.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.embed_tokens.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.lm_head.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
[INFO|trainer.py:749] 2025-12-05 04:51:51,710 >> Using auto half precision backend
[WARNING|2025-12-05 04:51:51] llamafactory.train.callbacks:154 >> Previous trainer log in this folder will be deleted.
[WARNING|trainer.py:982] 2025-12-05 04:51:51,713 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
[DEBUG|trainer.py:2373] 2025-12-05 04:51:51,999 >> Currently training with a batch size of: 2
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 8. Using DeepSpeed's value.
[INFO|deepspeed.py:383] 2025-12-05 04:51:52,016 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Currently training with a batch size of: 2
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Currently training with a batch size of: 2
Currently training with a batch size of: 2
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Currently training with a batch size of: 2
Currently training with a batch size of: 2
Currently training with a batch size of: 2
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Currently training with a batch size of: 2
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Using /tmp/.cache/torch_extensions as PyTorch extensions root...
Creating extension directory /tmp/.cache/torch_extensions/cpu_adam...
Detected CUDA files, patching ldflags
Emitting ninja build file /tmp/.cache/torch_extensions/cpu_adam/build.ninja...
/opt/conda/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module cpu_adam...
Using envvar MAX_JOBS (16) as the number of workers...
Using /tmp/.cache/torch_extensions as PyTorch extensions root...Using /tmp/.cache/torch_extensions as PyTorch extensions root...

Using /tmp/.cache/torch_extensions as PyTorch extensions root...
Using /tmp/.cache/torch_extensions as PyTorch extensions root...
Using /tmp/.cache/torch_extensions as PyTorch extensions root...
Using /tmp/.cache/torch_extensions as PyTorch extensions root...
Using /tmp/.cache/torch_extensions as PyTorch extensions root...
[1/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.11/site-packages/torch/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o 
[2/3] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.11/site-packages/torch/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o 
[3/3] c++ cpu_adam.o cpu_adam_impl.o -shared -lcurand -L/usr/local/cuda/lib64 -L/opt/conda/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so
Loading extension module cpu_adam...
Time to load cpu_adam op: 28.942991733551025 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-12-05 04:52:22,213] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
Loading extension module cpu_adam...
Time to load cpu_adam op: 28.923698663711548 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-12-05 04:52:22,240] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
Loading extension module cpu_adam...
Loading extension module cpu_adam...
Time to load cpu_adam op: 28.913816928863525 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-12-05 04:52:22,272] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
Time to load cpu_adam op: 28.922203063964844 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-12-05 04:52:22,273] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
Loading extension module cpu_adam...
Time to load cpu_adam op: 29.03342580795288 seconds
Loading extension module cpu_adam...Loading extension module cpu_adam...

Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-12-05 04:52:22,292] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.9, git-hash=unknown, git-branch=unknown
[2025-12-05 04:52:22,292] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
Loading extension module cpu_adam...
Time to load cpu_adam op: 28.88532328605652 secondsTime to load cpu_adam op: 29.035581827163696 seconds

Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
Time to load cpu_adam op: 28.935102462768555 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-12-05 04:52:22,295] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
[2025-12-05 04:52:22,295] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
[2025-12-05 04:52:22,295] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 8
[2025-12-05 04:52:22,374] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-12-05 04:52:22,379] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-12-05 04:52:22,379] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-12-05 04:52:22,408] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2025-12-05 04:52:22,408] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2025-12-05 04:52:22,408] [INFO] [logging.py:107:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2025-12-05 04:52:22,408] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2025-12-05 04:52:22,629] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[2025-12-05 04:52:22,630] [INFO] [utils.py:782:see_memory_usage] MA 0.04 GB         Max_MA 3.05 GB         CA 0.08 GB         Max_CA 3 GB 
[2025-12-05 04:52:22,630] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 62.34 GB, percent = 3.1%
[2025-12-05 04:52:22,635] [INFO] [stage3.py:170:__init__] Reduce bucket size 12845056
[2025-12-05 04:52:22,635] [INFO] [stage3.py:171:__init__] Prefetch bucket size 11560550
[2025-12-05 04:52:22,852] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2025-12-05 04:52:22,853] [INFO] [utils.py:782:see_memory_usage] MA 0.04 GB         Max_MA 0.04 GB         CA 0.08 GB         Max_CA 0 GB 
[2025-12-05 04:52:22,853] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 62.34 GB, percent = 3.1%
Parameter Offload: Total persistent parameters: 8303616 in 676 params
[2025-12-05 04:52:23,653] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2025-12-05 04:52:23,654] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.04 GB         CA 0.08 GB         Max_CA 0 GB 
[2025-12-05 04:52:23,654] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 62.57 GB, percent = 3.1%
[2025-12-05 04:52:23,923] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[2025-12-05 04:52:23,923] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.08 GB         Max_CA 0 GB 
[2025-12-05 04:52:23,923] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 62.57 GB, percent = 3.1%
[2025-12-05 04:52:24,209] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 1
[2025-12-05 04:52:24,209] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.08 GB         Max_CA 0 GB 
[2025-12-05 04:52:24,210] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 62.73 GB, percent = 3.1%
[2025-12-05 04:52:24,480] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[2025-12-05 04:52:24,481] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.08 GB         Max_CA 0 GB 
[2025-12-05 04:52:24,481] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 62.72 GB, percent = 3.1%
[2025-12-05 04:52:24,759] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[2025-12-05 04:52:24,760] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.08 GB         Max_CA 0 GB 
[2025-12-05 04:52:24,760] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 62.73 GB, percent = 3.1%
[2025-12-05 04:52:25,032] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-12-05 04:52:25,033] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.08 GB         Max_CA 0 GB 
[2025-12-05 04:52:25,033] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 62.96 GB, percent = 3.1%
[2025-12-05 04:52:25,319] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-12-05 04:52:25,320] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.08 GB         Max_CA 0 GB 
[2025-12-05 04:52:25,320] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 62.99 GB, percent = 3.1%
[2025-12-05 04:52:25,321] [INFO] [stage3.py:534:_setup_for_real_optimizer] optimizer state initialized
***** Running training *****
***** Running training *****
  Num examples = 29,742
  Num examples = 29,742
  Num Epochs = 1
  Num Epochs = 1
***** Running training *****
  Instantaneous batch size per device = 2
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 128
***** Running training *****
  Num examples = 29,742
  Gradient Accumulation steps = 8
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Num Epochs = 1
  Total optimization steps = 233
  Gradient Accumulation steps = 8
  Num examples = 29,742
  Total optimization steps = 233
  Instantaneous batch size per device = 2
  Num Epochs = 1
***** Running training *****
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Instantaneous batch size per device = 2
  Gradient Accumulation steps = 8
  Total optimization steps = 233
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Num examples = 29,742
***** Running training *****
  Gradient Accumulation steps = 8
  Num Epochs = 1
  Total optimization steps = 233
  Instantaneous batch size per device = 2
  Num examples = 29,742
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Num Epochs = 1
  Gradient Accumulation steps = 8
  Instantaneous batch size per device = 2
  Total optimization steps = 233
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Gradient Accumulation steps = 8
  Total optimization steps = 233
***** Running training *****
  Num examples = 29,742
  Num Epochs = 1
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Gradient Accumulation steps = 8
  Total optimization steps = 233
  Number of trainable parameters = 20,185,088
  Number of trainable parameters = 20,185,088
  Number of trainable parameters = 20,185,088
  Number of trainable parameters = 20,185,088
  Number of trainable parameters = 20,185,088
  Number of trainable parameters = 20,185,088
  Number of trainable parameters = 20,185,088
[2025-12-05 04:52:25,733] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-12-05 04:52:25,734] [INFO] [utils.py:782:see_memory_usage] MA 0.02 GB         Max_MA 0.02 GB         CA 0.12 GB         Max_CA 0 GB 
[2025-12-05 04:52:25,734] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 63.1 GB, percent = 3.1%
[2025-12-05 04:52:25,734] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[2025-12-05 04:52:25,734] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-12-05 04:52:25,734] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-12-05 04:52:25,734] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2025-12-05 04:52:25,739] [INFO] [config.py:1003:print] DeepSpeedEngine configuration:
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   amp_enabled .................. False
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   amp_params ................... False
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   bfloat16_enabled ............. True
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   bfloat16_immediate_grad_update  True
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   checkpoint_parallel_write_pipeline  False
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   checkpoint_tag_validation_enabled  True
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   checkpoint_tag_validation_fail  False
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x15521cfa1e90>
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   communication_data_type ...... None
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   curriculum_enabled_legacy .... False
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   curriculum_params_legacy ..... False
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   data_efficiency_enabled ...... False
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   dataloader_drop_last ......... False
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   disable_allgather ............ False
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   dump_state ................... False
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   dynamic_loss_scale_args ...... None
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   eigenvalue_enabled ........... False
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   eigenvalue_gas_boundary_resolution  1
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   eigenvalue_layer_num ......... 0
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   eigenvalue_max_iter .......... 100
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   eigenvalue_stability ......... 1e-06
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   eigenvalue_tol ............... 0.01
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   eigenvalue_verbose ........... False
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   elasticity_enabled ........... False
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   fp16_auto_cast ............... None
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   fp16_enabled ................. False
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   fp16_master_weights_and_gradients  False
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   global_rank .................. 0
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   grad_accum_dtype ............. None
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   gradient_accumulation_steps .. 8
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   gradient_clipping ............ 1.0
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   gradient_predivide_factor .... 1.0
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   graph_harvesting ............. False
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   initial_dynamic_scale ........ 1
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   load_universal_checkpoint .... False
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   loss_scale ................... 1.0
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   memory_breakdown ............. False
[2025-12-05 04:52:25,740] [INFO] [config.py:1007:print]   mics_hierarchial_params_gather  False
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   mics_shard_size .............. -1
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   optimizer_legacy_fusion ...... False
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   optimizer_name ............... None
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   optimizer_params ............. None
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   pld_enabled .................. False
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   pld_params ................... False
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   prescale_gradients ........... False
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   scheduler_name ............... None
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   scheduler_params ............. None
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   seq_parallel_communication_data_type  torch.float32
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   sparse_attention ............. None
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   sparse_gradients_enabled ..... False
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   steps_per_print .............. inf
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   timers_config ................ enabled=True synchronized=True
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   train_batch_size ............. 128
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   train_micro_batch_size_per_gpu  2
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   use_data_before_expert_parallel_  False
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   use_node_local_storage ....... False
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   wall_clock_breakdown ......... False
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   weight_quantization_config ... None
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   world_size ................... 8
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   zero_allow_untested_optimizer  True
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=12845056 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=11560550 param_persistence_threshold=35840 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   zero_enabled ................. True
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   zero_force_ds_cpu_optimizer .. True
[2025-12-05 04:52:25,741] [INFO] [config.py:1007:print]   zero_optimization_stage ...... 3
[2025-12-05 04:52:25,741] [INFO] [config.py:993:print_user_config]   json = {
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 2, 
    "gradient_accumulation_steps": 8, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "offload_param": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "overlap_comm": false, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 1.284506e+07, 
        "stage3_prefetch_bucket_size": 1.156055e+07, 
        "stage3_param_persistence_threshold": 3.584000e+04, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "steps_per_print": inf
}
[INFO|trainer.py:2519] 2025-12-05 04:52:25,742 >> ***** Running training *****
[INFO|trainer.py:2520] 2025-12-05 04:52:25,742 >>   Num examples = 29,742
[INFO|trainer.py:2521] 2025-12-05 04:52:25,742 >>   Num Epochs = 1
[INFO|trainer.py:2522] 2025-12-05 04:52:25,742 >>   Instantaneous batch size per device = 2
[INFO|trainer.py:2525] 2025-12-05 04:52:25,742 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2526] 2025-12-05 04:52:25,742 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2527] 2025-12-05 04:52:25,742 >>   Total optimization steps = 233
[INFO|trainer.py:2528] 2025-12-05 04:52:25,746 >>   Number of trainable parameters = 20,185,088
[INFO|integration_utils.py:867] 2025-12-05 04:52:25,750 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Tracking run with wandb version 0.23.0
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /project/aip-wangcs/indrisch/LLaMA-Factory/wandb/wandb/offline-run-20251205_045225-ve0uo12o
  0%|          | 0/233 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /tmp/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /tmp/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /tmp/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /tmp/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /tmp/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /tmp/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /tmp/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /tmp/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
  0%|          | 1/233 [02:34<9:59:01, 154.92s/it]  1%|          | 2/233 [05:03<9:42:17, 151.25s/it]  1%|▏         | 3/233 [07:04<8:46:34, 137.37s/it]  2%|▏         | 4/233 [09:02<8:14:40, 129.61s/it]  2%|▏         | 5/233 [11:11<8:11:54, 129.45s/it]  3%|▎         | 6/233 [13:04<7:49:04, 123.99s/it]  3%|▎         | 7/233 [15:06<7:43:53, 123.16s/it]  3%|▎         | 8/233 [17:10<7:43:48, 123.68s/it]  4%|▍         | 9/233 [19:13<7:40:42, 123.41s/it]  4%|▍         | 10/233 [21:35<8:00:06, 129.18s/it]                                                   {'loss': 1.6643, 'grad_norm': 0.48734644055366516, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.04}
  4%|▍         | 10/233 [21:35<8:00:06, 129.18s/it][INFO|trainer.py:4643] 2025-12-05 05:14:04,164 >> 
***** Running Evaluation *****

***** Running Evaluation *****

***** Running Evaluation *****

***** Running Evaluation *****

***** Running Evaluation *****

***** Running Evaluation *****

***** Running Evaluation *****
  Num examples = 3305
  Num examples = 3305
  Batch size = 1
  Batch size = 1
  Num examples = 3305
  Num examples = 3305
[INFO|trainer.py:4645] 2025-12-05 05:14:04,164 >>   Num examples = 3305
  Batch size = 1
  Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
  Num examples = 3305
  Batch size = 1
  Batch size = 1
[INFO|trainer.py:4648] 2025-12-05 05:14:04,164 >>   Batch size = 1
  Num examples = 3305
  Batch size = 1

  0%|          | 0/414 [00:00<?, ?it/s][A
  0%|          | 2/414 [00:04<16:42,  2.43s/it][A
  1%|          | 3/414 [00:10<25:07,  3.67s/it][A
  1%|          | 4/414 [00:18<35:29,  5.19s/it][A
  1%|          | 5/414 [00:25<41:46,  6.13s/it][A
  1%|▏         | 6/414 [00:30<37:20,  5.49s/it][A
  2%|▏         | 7/414 [00:34<35:23,  5.22s/it][A
  2%|▏         | 8/414 [00:39<33:54,  5.01s/it][A
  2%|▏         | 9/414 [00:45<35:18,  5.23s/it][A
  2%|▏         | 10/414 [00:52<39:51,  5.92s/it][A
  3%|▎         | 11/414 [00:58<39:24,  5.87s/it][A
  3%|▎         | 12/414 [01:04<40:21,  6.02s/it][A
  3%|▎         | 13/414 [01:12<44:05,  6.60s/it][A
  3%|▎         | 14/414 [01:19<45:08,  6.77s/it][A
  4%|▎         | 15/414 [01:26<44:27,  6.68s/it][A
  4%|▍         | 16/414 [01:33<44:43,  6.74s/it][A
  4%|▍         | 17/414 [01:39<44:38,  6.75s/it][A
  4%|▍         | 18/414 [01:46<44:56,  6.81s/it][A
  5%|▍         | 19/414 [01:55<48:23,  7.35s/it][A
  5%|▍         | 20/414 [02:02<47:27,  7.23s/it][A
  5%|▌         | 21/414 [02:07<43:07,  6.58s/it][A
  5%|▌         | 22/414 [02:13<42:02,  6.43s/it][A
  6%|▌         | 23/414 [02:20<43:50,  6.73s/it][A
  6%|▌         | 24/414 [02:26<42:24,  6.52s/it][A
  6%|▌         | 25/414 [02:33<42:33,  6.56s/it][A
  6%|▋         | 26/414 [02:40<42:51,  6.63s/it][A
  7%|▋         | 27/414 [02:44<38:08,  5.91s/it][A
  7%|▋         | 28/414 [02:49<36:31,  5.68s/it][A
  7%|▋         | 29/414 [02:55<35:32,  5.54s/it][A
  7%|▋         | 30/414 [03:01<36:28,  5.70s/it][A
  7%|▋         | 31/414 [03:06<36:24,  5.70s/it][A
  8%|▊         | 32/414 [03:11<34:05,  5.35s/it][A
  8%|▊         | 33/414 [03:17<36:00,  5.67s/it][A
  8%|▊         | 34/414 [03:23<36:15,  5.73s/it][A
  8%|▊         | 35/414 [03:29<36:07,  5.72s/it][A
  9%|▊         | 36/414 [03:35<37:27,  5.95s/it][A
  9%|▉         | 37/414 [03:42<38:17,  6.09s/it][A
  9%|▉         | 38/414 [03:48<38:34,  6.16s/it][A
  9%|▉         | 39/414 [03:52<34:46,  5.56s/it][A
 10%|▉         | 40/414 [03:58<34:36,  5.55s/it][A
 10%|▉         | 41/414 [04:04<36:29,  5.87s/it][A
 10%|█         | 42/414 [04:10<36:28,  5.88s/it][A
 10%|█         | 43/414 [04:19<42:18,  6.84s/it][A
 11%|█         | 44/414 [04:30<49:43,  8.06s/it][A
 11%|█         | 45/414 [04:37<47:54,  7.79s/it][A
 11%|█         | 46/414 [04:42<41:50,  6.82s/it][A
 11%|█▏        | 47/414 [04:46<37:05,  6.06s/it][A
 12%|█▏        | 48/414 [04:50<32:53,  5.39s/it][A
 12%|█▏        | 49/414 [04:54<29:15,  4.81s/it][A
 12%|█▏        | 50/414 [04:58<27:46,  4.58s/it][A
 12%|█▏        | 51/414 [05:02<26:55,  4.45s/it][A
 13%|█▎        | 52/414 [05:06<26:38,  4.41s/it][A
 13%|█▎        | 53/414 [05:13<31:58,  5.32s/it][A
 13%|█▎        | 54/414 [05:22<37:52,  6.31s/it][A
 13%|█▎        | 55/414 [05:29<38:43,  6.47s/it][A
 14%|█▎        | 56/414 [05:35<38:35,  6.47s/it][A
 14%|█▍        | 57/414 [05:40<35:47,  6.01s/it][A
 14%|█▍        | 58/414 [05:46<35:16,  5.95s/it][A
 14%|█▍        | 59/414 [05:53<37:18,  6.31s/it][A
 14%|█▍        | 60/414 [05:59<36:28,  6.18s/it][A
 15%|█▍        | 61/414 [06:04<33:03,  5.62s/it][A
 15%|█▍        | 62/414 [06:08<30:40,  5.23s/it][A
 15%|█▌        | 63/414 [06:13<30:10,  5.16s/it][A
 15%|█▌        | 64/414 [06:18<29:49,  5.11s/it][A
 16%|█▌        | 65/414 [06:23<29:44,  5.11s/it][A
 16%|█▌        | 66/414 [06:29<30:37,  5.28s/it][A
 16%|█▌        | 67/414 [06:36<33:48,  5.84s/it][A
 16%|█▋        | 68/414 [06:43<35:54,  6.23s/it][A
 17%|█▋        | 69/414 [06:50<37:10,  6.47s/it][A
 17%|█▋        | 70/414 [06:57<38:11,  6.66s/it][A
 17%|█▋        | 71/414 [07:02<35:17,  6.17s/it][A
 17%|█▋        | 72/414 [07:07<33:05,  5.80s/it][A
 18%|█▊        | 73/414 [07:12<31:21,  5.52s/it][A
 18%|█▊        | 74/414 [07:16<28:51,  5.09s/it][A
 18%|█▊        | 75/414 [07:20<26:50,  4.75s/it][A
 18%|█▊        | 76/414 [07:24<26:19,  4.67s/it][A
 19%|█▊        | 77/414 [07:31<28:41,  5.11s/it][A
 19%|█▉        | 78/414 [07:36<30:01,  5.36s/it][A
 19%|█▉        | 79/414 [07:42<30:52,  5.53s/it][A
 19%|█▉        | 80/414 [07:48<30:21,  5.45s/it][A
 20%|█▉        | 81/414 [07:52<28:56,  5.21s/it][A
 20%|█▉        | 82/414 [07:58<29:07,  5.26s/it][A
 20%|██        | 83/414 [08:05<32:47,  5.94s/it][A
 20%|██        | 84/414 [08:13<35:18,  6.42s/it][A
 21%|██        | 85/414 [08:18<32:49,  5.99s/it][A
 21%|██        | 86/414 [08:23<30:57,  5.66s/it][A
 21%|██        | 87/414 [08:27<29:12,  5.36s/it][A
 21%|██▏       | 88/414 [08:32<28:15,  5.20s/it][A
 21%|██▏       | 89/414 [08:38<29:08,  5.38s/it][A
 22%|██▏       | 90/414 [08:45<31:52,  5.90s/it][A
 22%|██▏       | 91/414 [08:50<30:59,  5.76s/it][A
 22%|██▏       | 92/414 [08:56<30:22,  5.66s/it][A
 22%|██▏       | 93/414 [09:01<30:07,  5.63s/it][A
 23%|██▎       | 94/414 [09:07<29:29,  5.53s/it][A
 23%|██▎       | 95/414 [09:13<29:59,  5.64s/it][A
 23%|██▎       | 96/414 [09:19<30:31,  5.76s/it][A
 23%|██▎       | 97/414 [09:24<30:10,  5.71s/it][A
 24%|██▎       | 98/414 [09:28<27:33,  5.23s/it][A
 24%|██▍       | 99/414 [09:33<26:31,  5.05s/it][A
 24%|██▍       | 100/414 [09:39<28:24,  5.43s/it][A
 24%|██▍       | 101/414 [09:47<32:30,  6.23s/it][A
 25%|██▍       | 102/414 [09:54<33:17,  6.40s/it][A
 25%|██▍       | 103/414 [09:59<30:38,  5.91s/it][A
 25%|██▌       | 104/414 [10:04<28:35,  5.53s/it][A
 25%|██▌       | 105/414 [10:09<28:39,  5.57s/it][A
 26%|██▌       | 106/414 [10:17<31:34,  6.15s/it][A
 26%|██▌       | 107/414 [10:23<31:20,  6.13s/it][A
 26%|██▌       | 108/414 [10:28<29:19,  5.75s/it][A
 26%|██▋       | 109/414 [10:33<28:03,  5.52s/it][A
 27%|██▋       | 110/414 [10:39<28:38,  5.65s/it][A
 27%|██▋       | 111/414 [10:45<29:29,  5.84s/it][A
 27%|██▋       | 112/414 [10:50<28:01,  5.57s/it][A
 27%|██▋       | 113/414 [10:56<29:07,  5.80s/it][A
 28%|██▊       | 114/414 [11:03<30:57,  6.19s/it][A
 28%|██▊       | 115/414 [11:09<30:18,  6.08s/it][A
 28%|██▊       | 116/414 [11:15<30:15,  6.09s/it][A
 28%|██▊       | 117/414 [11:23<33:02,  6.67s/it][A
 29%|██▊       | 118/414 [11:32<36:23,  7.38s/it][A
 29%|██▊       | 119/414 [11:42<39:04,  7.95s/it][A
 29%|██▉       | 120/414 [11:50<39:09,  7.99s/it][A
 29%|██▉       | 121/414 [11:57<38:23,  7.86s/it][A
 29%|██▉       | 122/414 [12:05<37:19,  7.67s/it][A
 30%|██▉       | 123/414 [12:11<35:40,  7.36s/it][A
 30%|██▉       | 124/414 [12:19<35:53,  7.43s/it][A
 30%|███       | 125/414 [12:26<35:53,  7.45s/it][A
 30%|███       | 126/414 [12:32<33:14,  6.93s/it][A
 31%|███       | 127/414 [12:39<33:24,  6.98s/it][A
 31%|███       | 128/414 [12:48<36:07,  7.58s/it][A
 31%|███       | 129/414 [12:55<34:54,  7.35s/it][A
 31%|███▏      | 130/414 [13:02<34:07,  7.21s/it][A
 32%|███▏      | 131/414 [13:10<35:50,  7.60s/it][A
 32%|███▏      | 132/414 [13:18<35:45,  7.61s/it][A
 32%|███▏      | 133/414 [13:22<30:57,  6.61s/it][A
 32%|███▏      | 134/414 [13:28<29:53,  6.41s/it][A
 33%|███▎      | 135/414 [13:35<29:51,  6.42s/it][A
 33%|███▎      | 136/414 [13:42<31:35,  6.82s/it][A
 33%|███▎      | 137/414 [13:52<34:57,  7.57s/it][A
 33%|███▎      | 138/414 [13:59<34:06,  7.41s/it][A
 34%|███▎      | 139/414 [14:03<30:20,  6.62s/it][A
 34%|███▍      | 140/414 [14:08<27:39,  6.06s/it][A
 34%|███▍      | 141/414 [14:14<26:49,  5.89s/it][A
 34%|███▍      | 142/414 [14:20<27:42,  6.11s/it][A
 35%|███▍      | 143/414 [14:26<27:40,  6.13s/it][A
 35%|███▍      | 144/414 [14:31<25:20,  5.63s/it][A
 35%|███▌      | 145/414 [14:35<23:36,  5.26s/it][A
 35%|███▌      | 146/414 [14:40<22:04,  4.94s/it][A
 36%|███▌      | 147/414 [14:44<21:48,  4.90s/it][A
 36%|███▌      | 148/414 [14:50<22:25,  5.06s/it][A
 36%|███▌      | 149/414 [14:55<22:08,  5.01s/it][A
 36%|███▌      | 150/414 [14:59<21:34,  4.90s/it][A
 36%|███▋      | 151/414 [15:03<20:30,  4.68s/it][A
 37%|███▋      | 152/414 [15:08<20:51,  4.78s/it][A
 37%|███▋      | 153/414 [15:14<21:35,  4.96s/it][A
 37%|███▋      | 154/414 [15:18<20:33,  4.74s/it][A
 37%|███▋      | 155/414 [15:22<19:53,  4.61s/it][A
 38%|███▊      | 156/414 [15:28<20:43,  4.82s/it][A
 38%|███▊      | 157/414 [15:34<22:38,  5.28s/it][A
 38%|███▊      | 158/414 [15:40<23:58,  5.62s/it][A
 38%|███▊      | 159/414 [15:47<24:48,  5.84s/it][A
 39%|███▊      | 160/414 [15:52<24:30,  5.79s/it][A
 39%|███▉      | 161/414 [15:59<25:44,  6.10s/it][A
 39%|███▉      | 162/414 [16:05<25:40,  6.11s/it][A
 39%|███▉      | 163/414 [16:11<24:56,  5.96s/it][A
 40%|███▉      | 164/414 [16:18<26:15,  6.30s/it][A
 40%|███▉      | 165/414 [16:26<27:39,  6.66s/it][A
 40%|████      | 166/414 [16:33<28:32,  6.90s/it][A
 40%|████      | 167/414 [16:40<28:05,  6.83s/it][A
 41%|████      | 168/414 [16:45<26:25,  6.45s/it][A
 41%|████      | 169/414 [16:50<24:07,  5.91s/it][A
 41%|████      | 170/414 [16:55<22:36,  5.56s/it][A
 41%|████▏     | 171/414 [17:00<22:20,  5.52s/it][A
 42%|████▏     | 172/414 [17:06<23:03,  5.72s/it][A
 42%|████▏     | 173/414 [17:13<23:32,  5.86s/it][A
 42%|████▏     | 174/414 [17:19<23:36,  5.90s/it][A
 42%|████▏     | 175/414 [17:25<23:44,  5.96s/it][A
 43%|████▎     | 176/414 [17:29<22:13,  5.60s/it][A
 43%|████▎     | 177/414 [17:35<22:26,  5.68s/it][A
 43%|████▎     | 178/414 [17:42<23:24,  5.95s/it][A
 43%|████▎     | 179/414 [17:47<22:30,  5.75s/it][A
 43%|████▎     | 180/414 [17:53<22:22,  5.74s/it][A
 44%|████▎     | 181/414 [17:59<22:55,  5.90s/it][A
 44%|████▍     | 182/414 [18:05<22:35,  5.84s/it][A
 44%|████▍     | 183/414 [18:11<23:10,  6.02s/it][A
 44%|████▍     | 184/414 [18:17<22:52,  5.97s/it][A
 45%|████▍     | 185/414 [18:22<21:36,  5.66s/it][A
 45%|████▍     | 186/414 [18:29<22:56,  6.04s/it][A
 45%|████▌     | 187/414 [18:39<27:15,  7.20s/it][A
 45%|████▌     | 188/414 [18:48<29:50,  7.92s/it][A
 46%|████▌     | 189/414 [18:57<30:29,  8.13s/it][A
 46%|████▌     | 190/414 [19:05<30:26,  8.16s/it][A
 46%|████▌     | 191/414 [19:11<27:55,  7.51s/it][A
 46%|████▋     | 192/414 [19:17<25:53,  7.00s/it][A
 47%|████▋     | 193/414 [19:22<23:05,  6.27s/it][A
 47%|████▋     | 194/414 [19:27<21:55,  5.98s/it][A
 47%|████▋     | 195/414 [19:32<20:50,  5.71s/it][A
 47%|████▋     | 196/414 [19:37<20:16,  5.58s/it][A
 48%|████▊     | 197/414 [19:44<21:43,  6.01s/it][A
 48%|████▊     | 198/414 [19:50<21:44,  6.04s/it][A
 48%|████▊     | 199/414 [19:55<20:09,  5.63s/it][A
 48%|████▊     | 200/414 [20:01<20:07,  5.64s/it][A
 49%|████▊     | 201/414 [20:08<21:21,  6.02s/it][A
 49%|████▉     | 202/414 [20:14<21:46,  6.16s/it][A
 49%|████▉     | 203/414 [20:19<20:10,  5.74s/it][A
 49%|████▉     | 204/414 [20:23<18:37,  5.32s/it][A
 50%|████▉     | 205/414 [20:28<17:40,  5.07s/it][A
 50%|████▉     | 206/414 [20:33<18:12,  5.25s/it][A
 50%|█████     | 207/414 [20:39<18:33,  5.38s/it][A
 50%|█████     | 208/414 [20:45<19:07,  5.57s/it][A
 50%|█████     | 209/414 [20:53<21:14,  6.22s/it][A
 51%|█████     | 210/414 [20:59<21:29,  6.32s/it][A
 51%|█████     | 211/414 [21:05<20:11,  5.97s/it][A
 51%|█████     | 212/414 [21:10<19:54,  5.91s/it][A
 51%|█████▏    | 213/414 [21:18<21:36,  6.45s/it][A
 52%|█████▏    | 214/414 [21:25<21:52,  6.56s/it][A
 52%|█████▏    | 215/414 [21:32<21:50,  6.59s/it][A
 52%|█████▏    | 216/414 [21:40<24:02,  7.29s/it][A
 52%|█████▏    | 217/414 [21:48<24:23,  7.43s/it][A
 53%|█████▎    | 218/414 [21:53<22:07,  6.77s/it][A
 53%|█████▎    | 219/414 [21:59<21:08,  6.51s/it][A
 53%|█████▎    | 220/414 [22:05<19:52,  6.15s/it][A
 53%|█████▎    | 221/414 [22:09<18:27,  5.74s/it][A
 54%|█████▎    | 222/414 [22:16<18:58,  5.93s/it][A
 54%|█████▍    | 223/414 [22:22<19:32,  6.14s/it][A
 54%|█████▍    | 224/414 [22:30<20:23,  6.44s/it][A
 54%|█████▍    | 225/414 [22:36<20:42,  6.57s/it][A
 55%|█████▍    | 226/414 [22:43<20:30,  6.54s/it][A
 55%|█████▍    | 227/414 [22:51<21:53,  7.03s/it][A
 55%|█████▌    | 228/414 [22:58<21:25,  6.91s/it][A
 55%|█████▌    | 229/414 [23:03<19:56,  6.47s/it][A
 56%|█████▌    | 230/414 [23:10<20:30,  6.69s/it][A
 56%|█████▌    | 231/414 [23:17<20:17,  6.65s/it][A
 56%|█████▌    | 232/414 [23:23<19:16,  6.35s/it][A
 56%|█████▋    | 233/414 [23:29<19:31,  6.47s/it][A
 57%|█████▋    | 234/414 [23:36<19:47,  6.60s/it][A
 57%|█████▋    | 235/414 [23:44<20:44,  6.95s/it][A
 57%|█████▋    | 236/414 [23:51<20:56,  7.06s/it][A
 57%|█████▋    | 237/414 [23:57<19:32,  6.62s/it][A
 57%|█████▋    | 238/414 [24:01<17:07,  5.84s/it][A
 58%|█████▊    | 239/414 [24:06<16:42,  5.73s/it][A
 58%|█████▊    | 240/414 [24:14<18:03,  6.23s/it][A
 58%|█████▊    | 241/414 [24:19<17:25,  6.04s/it][A
 58%|█████▊    | 242/414 [24:23<15:31,  5.41s/it][A
 59%|█████▊    | 243/414 [24:28<14:50,  5.21s/it][A
 59%|█████▉    | 244/414 [24:34<15:21,  5.42s/it][A
 59%|█████▉    | 245/414 [24:40<15:33,  5.52s/it][A
 59%|█████▉    | 246/414 [24:44<14:41,  5.25s/it][A
 60%|█████▉    | 247/414 [24:50<14:31,  5.22s/it][A
 60%|█████▉    | 248/414 [24:55<14:22,  5.20s/it][A
 60%|██████    | 249/414 [24:59<13:56,  5.07s/it][A
 60%|██████    | 250/414 [25:04<13:22,  4.89s/it][A
 61%|██████    | 251/414 [25:10<13:53,  5.12s/it][A
 61%|██████    | 252/414 [25:16<14:34,  5.40s/it][A
 61%|██████    | 253/414 [25:23<16:02,  5.98s/it][A
 61%|██████▏   | 254/414 [25:31<17:25,  6.53s/it][A
 62%|██████▏   | 255/414 [25:35<15:39,  5.91s/it][A
 62%|██████▏   | 256/414 [25:40<14:43,  5.59s/it][A
 62%|██████▏   | 257/414 [25:45<14:18,  5.47s/it][A
 62%|██████▏   | 258/414 [25:50<13:43,  5.28s/it][A
 63%|██████▎   | 259/414 [25:55<13:06,  5.07s/it][A
 63%|██████▎   | 260/414 [26:01<13:39,  5.32s/it][A
 63%|██████▎   | 261/414 [26:07<14:10,  5.56s/it][A
 63%|██████▎   | 262/414 [26:13<14:58,  5.91s/it][A
 64%|██████▎   | 263/414 [26:21<15:55,  6.33s/it][A
 64%|██████▍   | 264/414 [26:27<16:03,  6.42s/it][A
 64%|██████▍   | 265/414 [26:34<15:48,  6.37s/it][A
 64%|██████▍   | 266/414 [26:40<15:45,  6.39s/it][A
 64%|██████▍   | 267/414 [26:48<16:37,  6.79s/it][A
 65%|██████▍   | 268/414 [26:54<15:55,  6.55s/it][A
 65%|██████▍   | 269/414 [26:58<14:10,  5.87s/it][A
 65%|██████▌   | 270/414 [27:03<13:17,  5.54s/it][A
 65%|██████▌   | 271/414 [27:08<12:59,  5.45s/it][A
 66%|██████▌   | 272/414 [27:14<13:11,  5.58s/it][A
 66%|██████▌   | 273/414 [27:20<13:42,  5.84s/it][A
 66%|██████▌   | 274/414 [27:26<13:43,  5.88s/it][A
 66%|██████▋   | 275/414 [27:32<13:18,  5.74s/it][A
 67%|██████▋   | 276/414 [27:37<12:56,  5.63s/it][A
 67%|██████▋   | 277/414 [27:42<12:26,  5.45s/it][A
 67%|██████▋   | 278/414 [27:46<11:28,  5.07s/it][A
 67%|██████▋   | 279/414 [27:51<11:17,  5.02s/it][A
 68%|██████▊   | 280/414 [27:57<11:41,  5.23s/it][A
 68%|██████▊   | 281/414 [28:02<11:27,  5.17s/it][A
 68%|██████▊   | 282/414 [28:08<11:51,  5.39s/it][A
 68%|██████▊   | 283/414 [28:14<12:13,  5.60s/it][A
 69%|██████▊   | 284/414 [28:19<11:41,  5.40s/it][A
 69%|██████▉   | 285/414 [28:25<12:08,  5.65s/it][A
 69%|██████▉   | 286/414 [28:33<13:10,  6.17s/it][A
 69%|██████▉   | 287/414 [28:40<13:38,  6.45s/it][A
 70%|██████▉   | 288/414 [28:46<13:17,  6.33s/it][A
 70%|██████▉   | 289/414 [28:50<12:07,  5.82s/it][A
 70%|███████   | 290/414 [28:57<12:29,  6.05s/it][A
 70%|███████   | 291/414 [29:04<12:53,  6.29s/it][A
 71%|███████   | 292/414 [29:08<11:31,  5.67s/it][A
 71%|███████   | 293/414 [29:16<12:51,  6.38s/it][A
 71%|███████   | 294/414 [29:25<14:27,  7.23s/it][A
 71%|███████▏  | 295/414 [29:33<14:42,  7.42s/it][A
 71%|███████▏  | 296/414 [29:41<14:38,  7.44s/it][A
 72%|███████▏  | 297/414 [29:46<13:31,  6.93s/it][A
 72%|███████▏  | 298/414 [29:54<13:56,  7.21s/it][A
 72%|███████▏  | 299/414 [30:01<13:37,  7.11s/it][A
 72%|███████▏  | 300/414 [30:06<12:25,  6.54s/it][A
 73%|███████▎  | 301/414 [30:12<11:45,  6.24s/it][A
 73%|███████▎  | 302/414 [30:17<10:47,  5.78s/it][A
 73%|███████▎  | 303/414 [30:22<10:20,  5.59s/it][A
 73%|███████▎  | 304/414 [30:30<11:39,  6.36s/it][A
 74%|███████▎  | 305/414 [30:38<12:35,  6.93s/it][A
 74%|███████▍  | 306/414 [30:44<11:48,  6.56s/it][A
 74%|███████▍  | 307/414 [30:49<11:11,  6.27s/it][A
 74%|███████▍  | 308/414 [30:57<11:31,  6.53s/it][A
 75%|███████▍  | 309/414 [31:04<11:48,  6.75s/it][A
 75%|███████▍  | 310/414 [31:08<10:37,  6.13s/it][A
 75%|███████▌  | 311/414 [31:13<09:56,  5.79s/it][A
 75%|███████▌  | 312/414 [31:19<09:41,  5.70s/it][A
 76%|███████▌  | 313/414 [31:24<09:19,  5.54s/it][A
 76%|███████▌  | 314/414 [31:29<08:50,  5.30s/it][A
 76%|███████▌  | 315/414 [31:35<09:01,  5.47s/it][A
 76%|███████▋  | 316/414 [31:44<10:38,  6.51s/it][A
 77%|███████▋  | 317/414 [31:51<11:08,  6.89s/it][A
 77%|███████▋  | 318/414 [31:57<10:27,  6.54s/it][A
 77%|███████▋  | 319/414 [32:03<10:03,  6.36s/it][A
 77%|███████▋  | 320/414 [32:08<09:17,  5.93s/it][A
 78%|███████▊  | 321/414 [32:14<09:25,  6.08s/it][A
 78%|███████▊  | 322/414 [32:21<09:29,  6.19s/it][A
 78%|███████▊  | 323/414 [32:26<08:43,  5.75s/it][A
 78%|███████▊  | 324/414 [32:30<08:08,  5.43s/it][A
 79%|███████▊  | 325/414 [32:35<07:52,  5.31s/it][A
 79%|███████▊  | 326/414 [32:40<07:32,  5.14s/it][A
 79%|███████▉  | 327/414 [32:47<08:14,  5.69s/it][A
 79%|███████▉  | 328/414 [32:54<08:39,  6.04s/it][A
 79%|███████▉  | 329/414 [33:01<08:48,  6.22s/it][A
 80%|███████▉  | 330/414 [33:09<09:29,  6.78s/it][A
 80%|███████▉  | 331/414 [33:16<09:31,  6.89s/it][A
 80%|████████  | 332/414 [33:23<09:22,  6.87s/it][A
 80%|████████  | 333/414 [33:29<08:57,  6.63s/it][A
 81%|████████  | 334/414 [33:33<07:57,  5.97s/it][A
 81%|████████  | 335/414 [33:38<07:18,  5.55s/it][A
 81%|████████  | 336/414 [33:43<07:04,  5.45s/it][A
 81%|████████▏ | 337/414 [33:48<06:41,  5.21s/it][A
 82%|████████▏ | 338/414 [33:53<06:52,  5.43s/it][A
 82%|████████▏ | 339/414 [34:01<07:31,  6.02s/it][A
 82%|████████▏ | 340/414 [34:09<08:14,  6.68s/it][A
 82%|████████▏ | 341/414 [34:16<08:10,  6.72s/it][A
 83%|████████▎ | 342/414 [34:20<07:09,  5.96s/it][A
 83%|████████▎ | 343/414 [34:27<07:29,  6.33s/it][A
 83%|████████▎ | 344/414 [34:36<08:22,  7.17s/it][A
 83%|████████▎ | 345/414 [34:43<08:03,  7.01s/it][A
 84%|████████▎ | 346/414 [34:48<07:08,  6.31s/it][A
 84%|████████▍ | 347/414 [34:53<06:45,  6.06s/it][A
 84%|████████▍ | 348/414 [34:59<06:34,  5.98s/it][A
 84%|████████▍ | 349/414 [35:04<06:12,  5.73s/it][A
 85%|████████▍ | 350/414 [35:11<06:37,  6.21s/it][A
 85%|████████▍ | 351/414 [35:19<06:52,  6.55s/it][A
 85%|████████▌ | 352/414 [35:24<06:12,  6.02s/it][A
 85%|████████▌ | 353/414 [35:28<05:39,  5.56s/it][A
 86%|████████▌ | 354/414 [35:34<05:31,  5.53s/it][A
 86%|████████▌ | 355/414 [35:39<05:21,  5.44s/it][A
 86%|████████▌ | 356/414 [35:44<05:05,  5.27s/it][A
 86%|████████▌ | 357/414 [35:50<05:14,  5.51s/it][A
 86%|████████▋ | 358/414 [35:57<05:43,  6.14s/it][A
 87%|████████▋ | 359/414 [36:04<05:38,  6.16s/it][A
 87%|████████▋ | 360/414 [36:08<05:09,  5.73s/it][A
 87%|████████▋ | 361/414 [36:13<04:41,  5.32s/it][A
 87%|████████▋ | 362/414 [36:19<04:54,  5.66s/it][A
 88%|████████▊ | 363/414 [36:27<05:17,  6.22s/it][A
 88%|████████▊ | 364/414 [36:32<04:59,  5.99s/it][A
 88%|████████▊ | 365/414 [36:38<04:55,  6.03s/it][A
 88%|████████▊ | 366/414 [36:45<05:06,  6.39s/it][A
 89%|████████▊ | 367/414 [36:52<04:58,  6.35s/it][A
 89%|████████▉ | 368/414 [36:58<04:51,  6.34s/it][A
 89%|████████▉ | 369/414 [37:06<05:02,  6.73s/it][A
 89%|████████▉ | 370/414 [37:12<04:54,  6.68s/it][A
 90%|████████▉ | 371/414 [37:19<04:44,  6.61s/it][A
 90%|████████▉ | 372/414 [37:26<04:41,  6.70s/it][A
 90%|█████████ | 373/414 [37:33<04:45,  6.96s/it][A
 90%|█████████ | 374/414 [37:42<04:55,  7.39s/it][A
 91%|█████████ | 375/414 [37:48<04:32,  6.99s/it][A
 91%|█████████ | 376/414 [37:52<03:53,  6.14s/it][A
 91%|█████████ | 377/414 [37:58<03:44,  6.06s/it][A
 91%|█████████▏| 378/414 [38:05<03:52,  6.45s/it][A
 92%|█████████▏| 379/414 [38:10<03:35,  6.16s/it][A
 92%|█████████▏| 380/414 [38:18<03:40,  6.48s/it][A
 92%|█████████▏| 381/414 [38:28<04:13,  7.69s/it][A
 92%|█████████▏| 382/414 [38:36<04:05,  7.68s/it][A
 93%|█████████▎| 383/414 [38:40<03:29,  6.77s/it][A
 93%|█████████▎| 384/414 [38:46<03:16,  6.54s/it][A
 93%|█████████▎| 385/414 [38:53<03:06,  6.43s/it][A
 93%|█████████▎| 386/414 [38:58<02:47,  5.97s/it][A
 93%|█████████▎| 387/414 [39:02<02:31,  5.62s/it][A
 94%|█████████▎| 388/414 [39:08<02:22,  5.48s/it][A
 94%|█████████▍| 389/414 [39:12<02:10,  5.21s/it][A
 94%|█████████▍| 390/414 [39:16<01:57,  4.91s/it][A
 94%|█████████▍| 391/414 [39:21<01:51,  4.84s/it][A
 95%|█████████▍| 392/414 [39:26<01:46,  4.85s/it][A
 95%|█████████▍| 393/414 [39:33<01:53,  5.39s/it][A
 95%|█████████▌| 394/414 [39:40<01:59,  5.98s/it][A
 95%|█████████▌| 395/414 [39:45<01:48,  5.72s/it][A
 96%|█████████▌| 396/414 [39:52<01:47,  6.00s/it][A
 96%|█████████▌| 397/414 [40:00<01:55,  6.78s/it][A
 96%|█████████▌| 398/414 [40:06<01:44,  6.53s/it][A
 96%|█████████▋| 399/414 [40:10<01:26,  5.73s/it][A
 97%|█████████▋| 400/414 [40:18<01:28,  6.31s/it][A
 97%|█████████▋| 401/414 [40:28<01:38,  7.55s/it][A
 97%|█████████▋| 402/414 [40:34<01:25,  7.16s/it][A
 97%|█████████▋| 403/414 [40:39<01:09,  6.36s/it][A
 98%|█████████▊| 404/414 [40:43<00:57,  5.78s/it][A
 98%|█████████▊| 405/414 [40:48<00:48,  5.36s/it][A
 98%|█████████▊| 406/414 [40:54<00:45,  5.74s/it][A
 98%|█████████▊| 407/414 [41:02<00:44,  6.34s/it][A
 99%|█████████▊| 408/414 [41:08<00:37,  6.17s/it][A
 99%|█████████▉| 409/414 [41:14<00:30,  6.19s/it][A
 99%|█████████▉| 410/414 [41:20<00:24,  6.24s/it][A
 99%|█████████▉| 411/414 [41:25<00:16,  5.66s/it][A
100%|█████████▉| 412/414 [41:31<00:11,  5.91s/it][A
100%|█████████▉| 413/414 [41:38<00:06,  6.20s/it][A
100%|██████████| 414/414 [41:41<00:00,  5.20s/it][A                                                   
                                                 [A{'eval_loss': 1.6312347650527954, 'eval_runtime': 2508.8838, 'eval_samples_per_second': 1.317, 'eval_steps_per_second': 0.165, 'epoch': 0.04}
  4%|▍         | 10/233 [1:03:24<8:00:06, 129.18s/it]
100%|██████████| 414/414 [41:41<00:00,  5.20s/it][A
                                                 [A  5%|▍         | 11/233 [1:05:25<55:08:59, 894.32s/it]  5%|▌         | 12/233 [1:07:27<40:28:36, 659.35s/it]  6%|▌         | 13/233 [1:09:47<30:41:31, 502.23s/it]  6%|▌         | 14/233 [1:11:37<23:20:09, 383.61s/it]  6%|▋         | 15/233 [1:13:49<18:38:31, 307.85s/it]  7%|▋         | 16/233 [1:16:09<15:31:01, 257.42s/it]  7%|▋         | 17/233 [1:17:58<12:46:00, 212.78s/it]  8%|▊         | 18/233 [1:20:05<11:09:42, 186.90s/it]  8%|▊         | 19/233 [1:22:10<10:00:04, 168.24s/it]  9%|▊         | 20/233 [1:24:18<9:14:38, 156.24s/it]                                                      {'loss': 1.4757, 'grad_norm': 0.37688130140304565, 'learning_rate': 7.916666666666666e-05, 'epoch': 0.09}
  9%|▊         | 20/233 [1:24:18<9:14:38, 156.24s/it]
***** Running Evaluation *****

***** Running Evaluation *****

***** Running Evaluation *****

***** Running Evaluation *****

***** Running Evaluation *****
  Num examples = 3305
  Num examples = 3305

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1
  Batch size = 1
  Batch size = 1
  Num examples = 3305
  Batch size = 1
  Num examples = 3305
  Num examples = 3305
  Batch size = 1
  Batch size = 1
[INFO|trainer.py:4643] 2025-12-05 06:16:46,782 >> 
***** Running Evaluation *****
[INFO|trainer.py:4645] 2025-12-05 06:16:46,782 >>   Num examples = 3305
[INFO|trainer.py:4648] 2025-12-05 06:16:46,782 >>   Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

  0%|          | 0/414 [00:00<?, ?it/s][A
  0%|          | 2/414 [00:04<16:18,  2.38s/it][A
  1%|          | 3/414 [00:10<24:48,  3.62s/it][A
  1%|          | 4/414 [00:17<35:30,  5.20s/it][A
  1%|          | 5/414 [00:25<41:24,  6.08s/it][A
  1%|▏         | 6/414 [00:29<37:06,  5.46s/it][A
  2%|▏         | 7/414 [00:34<35:18,  5.21s/it][A
  2%|▏         | 8/414 [00:39<33:50,  5.00s/it][A
  2%|▏         | 9/414 [00:44<35:11,  5.21s/it][A
  2%|▏         | 10/414 [00:52<39:31,  5.87s/it][A
  3%|▎         | 11/414 [00:57<39:07,  5.82s/it][A
  3%|▎         | 12/414 [01:04<40:01,  5.97s/it][A
  3%|▎         | 13/414 [01:11<43:32,  6.52s/it][A
  3%|▎         | 14/414 [01:19<44:48,  6.72s/it][A
  4%|▎         | 15/414 [01:25<44:08,  6.64s/it][A
  4%|▍         | 16/414 [01:32<44:35,  6.72s/it][A
  4%|▍         | 17/414 [01:39<44:35,  6.74s/it][A
  4%|▍         | 18/414 [01:46<44:51,  6.80s/it][A
  5%|▍         | 19/414 [01:54<48:00,  7.29s/it][A
  5%|▍         | 20/414 [02:01<47:11,  7.19s/it][A
  5%|▌         | 21/414 [02:06<43:01,  6.57s/it][A
  5%|▌         | 22/414 [02:12<41:57,  6.42s/it][A
  6%|▌         | 23/414 [02:20<43:56,  6.74s/it][A
  6%|▌         | 24/414 [02:26<42:21,  6.52s/it][A
  6%|▌         | 25/414 [02:32<42:34,  6.57s/it][A
  6%|▋         | 26/414 [02:39<42:36,  6.59s/it][A
  7%|▋         | 27/414 [02:43<38:01,  5.90s/it][A
  7%|▋         | 28/414 [02:49<36:24,  5.66s/it][A
  7%|▋         | 29/414 [02:54<35:29,  5.53s/it][A
  7%|▋         | 30/414 [03:00<36:23,  5.69s/it][A
  7%|▋         | 31/414 [03:06<36:20,  5.69s/it][A
  8%|▊         | 32/414 [03:10<34:02,  5.35s/it][A
  8%|▊         | 33/414 [03:17<36:06,  5.69s/it][A
  8%|▊         | 34/414 [03:22<36:24,  5.75s/it][A
  8%|▊         | 35/414 [03:28<36:12,  5.73s/it][A
  9%|▊         | 36/414 [03:35<37:27,  5.95s/it][A
  9%|▉         | 37/414 [03:41<38:15,  6.09s/it][A
  9%|▉         | 38/414 [03:47<38:29,  6.14s/it][A
  9%|▉         | 39/414 [03:51<34:33,  5.53s/it][A
 10%|▉         | 40/414 [03:57<34:32,  5.54s/it][A
 10%|▉         | 41/414 [04:04<36:30,  5.87s/it][A
 10%|█         | 42/414 [04:09<36:29,  5.89s/it][A
 10%|█         | 43/414 [04:19<42:19,  6.84s/it][A
 11%|█         | 44/414 [04:29<49:19,  8.00s/it][A
 11%|█         | 45/414 [04:36<47:31,  7.73s/it][A
 11%|█         | 46/414 [04:41<41:35,  6.78s/it][A
 11%|█▏        | 47/414 [04:45<36:54,  6.03s/it][A
 12%|█▏        | 48/414 [04:49<32:39,  5.35s/it][A
 12%|█▏        | 49/414 [04:52<29:11,  4.80s/it][A
 12%|█▏        | 50/414 [04:57<27:48,  4.58s/it][A
 12%|█▏        | 51/414 [05:01<26:59,  4.46s/it][A
 13%|█▎        | 52/414 [05:05<26:41,  4.42s/it][A
 13%|█▎        | 53/414 [05:12<31:57,  5.31s/it][A
 13%|█▎        | 54/414 [05:21<37:32,  6.26s/it][A
 13%|█▎        | 55/414 [05:28<38:28,  6.43s/it][A
 14%|█▎        | 56/414 [05:34<38:28,  6.45s/it][A
 14%|█▍        | 57/414 [05:39<35:46,  6.01s/it][A
 14%|█▍        | 58/414 [05:45<35:18,  5.95s/it][A
 14%|█▍        | 59/414 [05:52<37:21,  6.31s/it][A
 14%|█▍        | 60/414 [05:58<36:33,  6.20s/it][A
 15%|█▍        | 61/414 [06:02<33:07,  5.63s/it][A
 15%|█▍        | 62/414 [06:07<30:39,  5.22s/it][A
 15%|█▌        | 63/414 [06:12<30:07,  5.15s/it][A
 15%|█▌        | 64/414 [06:17<29:48,  5.11s/it][A
 16%|█▌        | 65/414 [06:22<29:45,  5.12s/it][A
 16%|█▌        | 66/414 [06:27<30:33,  5.27s/it][A
 16%|█▌        | 67/414 [06:35<33:50,  5.85s/it][A
 16%|█▋        | 68/414 [06:42<35:54,  6.23s/it][A
 17%|█▋        | 69/414 [06:49<37:14,  6.48s/it][A
 17%|█▋        | 70/414 [06:56<38:18,  6.68s/it][A
 17%|█▋        | 71/414 [07:01<35:27,  6.20s/it][A
 17%|█▋        | 72/414 [07:06<33:16,  5.84s/it][A
 18%|█▊        | 73/414 [07:11<31:31,  5.55s/it][A
 18%|█▊        | 74/414 [07:15<29:02,  5.13s/it][A
 18%|█▊        | 75/414 [07:19<27:02,  4.78s/it][A
 18%|█▊        | 76/414 [07:24<26:33,  4.72s/it][A
 19%|█▊        | 77/414 [07:30<28:59,  5.16s/it][A
 19%|█▉        | 78/414 [07:36<30:14,  5.40s/it][A
 19%|█▉        | 79/414 [07:42<31:02,  5.56s/it][A
 19%|█▉        | 80/414 [07:47<30:25,  5.47s/it][A
 20%|█▉        | 81/414 [07:52<28:57,  5.22s/it][A
 20%|█▉        | 82/414 [07:57<29:04,  5.25s/it][A
 20%|██        | 83/414 [08:04<32:34,  5.91s/it][A
 20%|██        | 84/414 [08:12<35:06,  6.38s/it][A
 21%|██        | 85/414 [08:17<32:33,  5.94s/it][A
 21%|██        | 86/414 [08:22<30:44,  5.62s/it][A
 21%|██        | 87/414 [08:26<28:57,  5.31s/it][A
 21%|██▏       | 88/414 [08:31<28:07,  5.18s/it][A
 21%|██▏       | 89/414 [08:37<29:00,  5.36s/it][A
 22%|██▏       | 90/414 [08:44<31:45,  5.88s/it][A
 22%|██▏       | 91/414 [08:49<30:47,  5.72s/it][A
 22%|██▏       | 92/414 [08:55<30:17,  5.64s/it][A
 22%|██▏       | 93/414 [09:00<29:58,  5.60s/it][A
 23%|██▎       | 94/414 [09:06<29:25,  5.52s/it][A
 23%|██▎       | 95/414 [09:11<29:53,  5.62s/it][A
 23%|██▎       | 96/414 [09:18<30:26,  5.74s/it][A
 23%|██▎       | 97/414 [09:23<30:03,  5.69s/it][A
 24%|██▎       | 98/414 [09:27<27:26,  5.21s/it][A
 24%|██▍       | 99/414 [09:32<26:28,  5.04s/it][A
 24%|██▍       | 100/414 [09:38<28:16,  5.40s/it][A
 24%|██▍       | 101/414 [09:46<32:21,  6.20s/it][A
 25%|██▍       | 102/414 [09:53<33:05,  6.36s/it][A
 25%|██▍       | 103/414 [09:58<30:30,  5.88s/it][A
 25%|██▌       | 104/414 [10:02<28:26,  5.51s/it][A
 25%|██▌       | 105/414 [10:08<28:34,  5.55s/it][A
 26%|██▌       | 106/414 [10:15<31:18,  6.10s/it][A
 26%|██▌       | 107/414 [10:21<31:07,  6.08s/it][A
 26%|██▌       | 108/414 [10:26<29:13,  5.73s/it][A
 26%|██▋       | 109/414 [10:31<27:59,  5.51s/it][A
 27%|██▋       | 110/414 [10:37<28:29,  5.62s/it][A
 27%|██▋       | 111/414 [10:43<29:26,  5.83s/it][A
 27%|██▋       | 112/414 [10:48<27:59,  5.56s/it][A
 27%|██▋       | 113/414 [10:55<28:58,  5.78s/it][A
 28%|██▊       | 114/414 [11:02<30:52,  6.18s/it][A
 28%|██▊       | 115/414 [11:08<30:15,  6.07s/it][A
 28%|██▊       | 116/414 [11:14<30:11,  6.08s/it][A
 28%|██▊       | 117/414 [11:22<32:57,  6.66s/it][A
 29%|██▊       | 118/414 [11:31<36:16,  7.35s/it][A
 29%|██▊       | 119/414 [11:40<38:49,  7.90s/it][A
 29%|██▉       | 120/414 [11:48<38:54,  7.94s/it][A
 29%|██▉       | 121/414 [11:55<38:15,  7.83s/it][A
 29%|██▉       | 122/414 [12:03<37:13,  7.65s/it][A
 30%|██▉       | 123/414 [12:09<35:35,  7.34s/it][A
 30%|██▉       | 124/414 [12:17<35:45,  7.40s/it][A
 30%|███       | 125/414 [12:24<35:52,  7.45s/it][A
 30%|███       | 126/414 [12:30<33:11,  6.92s/it][A
 31%|███       | 127/414 [12:37<33:25,  6.99s/it][A
 31%|███       | 128/414 [12:46<35:51,  7.52s/it][A
 31%|███       | 129/414 [12:53<34:44,  7.31s/it][A
 31%|███▏      | 130/414 [13:00<34:03,  7.19s/it][A
 32%|███▏      | 131/414 [13:08<35:44,  7.58s/it][A
 32%|███▏      | 132/414 [13:16<35:33,  7.56s/it][A
 32%|███▏      | 133/414 [13:20<30:49,  6.58s/it][A
 32%|███▏      | 134/414 [13:26<29:48,  6.39s/it][A
 33%|███▎      | 135/414 [13:32<29:48,  6.41s/it][A
 33%|███▎      | 136/414 [13:40<31:36,  6.82s/it][A
 33%|███▎      | 137/414 [13:49<34:51,  7.55s/it][A
 33%|███▎      | 138/414 [13:57<34:02,  7.40s/it][A
 34%|███▎      | 139/414 [14:01<30:17,  6.61s/it][A
 34%|███▍      | 140/414 [14:06<27:34,  6.04s/it][A
 34%|███▍      | 141/414 [14:12<26:48,  5.89s/it][A
 34%|███▍      | 142/414 [14:18<27:44,  6.12s/it][A
 35%|███▍      | 143/414 [14:24<27:42,  6.13s/it][A
 35%|███▍      | 144/414 [14:29<25:22,  5.64s/it][A
 35%|███▌      | 145/414 [14:33<23:38,  5.27s/it][A
 35%|███▌      | 146/414 [14:37<22:03,  4.94s/it][A
 36%|███▌      | 147/414 [14:42<21:48,  4.90s/it][A
 36%|███▌      | 148/414 [14:48<22:22,  5.05s/it][A
 36%|███▌      | 149/414 [14:53<22:03,  5.00s/it][A
 36%|███▌      | 150/414 [14:57<21:33,  4.90s/it][A
 36%|███▋      | 151/414 [15:01<20:30,  4.68s/it][A
 37%|███▋      | 152/414 [15:06<20:52,  4.78s/it][A
 37%|███▋      | 153/414 [15:12<21:38,  4.97s/it][A
 37%|███▋      | 154/414 [15:16<20:37,  4.76s/it][A
 37%|███▋      | 155/414 [15:20<19:54,  4.61s/it][A
 38%|███▊      | 156/414 [15:26<20:40,  4.81s/it][A
 38%|███▊      | 157/414 [15:32<22:37,  5.28s/it][A
 38%|███▊      | 158/414 [15:38<24:01,  5.63s/it][A
 38%|███▊      | 159/414 [15:45<24:49,  5.84s/it][A
 39%|███▊      | 160/414 [15:50<24:32,  5.80s/it][A
 39%|███▉      | 161/414 [15:57<25:41,  6.09s/it][A
 39%|███▉      | 162/414 [16:03<25:37,  6.10s/it][A
 39%|███▉      | 163/414 [16:09<24:57,  5.96s/it][A
 40%|███▉      | 164/414 [16:16<26:14,  6.30s/it][A
 40%|███▉      | 165/414 [16:24<27:38,  6.66s/it][A
 40%|████      | 166/414 [16:31<28:31,  6.90s/it][A
 40%|████      | 167/414 [16:38<28:04,  6.82s/it][A
 41%|████      | 168/414 [16:43<26:26,  6.45s/it][A
 41%|████      | 169/414 [16:48<24:10,  5.92s/it][A
 41%|████      | 170/414 [16:53<22:38,  5.57s/it][A
 41%|████▏     | 171/414 [16:58<22:22,  5.53s/it][A
 42%|████▏     | 172/414 [17:04<23:01,  5.71s/it][A
 42%|████▏     | 173/414 [17:10<23:31,  5.86s/it][A
 42%|████▏     | 174/414 [17:16<23:38,  5.91s/it][A
 42%|████▏     | 175/414 [17:23<23:46,  5.97s/it][A
 43%|████▎     | 176/414 [17:27<22:16,  5.61s/it][A
 43%|████▎     | 177/414 [17:33<22:26,  5.68s/it][A
 43%|████▎     | 178/414 [17:40<23:24,  5.95s/it][A
 43%|████▎     | 179/414 [17:45<22:34,  5.76s/it][A
 43%|████▎     | 180/414 [17:51<22:20,  5.73s/it][A
 44%|████▎     | 181/414 [17:57<22:51,  5.89s/it][A
 44%|████▍     | 182/414 [18:03<22:33,  5.84s/it][A
 44%|████▍     | 183/414 [18:09<23:07,  6.01s/it][A
 44%|████▍     | 184/414 [18:15<22:49,  5.95s/it][A
 45%|████▍     | 185/414 [18:20<21:34,  5.65s/it][A
 45%|████▍     | 186/414 [18:27<22:54,  6.03s/it][A
 45%|████▌     | 187/414 [18:37<27:22,  7.24s/it][A
 45%|████▌     | 188/414 [18:47<29:58,  7.96s/it][A
 46%|████▌     | 189/414 [18:55<30:34,  8.15s/it][A
 46%|████▌     | 190/414 [19:03<30:25,  8.15s/it][A
 46%|████▌     | 191/414 [19:09<27:53,  7.51s/it][A
 46%|████▋     | 192/414 [19:15<25:51,  6.99s/it][A
 47%|████▋     | 193/414 [19:20<22:59,  6.24s/it][A
 47%|████▋     | 194/414 [19:25<21:48,  5.95s/it][A
 47%|████▋     | 195/414 [19:30<20:42,  5.67s/it][A
 47%|████▋     | 196/414 [19:35<20:10,  5.55s/it][A
 48%|████▊     | 197/414 [19:42<21:37,  5.98s/it][A
 48%|████▊     | 198/414 [19:48<21:41,  6.02s/it][A
 48%|████▊     | 199/414 [19:53<20:08,  5.62s/it][A
 48%|████▊     | 200/414 [19:59<20:07,  5.64s/it][A
 49%|████▊     | 201/414 [20:05<21:22,  6.02s/it][A
 49%|████▉     | 202/414 [20:12<21:44,  6.15s/it][A
 49%|████▉     | 203/414 [20:17<20:10,  5.74s/it][A
 49%|████▉     | 204/414 [20:21<18:38,  5.33s/it][A
 50%|████▉     | 205/414 [20:26<17:41,  5.08s/it][A
 50%|████▉     | 206/414 [20:31<18:14,  5.26s/it][A
 50%|█████     | 207/414 [20:37<18:33,  5.38s/it][A
 50%|█████     | 208/414 [20:43<19:06,  5.56s/it][A
 50%|█████     | 209/414 [20:51<21:15,  6.22s/it][A
 51%|█████     | 210/414 [20:57<21:27,  6.31s/it][A
 51%|█████     | 211/414 [21:02<20:10,  5.97s/it][A
 51%|█████     | 212/414 [21:08<19:53,  5.91s/it][A
 51%|█████▏    | 213/414 [21:16<21:33,  6.44s/it][A
 52%|█████▏    | 214/414 [21:23<21:51,  6.56s/it][A
 52%|█████▏    | 215/414 [21:29<21:49,  6.58s/it][A
 52%|█████▏    | 216/414 [21:38<24:01,  7.28s/it][A
 52%|█████▏    | 217/414 [21:46<24:20,  7.42s/it][A
 53%|█████▎    | 218/414 [21:51<21:59,  6.73s/it][A
 53%|█████▎    | 219/414 [21:57<21:07,  6.50s/it][A
 53%|█████▎    | 220/414 [22:02<19:54,  6.16s/it][A
 53%|█████▎    | 221/414 [22:07<18:30,  5.75s/it][A
 54%|█████▎    | 222/414 [22:14<19:01,  5.94s/it][A
 54%|█████▍    | 223/414 [22:20<19:34,  6.15s/it][A
 54%|█████▍    | 224/414 [22:27<20:19,  6.42s/it][A
 54%|█████▍    | 225/414 [22:34<20:37,  6.55s/it][A
 55%|█████▍    | 226/414 [22:41<20:24,  6.51s/it][A
 55%|█████▍    | 227/414 [22:49<21:51,  7.01s/it][A
 55%|█████▌    | 228/414 [22:55<21:21,  6.89s/it][A
 55%|█████▌    | 229/414 [23:01<19:51,  6.44s/it][A
 56%|█████▌    | 230/414 [23:08<20:24,  6.65s/it][A
 56%|█████▌    | 231/414 [23:14<20:14,  6.63s/it][A
 56%|█████▌    | 232/414 [23:20<19:13,  6.34s/it][A
 56%|█████▋    | 233/414 [23:27<19:28,  6.45s/it][A
 57%|█████▋    | 234/414 [23:34<19:45,  6.58s/it][A
 57%|█████▋    | 235/414 [23:41<20:41,  6.93s/it][A
 57%|█████▋    | 236/414 [23:49<20:54,  7.05s/it][A
 57%|█████▋    | 237/414 [23:54<19:31,  6.62s/it][A
 57%|█████▋    | 238/414 [23:58<17:05,  5.83s/it][A
 58%|█████▊    | 239/414 [24:04<16:40,  5.72s/it][A
 58%|█████▊    | 240/414 [24:11<18:03,  6.23s/it][A
 58%|█████▊    | 241/414 [24:17<17:25,  6.04s/it][A
 58%|█████▊    | 242/414 [24:21<15:30,  5.41s/it][A
 59%|█████▊    | 243/414 [24:26<14:52,  5.22s/it][A
 59%|█████▉    | 244/414 [24:32<15:23,  5.43s/it][A
 59%|█████▉    | 245/414 [24:37<15:36,  5.54s/it][A
 59%|█████▉    | 246/414 [24:42<14:46,  5.28s/it][A
 60%|█████▉    | 247/414 [24:47<14:33,  5.23s/it][A
 60%|█████▉    | 248/414 [24:52<14:24,  5.21s/it][A
 60%|██████    | 249/414 [24:57<13:58,  5.08s/it][A
 60%|██████    | 250/414 [25:01<13:20,  4.88s/it][A
 61%|██████    | 251/414 [25:07<13:52,  5.10s/it][A
 61%|██████    | 252/414 [25:13<14:33,  5.39s/it][A
 61%|██████    | 253/414 [25:20<16:03,  5.99s/it][A
 61%|██████▏   | 254/414 [25:28<17:25,  6.53s/it][A
 62%|██████▏   | 255/414 [25:33<15:38,  5.90s/it][A
 62%|██████▏   | 256/414 [25:38<14:40,  5.57s/it][A
 62%|██████▏   | 257/414 [25:43<14:15,  5.45s/it][A
 62%|██████▏   | 258/414 [25:48<13:41,  5.27s/it][A
 63%|██████▎   | 259/414 [25:52<13:04,  5.06s/it][A
 63%|██████▎   | 260/414 [25:58<13:37,  5.31s/it][A
 63%|██████▎   | 261/414 [26:04<14:09,  5.56s/it][A
 63%|██████▎   | 262/414 [26:11<14:57,  5.91s/it][A
 64%|██████▎   | 263/414 [26:18<15:53,  6.31s/it][A
 64%|██████▍   | 264/414 [26:25<15:58,  6.39s/it][A
 64%|██████▍   | 265/414 [26:31<15:41,  6.32s/it][A
 64%|██████▍   | 266/414 [26:37<15:39,  6.35s/it][A
 64%|██████▍   | 267/414 [26:45<16:34,  6.77s/it][A
 65%|██████▍   | 268/414 [26:51<15:56,  6.55s/it][A
 65%|██████▍   | 269/414 [26:55<14:08,  5.85s/it][A
 65%|██████▌   | 270/414 [27:00<13:15,  5.52s/it][A
 65%|██████▌   | 271/414 [27:05<12:56,  5.43s/it][A
 66%|██████▌   | 272/414 [27:11<13:10,  5.57s/it][A
 66%|██████▌   | 273/414 [27:18<13:42,  5.83s/it][A
 66%|██████▌   | 274/414 [27:24<13:45,  5.90s/it][A
 66%|██████▋   | 275/414 [27:29<13:19,  5.75s/it][A
 67%|██████▋   | 276/414 [27:34<12:56,  5.63s/it][A
 67%|██████▋   | 277/414 [27:39<12:27,  5.45s/it][A
 67%|██████▋   | 278/414 [27:44<11:30,  5.08s/it][A
 67%|██████▋   | 279/414 [27:48<11:16,  5.01s/it][A
 68%|██████▊   | 280/414 [27:54<11:39,  5.22s/it][A
 68%|██████▊   | 281/414 [27:59<11:26,  5.16s/it][A
 68%|██████▊   | 282/414 [28:05<11:51,  5.39s/it][A
 68%|██████▊   | 283/414 [28:11<12:13,  5.60s/it][A
 69%|██████▊   | 284/414 [28:16<11:42,  5.40s/it][A
 69%|██████▉   | 285/414 [28:22<12:05,  5.62s/it][A
 69%|██████▉   | 286/414 [28:30<13:06,  6.14s/it][A
 69%|██████▉   | 287/414 [28:37<13:35,  6.42s/it][A
 70%|██████▉   | 288/414 [28:43<13:15,  6.31s/it][A
 70%|██████▉   | 289/414 [28:47<12:07,  5.82s/it][A
 70%|███████   | 290/414 [28:54<12:21,  5.98s/it][A
 70%|███████   | 291/414 [29:01<12:48,  6.25s/it][A
 71%|███████   | 292/414 [29:05<11:29,  5.65s/it][A
 71%|███████   | 293/414 [29:13<12:47,  6.35s/it][A
 71%|███████   | 294/414 [29:22<14:24,  7.20s/it][A
 71%|███████▏  | 295/414 [29:30<14:40,  7.40s/it][A
 71%|███████▏  | 296/414 [29:37<14:37,  7.43s/it][A
 72%|███████▏  | 297/414 [29:43<13:28,  6.91s/it][A
 72%|███████▏  | 298/414 [29:51<13:54,  7.19s/it][A
 72%|███████▏  | 299/414 [29:58<13:36,  7.10s/it][A
 72%|███████▏  | 300/414 [30:03<12:24,  6.53s/it][A
 73%|███████▎  | 301/414 [30:09<11:44,  6.24s/it][A
 73%|███████▎  | 302/414 [30:13<10:46,  5.77s/it][A
 73%|███████▎  | 303/414 [30:19<10:20,  5.59s/it][A
 73%|███████▎  | 304/414 [30:27<11:38,  6.35s/it][A
 74%|███████▎  | 305/414 [30:35<12:30,  6.89s/it][A
 74%|███████▍  | 306/414 [30:40<11:45,  6.53s/it][A
 74%|███████▍  | 307/414 [30:46<11:07,  6.24s/it][A
 74%|███████▍  | 308/414 [30:53<11:28,  6.49s/it][A
 75%|███████▍  | 309/414 [31:00<11:46,  6.73s/it][A
 75%|███████▍  | 310/414 [31:05<10:39,  6.15s/it][A
 75%|███████▌  | 311/414 [31:10<09:58,  5.81s/it][A
 75%|███████▌  | 312/414 [31:16<09:41,  5.70s/it][A
 76%|███████▌  | 313/414 [31:21<09:18,  5.53s/it][A
 76%|███████▌  | 314/414 [31:26<08:50,  5.30s/it][A
 76%|███████▌  | 315/414 [31:31<09:01,  5.47s/it][A
 76%|███████▋  | 316/414 [31:40<10:36,  6.49s/it][A
 77%|███████▋  | 317/414 [31:48<11:03,  6.84s/it][A
 77%|███████▋  | 318/414 [31:54<10:26,  6.52s/it][A
 77%|███████▋  | 319/414 [32:00<10:03,  6.35s/it][A
 77%|███████▋  | 320/414 [32:05<09:16,  5.92s/it][A
 78%|███████▊  | 321/414 [32:11<09:25,  6.08s/it][A
 78%|███████▊  | 322/414 [32:18<09:30,  6.20s/it][A
 78%|███████▊  | 323/414 [32:22<08:43,  5.75s/it][A
 78%|███████▊  | 324/414 [32:27<08:09,  5.44s/it][A
 79%|███████▊  | 325/414 [32:32<07:51,  5.30s/it][A
 79%|███████▊  | 326/414 [32:37<07:33,  5.15s/it][A
 79%|███████▉  | 327/414 [32:44<08:15,  5.70s/it][A
 79%|███████▉  | 328/414 [32:51<08:39,  6.04s/it][A
 79%|███████▉  | 329/414 [32:57<08:46,  6.20s/it][A
 80%|███████▉  | 330/414 [33:05<09:27,  6.76s/it][A
 80%|███████▉  | 331/414 [33:12<09:31,  6.89s/it][A
 80%|████████  | 332/414 [33:19<09:24,  6.88s/it][A
 80%|████████  | 333/414 [33:25<08:58,  6.64s/it][A
 81%|████████  | 334/414 [33:30<07:58,  5.98s/it][A
 81%|████████  | 335/414 [33:34<07:18,  5.55s/it][A
 81%|████████  | 336/414 [33:39<07:04,  5.44s/it][A
 81%|████████▏ | 337/414 [33:44<06:40,  5.20s/it][A
 82%|████████▏ | 338/414 [33:50<06:51,  5.41s/it][A
 82%|████████▏ | 339/414 [33:57<07:30,  6.01s/it][A
 82%|████████▏ | 340/414 [34:06<08:13,  6.67s/it][A
 82%|████████▏ | 341/414 [34:12<08:08,  6.69s/it][A
 83%|████████▎ | 342/414 [34:17<07:08,  5.95s/it][A
 83%|████████▎ | 343/414 [34:24<07:27,  6.31s/it][A
 83%|████████▎ | 344/414 [34:33<08:20,  7.15s/it][A
 83%|████████▎ | 345/414 [34:39<08:02,  6.99s/it][A
 84%|████████▎ | 346/414 [34:44<07:10,  6.34s/it][A
 84%|████████▍ | 347/414 [34:50<06:47,  6.08s/it][A
 84%|████████▍ | 348/414 [34:56<06:35,  5.99s/it][A
 84%|████████▍ | 349/414 [35:01<06:12,  5.73s/it][A
 85%|████████▍ | 350/414 [35:08<06:36,  6.19s/it][A
 85%|████████▍ | 351/414 [35:15<06:51,  6.53s/it][A
 85%|████████▌ | 352/414 [35:20<06:11,  6.00s/it][A
 85%|████████▌ | 353/414 [35:24<05:38,  5.55s/it][A
 86%|████████▌ | 354/414 [35:30<05:30,  5.50s/it][A
 86%|████████▌ | 355/414 [35:35<05:20,  5.43s/it][A
 86%|████████▌ | 356/414 [35:40<05:04,  5.25s/it][A
 86%|████████▌ | 357/414 [35:46<05:12,  5.48s/it][A
 86%|████████▋ | 358/414 [35:54<05:42,  6.11s/it][A
 87%|████████▋ | 359/414 [36:00<05:37,  6.14s/it][A
 87%|████████▋ | 360/414 [36:04<05:07,  5.70s/it][A
 87%|████████▋ | 361/414 [36:09<04:40,  5.29s/it][A
 87%|████████▋ | 362/414 [36:15<04:53,  5.65s/it][A
 88%|████████▊ | 363/414 [36:23<05:17,  6.23s/it][A
 88%|████████▊ | 364/414 [36:28<04:59,  5.98s/it][A
 88%|████████▊ | 365/414 [36:34<04:55,  6.04s/it][A
 88%|████████▊ | 366/414 [36:42<05:07,  6.41s/it][A
 89%|████████▊ | 367/414 [36:48<04:59,  6.37s/it][A
 89%|████████▉ | 368/414 [36:54<04:52,  6.35s/it][A
 89%|████████▉ | 369/414 [37:02<05:03,  6.74s/it][A
 89%|████████▉ | 370/414 [37:09<04:54,  6.69s/it][A
 90%|████████▉ | 371/414 [37:15<04:43,  6.60s/it][A
 90%|████████▉ | 372/414 [37:22<04:40,  6.68s/it][A
 90%|█████████ | 373/414 [37:29<04:44,  6.93s/it][A
 90%|█████████ | 374/414 [37:38<04:54,  7.36s/it][A
 91%|█████████ | 375/414 [37:44<04:32,  6.98s/it][A
 91%|█████████ | 376/414 [37:48<03:53,  6.14s/it][A
 91%|█████████ | 377/414 [37:54<03:44,  6.07s/it][A
 91%|█████████▏| 378/414 [38:01<03:52,  6.46s/it][A
 92%|█████████▏| 379/414 [38:07<03:35,  6.17s/it][A
 92%|█████████▏| 380/414 [38:14<03:42,  6.54s/it][A
 92%|█████████▏| 381/414 [38:25<04:15,  7.73s/it][A
 92%|█████████▏| 382/414 [38:32<04:06,  7.71s/it][A
 93%|█████████▎| 383/414 [38:37<03:31,  6.81s/it][A
 93%|█████████▎| 384/414 [38:43<03:16,  6.57s/it][A
 93%|█████████▎| 385/414 [38:49<03:06,  6.44s/it][A
 93%|█████████▎| 386/414 [38:54<02:47,  5.99s/it][A
 93%|█████████▎| 387/414 [38:59<02:31,  5.62s/it][A
 94%|█████████▎| 388/414 [39:04<02:22,  5.47s/it][A
 94%|█████████▍| 389/414 [39:09<02:10,  5.20s/it][A
 94%|█████████▍| 390/414 [39:13<01:57,  4.90s/it][A
 94%|█████████▍| 391/414 [39:17<01:50,  4.82s/it][A
 95%|█████████▍| 392/414 [39:22<01:46,  4.84s/it][A
 95%|█████████▍| 393/414 [39:29<01:52,  5.38s/it][A
 95%|█████████▌| 394/414 [39:36<01:59,  5.97s/it][A
 95%|█████████▌| 395/414 [39:41<01:48,  5.72s/it][A
 96%|█████████▌| 396/414 [39:48<01:48,  6.01s/it][A
 96%|█████████▌| 397/414 [39:56<01:54,  6.75s/it][A
 96%|█████████▌| 398/414 [40:02<01:44,  6.52s/it][A
 96%|█████████▋| 399/414 [40:06<01:25,  5.73s/it][A
 97%|█████████▋| 400/414 [40:14<01:28,  6.31s/it][A
 97%|█████████▋| 401/414 [40:24<01:37,  7.51s/it][A
 97%|█████████▋| 402/414 [40:31<01:25,  7.13s/it][A
 97%|█████████▋| 403/414 [40:35<01:09,  6.35s/it][A
 98%|█████████▊| 404/414 [40:40<00:57,  5.76s/it][A
 98%|█████████▊| 405/414 [40:44<00:48,  5.36s/it][A
 98%|█████████▊| 406/414 [40:51<00:45,  5.74s/it][A
 98%|█████████▊| 407/414 [40:58<00:44,  6.33s/it][A
 99%|█████████▊| 408/414 [41:04<00:37,  6.17s/it][A
 99%|█████████▉| 409/414 [41:10<00:30,  6.17s/it][A
 99%|█████████▉| 410/414 [41:17<00:24,  6.24s/it][A
 99%|█████████▉| 411/414 [41:21<00:17,  5.67s/it][A
100%|█████████▉| 412/414 [41:27<00:11,  5.90s/it][A
100%|█████████▉| 413/414 [41:34<00:06,  6.20s/it][A
100%|██████████| 414/414 [41:37<00:00,  5.21s/it][A                                                     
                                                 [A{'eval_loss': 1.2971813678741455, 'eval_runtime': 2504.9156, 'eval_samples_per_second': 1.319, 'eval_steps_per_second': 0.165, 'epoch': 0.09}
  9%|▊         | 20/233 [2:06:03<9:14:38, 156.24s/it]
100%|██████████| 414/414 [41:38<00:00,  5.21s/it][A
                                                 [A  9%|▉         | 21/233 [2:07:56<52:42:46, 895.13s/it]  9%|▉         | 22/233 [2:10:10<39:05:07, 666.86s/it] 10%|▉         | 23/233 [2:12:24<29:33:41, 506.77s/it] 10%|█         | 24/233 [2:14:35<22:53:16, 394.24s/it] 11%|█         | 25/233 [2:16:41<18:06:53, 313.53s/it] 11%|█         | 26/233 [2:18:59<14:59:52, 260.84s/it] 12%|█▏        | 27/233 [2:20:56<12:27:32, 217.73s/it] 12%|█▏        | 28/233 [2:22:52<10:40:13, 187.38s/it] 12%|█▏        | 29/233 [2:25:12<9:48:33, 173.10s/it]  13%|█▎        | 30/233 [2:27:23<9:02:22, 160.31s/it]                                                     {'loss': 1.2282, 'grad_norm': 0.33568039536476135, 'learning_rate': 9.985884939360872e-05, 'epoch': 0.13}
 13%|█▎        | 30/233 [2:27:23<9:02:22, 160.31s/it]
***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****

***** Running Evaluation *****
[INFO|trainer.py:4643] 2025-12-05 07:19:51,399 >> 
***** Running Evaluation *****

***** Running Evaluation *****
  Num examples = 3305

***** Running Evaluation *****
  Batch size = 1
  Num examples = 3305

***** Running Evaluation *****
  Batch size = 1
  Num examples = 3305
  Num examples = 3305
  Batch size = 1
  Batch size = 1
  Num examples = 3305
[INFO|trainer.py:4645] 2025-12-05 07:19:51,399 >>   Num examples = 3305
  Batch size = 1
[INFO|trainer.py:4648] 2025-12-05 07:19:51,399 >>   Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

  0%|          | 0/414 [00:00<?, ?it/s][A
  0%|          | 2/414 [00:04<16:16,  2.37s/it][A
  1%|          | 3/414 [00:10<24:47,  3.62s/it][A
  1%|          | 4/414 [00:17<35:07,  5.14s/it][A
  1%|          | 5/414 [00:25<41:00,  6.02s/it][A
  1%|▏         | 6/414 [00:29<36:53,  5.43s/it][A
  2%|▏         | 7/414 [00:34<35:09,  5.18s/it][A
  2%|▏         | 8/414 [00:38<33:43,  4.98s/it][A
  2%|▏         | 9/414 [00:44<35:10,  5.21s/it][A
  2%|▏         | 10/414 [00:52<39:38,  5.89s/it][A
  3%|▎         | 11/414 [00:57<39:11,  5.83s/it][A
  3%|▎         | 12/414 [01:04<40:09,  5.99s/it][A
  3%|▎         | 13/414 [01:11<43:37,  6.53s/it][A
  3%|▎         | 14/414 [01:19<44:54,  6.74s/it][A
  4%|▎         | 15/414 [01:25<44:14,  6.65s/it][A
  4%|▍         | 16/414 [01:32<44:39,  6.73s/it][A
  4%|▍         | 17/414 [01:39<44:40,  6.75s/it][A
  4%|▍         | 18/414 [01:46<44:55,  6.81s/it][A
  5%|▍         | 19/414 [01:54<48:00,  7.29s/it][A
  5%|▍         | 20/414 [02:01<47:25,  7.22s/it][A
  5%|▌         | 21/414 [02:06<43:12,  6.60s/it][A
  5%|▌         | 22/414 [02:12<42:15,  6.47s/it][A
  6%|▌         | 23/414 [02:20<44:06,  6.77s/it][A
  6%|▌         | 24/414 [02:26<42:35,  6.55s/it][A
  6%|▌         | 25/414 [02:33<42:47,  6.60s/it][A
  6%|▋         | 26/414 [02:39<42:46,  6.61s/it][A
  7%|▋         | 27/414 [02:44<38:08,  5.91s/it][A
  7%|▋         | 28/414 [02:49<36:28,  5.67s/it][A
  7%|▋         | 29/414 [02:54<35:37,  5.55s/it][A
  7%|▋         | 30/414 [03:00<36:34,  5.72s/it][A
  7%|▋         | 31/414 [03:06<36:25,  5.71s/it][A
  8%|▊         | 32/414 [03:10<34:05,  5.35s/it][A
  8%|▊         | 33/414 [03:17<36:09,  5.69s/it][A
  8%|▊         | 34/414 [03:23<36:25,  5.75s/it][A
  8%|▊         | 35/414 [03:28<36:12,  5.73s/it][A
  9%|▊         | 36/414 [03:35<37:29,  5.95s/it][A
  9%|▉         | 37/414 [03:41<38:21,  6.11s/it][A
  9%|▉         | 38/414 [03:48<38:36,  6.16s/it][A
  9%|▉         | 39/414 [03:52<34:41,  5.55s/it][A
 10%|▉         | 40/414 [03:57<34:36,  5.55s/it][A
 10%|▉         | 41/414 [04:04<36:30,  5.87s/it][A
 10%|█         | 42/414 [04:10<36:31,  5.89s/it][A
 10%|█         | 43/414 [04:19<42:18,  6.84s/it][A
 11%|█         | 44/414 [04:30<49:17,  7.99s/it][A
 11%|█         | 45/414 [04:37<47:30,  7.73s/it][A
 11%|█         | 46/414 [04:41<41:38,  6.79s/it][A
 11%|█▏        | 47/414 [04:46<36:59,  6.05s/it][A
 12%|█▏        | 48/414 [04:49<32:45,  5.37s/it][A
 12%|█▏        | 49/414 [04:53<29:13,  4.80s/it][A
 12%|█▏        | 50/414 [04:57<27:45,  4.58s/it][A
 12%|█▏        | 51/414 [05:01<26:58,  4.46s/it][A
 13%|█▎        | 52/414 [05:05<26:39,  4.42s/it][A
 13%|█▎        | 53/414 [05:13<32:02,  5.32s/it][A
 13%|█▎        | 54/414 [05:21<37:34,  6.26s/it][A
 13%|█▎        | 55/414 [05:28<38:34,  6.45s/it][A
 14%|█▎        | 56/414 [05:35<38:35,  6.47s/it][A
 14%|█▍        | 57/414 [05:40<35:49,  6.02s/it][A
 14%|█▍        | 58/414 [05:46<35:21,  5.96s/it][A
 14%|█▍        | 59/414 [05:53<37:24,  6.32s/it][A
 14%|█▍        | 60/414 [05:59<36:34,  6.20s/it][A
 15%|█▍        | 61/414 [06:03<33:09,  5.64s/it][A
 15%|█▍        | 62/414 [06:07<30:43,  5.24s/it][A
 15%|█▌        | 63/414 [06:12<30:08,  5.15s/it][A
 15%|█▌        | 64/414 [06:17<29:49,  5.11s/it][A
 16%|█▌        | 65/414 [06:22<29:46,  5.12s/it][A
 16%|█▌        | 66/414 [06:28<30:33,  5.27s/it][A
 16%|█▌        | 67/414 [06:35<33:50,  5.85s/it][A
 16%|█▋        | 68/414 [06:42<35:57,  6.24s/it][A
 17%|█▋        | 69/414 [06:49<37:17,  6.49s/it][A
 17%|█▋        | 70/414 [06:57<38:22,  6.69s/it][A
 17%|█▋        | 71/414 [07:02<35:26,  6.20s/it][A
 17%|█▋        | 72/414 [07:07<33:17,  5.84s/it][A
 18%|█▊        | 73/414 [07:11<31:32,  5.55s/it][A
 18%|█▊        | 74/414 [07:16<29:02,  5.12s/it][A
 18%|█▊        | 75/414 [07:20<27:01,  4.78s/it][A
 18%|█▊        | 76/414 [07:24<26:27,  4.70s/it][A
 19%|█▊        | 77/414 [07:30<28:49,  5.13s/it][A
 19%|█▉        | 78/414 [07:36<30:06,  5.38s/it][A
 19%|█▉        | 79/414 [07:42<30:54,  5.54s/it][A
 19%|█▉        | 80/414 [07:47<30:23,  5.46s/it][A
 20%|█▉        | 81/414 [07:52<29:00,  5.23s/it][A
 20%|█▉        | 82/414 [07:57<29:06,  5.26s/it][A
 20%|██        | 83/414 [08:05<32:37,  5.91s/it][A
 20%|██        | 84/414 [08:12<35:08,  6.39s/it][A
 21%|██        | 85/414 [08:17<32:36,  5.95s/it][A
 21%|██        | 86/414 [08:22<30:47,  5.63s/it][A
 21%|██        | 87/414 [08:27<28:59,  5.32s/it][A
 21%|██▏       | 88/414 [08:32<28:10,  5.19s/it][A
 21%|██▏       | 89/414 [08:37<28:59,  5.35s/it][A
 22%|██▏       | 90/414 [08:44<31:48,  5.89s/it][A
 22%|██▏       | 91/414 [08:50<30:50,  5.73s/it][A
 22%|██▏       | 92/414 [08:55<30:20,  5.65s/it][A
 22%|██▏       | 93/414 [09:01<30:04,  5.62s/it][A
 23%|██▎       | 94/414 [09:06<29:28,  5.53s/it][A
 23%|██▎       | 95/414 [09:12<29:58,  5.64s/it][A
 23%|██▎       | 96/414 [09:18<30:30,  5.76s/it][A
 23%|██▎       | 97/414 [09:24<30:11,  5.72s/it][A
 24%|██▎       | 98/414 [09:28<27:33,  5.23s/it][A
 24%|██▍       | 99/414 [09:33<26:35,  5.06s/it][A
 24%|██▍       | 100/414 [09:39<28:27,  5.44s/it][A
 24%|██▍       | 101/414 [09:47<32:31,  6.24s/it][A
 25%|██▍       | 102/414 [09:54<33:15,  6.40s/it][A
 25%|██▍       | 103/414 [09:58<30:36,  5.91s/it][A
 25%|██▌       | 104/414 [10:03<28:29,  5.52s/it][A
 25%|██▌       | 105/414 [10:09<28:38,  5.56s/it][A
 26%|██▌       | 106/414 [10:16<31:21,  6.11s/it][A
 26%|██▌       | 107/414 [10:22<31:10,  6.09s/it][A
 26%|██▌       | 108/414 [10:27<29:15,  5.74s/it][A
 26%|██▋       | 109/414 [10:32<28:00,  5.51s/it][A
 27%|██▋       | 110/414 [10:38<28:30,  5.63s/it][A
 27%|██▋       | 111/414 [10:44<29:25,  5.83s/it][A
 27%|██▋       | 112/414 [10:49<27:58,  5.56s/it][A
 27%|██▋       | 113/414 [10:56<29:05,  5.80s/it][A
 28%|██▊       | 114/414 [11:03<30:59,  6.20s/it][A
 28%|██▊       | 115/414 [11:08<30:21,  6.09s/it][A
 28%|██▊       | 116/414 [11:15<30:20,  6.11s/it][A
 28%|██▊       | 117/414 [11:23<33:06,  6.69s/it][A
 29%|██▊       | 118/414 [11:32<36:23,  7.38s/it][A
 29%|██▊       | 119/414 [11:41<38:53,  7.91s/it][A
 29%|██▉       | 120/414 [11:49<39:02,  7.97s/it][A
 29%|██▉       | 121/414 [11:57<38:26,  7.87s/it][A
 29%|██▉       | 122/414 [12:04<37:23,  7.68s/it][A
 30%|██▉       | 123/414 [12:10<35:40,  7.36s/it][A
 30%|██▉       | 124/414 [12:18<35:50,  7.41s/it][A
 30%|███       | 125/414 [12:26<35:56,  7.46s/it][A
 30%|███       | 126/414 [12:31<33:21,  6.95s/it][A
 31%|███       | 127/414 [12:38<33:31,  7.01s/it][A
 31%|███       | 128/414 [12:47<36:04,  7.57s/it][A
 31%|███       | 129/414 [12:54<34:51,  7.34s/it][A
 31%|███▏      | 130/414 [13:01<34:05,  7.20s/it][A
 32%|███▏      | 131/414 [13:10<35:49,  7.59s/it][A
 32%|███▏      | 132/414 [13:17<35:37,  7.58s/it][A
 32%|███▏      | 133/414 [13:21<30:53,  6.59s/it][A
 32%|███▏      | 134/414 [13:27<29:49,  6.39s/it][A
 33%|███▎      | 135/414 [13:34<29:51,  6.42s/it][A
 33%|███▎      | 136/414 [13:42<31:36,  6.82s/it][A
 33%|███▎      | 137/414 [13:51<34:50,  7.55s/it][A
 33%|███▎      | 138/414 [13:58<34:04,  7.41s/it][A
 34%|███▎      | 139/414 [14:03<30:19,  6.62s/it][A
 34%|███▍      | 140/414 [14:07<27:39,  6.06s/it][A
 34%|███▍      | 141/414 [14:13<26:53,  5.91s/it][A
 34%|███▍      | 142/414 [14:20<27:48,  6.13s/it][A
 35%|███▍      | 143/414 [14:26<27:44,  6.14s/it][A
 35%|███▍      | 144/414 [14:30<25:25,  5.65s/it][A
 35%|███▌      | 145/414 [14:35<23:38,  5.27s/it][A
 35%|███▌      | 146/414 [14:39<22:12,  4.97s/it][A
 36%|███▌      | 147/414 [14:44<21:56,  4.93s/it][A
 36%|███▌      | 148/414 [14:49<22:30,  5.08s/it][A
 36%|███▌      | 149/414 [14:54<22:09,  5.02s/it][A
 36%|███▌      | 150/414 [14:59<21:36,  4.91s/it][A
 36%|███▋      | 151/414 [15:03<20:30,  4.68s/it][A
 37%|███▋      | 152/414 [15:08<20:56,  4.79s/it][A
 37%|███▋      | 153/414 [15:13<21:40,  4.98s/it][A
 37%|███▋      | 154/414 [15:18<20:38,  4.76s/it][A
 37%|███▋      | 155/414 [15:22<19:59,  4.63s/it][A
 38%|███▊      | 156/414 [15:27<20:46,  4.83s/it][A
 38%|███▊      | 157/414 [15:34<22:39,  5.29s/it][A
 38%|███▊      | 158/414 [15:40<24:11,  5.67s/it][A
 38%|███▊      | 159/414 [15:46<24:57,  5.87s/it][A
 39%|███▊      | 160/414 [15:52<24:35,  5.81s/it][A
 39%|███▉      | 161/414 [15:59<25:47,  6.12s/it][A
 39%|███▉      | 162/414 [16:05<25:41,  6.12s/it][A
 39%|███▉      | 163/414 [16:11<25:05,  6.00s/it][A
 40%|███▉      | 164/414 [16:18<26:26,  6.34s/it][A
 40%|███▉      | 165/414 [16:25<27:46,  6.69s/it][A
 40%|████      | 166/414 [16:33<28:37,  6.93s/it][A
 40%|████      | 167/414 [16:40<28:15,  6.87s/it][A
 41%|████      | 168/414 [16:45<26:35,  6.49s/it][A
 41%|████      | 169/414 [16:50<24:15,  5.94s/it][A
 41%|████      | 170/414 [16:55<22:42,  5.58s/it][A
 41%|████▏     | 171/414 [17:00<22:27,  5.55s/it][A
 42%|████▏     | 172/414 [17:06<23:06,  5.73s/it][A
 42%|████▏     | 173/414 [17:12<23:35,  5.87s/it][A
 42%|████▏     | 174/414 [17:19<23:40,  5.92s/it][A
 42%|████▏     | 175/414 [17:25<23:46,  5.97s/it][A
 43%|████▎     | 176/414 [17:29<22:17,  5.62s/it][A
 43%|████▎     | 177/414 [17:35<22:26,  5.68s/it][A
 43%|████▎     | 178/414 [17:42<23:25,  5.96s/it][A
 43%|████▎     | 179/414 [17:47<22:32,  5.75s/it][A
 43%|████▎     | 180/414 [17:53<22:22,  5.74s/it][A
 44%|████▎     | 181/414 [17:59<22:53,  5.89s/it][A
 44%|████▍     | 182/414 [18:05<22:31,  5.83s/it][A
 44%|████▍     | 183/414 [18:11<23:05,  6.00s/it][A
 44%|████▍     | 184/414 [18:17<22:50,  5.96s/it][A
 45%|████▍     | 185/414 [18:22<21:35,  5.66s/it][A
 45%|████▍     | 186/414 [18:29<22:53,  6.02s/it][A
 45%|████▌     | 187/414 [18:39<27:18,  7.22s/it][A
 45%|████▌     | 188/414 [18:48<29:53,  7.94s/it][A
 46%|████▌     | 189/414 [18:57<30:31,  8.14s/it][A
 46%|████▌     | 190/414 [19:05<30:25,  8.15s/it][A
 46%|████▌     | 191/414 [19:11<27:52,  7.50s/it][A
 46%|████▋     | 192/414 [19:17<25:53,  7.00s/it][A
 47%|████▋     | 193/414 [19:22<23:03,  6.26s/it][A
 47%|████▋     | 194/414 [19:27<21:52,  5.97s/it][A
 47%|████▋     | 195/414 [19:32<20:47,  5.70s/it][A
 47%|████▋     | 196/414 [19:37<20:13,  5.57s/it][A
 48%|████▊     | 197/414 [19:44<21:43,  6.01s/it][A
 48%|████▊     | 198/414 [19:50<21:46,  6.05s/it][A
 48%|████▊     | 199/414 [19:55<20:13,  5.64s/it][A
 48%|████▊     | 200/414 [20:01<20:11,  5.66s/it][A
 49%|████▊     | 201/414 [20:08<21:26,  6.04s/it][A
 49%|████▉     | 202/414 [20:14<21:50,  6.18s/it][A
 49%|████▉     | 203/414 [20:19<20:12,  5.75s/it][A
 49%|████▉     | 204/414 [20:23<18:38,  5.33s/it][A
 50%|████▉     | 205/414 [20:28<17:40,  5.08s/it][A
 50%|████▉     | 206/414 [20:34<18:15,  5.27s/it][A
 50%|█████     | 207/414 [20:39<18:36,  5.40s/it][A
 50%|█████     | 208/414 [20:45<19:11,  5.59s/it][A
 50%|█████     | 209/414 [20:53<21:16,  6.23s/it][A
 51%|█████     | 210/414 [20:59<21:28,  6.32s/it][A
 51%|█████     | 211/414 [21:05<20:11,  5.97s/it][A
 51%|█████     | 212/414 [21:10<19:55,  5.92s/it][A
 51%|█████▏    | 213/414 [21:18<21:35,  6.44s/it][A
 52%|█████▏    | 214/414 [21:25<21:53,  6.57s/it][A
 52%|█████▏    | 215/414 [21:32<21:49,  6.58s/it][A
 52%|█████▏    | 216/414 [21:41<24:03,  7.29s/it][A
 52%|█████▏    | 217/414 [21:48<24:24,  7.43s/it][A
 53%|█████▎    | 218/414 [21:54<22:07,  6.77s/it][A
 53%|█████▎    | 219/414 [21:59<21:11,  6.52s/it][A
 53%|█████▎    | 220/414 [22:05<19:55,  6.16s/it][A
 53%|█████▎    | 221/414 [22:10<18:32,  5.77s/it][A
 54%|█████▎    | 222/414 [22:16<19:06,  5.97s/it][A
 54%|█████▍    | 223/414 [22:23<19:36,  6.16s/it][A
 54%|█████▍    | 224/414 [22:30<20:25,  6.45s/it][A
 54%|█████▍    | 225/414 [22:37<20:44,  6.58s/it][A
 55%|█████▍    | 226/414 [22:43<20:30,  6.54s/it][A
 55%|█████▍    | 227/414 [22:51<21:56,  7.04s/it][A
 55%|█████▌    | 228/414 [22:58<21:29,  6.93s/it][A
 55%|█████▌    | 229/414 [23:03<19:56,  6.47s/it][A
 56%|█████▌    | 230/414 [23:11<20:35,  6.71s/it][A
 56%|█████▌    | 231/414 [23:17<20:19,  6.66s/it][A
 56%|█████▌    | 232/414 [23:23<19:14,  6.35s/it][A
 56%|█████▋    | 233/414 [23:30<19:28,  6.45s/it][A
 57%|█████▋    | 234/414 [23:36<19:45,  6.59s/it][A
 57%|█████▋    | 235/414 [23:44<20:40,  6.93s/it][A
 57%|█████▋    | 236/414 [23:51<20:54,  7.05s/it][A
 57%|█████▋    | 237/414 [23:57<19:33,  6.63s/it][A
 57%|█████▋    | 238/414 [24:01<17:07,  5.84s/it][A
 58%|█████▊    | 239/414 [24:07<16:42,  5.73s/it][A
 58%|█████▊    | 240/414 [24:14<18:05,  6.24s/it][A
 58%|█████▊    | 241/414 [24:20<17:28,  6.06s/it][A
 58%|█████▊    | 242/414 [24:24<15:33,  5.43s/it][A
 59%|█████▊    | 243/414 [24:28<14:54,  5.23s/it][A
 59%|█████▉    | 244/414 [24:34<15:21,  5.42s/it][A
 59%|█████▉    | 245/414 [24:40<15:31,  5.51s/it][A
 59%|█████▉    | 246/414 [24:45<14:42,  5.25s/it][A
 60%|█████▉    | 247/414 [24:50<14:30,  5.21s/it][A
 60%|█████▉    | 248/414 [24:55<14:21,  5.19s/it][A
 60%|██████    | 249/414 [25:00<13:55,  5.07s/it][A
 60%|██████    | 250/414 [25:04<13:22,  4.89s/it][A
 61%|██████    | 251/414 [25:10<13:52,  5.11s/it][A
 61%|██████    | 252/414 [25:16<14:33,  5.39s/it][A
 61%|██████    | 253/414 [25:23<16:02,  5.98s/it][A
 61%|██████▏   | 254/414 [25:31<17:24,  6.53s/it][A
 62%|██████▏   | 255/414 [25:35<15:38,  5.90s/it][A
 62%|██████▏   | 256/414 [25:40<14:45,  5.60s/it][A
 62%|██████▏   | 257/414 [25:46<14:19,  5.47s/it][A
 62%|██████▏   | 258/414 [25:50<13:45,  5.29s/it][A
 63%|██████▎   | 259/414 [25:55<13:06,  5.07s/it][A
 63%|██████▎   | 260/414 [26:01<13:38,  5.32s/it][A
 63%|██████▎   | 261/414 [26:07<14:09,  5.55s/it][A
 63%|██████▎   | 262/414 [26:14<14:57,  5.91s/it][A
 64%|██████▎   | 263/414 [26:21<15:53,  6.31s/it][A
 64%|██████▍   | 264/414 [26:27<15:59,  6.39s/it][A
 64%|██████▍   | 265/414 [26:34<15:44,  6.34s/it][A
 64%|██████▍   | 266/414 [26:40<15:43,  6.37s/it][A
 64%|██████▍   | 267/414 [26:48<16:36,  6.78s/it][A
 65%|██████▍   | 268/414 [26:54<15:56,  6.55s/it][A
 65%|██████▍   | 269/414 [26:58<14:10,  5.86s/it][A
 65%|██████▌   | 270/414 [27:03<13:18,  5.54s/it][A
 65%|██████▌   | 271/414 [27:08<12:58,  5.44s/it][A
 66%|██████▌   | 272/414 [27:14<13:09,  5.56s/it][A
 66%|██████▌   | 273/414 [27:20<13:38,  5.80s/it][A
 66%|██████▌   | 274/414 [27:26<13:41,  5.87s/it][A
 66%|██████▋   | 275/414 [27:32<13:16,  5.73s/it][A
 67%|██████▋   | 276/414 [27:37<12:53,  5.61s/it][A
 67%|██████▋   | 277/414 [27:42<12:26,  5.45s/it][A
 67%|██████▋   | 278/414 [27:46<11:29,  5.07s/it][A
 67%|██████▋   | 279/414 [27:51<11:16,  5.01s/it][A
 68%|██████▊   | 280/414 [27:57<11:40,  5.23s/it][A
 68%|██████▊   | 281/414 [28:02<11:27,  5.17s/it][A
 68%|██████▊   | 282/414 [28:08<11:52,  5.40s/it][A
 68%|██████▊   | 283/414 [28:14<12:15,  5.61s/it][A
 69%|██████▊   | 284/414 [28:19<11:44,  5.42s/it][A
 69%|██████▉   | 285/414 [28:25<12:06,  5.63s/it][A
 69%|██████▉   | 286/414 [28:33<13:06,  6.14s/it][A
 69%|██████▉   | 287/414 [28:40<13:36,  6.43s/it][A
 70%|██████▉   | 288/414 [28:46<13:17,  6.33s/it][A
 70%|██████▉   | 289/414 [28:50<12:08,  5.83s/it][A
 70%|███████   | 290/414 [28:57<12:22,  5.98s/it][A
 70%|███████   | 291/414 [29:04<12:49,  6.25s/it][A
 71%|███████   | 292/414 [29:08<11:28,  5.65s/it][A
 71%|███████   | 293/414 [29:16<12:49,  6.36s/it][A
 71%|███████   | 294/414 [29:25<14:27,  7.23s/it][A
 71%|███████▏  | 295/414 [29:33<14:42,  7.42s/it][A
 71%|███████▏  | 296/414 [29:40<14:37,  7.44s/it][A
 72%|███████▏  | 297/414 [29:46<13:29,  6.92s/it][A
 72%|███████▏  | 298/414 [29:54<13:55,  7.20s/it][A
 72%|███████▏  | 299/414 [30:01<13:38,  7.11s/it][A
 72%|███████▏  | 300/414 [30:06<12:25,  6.54s/it][A
 73%|███████▎  | 301/414 [30:12<11:44,  6.23s/it][A
 73%|███████▎  | 302/414 [30:16<10:46,  5.77s/it][A
 73%|███████▎  | 303/414 [30:21<10:20,  5.59s/it][A
 73%|███████▎  | 304/414 [30:30<11:39,  6.36s/it][A
 74%|███████▎  | 305/414 [30:38<12:32,  6.90s/it][A
 74%|███████▍  | 306/414 [30:44<11:46,  6.54s/it][A
 74%|███████▍  | 307/414 [30:49<11:09,  6.25s/it][A
 74%|███████▍  | 308/414 [30:56<11:29,  6.51s/it][A
 75%|███████▍  | 309/414 [31:04<11:48,  6.75s/it][A
 75%|███████▍  | 310/414 [31:08<10:38,  6.14s/it][A
 75%|███████▌  | 311/414 [31:13<09:57,  5.80s/it][A
 75%|███████▌  | 312/414 [31:19<09:40,  5.69s/it][A
 76%|███████▌  | 313/414 [31:24<09:18,  5.53s/it][A
 76%|███████▌  | 314/414 [31:29<08:50,  5.30s/it][A
 76%|███████▌  | 315/414 [31:34<09:01,  5.47s/it][A
 76%|███████▋  | 316/414 [31:43<10:40,  6.53s/it][A
 77%|███████▋  | 317/414 [31:51<11:07,  6.89s/it][A
 77%|███████▋  | 318/414 [31:57<10:28,  6.55s/it][A
 77%|███████▋  | 319/414 [32:03<10:04,  6.36s/it][A
 77%|███████▋  | 320/414 [32:08<09:18,  5.94s/it][A
 78%|███████▊  | 321/414 [32:14<09:26,  6.09s/it][A
 78%|███████▊  | 322/414 [32:21<09:29,  6.19s/it][A
 78%|███████▊  | 323/414 [32:25<08:43,  5.75s/it][A
 78%|███████▊  | 324/414 [32:30<08:08,  5.43s/it][A
 79%|███████▊  | 325/414 [32:35<07:50,  5.29s/it][A
 79%|███████▊  | 326/414 [32:40<07:32,  5.14s/it][A
 79%|███████▉  | 327/414 [32:47<08:16,  5.71s/it][A
 79%|███████▉  | 328/414 [32:54<08:40,  6.06s/it][A
 79%|███████▉  | 329/414 [33:00<08:48,  6.22s/it][A
 80%|███████▉  | 330/414 [33:08<09:28,  6.77s/it][A
 80%|███████▉  | 331/414 [33:16<09:31,  6.89s/it][A
 80%|████████  | 332/414 [33:22<09:24,  6.88s/it][A
 80%|████████  | 333/414 [33:29<08:58,  6.64s/it][A
 81%|████████  | 334/414 [33:33<07:58,  5.98s/it][A
 81%|████████  | 335/414 [33:38<07:18,  5.55s/it][A
 81%|████████  | 336/414 [33:43<07:04,  5.44s/it][A
 81%|████████▏ | 337/414 [33:47<06:41,  5.22s/it][A
 82%|████████▏ | 338/414 [33:53<06:52,  5.42s/it][A
 82%|████████▏ | 339/414 [34:01<07:30,  6.01s/it][A
 82%|████████▏ | 340/414 [34:09<08:13,  6.67s/it][A
 82%|████████▏ | 341/414 [34:16<08:09,  6.71s/it][A
 83%|████████▎ | 342/414 [34:20<07:09,  5.97s/it][A
 83%|████████▎ | 343/414 [34:27<07:28,  6.31s/it][A
 83%|████████▎ | 344/414 [34:36<08:21,  7.17s/it][A
 83%|████████▎ | 345/414 [34:43<08:04,  7.02s/it][A
 84%|████████▎ | 346/414 [34:47<07:08,  6.31s/it][A
 84%|████████▍ | 347/414 [34:53<06:46,  6.06s/it][A
 84%|████████▍ | 348/414 [34:59<06:34,  5.98s/it][A
 84%|████████▍ | 349/414 [35:04<06:12,  5.72s/it][A
 85%|████████▍ | 350/414 [35:11<06:38,  6.22s/it][A
 85%|████████▍ | 351/414 [35:19<06:54,  6.57s/it][A
 85%|████████▌ | 352/414 [35:23<06:13,  6.03s/it][A
 85%|████████▌ | 353/414 [35:28<05:39,  5.56s/it][A
 86%|████████▌ | 354/414 [35:33<05:31,  5.52s/it][A
 86%|████████▌ | 355/414 [35:39<05:21,  5.45s/it][A
 86%|████████▌ | 356/414 [35:43<05:05,  5.27s/it][A
 86%|████████▌ | 357/414 [35:50<05:14,  5.51s/it][A
 86%|████████▋ | 358/414 [35:57<05:43,  6.14s/it][A
 87%|████████▋ | 359/414 [36:03<05:38,  6.16s/it][A
 87%|████████▋ | 360/414 [36:08<05:08,  5.72s/it][A
 87%|████████▋ | 361/414 [36:12<04:40,  5.29s/it][A
 87%|████████▋ | 362/414 [36:19<04:53,  5.65s/it][A
 88%|████████▊ | 363/414 [36:26<05:17,  6.23s/it][A
 88%|████████▊ | 364/414 [36:32<04:59,  5.99s/it][A
 88%|████████▊ | 365/414 [36:38<04:56,  6.05s/it][A
 88%|████████▊ | 366/414 [36:45<05:08,  6.42s/it][A
 89%|████████▊ | 367/414 [36:52<04:59,  6.38s/it][A
 89%|████████▉ | 368/414 [36:58<04:52,  6.35s/it][A
 89%|████████▉ | 369/414 [37:06<05:03,  6.75s/it][A
 89%|████████▉ | 370/414 [37:12<04:54,  6.70s/it][A
 90%|████████▉ | 371/414 [37:19<04:45,  6.65s/it][A
 90%|████████▉ | 372/414 [37:26<04:43,  6.75s/it][A
 90%|█████████ | 373/414 [37:33<04:46,  6.98s/it][A
 90%|█████████ | 374/414 [37:42<04:56,  7.41s/it][A
 91%|█████████ | 375/414 [37:48<04:33,  7.02s/it][A
 91%|█████████ | 376/414 [37:52<03:54,  6.16s/it][A
 91%|█████████ | 377/414 [37:58<03:45,  6.08s/it][A
 91%|█████████▏| 378/414 [38:05<03:53,  6.47s/it][A
 92%|█████████▏| 379/414 [38:11<03:36,  6.17s/it][A
 92%|█████████▏| 380/414 [38:18<03:40,  6.48s/it][A
 92%|█████████▏| 381/414 [38:28<04:13,  7.68s/it][A
 92%|█████████▏| 382/414 [38:36<04:05,  7.68s/it][A
 93%|█████████▎| 383/414 [38:41<03:30,  6.77s/it][A
 93%|█████████▎| 384/414 [38:47<03:16,  6.54s/it][A
 93%|█████████▎| 385/414 [38:53<03:05,  6.41s/it][A
 93%|█████████▎| 386/414 [38:58<02:46,  5.96s/it][A
 93%|█████████▎| 387/414 [39:02<02:31,  5.60s/it][A
 94%|█████████▎| 388/414 [39:08<02:21,  5.45s/it][A
 94%|█████████▍| 389/414 [39:12<02:09,  5.20s/it][A
 94%|█████████▍| 390/414 [39:16<01:57,  4.90s/it][A
 94%|█████████▍| 391/414 [39:21<01:50,  4.81s/it][A
 95%|█████████▍| 392/414 [39:26<01:46,  4.82s/it][A
 95%|█████████▍| 393/414 [39:32<01:52,  5.37s/it][A
 95%|█████████▌| 394/414 [39:40<01:59,  5.98s/it][A
 95%|█████████▌| 395/414 [39:45<01:48,  5.72s/it][A
 96%|█████████▌| 396/414 [39:52<01:48,  6.01s/it][A
 96%|█████████▌| 397/414 [40:00<01:54,  6.75s/it][A
 96%|█████████▌| 398/414 [40:06<01:44,  6.53s/it][A
 96%|█████████▋| 399/414 [40:10<01:26,  5.73s/it][A
 97%|█████████▋| 400/414 [40:18<01:28,  6.33s/it][A
 97%|█████████▋| 401/414 [40:28<01:38,  7.54s/it][A
 97%|█████████▋| 402/414 [40:34<01:25,  7.15s/it][A
 97%|█████████▋| 403/414 [40:39<01:10,  6.39s/it][A
 98%|█████████▊| 404/414 [40:43<00:58,  5.80s/it][A
 98%|█████████▊| 405/414 [40:48<00:48,  5.38s/it][A
 98%|█████████▊| 406/414 [40:54<00:46,  5.76s/it][A
 98%|█████████▊| 407/414 [41:02<00:44,  6.35s/it][A
 99%|█████████▊| 408/414 [41:08<00:37,  6.18s/it][A
 99%|█████████▉| 409/414 [41:14<00:30,  6.17s/it][A
 99%|█████████▉| 410/414 [41:20<00:24,  6.24s/it][A
 99%|█████████▉| 411/414 [41:25<00:17,  5.67s/it][A
100%|█████████▉| 412/414 [41:31<00:11,  5.91s/it][A
100%|█████████▉| 413/414 [41:38<00:06,  6.21s/it][A
100%|██████████| 414/414 [41:41<00:00,  5.22s/it][A                                                     
                                                 [A{'eval_loss': 1.1047626733779907, 'eval_runtime': 2508.8114, 'eval_samples_per_second': 1.317, 'eval_steps_per_second': 0.165, 'epoch': 0.13}
 13%|█▎        | 30/233 [3:09:11<9:02:22, 160.31s/it]
100%|██████████| 414/414 [41:41<00:00,  5.22s/it][A
                                                 [A 13%|█▎        | 31/233 [3:11:14<50:35:58, 901.77s/it] 14%|█▎        | 32/233 [3:13:16<37:16:41, 667.67s/it] 14%|█▍        | 33/233 [3:15:24<28:05:37, 505.69s/it] 15%|█▍        | 34/233 [3:17:27<21:36:43, 390.97s/it] 15%|█▌        | 35/233 [3:19:19<16:53:40, 307.18s/it] 15%|█▌        | 36/233 [3:21:25<13:50:47, 253.03s/it] 16%|█▌        | 37/233 [3:23:31<11:42:02, 214.91s/it] 16%|█▋        | 38/233 [3:25:25<10:00:19, 184.72s/it] 17%|█▋        | 39/233 [3:27:33<9:01:33, 167.49s/it]  17%|█▋        | 40/233 [3:29:41<8:21:15, 155.83s/it]                                                     {'loss': 1.0809, 'grad_norm': 0.28603240847587585, 'learning_rate': 9.873442168142159e-05, 'epoch': 0.17}
 17%|█▋        | 40/233 [3:29:41<8:21:15, 155.83s/it]
***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
[INFO|trainer.py:4643] 2025-12-05 08:22:10,183 >> 
***** Running Evaluation *****
  Batch size = 1

***** Running Evaluation *****

***** Running Evaluation *****

***** Running Evaluation *****
  Num examples = 3305
  Num examples = 3305
  Batch size = 1
  Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
[INFO|trainer.py:4645] 2025-12-05 08:22:10,183 >>   Num examples = 3305
  Batch size = 1
  Num examples = 3305
[INFO|trainer.py:4648] 2025-12-05 08:22:10,183 >>   Batch size = 1
  Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

  0%|          | 0/414 [00:00<?, ?it/s][A
  0%|          | 2/414 [00:04<16:20,  2.38s/it][A
  1%|          | 3/414 [00:10<24:54,  3.64s/it][A
  1%|          | 4/414 [00:17<35:14,  5.16s/it][A
  1%|          | 5/414 [00:25<41:06,  6.03s/it][A
  1%|▏         | 6/414 [00:29<36:54,  5.43s/it][A
  2%|▏         | 7/414 [00:34<35:05,  5.17s/it][A
  2%|▏         | 8/414 [00:38<33:39,  4.97s/it][A
  2%|▏         | 9/414 [00:44<35:06,  5.20s/it][A
  2%|▏         | 10/414 [00:51<39:27,  5.86s/it][A
  3%|▎         | 11/414 [00:57<39:09,  5.83s/it][A
  3%|▎         | 12/414 [01:04<40:10,  6.00s/it][A
  3%|▎         | 13/414 [01:11<43:45,  6.55s/it][A
  3%|▎         | 14/414 [01:19<44:56,  6.74s/it][A
  4%|▎         | 15/414 [01:25<44:15,  6.65s/it][A
  4%|▍         | 16/414 [01:32<44:42,  6.74s/it][A
  4%|▍         | 17/414 [01:39<44:41,  6.75s/it][A
  4%|▍         | 18/414 [01:46<44:56,  6.81s/it][A
  5%|▍         | 19/414 [01:54<48:07,  7.31s/it][A
  5%|▍         | 20/414 [02:01<47:12,  7.19s/it][A
  5%|▌         | 21/414 [02:06<42:58,  6.56s/it][A
  5%|▌         | 22/414 [02:12<41:54,  6.41s/it][A
  6%|▌         | 23/414 [02:20<43:49,  6.72s/it][A
  6%|▌         | 24/414 [02:26<42:22,  6.52s/it][A
  6%|▌         | 25/414 [02:32<42:34,  6.57s/it][A
  6%|▋         | 26/414 [02:39<42:40,  6.60s/it][A
  7%|▋         | 27/414 [02:43<38:04,  5.90s/it][A
  7%|▋         | 28/414 [02:49<36:29,  5.67s/it][A
  7%|▋         | 29/414 [02:54<35:35,  5.55s/it][A
  7%|▋         | 30/414 [03:00<36:33,  5.71s/it][A
  7%|▋         | 31/414 [03:06<36:34,  5.73s/it][A
  8%|▊         | 32/414 [03:10<34:14,  5.38s/it][A
  8%|▊         | 33/414 [03:17<36:11,  5.70s/it][A
  8%|▊         | 34/414 [03:23<36:25,  5.75s/it][A
  8%|▊         | 35/414 [03:28<36:14,  5.74s/it][A
  9%|▊         | 36/414 [03:35<37:29,  5.95s/it][A
  9%|▉         | 37/414 [03:41<38:25,  6.12s/it][A
  9%|▉         | 38/414 [03:48<38:40,  6.17s/it][A
  9%|▉         | 39/414 [03:52<34:48,  5.57s/it][A
 10%|▉         | 40/414 [03:57<34:43,  5.57s/it][A
 10%|▉         | 41/414 [04:04<36:34,  5.88s/it][A
 10%|█         | 42/414 [04:10<36:34,  5.90s/it][A
 10%|█         | 43/414 [04:19<42:20,  6.85s/it][A
 11%|█         | 44/414 [04:30<49:26,  8.02s/it][A
 11%|█         | 45/414 [04:37<47:37,  7.74s/it][A
 11%|█         | 46/414 [04:41<41:41,  6.80s/it][A
 11%|█▏        | 47/414 [04:46<37:01,  6.05s/it][A
 12%|█▏        | 48/414 [04:49<32:46,  5.37s/it][A
 12%|█▏        | 49/414 [04:53<29:11,  4.80s/it][A
 12%|█▏        | 50/414 [04:57<27:44,  4.57s/it][A
 12%|█▏        | 51/414 [05:01<26:57,  4.45s/it][A
 13%|█▎        | 52/414 [05:05<26:36,  4.41s/it][A
 13%|█▎        | 53/414 [05:13<31:59,  5.32s/it][A
 13%|█▎        | 54/414 [05:21<37:28,  6.25s/it][A
 13%|█▎        | 55/414 [05:28<38:25,  6.42s/it][A
 14%|█▎        | 56/414 [05:35<38:23,  6.44s/it][A
 14%|█▍        | 57/414 [05:40<35:41,  6.00s/it][A
 14%|█▍        | 58/414 [05:45<35:13,  5.94s/it][A
 14%|█▍        | 59/414 [05:53<37:22,  6.32s/it][A
 14%|█▍        | 60/414 [05:58<36:31,  6.19s/it][A
 15%|█▍        | 61/414 [06:03<33:05,  5.62s/it][A
 15%|█▍        | 62/414 [06:07<30:38,  5.22s/it][A
 15%|█▌        | 63/414 [06:12<30:05,  5.14s/it][A
 15%|█▌        | 64/414 [06:17<29:48,  5.11s/it][A
 16%|█▌        | 65/414 [06:22<29:49,  5.13s/it][A
 16%|█▌        | 66/414 [06:28<30:39,  5.29s/it][A
 16%|█▌        | 67/414 [06:35<33:51,  5.85s/it][A
 16%|█▋        | 68/414 [06:42<35:57,  6.24s/it][A
 17%|█▋        | 69/414 [06:49<37:15,  6.48s/it][A
 17%|█▋        | 70/414 [06:56<38:18,  6.68s/it][A
 17%|█▋        | 71/414 [07:01<35:29,  6.21s/it][A
 17%|█▋        | 72/414 [07:06<33:16,  5.84s/it][A
 18%|█▊        | 73/414 [07:11<31:27,  5.54s/it][A
 18%|█▊        | 74/414 [07:15<28:59,  5.12s/it][A
 18%|█▊        | 75/414 [07:19<26:59,  4.78s/it][A
 18%|█▊        | 76/414 [07:24<26:26,  4.69s/it][A
 19%|█▊        | 77/414 [07:30<28:42,  5.11s/it][A
 19%|█▉        | 78/414 [07:36<29:56,  5.35s/it][A
 19%|█▉        | 79/414 [07:42<30:48,  5.52s/it][A
 19%|█▉        | 80/414 [07:47<30:20,  5.45s/it][A
 20%|█▉        | 81/414 [07:52<29:00,  5.23s/it][A
 20%|█▉        | 82/414 [07:57<29:06,  5.26s/it][A
 20%|██        | 83/414 [08:05<32:43,  5.93s/it][A
 20%|██        | 84/414 [08:12<35:14,  6.41s/it][A
 21%|██        | 85/414 [08:17<32:37,  5.95s/it][A
 21%|██        | 86/414 [08:22<30:46,  5.63s/it][A
 21%|██        | 87/414 [08:26<29:01,  5.32s/it][A
 21%|██▏       | 88/414 [08:31<28:13,  5.20s/it][A
 21%|██▏       | 89/414 [08:37<29:09,  5.38s/it][A
 22%|██▏       | 90/414 [08:44<31:52,  5.90s/it][A
 22%|██▏       | 91/414 [08:50<30:56,  5.75s/it][A
 22%|██▏       | 92/414 [08:55<30:19,  5.65s/it][A
 22%|██▏       | 93/414 [09:01<29:54,  5.59s/it][A
 23%|██▎       | 94/414 [09:06<29:21,  5.50s/it][A
 23%|██▎       | 95/414 [09:12<29:55,  5.63s/it][A
 23%|██▎       | 96/414 [09:18<30:23,  5.74s/it][A
 23%|██▎       | 97/414 [09:23<30:07,  5.70s/it][A
 24%|██▎       | 98/414 [09:28<27:32,  5.23s/it][A
 24%|██▍       | 99/414 [09:32<26:31,  5.05s/it][A
 24%|██▍       | 100/414 [09:38<28:23,  5.42s/it][A
 24%|██▍       | 101/414 [09:46<32:23,  6.21s/it][A
 25%|██▍       | 102/414 [09:53<32:59,  6.34s/it][A
 25%|██▍       | 103/414 [09:58<30:23,  5.86s/it][A
 25%|██▌       | 104/414 [10:03<28:26,  5.50s/it][A
 25%|██▌       | 105/414 [10:08<28:38,  5.56s/it][A
 26%|██▌       | 106/414 [10:16<31:19,  6.10s/it][A
 26%|██▌       | 107/414 [10:22<31:09,  6.09s/it][A
 26%|██▌       | 108/414 [10:27<29:12,  5.73s/it][A
 26%|██▋       | 109/414 [10:32<27:55,  5.49s/it][A
 27%|██▋       | 110/414 [10:37<28:31,  5.63s/it][A
 27%|██▋       | 111/414 [10:44<29:27,  5.83s/it][A
 27%|██▋       | 112/414 [10:49<27:59,  5.56s/it][A
 27%|██▋       | 113/414 [10:55<28:59,  5.78s/it][A
 28%|██▊       | 114/414 [11:02<30:58,  6.19s/it][A
 28%|██▊       | 115/414 [11:08<30:17,  6.08s/it][A
 28%|██▊       | 116/414 [11:14<30:12,  6.08s/it][A
 28%|██▊       | 117/414 [11:22<32:56,  6.66s/it][A
 29%|██▊       | 118/414 [11:31<36:21,  7.37s/it][A
 29%|██▊       | 119/414 [11:40<38:56,  7.92s/it][A
 29%|██▉       | 120/414 [11:48<39:01,  7.96s/it][A
 29%|██▉       | 121/414 [11:56<38:24,  7.86s/it][A
 29%|██▉       | 122/414 [12:03<37:19,  7.67s/it][A
 30%|██▉       | 123/414 [12:10<35:39,  7.35s/it][A
 30%|██▉       | 124/414 [12:17<35:48,  7.41s/it][A
 30%|███       | 125/414 [12:25<35:53,  7.45s/it][A
 30%|███       | 126/414 [12:31<33:15,  6.93s/it][A
 31%|███       | 127/414 [12:38<33:22,  6.98s/it][A
 31%|███       | 128/414 [12:47<35:57,  7.54s/it][A
 31%|███       | 129/414 [12:53<34:48,  7.33s/it][A
 31%|███▏      | 130/414 [13:00<34:05,  7.20s/it][A
 32%|███▏      | 131/414 [13:09<35:41,  7.57s/it][A
 32%|███▏      | 132/414 [13:16<35:30,  7.55s/it][A
 32%|███▏      | 133/414 [13:21<30:48,  6.58s/it][A
 32%|███▏      | 134/414 [13:26<29:48,  6.39s/it][A
 33%|███▎      | 135/414 [13:33<29:48,  6.41s/it][A
 33%|███▎      | 136/414 [13:41<31:35,  6.82s/it][A
 33%|███▎      | 137/414 [13:50<34:55,  7.56s/it][A
 33%|███▎      | 138/414 [13:57<34:09,  7.42s/it][A
 34%|███▎      | 139/414 [14:02<30:23,  6.63s/it][A
 34%|███▍      | 140/414 [14:07<27:39,  6.06s/it][A
 34%|███▍      | 141/414 [14:12<26:52,  5.91s/it][A
 34%|███▍      | 142/414 [14:19<27:46,  6.13s/it][A
 35%|███▍      | 143/414 [14:25<27:44,  6.14s/it][A
 35%|███▍      | 144/414 [14:29<25:23,  5.64s/it][A
 35%|███▌      | 145/414 [14:34<23:38,  5.27s/it][A
 35%|███▌      | 146/414 [14:38<22:06,  4.95s/it][A
 36%|███▌      | 147/414 [14:43<21:52,  4.91s/it][A
 36%|███▌      | 148/414 [14:48<22:30,  5.08s/it][A
 36%|███▌      | 149/414 [14:53<22:09,  5.02s/it][A
 36%|███▌      | 150/414 [14:58<21:34,  4.90s/it][A
 36%|███▋      | 151/414 [15:02<20:29,  4.67s/it][A
 37%|███▋      | 152/414 [15:07<20:52,  4.78s/it][A
 37%|███▋      | 153/414 [15:12<21:36,  4.97s/it][A
 37%|███▋      | 154/414 [15:17<20:31,  4.74s/it][A
 37%|███▋      | 155/414 [15:21<19:53,  4.61s/it][A
 38%|███▊      | 156/414 [15:26<20:46,  4.83s/it][A
 38%|███▊      | 157/414 [15:33<22:39,  5.29s/it][A
 38%|███▊      | 158/414 [15:39<24:01,  5.63s/it][A
 38%|███▊      | 159/414 [15:45<24:50,  5.84s/it][A
 39%|███▊      | 160/414 [15:51<24:41,  5.83s/it][A
 39%|███▉      | 161/414 [15:58<25:47,  6.12s/it][A
 39%|███▉      | 162/414 [16:04<25:43,  6.12s/it][A
 39%|███▉      | 163/414 [16:10<25:04,  5.99s/it][A
 40%|███▉      | 164/414 [16:17<26:22,  6.33s/it][A
 40%|███▉      | 165/414 [16:24<27:41,  6.67s/it][A
 40%|████      | 166/414 [16:32<28:32,  6.91s/it][A
 40%|████      | 167/414 [16:39<28:07,  6.83s/it][A
 41%|████      | 168/414 [16:44<26:26,  6.45s/it][A
 41%|████      | 169/414 [16:49<24:07,  5.91s/it][A
 41%|████      | 170/414 [16:54<22:37,  5.56s/it][A
 41%|████▏     | 171/414 [16:59<22:21,  5.52s/it][A
 42%|████▏     | 172/414 [17:05<23:05,  5.72s/it][A
 42%|████▏     | 173/414 [17:11<23:35,  5.87s/it][A
 42%|████▏     | 174/414 [17:17<23:39,  5.92s/it][A
 42%|████▏     | 175/414 [17:23<23:46,  5.97s/it][A
 43%|████▎     | 176/414 [17:28<22:16,  5.62s/it][A
 43%|████▎     | 177/414 [17:34<22:26,  5.68s/it][A
 43%|████▎     | 178/414 [17:41<23:24,  5.95s/it][A
 43%|████▎     | 179/414 [17:46<22:31,  5.75s/it][A
 43%|████▎     | 180/414 [17:52<22:18,  5.72s/it][A
 44%|████▎     | 181/414 [17:58<22:53,  5.89s/it][A
 44%|████▍     | 182/414 [18:04<22:33,  5.83s/it][A
 44%|████▍     | 183/414 [18:10<23:08,  6.01s/it][A
 44%|████▍     | 184/414 [18:16<22:49,  5.96s/it][A
 45%|████▍     | 185/414 [18:21<21:33,  5.65s/it][A
 45%|████▍     | 186/414 [18:28<22:51,  6.01s/it][A
 45%|████▌     | 187/414 [18:38<27:15,  7.20s/it][A
 45%|████▌     | 188/414 [18:47<29:49,  7.92s/it][A
 46%|████▌     | 189/414 [18:56<30:27,  8.12s/it][A
 46%|████▌     | 190/414 [19:04<30:21,  8.13s/it][A
 46%|████▌     | 191/414 [19:10<27:50,  7.49s/it][A
 46%|████▋     | 192/414 [19:16<25:51,  6.99s/it][A
 47%|████▋     | 193/414 [19:21<23:16,  6.32s/it][A
 47%|████▋     | 194/414 [19:26<22:05,  6.02s/it][A
 47%|████▋     | 195/414 [19:31<20:55,  5.73s/it][A
 47%|████▋     | 196/414 [19:36<20:18,  5.59s/it][A
 48%|████▊     | 197/414 [19:43<21:44,  6.01s/it][A
 48%|████▊     | 198/414 [19:49<21:46,  6.05s/it][A
 48%|████▊     | 199/414 [19:54<20:11,  5.63s/it][A
 48%|████▊     | 200/414 [20:00<20:11,  5.66s/it][A
 49%|████▊     | 201/414 [20:07<21:23,  6.03s/it][A
 49%|████▉     | 202/414 [20:13<21:46,  6.16s/it][A
 49%|████▉     | 203/414 [20:18<20:09,  5.73s/it][A
 49%|████▉     | 204/414 [20:22<18:36,  5.32s/it][A
 50%|████▉     | 205/414 [20:27<17:41,  5.08s/it][A
 50%|████▉     | 206/414 [20:32<18:15,  5.27s/it][A
 50%|█████     | 207/414 [20:38<18:35,  5.39s/it][A
 50%|█████     | 208/414 [20:44<19:08,  5.58s/it][A
 50%|█████     | 209/414 [20:52<21:19,  6.24s/it][A
 51%|█████     | 210/414 [20:58<21:31,  6.33s/it][A
 51%|█████     | 211/414 [21:03<20:10,  5.96s/it][A
 51%|█████     | 212/414 [21:09<19:55,  5.92s/it][A
 51%|█████▏    | 213/414 [21:17<21:34,  6.44s/it][A
 52%|█████▏    | 214/414 [21:24<21:52,  6.56s/it][A
 52%|█████▏    | 215/414 [21:30<21:47,  6.57s/it][A
 52%|█████▏    | 216/414 [21:39<24:01,  7.28s/it][A
 52%|█████▏    | 217/414 [21:47<24:22,  7.42s/it][A
 53%|█████▎    | 218/414 [21:52<22:04,  6.76s/it][A
 53%|█████▎    | 219/414 [21:58<21:09,  6.51s/it][A
 53%|█████▎    | 220/414 [22:04<19:53,  6.15s/it][A
 53%|█████▎    | 221/414 [22:08<18:27,  5.74s/it][A
 54%|█████▎    | 222/414 [22:15<18:58,  5.93s/it][A
 54%|█████▍    | 223/414 [22:21<19:31,  6.13s/it][A
 54%|█████▍    | 224/414 [22:28<20:21,  6.43s/it][A
 54%|█████▍    | 225/414 [22:35<20:41,  6.57s/it][A
 55%|█████▍    | 226/414 [22:42<20:32,  6.56s/it][A
 55%|█████▍    | 227/414 [22:50<21:57,  7.05s/it][A
 55%|█████▌    | 228/414 [22:57<21:26,  6.92s/it][A
 55%|█████▌    | 229/414 [23:02<19:58,  6.48s/it][A
 56%|█████▌    | 230/414 [23:09<20:31,  6.69s/it][A
 56%|█████▌    | 231/414 [23:16<20:19,  6.66s/it][A
 56%|█████▌    | 232/414 [23:22<19:16,  6.35s/it][A
 56%|█████▋    | 233/414 [23:28<19:31,  6.47s/it][A
 57%|█████▋    | 234/414 [23:35<19:46,  6.59s/it][A
 57%|█████▋    | 235/414 [23:43<20:43,  6.95s/it][A
 57%|█████▋    | 236/414 [23:50<20:53,  7.04s/it][A
 57%|█████▋    | 237/414 [23:56<19:35,  6.64s/it][A
 57%|█████▋    | 238/414 [24:00<17:05,  5.83s/it][A
 58%|█████▊    | 239/414 [24:05<16:40,  5.72s/it][A
 58%|█████▊    | 240/414 [24:13<18:03,  6.23s/it][A
 58%|█████▊    | 241/414 [24:18<17:24,  6.04s/it][A
 58%|█████▊    | 242/414 [24:22<15:27,  5.39s/it][A
 59%|█████▊    | 243/414 [24:27<14:49,  5.20s/it][A
 59%|█████▉    | 244/414 [24:33<15:20,  5.42s/it][A
 59%|█████▉    | 245/414 [24:39<15:34,  5.53s/it][A
 59%|█████▉    | 246/414 [24:43<14:44,  5.26s/it][A
 60%|█████▉    | 247/414 [24:48<14:33,  5.23s/it][A
 60%|█████▉    | 248/414 [24:54<14:23,  5.20s/it][A
 60%|██████    | 249/414 [24:58<13:56,  5.07s/it][A
 60%|██████    | 250/414 [25:03<13:22,  4.89s/it][A
 61%|██████    | 251/414 [25:08<13:54,  5.12s/it][A
 61%|██████    | 252/414 [25:15<14:40,  5.44s/it][A
 61%|██████    | 253/414 [25:22<16:08,  6.02s/it][A
 61%|██████▏   | 254/414 [25:30<17:30,  6.56s/it][A
 62%|██████▏   | 255/414 [25:34<15:43,  5.93s/it][A
 62%|██████▏   | 256/414 [25:39<14:44,  5.60s/it][A
 62%|██████▏   | 257/414 [25:44<14:18,  5.47s/it][A
 62%|██████▏   | 258/414 [25:49<13:43,  5.28s/it][A
 63%|██████▎   | 259/414 [25:54<13:04,  5.06s/it][A
 63%|██████▎   | 260/414 [26:00<13:39,  5.32s/it][A
 63%|██████▎   | 261/414 [26:06<14:09,  5.55s/it][A
 63%|██████▎   | 262/414 [26:12<14:58,  5.91s/it][A
 64%|██████▎   | 263/414 [26:20<15:53,  6.31s/it][A
 64%|██████▍   | 264/414 [26:26<15:58,  6.39s/it][A
 64%|██████▍   | 265/414 [26:32<15:43,  6.34s/it][A
 64%|██████▍   | 266/414 [26:39<15:39,  6.35s/it][A
 64%|██████▍   | 267/414 [26:47<16:31,  6.75s/it][A
 65%|██████▍   | 268/414 [26:53<15:51,  6.52s/it][A
 65%|██████▍   | 269/414 [26:57<14:08,  5.85s/it][A
 65%|██████▌   | 270/414 [27:02<13:16,  5.53s/it][A
 65%|██████▌   | 271/414 [27:07<12:59,  5.45s/it][A
 66%|██████▌   | 272/414 [27:13<13:11,  5.57s/it][A
 66%|██████▌   | 273/414 [27:19<13:39,  5.81s/it][A
 66%|██████▌   | 274/414 [27:25<13:41,  5.87s/it][A
 66%|██████▋   | 275/414 [27:31<13:17,  5.74s/it][A
 67%|██████▋   | 276/414 [27:36<12:56,  5.63s/it][A
 67%|██████▋   | 277/414 [27:41<12:26,  5.45s/it][A
 67%|██████▋   | 278/414 [27:45<11:29,  5.07s/it][A
 67%|██████▋   | 279/414 [27:50<11:18,  5.02s/it][A
 68%|██████▊   | 280/414 [27:56<11:41,  5.24s/it][A
 68%|██████▊   | 281/414 [28:01<11:28,  5.17s/it][A
 68%|██████▊   | 282/414 [28:07<11:53,  5.40s/it][A
 68%|██████▊   | 283/414 [28:13<12:15,  5.62s/it][A
 69%|██████▊   | 284/414 [28:18<11:42,  5.40s/it][A
 69%|██████▉   | 285/414 [28:24<12:06,  5.63s/it][A
 69%|██████▉   | 286/414 [28:31<13:06,  6.15s/it][A
 69%|██████▉   | 287/414 [28:38<13:38,  6.45s/it][A
 70%|██████▉   | 288/414 [28:45<13:19,  6.34s/it][A
 70%|██████▉   | 289/414 [28:49<12:08,  5.83s/it][A
 70%|███████   | 290/414 [28:55<12:22,  5.99s/it][A
 70%|███████   | 291/414 [29:02<12:48,  6.25s/it][A
 71%|███████   | 292/414 [29:07<11:29,  5.65s/it][A
 71%|███████   | 293/414 [29:15<12:49,  6.36s/it][A
 71%|███████   | 294/414 [29:24<14:28,  7.24s/it][A
 71%|███████▏  | 295/414 [29:32<14:42,  7.41s/it][A
 71%|███████▏  | 296/414 [29:39<14:37,  7.44s/it][A
 72%|███████▏  | 297/414 [29:45<13:30,  6.93s/it][A
 72%|███████▏  | 298/414 [29:53<13:56,  7.21s/it][A
 72%|███████▏  | 299/414 [30:00<13:34,  7.08s/it][A
 72%|███████▏  | 300/414 [30:05<12:24,  6.53s/it][A
 73%|███████▎  | 301/414 [30:10<11:45,  6.24s/it][A
 73%|███████▎  | 302/414 [30:15<10:47,  5.78s/it][A
 73%|███████▎  | 303/414 [30:20<10:20,  5.59s/it][A
 73%|███████▎  | 304/414 [30:28<11:37,  6.34s/it][A
 74%|███████▎  | 305/414 [30:37<12:30,  6.88s/it][A
 74%|███████▍  | 306/414 [30:42<11:45,  6.53s/it][A
 74%|███████▍  | 307/414 [30:48<11:08,  6.25s/it][A
 74%|███████▍  | 308/414 [30:55<11:31,  6.53s/it][A
 75%|███████▍  | 309/414 [31:02<11:48,  6.75s/it][A
 75%|███████▍  | 310/414 [31:07<10:38,  6.14s/it][A
 75%|███████▌  | 311/414 [31:12<09:58,  5.81s/it][A
 75%|███████▌  | 312/414 [31:17<09:41,  5.70s/it][A
 76%|███████▌  | 313/414 [31:23<09:20,  5.55s/it][A
 76%|███████▌  | 314/414 [31:27<08:51,  5.32s/it][A
 76%|███████▌  | 315/414 [31:33<09:01,  5.47s/it][A
 76%|███████▋  | 316/414 [31:42<10:37,  6.51s/it][A
 77%|███████▋  | 317/414 [31:50<11:05,  6.87s/it][A
 77%|███████▋  | 318/414 [31:56<10:27,  6.53s/it][A
 77%|███████▋  | 319/414 [32:02<10:02,  6.35s/it][A
 77%|███████▋  | 320/414 [32:06<09:16,  5.92s/it][A
 78%|███████▊  | 321/414 [32:13<09:24,  6.07s/it][A
 78%|███████▊  | 322/414 [32:19<09:28,  6.18s/it][A
 78%|███████▊  | 323/414 [32:24<08:42,  5.74s/it][A
 78%|███████▊  | 324/414 [32:29<08:08,  5.43s/it][A
 79%|███████▊  | 325/414 [32:34<07:51,  5.30s/it][A
 79%|███████▊  | 326/414 [32:39<07:33,  5.15s/it][A
 79%|███████▉  | 327/414 [32:46<08:16,  5.70s/it][A
 79%|███████▉  | 328/414 [32:52<08:40,  6.05s/it][A
 79%|███████▉  | 329/414 [32:59<08:48,  6.21s/it][A
 80%|███████▉  | 330/414 [33:07<09:27,  6.76s/it][A
 80%|███████▉  | 331/414 [33:14<09:31,  6.88s/it][A
 80%|████████  | 332/414 [33:21<09:23,  6.87s/it][A
 80%|████████  | 333/414 [33:27<08:57,  6.64s/it][A
 81%|████████  | 334/414 [33:32<07:58,  5.98s/it][A
 81%|████████  | 335/414 [33:36<07:17,  5.54s/it][A
 81%|████████  | 336/414 [33:41<07:02,  5.42s/it][A
 81%|████████▏ | 337/414 [33:46<06:41,  5.21s/it][A
 82%|████████▏ | 338/414 [33:52<06:52,  5.42s/it][A
 82%|████████▏ | 339/414 [33:59<07:31,  6.02s/it][A
 82%|████████▏ | 340/414 [34:08<08:14,  6.69s/it][A
 82%|████████▏ | 341/414 [34:14<08:08,  6.69s/it][A
 83%|████████▎ | 342/414 [34:18<07:08,  5.95s/it][A
 83%|████████▎ | 343/414 [34:26<07:27,  6.30s/it][A
 83%|████████▎ | 344/414 [34:35<08:20,  7.15s/it][A
 83%|████████▎ | 345/414 [34:41<08:02,  6.99s/it][A
 84%|████████▎ | 346/414 [34:46<07:09,  6.31s/it][A
 84%|████████▍ | 347/414 [34:52<06:47,  6.08s/it][A
 84%|████████▍ | 348/414 [34:57<06:36,  6.00s/it][A
 84%|████████▍ | 349/414 [35:03<06:12,  5.74s/it][A
 85%|████████▍ | 350/414 [35:10<06:37,  6.21s/it][A
 85%|████████▍ | 351/414 [35:17<06:52,  6.55s/it][A
 85%|████████▌ | 352/414 [35:22<06:12,  6.01s/it][A
 85%|████████▌ | 353/414 [35:26<05:39,  5.56s/it][A
 86%|████████▌ | 354/414 [35:32<05:30,  5.51s/it][A
 86%|████████▌ | 355/414 [35:37<05:20,  5.44s/it][A
 86%|████████▌ | 356/414 [35:42<05:05,  5.27s/it][A
 86%|████████▌ | 357/414 [35:48<05:13,  5.50s/it][A
 86%|████████▋ | 358/414 [35:56<05:44,  6.14s/it][A
 87%|████████▋ | 359/414 [36:02<05:39,  6.16s/it][A
 87%|████████▋ | 360/414 [36:07<05:09,  5.73s/it][A
 87%|████████▋ | 361/414 [36:11<04:41,  5.31s/it][A
 87%|████████▋ | 362/414 [36:17<04:54,  5.66s/it][A
 88%|████████▊ | 363/414 [36:25<05:18,  6.24s/it][A
 88%|████████▊ | 364/414 [36:30<04:59,  6.00s/it][A
 88%|████████▊ | 365/414 [36:37<04:55,  6.03s/it][A
 88%|████████▊ | 366/414 [36:44<05:06,  6.39s/it][A
 89%|████████▊ | 367/414 [36:50<04:58,  6.35s/it][A
 89%|████████▉ | 368/414 [36:56<04:51,  6.33s/it][A
 89%|████████▉ | 369/414 [37:04<05:02,  6.72s/it][A
 89%|████████▉ | 370/414 [37:11<04:53,  6.68s/it][A
 90%|████████▉ | 371/414 [37:17<04:44,  6.61s/it][A
 90%|████████▉ | 372/414 [37:24<04:41,  6.70s/it][A
 90%|█████████ | 373/414 [37:31<04:44,  6.95s/it][A
 90%|█████████ | 374/414 [37:40<04:55,  7.38s/it][A
 91%|█████████ | 375/414 [37:46<04:32,  6.98s/it][A
 91%|█████████ | 376/414 [37:50<03:53,  6.14s/it][A
 91%|█████████ | 377/414 [37:56<03:44,  6.07s/it][A
 91%|█████████▏| 378/414 [38:03<03:53,  6.48s/it][A
 92%|█████████▏| 379/414 [38:09<03:36,  6.18s/it][A
 92%|█████████▏| 380/414 [38:16<03:40,  6.49s/it][A
 92%|█████████▏| 381/414 [38:27<04:13,  7.69s/it][A
 92%|█████████▏| 382/414 [38:34<04:05,  7.68s/it][A
 93%|█████████▎| 383/414 [38:39<03:29,  6.77s/it][A
 93%|█████████▎| 384/414 [38:45<03:15,  6.53s/it][A
 93%|█████████▎| 385/414 [38:51<03:06,  6.42s/it][A
 93%|█████████▎| 386/414 [38:56<02:47,  5.97s/it][A
 93%|█████████▎| 387/414 [39:01<02:31,  5.60s/it][A
 94%|█████████▎| 388/414 [39:06<02:21,  5.46s/it][A
 94%|█████████▍| 389/414 [39:10<02:09,  5.17s/it][A
 94%|█████████▍| 390/414 [39:14<01:56,  4.87s/it][A
 94%|█████████▍| 391/414 [39:19<01:50,  4.81s/it][A
 95%|█████████▍| 392/414 [39:24<01:46,  4.82s/it][A
 95%|█████████▍| 393/414 [39:31<01:52,  5.37s/it][A
 95%|█████████▌| 394/414 [39:38<01:59,  5.96s/it][A
 95%|█████████▌| 395/414 [39:43<01:48,  5.71s/it][A
 96%|█████████▌| 396/414 [39:50<01:47,  6.00s/it][A
 96%|█████████▌| 397/414 [39:58<01:54,  6.76s/it][A
 96%|█████████▌| 398/414 [40:04<01:44,  6.53s/it][A
 96%|█████████▋| 399/414 [40:08<01:26,  5.73s/it][A
 97%|█████████▋| 400/414 [40:16<01:28,  6.33s/it][A
 97%|█████████▋| 401/414 [40:26<01:37,  7.53s/it][A
 97%|█████████▋| 402/414 [40:32<01:25,  7.15s/it][A
 97%|█████████▋| 403/414 [40:37<01:09,  6.35s/it][A
 98%|█████████▊| 404/414 [40:41<00:57,  5.78s/it][A
 98%|█████████▊| 405/414 [40:46<00:48,  5.36s/it][A
 98%|█████████▊| 406/414 [40:52<00:46,  5.76s/it][A
 98%|█████████▊| 407/414 [41:00<00:44,  6.35s/it][A
 99%|█████████▊| 408/414 [41:06<00:37,  6.18s/it][A
 99%|█████████▉| 409/414 [41:12<00:30,  6.19s/it][A
 99%|█████████▉| 410/414 [41:19<00:24,  6.25s/it][A
 99%|█████████▉| 411/414 [41:23<00:16,  5.66s/it][A
100%|█████████▉| 412/414 [41:29<00:11,  5.89s/it][A
100%|█████████▉| 413/414 [41:36<00:06,  6.20s/it][A
100%|██████████| 414/414 [41:39<00:00,  5.20s/it][A                                                     
                                                 [A{'eval_loss': 1.0001577138900757, 'eval_runtime': 2506.7993, 'eval_samples_per_second': 1.318, 'eval_steps_per_second': 0.165, 'epoch': 0.17}
 17%|█▋        | 40/233 [4:11:28<8:21:15, 155.83s/it]
100%|██████████| 414/414 [41:39<00:00,  5.20s/it][A
                                                 [A 18%|█▊        | 41/233 [4:13:49<48:10:48, 903.38s/it] 18%|█▊        | 42/233 [4:15:52<35:30:19, 669.21s/it] 18%|█▊        | 43/233 [4:17:55<26:40:27, 505.41s/it] 19%|█▉        | 44/233 [4:20:07<20:39:28, 393.48s/it] 19%|█▉        | 45/233 [4:21:55<16:04:08, 307.70s/it] 20%|█▉        | 46/233 [4:23:57<13:05:37, 252.07s/it] 20%|██        | 47/233 [4:26:06<11:06:40, 215.06s/it] 21%|██        | 48/233 [4:28:28<9:55:58, 193.29s/it]  21%|██        | 49/233 [4:30:39<8:55:29, 174.62s/it] 21%|██▏       | 50/233 [4:32:46<8:08:45, 160.25s/it]                                                     {'loss': 0.9886, 'grad_norm': 0.24260303378105164, 'learning_rate': 9.651092459754877e-05, 'epoch': 0.22}
 21%|██▏       | 50/233 [4:32:46<8:08:45, 160.25s/it]
***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****

***** Running Evaluation *****

***** Running Evaluation *****
  Num examples = 3305
  Num examples = 3305
  Batch size = 1
  Batch size = 1
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1
[INFO|trainer.py:4643] 2025-12-05 09:25:14,958 >> 
***** Running Evaluation *****
[INFO|trainer.py:4645] 2025-12-05 09:25:14,958 >>   Num examples = 3305
[INFO|trainer.py:4648] 2025-12-05 09:25:14,958 >>   Batch size = 1

  0%|          | 0/414 [00:00<?, ?it/s][A
  0%|          | 2/414 [00:04<16:23,  2.39s/it][A
  1%|          | 3/414 [00:10<24:55,  3.64s/it][A
  1%|          | 4/414 [00:18<35:43,  5.23s/it][A
  1%|          | 5/414 [00:25<41:25,  6.08s/it][A
  1%|▏         | 6/414 [00:29<37:05,  5.46s/it][A
  2%|▏         | 7/414 [00:34<35:17,  5.20s/it][A
  2%|▏         | 8/414 [00:39<33:53,  5.01s/it][A
  2%|▏         | 9/414 [00:44<35:15,  5.22s/it][A
  2%|▏         | 10/414 [00:52<39:45,  5.90s/it][A
  3%|▎         | 11/414 [00:58<39:21,  5.86s/it][A
  3%|▎         | 12/414 [01:04<40:17,  6.01s/it][A
  3%|▎         | 13/414 [01:12<43:50,  6.56s/it][A
  3%|▎         | 14/414 [01:19<45:03,  6.76s/it][A
  4%|▎         | 15/414 [01:25<44:24,  6.68s/it][A
  4%|▍         | 16/414 [01:32<44:47,  6.75s/it][A
  4%|▍         | 17/414 [01:39<44:49,  6.77s/it][A
  4%|▍         | 18/414 [01:46<45:19,  6.87s/it][A
  5%|▍         | 19/414 [01:55<48:22,  7.35s/it][A
  5%|▍         | 20/414 [02:02<47:27,  7.23s/it][A
  5%|▌         | 21/414 [02:07<43:06,  6.58s/it][A
  5%|▌         | 22/414 [02:13<42:01,  6.43s/it][A
  6%|▌         | 23/414 [02:21<44:15,  6.79s/it][A
  6%|▌         | 24/414 [02:27<42:37,  6.56s/it][A
  6%|▌         | 25/414 [02:33<42:42,  6.59s/it][A
  6%|▋         | 26/414 [02:40<42:46,  6.61s/it][A
  7%|▋         | 27/414 [02:44<38:14,  5.93s/it][A
  7%|▋         | 28/414 [02:49<36:35,  5.69s/it][A
  7%|▋         | 29/414 [02:55<35:40,  5.56s/it][A
  7%|▋         | 30/414 [03:01<36:38,  5.73s/it][A
  7%|▋         | 31/414 [03:06<36:31,  5.72s/it][A
  8%|▊         | 32/414 [03:11<34:10,  5.37s/it][A
  8%|▊         | 33/414 [03:17<36:14,  5.71s/it][A
  8%|▊         | 34/414 [03:23<36:31,  5.77s/it][A
  8%|▊         | 35/414 [03:29<36:18,  5.75s/it][A
  9%|▊         | 36/414 [03:36<37:34,  5.96s/it][A
  9%|▉         | 37/414 [03:42<38:26,  6.12s/it][A
  9%|▉         | 38/414 [03:48<38:41,  6.17s/it][A
  9%|▉         | 39/414 [03:52<34:44,  5.56s/it][A
 10%|▉         | 40/414 [03:58<34:38,  5.56s/it][A
 10%|▉         | 41/414 [04:05<36:28,  5.87s/it][A
 10%|█         | 42/414 [04:10<36:28,  5.88s/it][A
 10%|█         | 43/414 [04:20<42:16,  6.84s/it][A
 11%|█         | 44/414 [04:30<49:24,  8.01s/it][A
 11%|█         | 45/414 [04:37<47:34,  7.74s/it][A
 11%|█         | 46/414 [04:42<41:41,  6.80s/it][A
 11%|█▏        | 47/414 [04:46<37:00,  6.05s/it][A
 12%|█▏        | 48/414 [04:50<32:45,  5.37s/it][A
 12%|█▏        | 49/414 [04:54<29:11,  4.80s/it][A
 12%|█▏        | 50/414 [04:58<27:42,  4.57s/it][A
 12%|█▏        | 51/414 [05:02<26:55,  4.45s/it][A
 13%|█▎        | 52/414 [05:06<26:40,  4.42s/it][A
 13%|█▎        | 53/414 [05:14<32:01,  5.32s/it][A
 13%|█▎        | 54/414 [05:22<37:35,  6.27s/it][A
 13%|█▎        | 55/414 [05:29<38:35,  6.45s/it][A
 14%|█▎        | 56/414 [05:35<38:32,  6.46s/it][A
 14%|█▍        | 57/414 [05:40<35:48,  6.02s/it][A
 14%|█▍        | 58/414 [05:46<35:17,  5.95s/it][A
 14%|█▍        | 59/414 [05:53<37:21,  6.31s/it][A
 14%|█▍        | 60/414 [05:59<36:31,  6.19s/it][A
 15%|█▍        | 61/414 [06:04<33:03,  5.62s/it][A
 15%|█▍        | 62/414 [06:08<30:37,  5.22s/it][A
 15%|█▌        | 63/414 [06:13<30:06,  5.15s/it][A
 15%|█▌        | 64/414 [06:18<29:47,  5.11s/it][A
 16%|█▌        | 65/414 [06:23<29:46,  5.12s/it][A
 16%|█▌        | 66/414 [06:29<30:39,  5.29s/it][A
 16%|█▌        | 67/414 [06:36<33:50,  5.85s/it][A
 16%|█▋        | 68/414 [06:43<35:58,  6.24s/it][A
 17%|█▋        | 69/414 [06:50<37:16,  6.48s/it][A
 17%|█▋        | 70/414 [06:57<38:17,  6.68s/it][A
 17%|█▋        | 71/414 [07:02<35:23,  6.19s/it][A
 17%|█▋        | 72/414 [07:07<33:12,  5.83s/it][A
 18%|█▊        | 73/414 [07:12<31:30,  5.54s/it][A
 18%|█▊        | 74/414 [07:16<29:00,  5.12s/it][A
 18%|█▊        | 75/414 [07:20<26:51,  4.76s/it][A
 18%|█▊        | 76/414 [07:25<26:21,  4.68s/it][A
 19%|█▊        | 77/414 [07:31<28:45,  5.12s/it][A
 19%|█▉        | 78/414 [07:37<30:05,  5.37s/it][A
 19%|█▉        | 79/414 [07:43<30:57,  5.54s/it][A
 19%|█▉        | 80/414 [07:48<30:25,  5.46s/it][A
 20%|█▉        | 81/414 [07:53<29:02,  5.23s/it][A
 20%|█▉        | 82/414 [07:58<29:08,  5.27s/it][A
 20%|██        | 83/414 [08:05<32:41,  5.93s/it][A
 20%|██        | 84/414 [08:13<35:12,  6.40s/it][A
 21%|██        | 85/414 [08:18<32:40,  5.96s/it][A
 21%|██        | 86/414 [08:23<30:50,  5.64s/it][A
 21%|██        | 87/414 [08:27<29:06,  5.34s/it][A
 21%|██▏       | 88/414 [08:32<28:13,  5.20s/it][A
 21%|██▏       | 89/414 [08:38<29:06,  5.37s/it][A
 22%|██▏       | 90/414 [08:45<31:52,  5.90s/it][A
 22%|██▏       | 91/414 [08:51<30:56,  5.75s/it][A
 22%|██▏       | 92/414 [08:56<30:25,  5.67s/it][A
 22%|██▏       | 93/414 [09:02<30:07,  5.63s/it][A
 23%|██▎       | 94/414 [09:07<29:30,  5.53s/it][A
 23%|██▎       | 95/414 [09:13<29:58,  5.64s/it][A
 23%|██▎       | 96/414 [09:19<30:32,  5.76s/it][A
 23%|██▎       | 97/414 [09:24<30:09,  5.71s/it][A
 24%|██▎       | 98/414 [09:29<27:32,  5.23s/it][A
 24%|██▍       | 99/414 [09:33<26:35,  5.06s/it][A
 24%|██▍       | 100/414 [09:39<28:23,  5.43s/it][A
 24%|██▍       | 101/414 [09:48<32:27,  6.22s/it][A
 25%|██▍       | 102/414 [09:54<33:14,  6.39s/it][A
 25%|██▍       | 103/414 [09:59<30:36,  5.90s/it][A
 25%|██▌       | 104/414 [10:04<28:31,  5.52s/it][A
 25%|██▌       | 105/414 [10:09<28:38,  5.56s/it][A
 26%|██▌       | 106/414 [10:17<31:22,  6.11s/it][A
 26%|██▌       | 107/414 [10:23<31:11,  6.10s/it][A
 26%|██▌       | 108/414 [10:28<29:14,  5.73s/it][A
 26%|██▋       | 109/414 [10:33<28:01,  5.51s/it][A
 27%|██▋       | 110/414 [10:39<28:35,  5.64s/it][A
 27%|██▋       | 111/414 [10:45<29:28,  5.84s/it][A
 27%|██▋       | 112/414 [10:50<27:58,  5.56s/it][A
 27%|██▋       | 113/414 [10:56<29:02,  5.79s/it][A
 28%|██▊       | 114/414 [11:03<31:01,  6.21s/it][A
 28%|██▊       | 115/414 [11:09<30:23,  6.10s/it][A
 28%|██▊       | 116/414 [11:15<30:19,  6.11s/it][A
 28%|██▊       | 117/414 [11:23<33:11,  6.71s/it][A
 29%|██▊       | 118/414 [11:32<36:30,  7.40s/it][A
 29%|██▊       | 119/414 [11:42<39:01,  7.94s/it][A
 29%|██▉       | 120/414 [11:50<39:06,  7.98s/it][A
 29%|██▉       | 121/414 [11:57<38:27,  7.88s/it][A
 29%|██▉       | 122/414 [12:05<37:25,  7.69s/it][A
 30%|██▉       | 123/414 [12:11<35:45,  7.37s/it][A
 30%|██▉       | 124/414 [12:19<35:54,  7.43s/it][A
 30%|███       | 125/414 [12:26<35:56,  7.46s/it][A
 30%|███       | 126/414 [12:32<33:19,  6.94s/it][A
 31%|███       | 127/414 [12:39<33:29,  7.00s/it][A
 31%|███       | 128/414 [12:48<36:02,  7.56s/it][A
 31%|███       | 129/414 [12:55<34:49,  7.33s/it][A
 31%|███▏      | 130/414 [13:02<34:04,  7.20s/it][A
 32%|███▏      | 131/414 [13:10<35:45,  7.58s/it][A
 32%|███▏      | 132/414 [13:18<35:40,  7.59s/it][A
 32%|███▏      | 133/414 [13:22<30:54,  6.60s/it][A
 32%|███▏      | 134/414 [13:28<29:53,  6.40s/it][A
 33%|███▎      | 135/414 [13:35<29:55,  6.44s/it][A
 33%|███▎      | 136/414 [13:42<31:39,  6.83s/it][A
 33%|███▎      | 137/414 [13:52<34:58,  7.58s/it][A
 33%|███▎      | 138/414 [13:59<34:11,  7.43s/it][A
 34%|███▎      | 139/414 [14:04<30:29,  6.65s/it][A
 34%|███▍      | 140/414 [14:08<27:45,  6.08s/it][A
 34%|███▍      | 141/414 [14:14<26:56,  5.92s/it][A
 34%|███▍      | 142/414 [14:21<27:49,  6.14s/it][A
 35%|███▍      | 143/414 [14:27<27:43,  6.14s/it][A
 35%|███▍      | 144/414 [14:31<25:22,  5.64s/it][A
 35%|███▌      | 145/414 [14:36<23:38,  5.27s/it][A
 35%|███▌      | 146/414 [14:40<22:07,  4.95s/it][A
 36%|███▌      | 147/414 [14:45<21:54,  4.92s/it][A
 36%|███▌      | 148/414 [14:50<22:30,  5.08s/it][A
 36%|███▌      | 149/414 [14:55<22:11,  5.02s/it][A
 36%|███▌      | 150/414 [15:00<21:35,  4.91s/it][A
 36%|███▋      | 151/414 [15:04<20:29,  4.68s/it][A
 37%|███▋      | 152/414 [15:09<20:52,  4.78s/it][A
 37%|███▋      | 153/414 [15:14<21:35,  4.97s/it][A
 37%|███▋      | 154/414 [15:18<20:36,  4.76s/it][A
 37%|███▋      | 155/414 [15:23<19:56,  4.62s/it][A
 38%|███▊      | 156/414 [15:28<20:47,  4.84s/it][A
 38%|███▊      | 157/414 [15:34<22:38,  5.29s/it][A
 38%|███▊      | 158/414 [15:41<24:00,  5.63s/it][A
 38%|███▊      | 159/414 [15:47<24:48,  5.84s/it][A
 39%|███▊      | 160/414 [15:53<24:30,  5.79s/it][A
 39%|███▉      | 161/414 [16:00<25:41,  6.09s/it][A
 39%|███▉      | 162/414 [16:06<25:37,  6.10s/it][A
 39%|███▉      | 163/414 [16:11<24:57,  5.97s/it][A
 40%|███▉      | 164/414 [16:19<26:17,  6.31s/it][A
 40%|███▉      | 165/414 [16:26<27:39,  6.67s/it][A
 40%|████      | 166/414 [16:33<28:30,  6.90s/it][A
 40%|████      | 167/414 [16:40<28:03,  6.82s/it][A
 41%|████      | 168/414 [16:46<26:31,  6.47s/it][A
 41%|████      | 169/414 [16:50<24:11,  5.92s/it][A
 41%|████      | 170/414 [16:55<22:41,  5.58s/it][A
 41%|████▏     | 171/414 [17:01<22:24,  5.53s/it][A
 42%|████▏     | 172/414 [17:07<23:07,  5.73s/it][A
 42%|████▏     | 173/414 [17:13<23:36,  5.88s/it][A
 42%|████▏     | 174/414 [17:19<23:38,  5.91s/it][A
 42%|████▏     | 175/414 [17:25<23:45,  5.96s/it][A
 43%|████▎     | 176/414 [17:30<22:16,  5.61s/it][A
 43%|████▎     | 177/414 [17:36<22:28,  5.69s/it][A
 43%|████▎     | 178/414 [17:42<23:25,  5.95s/it][A
 43%|████▎     | 179/414 [17:48<22:30,  5.75s/it][A
 43%|████▎     | 180/414 [17:53<22:21,  5.73s/it][A
 44%|████▎     | 181/414 [18:00<22:54,  5.90s/it][A
 44%|████▍     | 182/414 [18:05<22:34,  5.84s/it][A
 44%|████▍     | 183/414 [18:12<23:09,  6.02s/it][A
 44%|████▍     | 184/414 [18:18<22:56,  5.99s/it][A
 45%|████▍     | 185/414 [18:23<21:40,  5.68s/it][A
 45%|████▍     | 186/414 [18:29<22:53,  6.02s/it][A
 45%|████▌     | 187/414 [18:39<27:15,  7.20s/it][A
 45%|████▌     | 188/414 [18:49<29:49,  7.92s/it][A
 46%|████▌     | 189/414 [18:58<30:29,  8.13s/it][A
 46%|████▌     | 190/414 [19:06<30:26,  8.15s/it][A
 46%|████▌     | 191/414 [19:12<27:52,  7.50s/it][A
 46%|████▋     | 192/414 [19:18<25:51,  6.99s/it][A
 47%|████▋     | 193/414 [19:22<23:01,  6.25s/it][A
 47%|████▋     | 194/414 [19:27<21:53,  5.97s/it][A
 47%|████▋     | 195/414 [19:32<20:48,  5.70s/it][A
 47%|████▋     | 196/414 [19:38<20:15,  5.58s/it][A
 48%|████▊     | 197/414 [19:45<21:42,  6.00s/it][A
 48%|████▊     | 198/414 [19:51<21:44,  6.04s/it][A
 48%|████▊     | 199/414 [19:56<20:13,  5.64s/it][A
 48%|████▊     | 200/414 [20:01<20:11,  5.66s/it][A
 49%|████▊     | 201/414 [20:08<21:27,  6.04s/it][A
 49%|████▉     | 202/414 [20:15<21:48,  6.17s/it][A
 49%|████▉     | 203/414 [20:19<20:11,  5.74s/it][A
 49%|████▉     | 204/414 [20:24<18:39,  5.33s/it][A
 50%|████▉     | 205/414 [20:28<17:42,  5.08s/it][A
 50%|████▉     | 206/414 [20:34<18:16,  5.27s/it][A
 50%|█████     | 207/414 [20:40<18:38,  5.40s/it][A
 50%|█████     | 208/414 [20:46<19:11,  5.59s/it][A
 50%|█████     | 209/414 [20:54<21:18,  6.24s/it][A
 51%|█████     | 210/414 [21:00<21:30,  6.33s/it][A
 51%|█████     | 211/414 [21:05<20:13,  5.98s/it][A
 51%|█████     | 212/414 [21:11<19:56,  5.92s/it][A
 51%|█████▏    | 213/414 [21:19<21:34,  6.44s/it][A
 52%|█████▏    | 214/414 [21:26<21:54,  6.57s/it][A
 52%|█████▏    | 215/414 [21:32<21:49,  6.58s/it][A
 52%|█████▏    | 216/414 [21:41<24:04,  7.29s/it][A
 52%|█████▏    | 217/414 [21:49<24:23,  7.43s/it][A
 53%|█████▎    | 218/414 [21:54<22:04,  6.76s/it][A
 53%|█████▎    | 219/414 [22:00<21:08,  6.51s/it][A
 53%|█████▎    | 220/414 [22:05<19:55,  6.16s/it][A
 53%|█████▎    | 221/414 [22:10<18:31,  5.76s/it][A
 54%|█████▎    | 222/414 [22:17<19:03,  5.95s/it][A
 54%|█████▍    | 223/414 [22:23<19:34,  6.15s/it][A
 54%|█████▍    | 224/414 [22:30<20:21,  6.43s/it][A
 54%|█████▍    | 225/414 [22:37<20:41,  6.57s/it][A
 55%|█████▍    | 226/414 [22:44<20:31,  6.55s/it][A
 55%|█████▍    | 227/414 [22:52<21:56,  7.04s/it][A
 55%|█████▌    | 228/414 [22:58<21:26,  6.92s/it][A
 55%|█████▌    | 229/414 [23:04<19:56,  6.47s/it][A
 56%|█████▌    | 230/414 [23:11<20:28,  6.68s/it][A
 56%|█████▌    | 231/414 [23:18<20:23,  6.69s/it][A
 56%|█████▌    | 232/414 [23:23<19:18,  6.37s/it][A
 56%|█████▋    | 233/414 [23:30<19:33,  6.48s/it][A
 57%|█████▋    | 234/414 [23:37<19:50,  6.61s/it][A
 57%|█████▋    | 235/414 [23:45<20:46,  6.96s/it][A
 57%|█████▋    | 236/414 [23:52<20:59,  7.07s/it][A
 57%|█████▋    | 237/414 [23:58<19:39,  6.67s/it][A
 57%|█████▋    | 238/414 [24:02<17:10,  5.86s/it][A
 58%|█████▊    | 239/414 [24:07<16:46,  5.75s/it][A
 58%|█████▊    | 240/414 [24:15<18:10,  6.27s/it][A
 58%|█████▊    | 241/414 [24:20<17:30,  6.07s/it][A
 58%|█████▊    | 242/414 [24:24<15:35,  5.44s/it][A
 59%|█████▊    | 243/414 [24:29<14:53,  5.23s/it][A
 59%|█████▉    | 244/414 [24:35<15:21,  5.42s/it][A
 59%|█████▉    | 245/414 [24:41<15:32,  5.52s/it][A
 59%|█████▉    | 246/414 [24:45<14:42,  5.25s/it][A
 60%|█████▉    | 247/414 [24:51<14:32,  5.22s/it][A
 60%|█████▉    | 248/414 [24:56<14:22,  5.20s/it][A
 60%|██████    | 249/414 [25:00<13:57,  5.08s/it][A
 60%|██████    | 250/414 [25:05<13:24,  4.90s/it][A
 61%|██████    | 251/414 [25:11<13:55,  5.12s/it][A
 61%|██████    | 252/414 [25:17<14:37,  5.42s/it][A
 61%|██████    | 253/414 [25:24<16:05,  6.00s/it][A
 61%|██████▏   | 254/414 [25:32<17:27,  6.55s/it][A
 62%|██████▏   | 255/414 [25:36<15:42,  5.93s/it][A
 62%|██████▏   | 256/414 [25:41<14:44,  5.60s/it][A
 62%|██████▏   | 257/414 [25:46<14:18,  5.47s/it][A
 62%|██████▏   | 258/414 [25:51<13:44,  5.28s/it][A
 63%|██████▎   | 259/414 [25:56<13:05,  5.07s/it][A
 63%|██████▎   | 260/414 [26:02<13:38,  5.32s/it][A
 63%|██████▎   | 261/414 [26:08<14:08,  5.55s/it][A
 63%|██████▎   | 262/414 [26:15<14:58,  5.91s/it][A
 64%|██████▎   | 263/414 [26:22<15:52,  6.31s/it][A
 64%|██████▍   | 264/414 [26:28<15:58,  6.39s/it][A
 64%|██████▍   | 265/414 [26:35<15:44,  6.34s/it][A
 64%|██████▍   | 266/414 [26:41<15:42,  6.37s/it][A
 64%|██████▍   | 267/414 [26:49<16:39,  6.80s/it][A
 65%|██████▍   | 268/414 [26:55<15:59,  6.57s/it][A
 65%|██████▍   | 269/414 [26:59<14:14,  5.89s/it][A
 65%|██████▌   | 270/414 [27:04<13:21,  5.56s/it][A
 65%|██████▌   | 271/414 [27:09<13:01,  5.47s/it][A
 66%|██████▌   | 272/414 [27:15<13:12,  5.58s/it][A
 66%|██████▌   | 273/414 [27:21<13:41,  5.82s/it][A
 66%|██████▌   | 274/414 [27:27<13:41,  5.87s/it][A
 66%|██████▋   | 275/414 [27:33<13:15,  5.73s/it][A
 67%|██████▋   | 276/414 [27:38<12:53,  5.61s/it][A
 67%|██████▋   | 277/414 [27:43<12:25,  5.44s/it][A
 67%|██████▋   | 278/414 [27:47<11:28,  5.06s/it][A
 67%|██████▋   | 279/414 [27:52<11:16,  5.01s/it][A
 68%|██████▊   | 280/414 [27:58<11:41,  5.24s/it][A
 68%|██████▊   | 281/414 [28:03<11:27,  5.17s/it][A
 68%|██████▊   | 282/414 [28:09<11:51,  5.39s/it][A
 68%|██████▊   | 283/414 [28:15<12:13,  5.60s/it][A
 69%|██████▊   | 284/414 [28:20<11:42,  5.40s/it][A
 69%|██████▉   | 285/414 [28:26<12:07,  5.64s/it][A
 69%|██████▉   | 286/414 [28:34<13:09,  6.17s/it][A
 69%|██████▉   | 287/414 [28:41<13:39,  6.45s/it][A
 70%|██████▉   | 288/414 [28:47<13:19,  6.34s/it][A
 70%|██████▉   | 289/414 [28:51<12:09,  5.83s/it][A
 70%|███████   | 290/414 [28:58<12:23,  6.00s/it][A
 70%|███████   | 291/414 [29:05<12:50,  6.26s/it][A
 71%|███████   | 292/414 [29:09<11:30,  5.66s/it][A
 71%|███████   | 293/414 [29:17<12:50,  6.37s/it][A
 71%|███████   | 294/414 [29:26<14:26,  7.22s/it][A
 71%|███████▏  | 295/414 [29:34<14:42,  7.41s/it][A
 71%|███████▏  | 296/414 [29:42<14:38,  7.45s/it][A
 72%|███████▏  | 297/414 [29:47<13:30,  6.93s/it][A
 72%|███████▏  | 298/414 [29:55<13:56,  7.21s/it][A
 72%|███████▏  | 299/414 [30:02<13:37,  7.11s/it][A
 72%|███████▏  | 300/414 [30:07<12:26,  6.55s/it][A
 73%|███████▎  | 301/414 [30:13<11:45,  6.25s/it][A
 73%|███████▎  | 302/414 [30:17<10:47,  5.78s/it][A
 73%|███████▎  | 303/414 [30:23<10:21,  5.60s/it][A
 73%|███████▎  | 304/414 [30:31<11:39,  6.36s/it][A
 74%|███████▎  | 305/414 [30:39<12:33,  6.91s/it][A
 74%|███████▍  | 306/414 [30:45<11:46,  6.54s/it][A
 74%|███████▍  | 307/414 [30:50<11:08,  6.25s/it][A
 74%|███████▍  | 308/414 [30:57<11:31,  6.52s/it][A
 75%|███████▍  | 309/414 [31:05<11:50,  6.76s/it][A
 75%|███████▍  | 310/414 [31:09<10:38,  6.14s/it][A
 75%|███████▌  | 311/414 [31:14<09:58,  5.81s/it][A
 75%|███████▌  | 312/414 [31:20<09:42,  5.71s/it][A
 76%|███████▌  | 313/414 [31:25<09:21,  5.56s/it][A
 76%|███████▌  | 314/414 [31:30<08:51,  5.31s/it][A
 76%|███████▌  | 315/414 [31:36<09:02,  5.48s/it][A
 76%|███████▋  | 316/414 [31:45<10:38,  6.52s/it][A
 77%|███████▋  | 317/414 [31:52<11:06,  6.87s/it][A
 77%|███████▋  | 318/414 [31:58<10:26,  6.52s/it][A
 77%|███████▋  | 319/414 [32:04<10:03,  6.36s/it][A
 77%|███████▋  | 320/414 [32:09<09:17,  5.93s/it][A
 78%|███████▊  | 321/414 [32:15<09:25,  6.08s/it][A
 78%|███████▊  | 322/414 [32:22<09:28,  6.18s/it][A
 78%|███████▊  | 323/414 [32:27<08:43,  5.75s/it][A
 78%|███████▊  | 324/414 [32:31<08:09,  5.44s/it][A
 79%|███████▊  | 325/414 [32:36<07:52,  5.31s/it][A
 79%|███████▊  | 326/414 [32:41<07:32,  5.15s/it][A
 79%|███████▉  | 327/414 [32:48<08:19,  5.74s/it][A
 79%|███████▉  | 328/414 [32:55<08:43,  6.08s/it][A
 79%|███████▉  | 329/414 [33:02<08:51,  6.25s/it][A
 80%|███████▉  | 330/414 [33:10<09:30,  6.79s/it][A
 80%|███████▉  | 331/414 [33:17<09:32,  6.89s/it][A
 80%|████████  | 332/414 [33:24<09:24,  6.88s/it][A
 80%|████████  | 333/414 [33:30<08:58,  6.65s/it][A
 81%|████████  | 334/414 [33:34<07:59,  5.99s/it][A
 81%|████████  | 335/414 [33:39<07:18,  5.56s/it][A
 81%|████████  | 336/414 [33:44<07:05,  5.45s/it][A
 81%|████████▏ | 337/414 [33:49<06:42,  5.22s/it][A
 82%|████████▏ | 338/414 [33:55<06:53,  5.44s/it][A
 82%|████████▏ | 339/414 [34:02<07:32,  6.03s/it][A
 82%|████████▏ | 340/414 [34:10<08:14,  6.68s/it][A
 82%|████████▏ | 341/414 [34:17<08:09,  6.71s/it][A
 83%|████████▎ | 342/414 [34:21<07:08,  5.96s/it][A
 83%|████████▎ | 343/414 [34:28<07:28,  6.32s/it][A
 83%|████████▎ | 344/414 [34:38<08:20,  7.16s/it][A
 83%|████████▎ | 345/414 [34:44<08:02,  6.99s/it][A
 84%|████████▎ | 346/414 [34:49<07:08,  6.30s/it][A
 84%|████████▍ | 347/414 [34:54<06:46,  6.06s/it][A
 84%|████████▍ | 348/414 [35:00<06:34,  5.98s/it][A
 84%|████████▍ | 349/414 [35:05<06:11,  5.71s/it][A
 85%|████████▍ | 350/414 [35:13<06:37,  6.21s/it][A
 85%|████████▍ | 351/414 [35:20<06:52,  6.55s/it][A
 85%|████████▌ | 352/414 [35:25<06:12,  6.00s/it][A
 85%|████████▌ | 353/414 [35:29<05:38,  5.56s/it][A
 86%|████████▌ | 354/414 [35:35<05:31,  5.52s/it][A
 86%|████████▌ | 355/414 [35:40<05:20,  5.43s/it][A
 86%|████████▌ | 356/414 [35:45<05:05,  5.26s/it][A
 86%|████████▌ | 357/414 [35:51<05:13,  5.50s/it][A
 86%|████████▋ | 358/414 [35:58<05:44,  6.15s/it][A
 87%|████████▋ | 359/414 [36:05<05:39,  6.17s/it][A
 87%|████████▋ | 360/414 [36:09<05:09,  5.73s/it][A
 87%|████████▋ | 361/414 [36:14<04:41,  5.31s/it][A
 87%|████████▋ | 362/414 [36:20<04:54,  5.65s/it][A
 88%|████████▊ | 363/414 [36:28<05:18,  6.25s/it][A
 88%|████████▊ | 364/414 [36:33<05:00,  6.01s/it][A
 88%|████████▊ | 365/414 [36:39<04:55,  6.04s/it][A
 88%|████████▊ | 366/414 [36:47<05:09,  6.44s/it][A
 89%|████████▊ | 367/414 [36:53<05:00,  6.39s/it][A
 89%|████████▉ | 368/414 [36:59<04:52,  6.35s/it][A
 89%|████████▉ | 369/414 [37:07<05:03,  6.75s/it][A
 89%|████████▉ | 370/414 [37:13<04:54,  6.69s/it][A
 90%|████████▉ | 371/414 [37:20<04:44,  6.62s/it][A
 90%|████████▉ | 372/414 [37:27<04:41,  6.71s/it][A
 90%|█████████ | 373/414 [37:34<04:46,  6.98s/it][A
 90%|█████████ | 374/414 [37:43<04:55,  7.39s/it][A
 91%|█████████ | 375/414 [37:49<04:32,  7.00s/it][A
 91%|█████████ | 376/414 [37:53<03:53,  6.15s/it][A
 91%|█████████ | 377/414 [37:59<03:44,  6.06s/it][A
 91%|█████████▏| 378/414 [38:06<03:52,  6.46s/it][A
 92%|█████████▏| 379/414 [38:12<03:36,  6.17s/it][A
 92%|█████████▏| 380/414 [38:19<03:40,  6.49s/it][A
 92%|█████████▏| 381/414 [38:30<04:14,  7.72s/it][A
 92%|█████████▏| 382/414 [38:37<04:06,  7.70s/it][A
 93%|█████████▎| 383/414 [38:42<03:30,  6.80s/it][A
 93%|█████████▎| 384/414 [38:48<03:16,  6.57s/it][A
 93%|█████████▎| 385/414 [38:54<03:07,  6.45s/it][A
 93%|█████████▎| 386/414 [38:59<02:48,  6.01s/it][A
 93%|█████████▎| 387/414 [39:04<02:32,  5.64s/it][A
 94%|█████████▎| 388/414 [39:09<02:22,  5.49s/it][A
 94%|█████████▍| 389/414 [39:14<02:10,  5.22s/it][A
 94%|█████████▍| 390/414 [39:18<01:57,  4.91s/it][A
 94%|█████████▍| 391/414 [39:22<01:50,  4.81s/it][A
 95%|█████████▍| 392/414 [39:27<01:45,  4.81s/it][A
 95%|█████████▍| 393/414 [39:34<01:52,  5.36s/it][A
 95%|█████████▌| 394/414 [39:41<01:59,  5.97s/it][A
 95%|█████████▌| 395/414 [39:46<01:48,  5.71s/it][A
 96%|█████████▌| 396/414 [39:53<01:47,  5.99s/it][A
 96%|█████████▌| 397/414 [40:02<01:54,  6.74s/it][A
 96%|█████████▌| 398/414 [40:08<01:44,  6.53s/it][A
 96%|█████████▋| 399/414 [40:11<01:25,  5.73s/it][A
 97%|█████████▋| 400/414 [40:19<01:28,  6.31s/it][A
 97%|█████████▋| 401/414 [40:29<01:37,  7.52s/it][A
 97%|█████████▋| 402/414 [40:36<01:25,  7.15s/it][A
 97%|█████████▋| 403/414 [40:40<01:09,  6.34s/it][A
 98%|█████████▊| 404/414 [40:45<00:57,  5.77s/it][A
 98%|█████████▊| 405/414 [40:49<00:48,  5.37s/it][A
 98%|█████████▊| 406/414 [40:56<00:46,  5.76s/it][A
 98%|█████████▊| 407/414 [41:03<00:44,  6.34s/it][A
 99%|█████████▊| 408/414 [41:09<00:36,  6.16s/it][A
 99%|█████████▉| 409/414 [41:15<00:30,  6.17s/it][A
 99%|█████████▉| 410/414 [41:22<00:24,  6.24s/it][A
 99%|█████████▉| 411/414 [41:26<00:16,  5.66s/it][A
100%|█████████▉| 412/414 [41:33<00:11,  5.90s/it][A
100%|█████████▉| 413/414 [41:39<00:06,  6.19s/it][A
100%|██████████| 414/414 [41:42<00:00,  5.20s/it][A                                                     
                                                 [A{'eval_loss': 0.9413235187530518, 'eval_runtime': 2510.0642, 'eval_samples_per_second': 1.317, 'eval_steps_per_second': 0.165, 'epoch': 0.22}
 21%|██▏       | 50/233 [5:14:36<8:08:45, 160.25s/it]
100%|██████████| 414/414 [41:43<00:00,  5.20s/it][A
                                                 [A 22%|██▏       | 51/233 [5:17:02<45:57:02, 908.91s/it] 22%|██▏       | 52/233 [5:19:10<33:55:19, 674.69s/it] 23%|██▎       | 53/233 [5:21:15<25:29:30, 509.84s/it] 23%|██▎       | 54/233 [5:23:12<19:29:03, 391.86s/it] 24%|██▎       | 55/233 [5:25:06<15:15:09, 308.48s/it] 24%|██▍       | 56/233 [5:27:24<12:39:16, 257.38s/it] 24%|██▍       | 57/233 [5:29:49<10:56:04, 223.66s/it] 25%|██▍       | 58/233 [5:31:54<9:25:59, 194.05s/it]  25%|██▌       | 59/233 [5:34:10<8:32:36, 176.76s/it] 26%|██▌       | 60/233 [5:36:10<7:40:07, 159.58s/it]                                                     {'loss': 0.9599, 'grad_norm': 0.1921461969614029, 'learning_rate': 9.32385029361338e-05, 'epoch': 0.26}
 26%|██▌       | 60/233 [5:36:10<7:40:07, 159.58s/it]
***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1
[INFO|trainer.py:4643] 2025-12-05 10:28:38,624 >> 
***** Running Evaluation *****

***** Running Evaluation *****

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****
[INFO|trainer.py:4645] 2025-12-05 10:28:38,624 >>   Num examples = 3305

***** Running Evaluation *****
  Num examples = 3305

***** Running Evaluation *****
  Batch size = 1
  Num examples = 3305
[INFO|trainer.py:4648] 2025-12-05 10:28:38,624 >>   Batch size = 1
  Batch size = 1
  Num examples = 3305
  Batch size = 1
  Num examples = 3305
  Batch size = 1

  0%|          | 0/414 [00:00<?, ?it/s][A
  0%|          | 2/414 [00:04<16:18,  2.37s/it][A
  1%|          | 3/414 [00:10<24:53,  3.63s/it][A
  1%|          | 4/414 [00:17<35:14,  5.16s/it][A
  1%|          | 5/414 [00:25<41:04,  6.03s/it][A
  1%|▏         | 6/414 [00:29<36:53,  5.43s/it][A
  2%|▏         | 7/414 [00:34<35:11,  5.19s/it][A
  2%|▏         | 8/414 [00:39<33:49,  5.00s/it][A
  2%|▏         | 9/414 [00:44<35:12,  5.22s/it][A
  2%|▏         | 10/414 [00:52<39:36,  5.88s/it][A
  3%|▎         | 11/414 [00:57<39:13,  5.84s/it][A
  3%|▎         | 12/414 [01:04<40:16,  6.01s/it][A
  3%|▎         | 13/414 [01:12<43:45,  6.55s/it][A
  3%|▎         | 14/414 [01:19<44:58,  6.75s/it][A
  4%|▎         | 15/414 [01:25<44:14,  6.65s/it][A
  4%|▍         | 16/414 [01:32<44:43,  6.74s/it][A
  4%|▍         | 17/414 [01:39<44:41,  6.75s/it][A
  4%|▍         | 18/414 [01:46<45:01,  6.82s/it][A
  5%|▍         | 19/414 [01:54<48:09,  7.32s/it][A
  5%|▍         | 20/414 [02:01<47:14,  7.19s/it][A
  5%|▌         | 21/414 [02:06<42:58,  6.56s/it][A
  5%|▌         | 22/414 [02:12<42:02,  6.43s/it][A
  6%|▌         | 23/414 [02:20<43:56,  6.74s/it][A
  6%|▌         | 24/414 [02:26<42:27,  6.53s/it][A
  6%|▌         | 25/414 [02:33<42:39,  6.58s/it][A
  6%|▋         | 26/414 [02:39<42:46,  6.62s/it][A
  7%|▋         | 27/414 [02:44<38:08,  5.91s/it][A
  7%|▋         | 28/414 [02:49<36:27,  5.67s/it][A
  7%|▋         | 29/414 [02:54<35:36,  5.55s/it][A
  7%|▋         | 30/414 [03:00<36:34,  5.72s/it][A
  7%|▋         | 31/414 [03:06<36:28,  5.72s/it][A
  8%|▊         | 32/414 [03:10<34:08,  5.36s/it][A
  8%|▊         | 33/414 [03:17<36:10,  5.70s/it][A
  8%|▊         | 34/414 [03:23<36:25,  5.75s/it][A
  8%|▊         | 35/414 [03:28<36:14,  5.74s/it][A
  9%|▊         | 36/414 [03:35<37:32,  5.96s/it][A
  9%|▉         | 37/414 [03:41<38:30,  6.13s/it][A
  9%|▉         | 38/414 [03:48<38:44,  6.18s/it][A
  9%|▉         | 39/414 [03:52<34:42,  5.55s/it][A
 10%|▉         | 40/414 [03:57<34:39,  5.56s/it][A
 10%|▉         | 41/414 [04:04<36:32,  5.88s/it][A
 10%|█         | 42/414 [04:10<36:32,  5.89s/it][A
 10%|█         | 43/414 [04:19<42:21,  6.85s/it][A
 11%|█         | 44/414 [04:30<49:22,  8.01s/it][A
 11%|█         | 45/414 [04:37<47:35,  7.74s/it][A
 11%|█         | 46/414 [04:42<41:50,  6.82s/it][A
 11%|█▏        | 47/414 [04:46<37:12,  6.08s/it][A
 12%|█▏        | 48/414 [04:50<32:58,  5.40s/it][A
 12%|█▏        | 49/414 [04:53<29:25,  4.84s/it][A
 12%|█▏        | 50/414 [04:57<27:57,  4.61s/it][A
 12%|█▏        | 51/414 [05:01<27:07,  4.48s/it][A
 13%|█▎        | 52/414 [05:06<26:47,  4.44s/it][A
 13%|█▎        | 53/414 [05:13<32:13,  5.36s/it][A
 13%|█▎        | 54/414 [05:22<37:43,  6.29s/it][A
 13%|█▎        | 55/414 [05:29<38:39,  6.46s/it][A
 14%|█▎        | 56/414 [05:35<38:38,  6.48s/it][A
 14%|█▍        | 57/414 [05:40<35:49,  6.02s/it][A
 14%|█▍        | 58/414 [05:46<35:22,  5.96s/it][A
 14%|█▍        | 59/414 [05:53<37:22,  6.32s/it][A
 14%|█▍        | 60/414 [05:59<36:33,  6.20s/it][A
 15%|█▍        | 61/414 [06:03<33:08,  5.63s/it][A
 15%|█▍        | 62/414 [06:08<30:42,  5.24s/it][A
 15%|█▌        | 63/414 [06:13<30:10,  5.16s/it][A
 15%|█▌        | 64/414 [06:18<29:53,  5.12s/it][A
 16%|█▌        | 65/414 [06:23<30:07,  5.18s/it][A
 16%|█▌        | 66/414 [06:29<30:50,  5.32s/it][A
 16%|█▌        | 67/414 [06:36<34:03,  5.89s/it][A
 16%|█▋        | 68/414 [06:43<36:08,  6.27s/it][A
 17%|█▋        | 69/414 [06:50<37:24,  6.50s/it][A
 17%|█▋        | 70/414 [06:57<38:23,  6.70s/it][A
 17%|█▋        | 71/414 [07:02<35:29,  6.21s/it][A
 17%|█▋        | 72/414 [07:07<33:15,  5.84s/it][A
 18%|█▊        | 73/414 [07:12<31:30,  5.54s/it][A
 18%|█▊        | 74/414 [07:16<28:59,  5.12s/it][A
 18%|█▊        | 75/414 [07:20<27:00,  4.78s/it][A
 18%|█▊        | 76/414 [07:25<26:25,  4.69s/it][A
 19%|█▊        | 77/414 [07:31<28:47,  5.13s/it][A
 19%|█▉        | 78/414 [07:37<30:06,  5.38s/it][A
 19%|█▉        | 79/414 [07:43<30:57,  5.54s/it][A
 19%|█▉        | 80/414 [07:48<30:27,  5.47s/it][A
 20%|█▉        | 81/414 [07:53<29:04,  5.24s/it][A
 20%|█▉        | 82/414 [07:58<29:08,  5.27s/it][A
 20%|██        | 83/414 [08:05<32:38,  5.92s/it][A
 20%|██        | 84/414 [08:13<35:10,  6.40s/it][A
 21%|██        | 85/414 [08:18<32:36,  5.95s/it][A
 21%|██        | 86/414 [08:23<30:56,  5.66s/it][A
 21%|██        | 87/414 [08:27<29:04,  5.33s/it][A
 21%|██▏       | 88/414 [08:32<28:17,  5.21s/it][A
 21%|██▏       | 89/414 [08:38<29:06,  5.38s/it][A
 22%|██▏       | 90/414 [08:45<31:51,  5.90s/it][A
 22%|██▏       | 91/414 [08:51<30:54,  5.74s/it][A
 22%|██▏       | 92/414 [08:56<30:27,  5.68s/it][A
 22%|██▏       | 93/414 [09:02<30:09,  5.64s/it][A
 23%|██▎       | 94/414 [09:07<29:34,  5.55s/it][A
 23%|██▎       | 95/414 [09:13<30:09,  5.67s/it][A
 23%|██▎       | 96/414 [09:19<30:39,  5.79s/it][A
 23%|██▎       | 97/414 [09:25<30:19,  5.74s/it][A
 24%|██▎       | 98/414 [09:29<27:33,  5.23s/it][A
 24%|██▍       | 99/414 [09:33<26:32,  5.06s/it][A
 24%|██▍       | 100/414 [09:40<28:25,  5.43s/it][A
 24%|██▍       | 101/414 [09:48<32:31,  6.24s/it][A
 25%|██▍       | 102/414 [09:55<33:16,  6.40s/it][A
 25%|██▍       | 103/414 [09:59<30:35,  5.90s/it][A
 25%|██▌       | 104/414 [10:04<28:29,  5.51s/it][A
 25%|██▌       | 105/414 [10:10<28:37,  5.56s/it][A
 26%|██▌       | 106/414 [10:17<31:21,  6.11s/it][A
 26%|██▌       | 107/414 [10:23<31:12,  6.10s/it][A
 26%|██▌       | 108/414 [10:28<29:16,  5.74s/it][A
 26%|██▋       | 109/414 [10:33<28:00,  5.51s/it][A
 27%|██▋       | 110/414 [10:39<28:31,  5.63s/it][A
 27%|██▋       | 111/414 [10:45<29:30,  5.84s/it][A
 27%|██▋       | 112/414 [10:50<28:06,  5.58s/it][A
 27%|██▋       | 113/414 [10:57<29:10,  5.82s/it][A
 28%|██▊       | 114/414 [11:04<31:06,  6.22s/it][A
 28%|██▊       | 115/414 [11:09<30:23,  6.10s/it][A
 28%|██▊       | 116/414 [11:16<30:19,  6.11s/it][A
 28%|██▊       | 117/414 [11:24<33:09,  6.70s/it][A
 29%|██▊       | 118/414 [11:33<37:03,  7.51s/it][A
 29%|██▊       | 119/414 [11:42<39:24,  8.01s/it][A
 29%|██▉       | 120/414 [11:50<39:23,  8.04s/it][A
 29%|██▉       | 121/414 [11:58<38:38,  7.91s/it][A
 29%|██▉       | 122/414 [12:05<37:33,  7.72s/it][A
 30%|██▉       | 123/414 [12:12<35:48,  7.38s/it][A
 30%|██▉       | 124/414 [12:19<35:55,  7.43s/it][A
 30%|███       | 125/414 [12:27<35:58,  7.47s/it][A
 30%|███       | 126/414 [12:33<33:22,  6.95s/it][A
 31%|███       | 127/414 [12:40<33:32,  7.01s/it][A
 31%|███       | 128/414 [12:49<36:03,  7.57s/it][A
 31%|███       | 129/414 [12:56<34:53,  7.35s/it][A
 31%|███▏      | 130/414 [13:03<34:10,  7.22s/it][A
 32%|███▏      | 131/414 [13:11<35:51,  7.60s/it][A
 32%|███▏      | 132/414 [13:19<35:38,  7.58s/it][A
 32%|███▏      | 133/414 [13:23<31:06,  6.64s/it][A
 32%|███▏      | 134/414 [13:29<30:00,  6.43s/it][A
 33%|███▎      | 135/414 [13:35<29:58,  6.44s/it][A
 33%|███▎      | 136/414 [13:43<31:40,  6.84s/it][A
 33%|███▎      | 137/414 [13:52<34:54,  7.56s/it][A
 33%|███▎      | 138/414 [14:00<34:09,  7.42s/it][A
 34%|███▎      | 139/414 [14:04<30:21,  6.62s/it][A
 34%|███▍      | 140/414 [14:09<27:39,  6.06s/it][A
 34%|███▍      | 141/414 [14:15<26:52,  5.91s/it][A
 34%|███▍      | 142/414 [14:21<27:47,  6.13s/it][A
 35%|███▍      | 143/414 [14:27<27:45,  6.15s/it][A
 35%|███▍      | 144/414 [14:32<25:24,  5.65s/it][A
 35%|███▌      | 145/414 [14:36<23:40,  5.28s/it][A
 35%|███▌      | 146/414 [14:41<22:09,  4.96s/it][A
 36%|███▌      | 147/414 [14:45<21:55,  4.93s/it][A
 36%|███▌      | 148/414 [14:51<22:31,  5.08s/it][A
 36%|███▌      | 149/414 [14:56<22:08,  5.01s/it][A
 36%|███▌      | 150/414 [15:00<21:34,  4.90s/it][A
 36%|███▋      | 151/414 [15:04<20:29,  4.67s/it][A
 37%|███▋      | 152/414 [15:09<20:51,  4.78s/it][A
 37%|███▋      | 153/414 [15:15<21:37,  4.97s/it][A
 37%|███▋      | 154/414 [15:19<20:34,  4.75s/it][A
 37%|███▋      | 155/414 [15:23<19:53,  4.61s/it][A
 38%|███▊      | 156/414 [15:29<20:41,  4.81s/it][A
 38%|███▊      | 157/414 [15:35<22:36,  5.28s/it][A
 38%|███▊      | 158/414 [15:41<23:58,  5.62s/it][A
 38%|███▊      | 159/414 [15:48<24:49,  5.84s/it][A
 39%|███▊      | 160/414 [15:53<24:30,  5.79s/it][A
 39%|███▉      | 161/414 [16:00<25:42,  6.10s/it][A
 39%|███▉      | 162/414 [16:06<25:38,  6.11s/it][A
 39%|███▉      | 163/414 [16:12<25:00,  5.98s/it][A
 40%|███▉      | 164/414 [16:19<26:20,  6.32s/it][A
 40%|███▉      | 165/414 [16:27<27:44,  6.68s/it][A
 40%|████      | 166/414 [16:34<28:37,  6.93s/it][A
 40%|████      | 167/414 [16:41<28:12,  6.85s/it][A
 41%|████      | 168/414 [16:47<26:33,  6.48s/it][A
 41%|████      | 169/414 [16:51<24:14,  5.94s/it][A
 41%|████      | 170/414 [16:56<22:40,  5.58s/it][A
 41%|████▏     | 171/414 [17:01<22:23,  5.53s/it][A
 42%|████▏     | 172/414 [17:08<23:02,  5.71s/it][A
 42%|████▏     | 173/414 [17:14<23:34,  5.87s/it][A
 42%|████▏     | 174/414 [17:20<23:39,  5.91s/it][A
 42%|████▏     | 175/414 [17:26<23:48,  5.98s/it][A
 43%|████▎     | 176/414 [17:31<22:18,  5.63s/it][A
 43%|████▎     | 177/414 [17:37<22:29,  5.69s/it][A
 43%|████▎     | 178/414 [17:43<23:28,  5.97s/it][A
 43%|████▎     | 179/414 [17:48<22:34,  5.76s/it][A
 43%|████▎     | 180/414 [17:54<22:23,  5.74s/it][A
 44%|████▎     | 181/414 [18:00<22:54,  5.90s/it][A
 44%|████▍     | 182/414 [18:06<22:35,  5.84s/it][A
 44%|████▍     | 183/414 [18:13<23:10,  6.02s/it][A
 44%|████▍     | 184/414 [18:18<22:53,  5.97s/it][A
 45%|████▍     | 185/414 [18:23<21:40,  5.68s/it][A
 45%|████▍     | 186/414 [18:30<22:56,  6.04s/it][A
 45%|████▌     | 187/414 [18:40<27:17,  7.21s/it][A
 45%|████▌     | 188/414 [18:50<29:52,  7.93s/it][A
 46%|████▌     | 189/414 [18:58<30:34,  8.15s/it][A
 46%|████▌     | 190/414 [19:07<30:26,  8.16s/it][A
 46%|████▌     | 191/414 [19:13<27:56,  7.52s/it][A
 46%|████▋     | 192/414 [19:19<25:59,  7.03s/it][A
 47%|████▋     | 193/414 [19:23<23:06,  6.27s/it][A
 47%|████▋     | 194/414 [19:28<21:56,  5.98s/it][A
 47%|████▋     | 195/414 [19:33<20:50,  5.71s/it][A
 47%|████▋     | 196/414 [19:39<20:15,  5.58s/it][A
 48%|████▊     | 197/414 [19:46<21:43,  6.00s/it][A
 48%|████▊     | 198/414 [19:52<21:47,  6.05s/it][A
 48%|████▊     | 199/414 [19:57<20:13,  5.64s/it][A
 48%|████▊     | 200/414 [20:02<20:16,  5.68s/it][A
 49%|████▊     | 201/414 [20:09<21:33,  6.07s/it][A
 49%|████▉     | 202/414 [20:16<21:58,  6.22s/it][A
 49%|████▉     | 203/414 [20:21<20:20,  5.78s/it][A
 49%|████▉     | 204/414 [20:25<18:46,  5.36s/it][A
 50%|████▉     | 205/414 [20:30<17:46,  5.10s/it][A
 50%|████▉     | 206/414 [20:35<18:18,  5.28s/it][A
 50%|█████     | 207/414 [20:41<18:38,  5.40s/it][A
 50%|█████     | 208/414 [20:47<19:13,  5.60s/it][A
 50%|█████     | 209/414 [20:55<21:27,  6.28s/it][A
 51%|█████     | 210/414 [21:01<21:34,  6.35s/it][A
 51%|█████     | 211/414 [21:07<20:16,  5.99s/it][A
 51%|█████     | 212/414 [21:12<20:00,  5.94s/it][A
 51%|█████▏    | 213/414 [21:20<21:39,  6.47s/it][A
 52%|█████▏    | 214/414 [21:27<21:57,  6.59s/it][A
 52%|█████▏    | 215/414 [21:34<21:53,  6.60s/it][A
 52%|█████▏    | 216/414 [21:43<24:10,  7.33s/it][A
 52%|█████▏    | 217/414 [21:50<24:28,  7.46s/it][A
 53%|█████▎    | 218/414 [21:56<22:09,  6.78s/it][A
 53%|█████▎    | 219/414 [22:01<21:14,  6.54s/it][A
 53%|█████▎    | 220/414 [22:07<19:59,  6.18s/it][A
 53%|█████▎    | 221/414 [22:12<18:33,  5.77s/it][A
 54%|█████▎    | 222/414 [22:18<19:02,  5.95s/it][A
 54%|█████▍    | 223/414 [22:25<19:37,  6.16s/it][A
 54%|█████▍    | 224/414 [22:32<20:26,  6.46s/it][A
 54%|█████▍    | 225/414 [22:39<20:44,  6.59s/it][A
 55%|█████▍    | 226/414 [22:45<20:29,  6.54s/it][A
 55%|█████▍    | 227/414 [22:53<21:56,  7.04s/it][A
 55%|█████▌    | 228/414 [23:00<21:28,  6.93s/it][A
 55%|█████▌    | 229/414 [23:05<19:57,  6.47s/it][A
 56%|█████▌    | 230/414 [23:13<20:41,  6.75s/it][A
 56%|█████▌    | 231/414 [23:19<20:27,  6.71s/it][A
 56%|█████▌    | 232/414 [23:25<19:23,  6.39s/it][A
 56%|█████▋    | 233/414 [23:32<19:39,  6.52s/it][A
 57%|█████▋    | 234/414 [23:39<19:56,  6.65s/it][A
 57%|█████▋    | 235/414 [23:47<20:49,  6.98s/it][A
 57%|█████▋    | 236/414 [23:54<21:00,  7.08s/it][A
 57%|█████▋    | 237/414 [24:00<19:37,  6.65s/it][A
 57%|█████▋    | 238/414 [24:04<17:10,  5.86s/it][A
 58%|█████▊    | 239/414 [24:09<16:46,  5.75s/it][A
 58%|█████▊    | 240/414 [24:17<18:09,  6.26s/it][A
 58%|█████▊    | 241/414 [24:22<17:32,  6.08s/it][A
 58%|█████▊    | 242/414 [24:26<15:36,  5.44s/it][A
 59%|█████▊    | 243/414 [24:31<14:55,  5.23s/it][A
 59%|█████▉    | 244/414 [24:37<15:25,  5.44s/it][A
 59%|█████▉    | 245/414 [24:43<15:35,  5.54s/it][A
 59%|█████▉    | 246/414 [24:47<14:44,  5.26s/it][A
 60%|█████▉    | 247/414 [24:52<14:31,  5.22s/it][A
 60%|█████▉    | 248/414 [24:57<14:22,  5.20s/it][A
 60%|██████    | 249/414 [25:02<13:56,  5.07s/it][A
 60%|██████    | 250/414 [25:07<13:23,  4.90s/it][A
 61%|██████    | 251/414 [25:12<13:55,  5.12s/it][A
 61%|██████    | 252/414 [25:18<14:36,  5.41s/it][A
 61%|██████    | 253/414 [25:26<16:04,  5.99s/it][A
 61%|██████▏   | 254/414 [25:34<17:26,  6.54s/it][A
 62%|██████▏   | 255/414 [25:38<15:38,  5.90s/it][A
 62%|██████▏   | 256/414 [25:43<14:42,  5.58s/it][A
 62%|██████▏   | 257/414 [25:48<14:17,  5.46s/it][A
 62%|██████▏   | 258/414 [25:53<13:47,  5.31s/it][A
 63%|██████▎   | 259/414 [25:58<13:08,  5.09s/it][A
 63%|██████▎   | 260/414 [26:03<13:40,  5.33s/it][A
 63%|██████▎   | 261/414 [26:10<14:12,  5.57s/it][A
 63%|██████▎   | 262/414 [26:16<15:00,  5.93s/it][A
 64%|██████▎   | 263/414 [26:24<15:56,  6.34s/it][A
 64%|██████▍   | 264/414 [26:30<16:02,  6.42s/it][A
 64%|██████▍   | 265/414 [26:36<15:46,  6.35s/it][A
 64%|██████▍   | 266/414 [26:43<15:42,  6.37s/it][A
 64%|██████▍   | 267/414 [26:51<16:35,  6.77s/it][A
 65%|██████▍   | 268/414 [26:57<15:54,  6.54s/it][A
 65%|██████▍   | 269/414 [27:01<14:10,  5.86s/it][A
 65%|██████▌   | 270/414 [27:06<13:18,  5.55s/it][A
 65%|██████▌   | 271/414 [27:11<12:59,  5.45s/it][A
 66%|██████▌   | 272/414 [27:17<13:09,  5.56s/it][A
 66%|██████▌   | 273/414 [27:23<13:39,  5.81s/it][A
 66%|██████▌   | 274/414 [27:29<13:41,  5.87s/it][A
 66%|██████▋   | 275/414 [27:35<13:16,  5.73s/it][A
 67%|██████▋   | 276/414 [27:40<12:53,  5.60s/it][A
 67%|██████▋   | 277/414 [27:45<12:24,  5.44s/it][A
 67%|██████▋   | 278/414 [27:49<11:26,  5.05s/it][A
 67%|██████▋   | 279/414 [27:54<11:16,  5.01s/it][A
 68%|██████▊   | 280/414 [28:00<11:40,  5.22s/it][A
 68%|██████▊   | 281/414 [28:05<11:26,  5.16s/it][A
 68%|██████▊   | 282/414 [28:11<11:52,  5.39s/it][A
 68%|██████▊   | 283/414 [28:17<12:15,  5.62s/it][A
 69%|██████▊   | 284/414 [28:22<11:44,  5.42s/it][A
 69%|██████▉   | 285/414 [28:28<12:07,  5.64s/it][A
 69%|██████▉   | 286/414 [28:35<13:06,  6.14s/it][A
 69%|██████▉   | 287/414 [28:42<13:37,  6.44s/it][A
 70%|██████▉   | 288/414 [28:48<13:18,  6.34s/it][A
 70%|██████▉   | 289/414 [28:53<12:08,  5.83s/it][A
 70%|███████   | 290/414 [28:59<12:21,  5.98s/it][A
 70%|███████   | 291/414 [29:06<12:48,  6.25s/it][A
 71%|███████   | 292/414 [29:11<11:28,  5.64s/it][A
 71%|███████   | 293/414 [29:19<12:49,  6.36s/it][A
 71%|███████   | 294/414 [29:28<14:26,  7.22s/it][A
 71%|███████▏  | 295/414 [29:36<14:42,  7.42s/it][A
 71%|███████▏  | 296/414 [29:43<14:37,  7.44s/it][A
 72%|███████▏  | 297/414 [29:49<13:28,  6.91s/it][A
 72%|███████▏  | 298/414 [29:57<13:54,  7.20s/it][A
 72%|███████▏  | 299/414 [30:04<13:37,  7.11s/it][A
 72%|███████▏  | 300/414 [30:09<12:24,  6.53s/it][A
 73%|███████▎  | 301/414 [30:14<11:43,  6.23s/it][A
 73%|███████▎  | 302/414 [30:19<10:46,  5.77s/it][A
 73%|███████▎  | 303/414 [30:24<10:20,  5.59s/it][A
 73%|███████▎  | 304/414 [30:32<11:38,  6.35s/it][A
 74%|███████▎  | 305/414 [30:40<12:30,  6.89s/it][A
 74%|███████▍  | 306/414 [30:46<11:45,  6.53s/it][A
 74%|███████▍  | 307/414 [30:52<11:08,  6.25s/it][A
 74%|███████▍  | 308/414 [30:59<11:30,  6.52s/it][A
 75%|███████▍  | 309/414 [31:06<11:49,  6.76s/it][A
 75%|███████▍  | 310/414 [31:11<10:38,  6.14s/it][A
 75%|███████▌  | 311/414 [31:16<09:57,  5.80s/it][A
 75%|███████▌  | 312/414 [31:21<09:41,  5.70s/it][A
 76%|███████▌  | 313/414 [31:26<09:18,  5.53s/it][A
 76%|███████▌  | 314/414 [31:31<08:49,  5.30s/it][A
 76%|███████▌  | 315/414 [31:37<09:02,  5.48s/it][A
 76%|███████▋  | 316/414 [31:46<10:38,  6.52s/it][A
 77%|███████▋  | 317/414 [31:54<11:08,  6.89s/it][A
 77%|███████▋  | 318/414 [32:00<10:29,  6.56s/it][A
 77%|███████▋  | 319/414 [32:06<10:05,  6.38s/it][A
 77%|███████▋  | 320/414 [32:10<09:18,  5.94s/it][A
 78%|███████▊  | 321/414 [32:17<09:26,  6.09s/it][A
 78%|███████▊  | 322/414 [32:23<09:30,  6.20s/it][A
 78%|███████▊  | 323/414 [32:28<08:43,  5.75s/it][A
 78%|███████▊  | 324/414 [32:33<08:09,  5.43s/it][A
 79%|███████▊  | 325/414 [32:38<07:50,  5.29s/it][A
 79%|███████▊  | 326/414 [32:43<07:32,  5.15s/it][A
 79%|███████▉  | 327/414 [32:50<08:17,  5.71s/it][A
 79%|███████▉  | 328/414 [32:56<08:41,  6.06s/it][A
 79%|███████▉  | 329/414 [33:03<08:49,  6.23s/it][A
 80%|███████▉  | 330/414 [33:11<09:30,  6.79s/it][A
 80%|███████▉  | 331/414 [33:18<09:33,  6.90s/it][A
 80%|████████  | 332/414 [33:25<09:25,  6.90s/it][A
 80%|████████  | 333/414 [33:31<09:00,  6.67s/it][A
 81%|████████  | 334/414 [33:36<08:00,  6.01s/it][A
 81%|████████  | 335/414 [33:40<07:19,  5.57s/it][A
 81%|████████  | 336/414 [33:46<07:05,  5.45s/it][A
 81%|████████▏ | 337/414 [33:50<06:41,  5.22s/it][A
 82%|████████▏ | 338/414 [33:56<06:52,  5.43s/it][A
 82%|████████▏ | 339/414 [34:04<07:31,  6.02s/it][A
 82%|████████▏ | 340/414 [34:12<08:14,  6.69s/it][A
 82%|████████▏ | 341/414 [34:19<08:10,  6.72s/it][A
 83%|████████▎ | 342/414 [34:23<07:09,  5.96s/it][A
 83%|████████▎ | 343/414 [34:30<07:28,  6.32s/it][A
 83%|████████▎ | 344/414 [34:39<08:21,  7.16s/it][A
 83%|████████▎ | 345/414 [34:46<08:02,  7.00s/it][A
 84%|████████▎ | 346/414 [34:50<07:07,  6.29s/it][A
 84%|████████▍ | 347/414 [34:56<06:46,  6.07s/it][A
 84%|████████▍ | 348/414 [35:02<06:35,  5.99s/it][A
 84%|████████▍ | 349/414 [35:07<06:12,  5.73s/it][A
 85%|████████▍ | 350/414 [35:14<06:38,  6.23s/it][A
 85%|████████▍ | 351/414 [35:22<06:54,  6.59s/it][A
 85%|████████▌ | 352/414 [35:26<06:14,  6.04s/it][A
 85%|████████▌ | 353/414 [35:31<05:40,  5.58s/it][A
 86%|████████▌ | 354/414 [35:36<05:32,  5.54s/it][A
 86%|████████▌ | 355/414 [35:42<05:22,  5.46s/it][A
 86%|████████▌ | 356/414 [35:46<05:06,  5.29s/it][A
 86%|████████▌ | 357/414 [35:53<05:14,  5.51s/it][A
 86%|████████▋ | 358/414 [36:00<05:44,  6.15s/it][A
 87%|████████▋ | 359/414 [36:06<05:40,  6.19s/it][A
 87%|████████▋ | 360/414 [36:11<05:10,  5.75s/it][A
 87%|████████▋ | 361/414 [36:15<04:42,  5.33s/it][A
 87%|████████▋ | 362/414 [36:22<04:55,  5.67s/it][A
 88%|████████▊ | 363/414 [36:30<05:18,  6.25s/it][A
 88%|████████▊ | 364/414 [36:35<05:00,  6.00s/it][A
 88%|████████▊ | 365/414 [36:41<04:55,  6.04s/it][A
 88%|████████▊ | 366/414 [36:48<05:07,  6.40s/it][A
 89%|████████▊ | 367/414 [36:55<04:59,  6.37s/it][A
 89%|████████▉ | 368/414 [37:01<04:52,  6.35s/it][A
 89%|████████▉ | 369/414 [37:09<05:03,  6.74s/it][A
 89%|████████▉ | 370/414 [37:15<04:54,  6.70s/it][A
 90%|████████▉ | 371/414 [37:22<04:44,  6.62s/it][A
 90%|████████▉ | 372/414 [37:29<04:42,  6.72s/it][A
 90%|█████████ | 373/414 [37:36<04:45,  6.96s/it][A
 90%|█████████ | 374/414 [37:45<04:55,  7.39s/it][A
 91%|█████████ | 375/414 [37:51<04:32,  6.99s/it][A
 91%|█████████ | 376/414 [37:55<03:53,  6.15s/it][A
 91%|█████████ | 377/414 [38:01<03:44,  6.07s/it][A
 91%|█████████▏| 378/414 [38:08<03:53,  6.47s/it][A
 92%|█████████▏| 379/414 [38:14<03:35,  6.16s/it][A
 92%|█████████▏| 380/414 [38:21<03:40,  6.49s/it][A
 92%|█████████▏| 381/414 [38:31<04:13,  7.69s/it][A
 92%|█████████▏| 382/414 [38:39<04:05,  7.68s/it][A
 93%|█████████▎| 383/414 [38:44<03:29,  6.77s/it][A
 93%|█████████▎| 384/414 [38:50<03:16,  6.54s/it][A
 93%|█████████▎| 385/414 [38:56<03:06,  6.43s/it][A
 93%|█████████▎| 386/414 [39:01<02:47,  5.98s/it][A
 93%|█████████▎| 387/414 [39:05<02:31,  5.62s/it][A
 94%|█████████▎| 388/414 [39:11<02:22,  5.47s/it][A
 94%|█████████▍| 389/414 [39:15<02:10,  5.21s/it][A
 94%|█████████▍| 390/414 [39:19<01:57,  4.91s/it][A
 94%|█████████▍| 391/414 [39:24<01:51,  4.84s/it][A
 95%|█████████▍| 392/414 [39:29<01:46,  4.86s/it][A
 95%|█████████▍| 393/414 [39:36<01:53,  5.39s/it][A
 95%|█████████▌| 394/414 [39:43<01:59,  5.99s/it][A
 95%|█████████▌| 395/414 [39:48<01:48,  5.73s/it][A
 96%|█████████▌| 396/414 [39:55<01:48,  6.01s/it][A
 96%|█████████▌| 397/414 [40:03<01:54,  6.76s/it][A
 96%|█████████▌| 398/414 [40:09<01:44,  6.53s/it][A
 96%|█████████▋| 399/414 [40:13<01:25,  5.73s/it][A
 97%|█████████▋| 400/414 [40:21<01:28,  6.30s/it][A
 97%|█████████▋| 401/414 [40:31<01:37,  7.51s/it][A
 97%|█████████▋| 402/414 [40:37<01:25,  7.13s/it][A
 97%|█████████▋| 403/414 [40:42<01:09,  6.33s/it][A
 98%|█████████▊| 404/414 [40:46<00:57,  5.76s/it][A
 98%|█████████▊| 405/414 [40:51<00:48,  5.36s/it][A
 98%|█████████▊| 406/414 [40:57<00:45,  5.75s/it][A
 98%|█████████▊| 407/414 [41:05<00:44,  6.34s/it][A
 99%|█████████▊| 408/414 [41:11<00:37,  6.17s/it][A
 99%|█████████▉| 409/414 [41:17<00:30,  6.16s/it][A
 99%|█████████▉| 410/414 [41:23<00:24,  6.24s/it][A
 99%|█████████▉| 411/414 [41:28<00:16,  5.67s/it][A
100%|█████████▉| 412/414 [41:34<00:11,  5.90s/it][A
100%|█████████▉| 413/414 [41:41<00:06,  6.20s/it][A
100%|██████████| 414/414 [41:44<00:00,  5.21s/it][A                                                     
                                                 [A{'eval_loss': 0.9025837182998657, 'eval_runtime': 2511.6431, 'eval_samples_per_second': 1.316, 'eval_steps_per_second': 0.165, 'epoch': 0.26}
 26%|██▌       | 60/233 [6:18:01<7:40:07, 159.58s/it]
100%|██████████| 414/414 [41:44<00:00,  5.21s/it][A
                                                 [A 26%|██▌       | 61/233 [6:20:36<43:33:01, 911.52s/it] 27%|██▋       | 62/233 [6:22:35<32:00:37, 673.90s/it] 27%|██▋       | 63/233 [6:24:32<23:55:27, 506.63s/it] 27%|██▋       | 64/233 [6:26:35<18:22:59, 391.59s/it] 28%|██▊       | 65/233 [6:29:06<14:54:21, 319.42s/it] 28%|██▊       | 66/233 [6:31:12<12:08:05, 261.59s/it] 29%|██▉       | 67/233 [6:33:04<9:58:51, 216.45s/it]  29%|██▉       | 68/233 [6:35:15<8:44:44, 190.81s/it] 30%|██▉       | 69/233 [6:37:20<7:48:14, 171.31s/it] 30%|███       | 70/233 [6:39:28<7:09:34, 158.12s/it]                                                     {'loss': 0.8963, 'grad_norm': 0.2276197373867035, 'learning_rate': 8.899095706856122e-05, 'epoch': 0.3}
 30%|███       | 70/233 [6:39:28<7:09:34, 158.12s/it]
***** Running Evaluation *****

***** Running Evaluation *****
  Num examples = 3305
  Num examples = 3305
  Batch size = 1
  Batch size = 1

***** Running Evaluation *****

***** Running Evaluation *****

***** Running Evaluation *****

***** Running Evaluation *****

***** Running Evaluation *****
  Num examples = 3305
  Num examples = 3305
  Num examples = 3305
  Num examples = 3305
  Batch size = 1
  Batch size = 1
  Num examples = 3305
  Batch size = 1
  Batch size = 1
  Batch size = 1
[INFO|trainer.py:4643] 2025-12-05 11:31:56,579 >> 
***** Running Evaluation *****
[INFO|trainer.py:4645] 2025-12-05 11:31:56,579 >>   Num examples = 3305
[INFO|trainer.py:4648] 2025-12-05 11:31:56,579 >>   Batch size = 1

  0%|          | 0/414 [00:00<?, ?it/s][A
  0%|          | 2/414 [00:04<16:25,  2.39s/it][A
  1%|          | 3/414 [00:10<24:58,  3.65s/it][A
  1%|          | 4/414 [00:17<35:26,  5.19s/it][A
  1%|          | 5/414 [00:25<41:16,  6.06s/it][A
  1%|▏         | 6/414 [00:29<37:04,  5.45s/it][A
  2%|▏         | 7/414 [00:34<35:16,  5.20s/it][A
  2%|▏         | 8/414 [00:39<33:52,  5.01s/it][A
  2%|▏         | 9/414 [00:44<35:15,  5.22s/it][A
  2%|▏         | 10/414 [00:52<39:40,  5.89s/it][A
  3%|▎         | 11/414 [00:57<39:17,  5.85s/it][A
  3%|▎         | 12/414 [01:04<40:10,  6.00s/it][A
  3%|▎         | 13/414 [01:12<43:43,  6.54s/it][A
  3%|▎         | 14/414 [01:19<44:54,  6.74s/it][A
  4%|▎         | 15/414 [01:25<44:12,  6.65s/it][A
  4%|▍         | 16/414 [01:32<44:34,  6.72s/it][A
  4%|▍         | 17/414 [01:39<44:34,  6.74s/it][A
  4%|▍         | 18/414 [01:46<44:52,  6.80s/it][A
  5%|▍         | 19/414 [01:54<47:57,  7.29s/it][A
  5%|▍         | 20/414 [02:01<47:05,  7.17s/it][A
  5%|▌         | 21/414 [02:06<42:57,  6.56s/it][A
  5%|▌         | 22/414 [02:12<41:56,  6.42s/it][A
  6%|▌         | 23/414 [02:20<43:48,  6.72s/it][A
  6%|▌         | 24/414 [02:26<42:19,  6.51s/it][A
  6%|▌         | 25/414 [02:33<42:33,  6.56s/it][A
  6%|▋         | 26/414 [02:39<42:38,  6.59s/it][A
  7%|▋         | 27/414 [02:43<38:03,  5.90s/it][A
  7%|▋         | 28/414 [02:49<36:27,  5.67s/it][A
  7%|▋         | 29/414 [02:54<35:32,  5.54s/it][A
  7%|▋         | 30/414 [03:00<36:28,  5.70s/it][A
  7%|▋         | 31/414 [03:06<36:28,  5.71s/it][A
  8%|▊         | 32/414 [03:10<34:07,  5.36s/it][A
  8%|▊         | 33/414 [03:17<36:13,  5.70s/it][A
  8%|▊         | 34/414 [03:23<36:24,  5.75s/it][A
  8%|▊         | 35/414 [03:28<36:10,  5.73s/it][A
  9%|▊         | 36/414 [03:35<37:27,  5.95s/it][A
  9%|▉         | 37/414 [03:41<38:17,  6.09s/it][A
  9%|▉         | 38/414 [03:47<38:28,  6.14s/it][A
  9%|▉         | 39/414 [03:52<34:36,  5.54s/it][A
 10%|▉         | 40/414 [03:57<34:32,  5.54s/it][A
 10%|▉         | 41/414 [04:04<36:28,  5.87s/it][A
 10%|█         | 42/414 [04:10<36:28,  5.88s/it][A
 10%|█         | 43/414 [04:19<42:17,  6.84s/it][A
 11%|█         | 44/414 [04:29<49:21,  8.00s/it][A
 11%|█         | 45/414 [04:37<47:34,  7.73s/it][A
 11%|█         | 46/414 [04:41<41:37,  6.79s/it][A
 11%|█▏        | 47/414 [04:45<36:58,  6.05s/it][A
 12%|█▏        | 48/414 [04:49<32:46,  5.37s/it][A
 12%|█▏        | 49/414 [04:53<29:13,  4.80s/it][A
 12%|█▏        | 50/414 [04:57<27:47,  4.58s/it][A
 12%|█▏        | 51/414 [05:01<27:01,  4.47s/it][A
 13%|█▎        | 52/414 [05:05<26:42,  4.43s/it][A
 13%|█▎        | 53/414 [05:13<32:03,  5.33s/it][A
 13%|█▎        | 54/414 [05:21<37:35,  6.26s/it][A
 13%|█▎        | 55/414 [05:28<38:31,  6.44s/it][A
 14%|█▎        | 56/414 [05:34<38:28,  6.45s/it][A
 14%|█▍        | 57/414 [05:39<35:44,  6.01s/it][A
 14%|█▍        | 58/414 [05:45<35:20,  5.96s/it][A
 14%|█▍        | 59/414 [05:52<37:21,  6.32s/it][A
 14%|█▍        | 60/414 [05:58<36:29,  6.18s/it][A
 15%|█▍        | 61/414 [06:03<33:06,  5.63s/it][A
 15%|█▍        | 62/414 [06:07<30:36,  5.22s/it][A
 15%|█▌        | 63/414 [06:12<30:06,  5.15s/it][A
 15%|█▌        | 64/414 [06:17<29:49,  5.11s/it][A
 16%|█▌        | 65/414 [06:22<29:47,  5.12s/it][A
 16%|█▌        | 66/414 [06:28<30:40,  5.29s/it][A
 16%|█▌        | 67/414 [06:35<33:55,  5.87s/it][A
 16%|█▋        | 68/414 [06:42<36:03,  6.25s/it][A
 17%|█▋        | 69/414 [06:49<37:20,  6.49s/it][A
 17%|█▋        | 70/414 [06:56<38:20,  6.69s/it][A
 17%|█▋        | 71/414 [07:01<35:27,  6.20s/it][A
 17%|█▋        | 72/414 [07:06<33:16,  5.84s/it][A
 18%|█▊        | 73/414 [07:11<31:31,  5.55s/it][A
 18%|█▊        | 74/414 [07:15<29:00,  5.12s/it][A
 18%|█▊        | 75/414 [07:19<26:58,  4.77s/it][A
 18%|█▊        | 76/414 [07:24<26:31,  4.71s/it][A
 19%|█▊        | 77/414 [07:30<28:56,  5.15s/it][A
 19%|█▉        | 78/414 [07:36<30:10,  5.39s/it][A
 19%|█▉        | 79/414 [07:42<30:58,  5.55s/it][A
 19%|█▉        | 80/414 [07:47<30:20,  5.45s/it][A
 20%|█▉        | 81/414 [07:52<28:55,  5.21s/it][A
 20%|█▉        | 82/414 [07:57<29:02,  5.25s/it][A
 20%|██        | 83/414 [08:05<32:41,  5.93s/it][A
 20%|██        | 84/414 [08:12<35:13,  6.40s/it][A
 21%|██        | 85/414 [08:17<32:39,  5.95s/it][A
 21%|██        | 86/414 [08:22<30:51,  5.65s/it][A
 21%|██        | 87/414 [08:27<29:04,  5.34s/it][A
 21%|██▏       | 88/414 [08:31<28:12,  5.19s/it][A
 21%|██▏       | 89/414 [08:37<29:06,  5.37s/it][A
 22%|██▏       | 90/414 [08:44<31:50,  5.90s/it][A
 22%|██▏       | 91/414 [08:50<30:53,  5.74s/it][A
 22%|██▏       | 92/414 [08:55<30:20,  5.65s/it][A
 22%|██▏       | 93/414 [09:01<30:00,  5.61s/it][A
 23%|██▎       | 94/414 [09:06<29:31,  5.54s/it][A
 23%|██▎       | 95/414 [09:12<30:00,  5.64s/it][A
 23%|██▎       | 96/414 [09:18<30:31,  5.76s/it][A
 23%|██▎       | 97/414 [09:24<30:09,  5.71s/it][A
 24%|██▎       | 98/414 [09:28<27:34,  5.24s/it][A
 24%|██▍       | 99/414 [09:32<26:32,  5.06s/it][A
 24%|██▍       | 100/414 [09:39<28:21,  5.42s/it][A
 24%|██▍       | 101/414 [09:47<32:23,  6.21s/it][A
 25%|██▍       | 102/414 [09:53<33:08,  6.37s/it][A
 25%|██▍       | 103/414 [09:58<30:30,  5.88s/it][A
 25%|██▌       | 104/414 [10:03<28:27,  5.51s/it][A
 25%|██▌       | 105/414 [10:09<28:39,  5.57s/it][A
 26%|██▌       | 106/414 [10:16<31:23,  6.12s/it][A
 26%|██▌       | 107/414 [10:22<31:14,  6.10s/it][A
 26%|██▌       | 108/414 [10:27<29:18,  5.75s/it][A
 26%|██▋       | 109/414 [10:32<28:00,  5.51s/it][A
 27%|██▋       | 110/414 [10:38<28:34,  5.64s/it][A
 27%|██▋       | 111/414 [10:44<29:30,  5.84s/it][A
 27%|██▋       | 112/414 [10:49<28:03,  5.58s/it][A
 27%|██▋       | 113/414 [10:55<29:07,  5.81s/it][A
 28%|██▊       | 114/414 [11:03<31:05,  6.22s/it][A
 28%|██▊       | 115/414 [11:08<30:20,  6.09s/it][A
 28%|██▊       | 116/414 [11:15<30:19,  6.10s/it][A
 28%|██▊       | 117/414 [11:23<33:04,  6.68s/it][A
 29%|██▊       | 118/414 [11:32<36:23,  7.38s/it][A
 29%|██▊       | 119/414 [11:41<38:56,  7.92s/it][A
 29%|██▉       | 120/414 [11:49<39:02,  7.97s/it][A
 29%|██▉       | 121/414 [11:56<38:26,  7.87s/it][A
 29%|██▉       | 122/414 [12:04<37:20,  7.67s/it][A
 30%|██▉       | 123/414 [12:10<35:40,  7.35s/it][A
 30%|██▉       | 124/414 [12:18<35:47,  7.40s/it][A
 30%|███       | 125/414 [12:25<35:52,  7.45s/it][A
 30%|███       | 126/414 [12:31<33:12,  6.92s/it][A
 31%|███       | 127/414 [12:38<33:20,  6.97s/it][A
 31%|███       | 128/414 [12:47<35:52,  7.53s/it][A
 31%|███       | 129/414 [12:54<34:41,  7.30s/it][A
 31%|███▏      | 130/414 [13:01<34:01,  7.19s/it][A
 32%|███▏      | 131/414 [13:09<35:43,  7.58s/it][A
 32%|███▏      | 132/414 [13:17<35:34,  7.57s/it][A
 32%|███▏      | 133/414 [13:21<30:54,  6.60s/it][A
 32%|███▏      | 134/414 [13:27<29:53,  6.40s/it][A
 33%|███▎      | 135/414 [13:33<29:51,  6.42s/it][A
 33%|███▎      | 136/414 [13:41<31:37,  6.82s/it][A
 33%|███▎      | 137/414 [13:50<34:55,  7.56s/it][A
 33%|███▎      | 138/414 [13:58<34:06,  7.41s/it][A
 34%|███▎      | 139/414 [14:02<30:20,  6.62s/it][A
 34%|███▍      | 140/414 [14:07<27:38,  6.05s/it][A
 34%|███▍      | 141/414 [14:13<26:53,  5.91s/it][A
 34%|███▍      | 142/414 [14:19<27:49,  6.14s/it][A
 35%|███▍      | 143/414 [14:25<27:48,  6.16s/it][A
 35%|███▍      | 144/414 [14:30<25:27,  5.66s/it][A
 35%|███▌      | 145/414 [14:34<23:43,  5.29s/it][A
 35%|███▌      | 146/414 [14:39<22:08,  4.96s/it][A
 36%|███▌      | 147/414 [14:43<21:53,  4.92s/it][A
 36%|███▌      | 148/414 [14:49<22:27,  5.07s/it][A
 36%|███▌      | 149/414 [14:54<22:06,  5.01s/it][A
 36%|███▌      | 150/414 [14:58<21:33,  4.90s/it][A
 36%|███▋      | 151/414 [15:02<20:28,  4.67s/it][A
 37%|███▋      | 152/414 [15:07<20:48,  4.77s/it][A
 37%|███▋      | 153/414 [15:13<21:34,  4.96s/it][A
 37%|███▋      | 154/414 [15:17<20:31,  4.73s/it][A
 37%|███▋      | 155/414 [15:21<19:51,  4.60s/it][A
 38%|███▊      | 156/414 [15:27<20:42,  4.82s/it][A
 38%|███▊      | 157/414 [15:33<22:35,  5.28s/it][A
 38%|███▊      | 158/414 [15:39<23:57,  5.62s/it][A
 38%|███▊      | 159/414 [15:46<24:48,  5.84s/it][A
 39%|███▊      | 160/414 [15:51<24:27,  5.78s/it][A
 39%|███▉      | 161/414 [15:58<25:37,  6.08s/it][A
 39%|███▉      | 162/414 [16:04<25:34,  6.09s/it][A
 39%|███▉      | 163/414 [16:10<24:56,  5.96s/it][A
 40%|███▉      | 164/414 [16:17<26:14,  6.30s/it][A
 40%|███▉      | 165/414 [16:25<27:39,  6.66s/it][A
 40%|████      | 166/414 [16:32<28:32,  6.90s/it][A
 40%|████      | 167/414 [16:39<28:05,  6.82s/it][A
 41%|████      | 168/414 [16:44<26:26,  6.45s/it][A
 41%|████      | 169/414 [16:49<24:06,  5.90s/it][A
 41%|████      | 170/414 [16:54<22:37,  5.56s/it][A
 41%|████▏     | 171/414 [16:59<22:19,  5.51s/it][A
 42%|████▏     | 172/414 [17:05<23:01,  5.71s/it][A
 42%|████▏     | 173/414 [17:11<23:31,  5.86s/it][A
 42%|████▏     | 174/414 [17:17<23:37,  5.91s/it][A
 42%|████▏     | 175/414 [17:24<23:45,  5.96s/it][A
 43%|████▎     | 176/414 [17:28<22:16,  5.61s/it][A
 43%|████▎     | 177/414 [17:34<22:28,  5.69s/it][A
 43%|████▎     | 178/414 [17:41<23:26,  5.96s/it][A
 43%|████▎     | 179/414 [17:46<22:35,  5.77s/it][A
 43%|████▎     | 180/414 [17:52<22:22,  5.74s/it][A
 44%|████▎     | 181/414 [17:58<22:54,  5.90s/it][A
 44%|████▍     | 182/414 [18:04<22:37,  5.85s/it][A
 44%|████▍     | 183/414 [18:10<23:12,  6.03s/it][A
 44%|████▍     | 184/414 [18:16<22:52,  5.97s/it][A
 45%|████▍     | 185/414 [18:21<21:37,  5.67s/it][A
 45%|████▍     | 186/414 [18:28<22:56,  6.04s/it][A
 45%|████▌     | 187/414 [18:38<27:21,  7.23s/it][A
 45%|████▌     | 188/414 [18:48<29:55,  7.94s/it][A
 46%|████▌     | 189/414 [18:56<30:32,  8.14s/it][A
 46%|████▌     | 190/414 [19:04<30:24,  8.15s/it][A
 46%|████▌     | 191/414 [19:10<27:51,  7.49s/it][A
 46%|████▋     | 192/414 [19:16<25:50,  6.98s/it][A
 47%|████▋     | 193/414 [19:21<23:01,  6.25s/it][A
 47%|████▋     | 194/414 [19:26<21:54,  5.97s/it][A
 47%|████▋     | 195/414 [19:31<20:46,  5.69s/it][A
 47%|████▋     | 196/414 [19:36<20:12,  5.56s/it][A
 48%|████▊     | 197/414 [19:43<21:39,  5.99s/it][A
 48%|████▊     | 198/414 [19:49<21:45,  6.04s/it][A
 48%|████▊     | 199/414 [19:54<20:11,  5.63s/it][A
 48%|████▊     | 200/414 [20:00<20:11,  5.66s/it][A
 49%|████▊     | 201/414 [20:07<21:26,  6.04s/it][A
 49%|████▉     | 202/414 [20:13<21:49,  6.18s/it][A
 49%|████▉     | 203/414 [20:18<20:10,  5.74s/it][A
 49%|████▉     | 204/414 [20:22<18:36,  5.32s/it][A
 50%|████▉     | 205/414 [20:27<17:39,  5.07s/it][A
 50%|████▉     | 206/414 [20:33<18:15,  5.27s/it][A
 50%|█████     | 207/414 [20:38<18:36,  5.39s/it][A
 50%|█████     | 208/414 [20:44<19:09,  5.58s/it][A
 50%|█████     | 209/414 [20:52<21:14,  6.22s/it][A
 51%|█████     | 210/414 [20:58<21:28,  6.32s/it][A
 51%|█████     | 211/414 [21:04<20:11,  5.97s/it][A
 51%|█████     | 212/414 [21:09<19:58,  5.93s/it][A
 51%|█████▏    | 213/414 [21:17<21:39,  6.47s/it][A
 52%|█████▏    | 214/414 [21:24<21:55,  6.58s/it][A
 52%|█████▏    | 215/414 [21:31<21:53,  6.60s/it][A
 52%|█████▏    | 216/414 [21:40<24:05,  7.30s/it][A
 52%|█████▏    | 217/414 [21:47<24:23,  7.43s/it][A
 53%|█████▎    | 218/414 [21:53<22:04,  6.76s/it][A
 53%|█████▎    | 219/414 [21:59<21:11,  6.52s/it][A
 53%|█████▎    | 220/414 [22:04<19:57,  6.17s/it][A
 53%|█████▎    | 221/414 [22:09<18:33,  5.77s/it][A
 54%|█████▎    | 222/414 [22:15<19:05,  5.96s/it][A
 54%|█████▍    | 223/414 [22:22<19:37,  6.17s/it][A
 54%|█████▍    | 224/414 [22:29<20:24,  6.45s/it][A
 54%|█████▍    | 225/414 [22:36<20:43,  6.58s/it][A
 55%|█████▍    | 226/414 [22:42<20:30,  6.55s/it][A
 55%|█████▍    | 227/414 [22:50<21:53,  7.03s/it][A
 55%|█████▌    | 228/414 [22:57<21:24,  6.91s/it][A
 55%|█████▌    | 229/414 [23:02<19:54,  6.46s/it][A
 56%|█████▌    | 230/414 [23:10<20:28,  6.68s/it][A
 56%|█████▌    | 231/414 [23:16<20:17,  6.65s/it][A
 56%|█████▌    | 232/414 [23:22<19:15,  6.35s/it][A
 56%|█████▋    | 233/414 [23:29<19:29,  6.46s/it][A
 57%|█████▋    | 234/414 [23:35<19:46,  6.59s/it][A
 57%|█████▋    | 235/414 [23:43<20:41,  6.94s/it][A
 57%|█████▋    | 236/414 [23:51<20:55,  7.05s/it][A
 57%|█████▋    | 237/414 [23:56<19:36,  6.65s/it][A
 57%|█████▋    | 238/414 [24:00<17:05,  5.83s/it][A
 58%|█████▊    | 239/414 [24:06<16:40,  5.72s/it][A
 58%|█████▊    | 240/414 [24:13<18:03,  6.23s/it][A
 58%|█████▊    | 241/414 [24:19<17:24,  6.04s/it][A
 58%|█████▊    | 242/414 [24:23<15:29,  5.41s/it][A
 59%|█████▊    | 243/414 [24:27<14:49,  5.20s/it][A
 59%|█████▉    | 244/414 [24:33<15:20,  5.42s/it][A
 59%|█████▉    | 245/414 [24:39<15:32,  5.52s/it][A
 59%|█████▉    | 246/414 [24:44<14:41,  5.25s/it][A
 60%|█████▉    | 247/414 [24:49<14:31,  5.22s/it][A
 60%|█████▉    | 248/414 [24:54<14:21,  5.19s/it][A
 60%|██████    | 249/414 [24:59<13:54,  5.06s/it][A
 60%|██████    | 250/414 [25:03<13:20,  4.88s/it][A
 61%|██████    | 251/414 [25:09<13:51,  5.10s/it][A
 61%|██████    | 252/414 [25:15<14:34,  5.40s/it][A
 61%|██████    | 253/414 [25:22<16:03,  5.99s/it][A
 61%|██████▏   | 254/414 [25:30<17:26,  6.54s/it][A
 62%|██████▏   | 255/414 [25:34<15:39,  5.91s/it][A
 62%|██████▏   | 256/414 [25:39<14:41,  5.58s/it][A
 62%|██████▏   | 257/414 [25:44<14:17,  5.46s/it][A
 62%|██████▏   | 258/414 [25:49<13:42,  5.27s/it][A
 63%|██████▎   | 259/414 [25:54<13:04,  5.06s/it][A
 63%|██████▎   | 260/414 [26:00<13:38,  5.32s/it][A
 63%|██████▎   | 261/414 [26:06<14:10,  5.56s/it][A
 63%|██████▎   | 262/414 [26:13<14:59,  5.92s/it][A
 64%|██████▎   | 263/414 [26:20<15:54,  6.32s/it][A
 64%|██████▍   | 264/414 [26:26<15:59,  6.40s/it][A
 64%|██████▍   | 265/414 [26:33<15:43,  6.33s/it][A
 64%|██████▍   | 266/414 [26:39<15:40,  6.35s/it][A
 64%|██████▍   | 267/414 [26:47<16:33,  6.76s/it][A
 65%|██████▍   | 268/414 [26:53<15:53,  6.53s/it][A
 65%|██████▍   | 269/414 [26:57<14:09,  5.86s/it][A
 65%|██████▌   | 270/414 [27:02<13:17,  5.54s/it][A
 65%|██████▌   | 271/414 [27:07<12:58,  5.44s/it][A
 66%|██████▌   | 272/414 [27:13<13:10,  5.57s/it][A
 66%|██████▌   | 273/414 [27:19<13:38,  5.81s/it][A
 66%|██████▌   | 274/414 [27:25<13:40,  5.86s/it][A
 66%|██████▋   | 275/414 [27:31<13:15,  5.72s/it][A
 67%|██████▋   | 276/414 [27:36<12:53,  5.60s/it][A
 67%|██████▋   | 277/414 [27:41<12:24,  5.44s/it][A
 67%|██████▋   | 278/414 [27:45<11:26,  5.05s/it][A
 67%|██████▋   | 279/414 [27:50<11:14,  5.00s/it][A
 68%|██████▊   | 280/414 [27:56<11:39,  5.22s/it][A
 68%|██████▊   | 281/414 [28:01<11:27,  5.17s/it][A
 68%|██████▊   | 282/414 [28:07<11:54,  5.41s/it][A
 68%|██████▊   | 283/414 [28:13<12:17,  5.63s/it][A
 69%|██████▊   | 284/414 [28:18<11:44,  5.42s/it][A
 69%|██████▉   | 285/414 [28:24<12:07,  5.64s/it][A
 69%|██████▉   | 286/414 [28:31<13:08,  6.16s/it][A
 69%|██████▉   | 287/414 [28:38<13:37,  6.43s/it][A
 70%|██████▉   | 288/414 [28:44<13:16,  6.32s/it][A
 70%|██████▉   | 289/414 [28:49<12:06,  5.81s/it][A
 70%|███████   | 290/414 [28:55<12:18,  5.96s/it][A
 70%|███████   | 291/414 [29:02<12:46,  6.23s/it][A
 71%|███████   | 292/414 [29:06<11:26,  5.63s/it][A
 71%|███████   | 293/414 [29:15<12:47,  6.35s/it][A
 71%|███████   | 294/414 [29:24<14:24,  7.20s/it][A
 71%|███████▏  | 295/414 [29:32<14:40,  7.40s/it][A
 71%|███████▏  | 296/414 [29:39<14:36,  7.42s/it][A
 72%|███████▏  | 297/414 [29:45<13:27,  6.90s/it][A
 72%|███████▏  | 298/414 [29:53<13:52,  7.18s/it][A
 72%|███████▏  | 299/414 [29:59<13:34,  7.08s/it][A
 72%|███████▏  | 300/414 [30:05<12:23,  6.53s/it][A
 73%|███████▎  | 301/414 [30:10<11:43,  6.23s/it][A
 73%|███████▎  | 302/414 [30:15<10:46,  5.77s/it][A
 73%|███████▎  | 303/414 [30:20<10:19,  5.58s/it][A
 73%|███████▎  | 304/414 [30:28<11:37,  6.34s/it][A
 74%|███████▎  | 305/414 [30:36<12:30,  6.88s/it][A
 74%|███████▍  | 306/414 [30:42<11:45,  6.53s/it][A
 74%|███████▍  | 307/414 [30:48<11:09,  6.25s/it][A
 74%|███████▍  | 308/414 [30:55<11:30,  6.51s/it][A
 75%|███████▍  | 309/414 [31:02<11:47,  6.74s/it][A
 75%|███████▍  | 310/414 [31:07<10:37,  6.13s/it][A
 75%|███████▌  | 311/414 [31:12<09:56,  5.79s/it][A
 75%|███████▌  | 312/414 [31:17<09:40,  5.69s/it][A
 76%|███████▌  | 313/414 [31:22<09:19,  5.54s/it][A
 76%|███████▌  | 314/414 [31:27<08:50,  5.31s/it][A
 76%|███████▌  | 315/414 [31:33<09:01,  5.47s/it][A
 76%|███████▋  | 316/414 [31:42<10:38,  6.52s/it][A
 77%|███████▋  | 317/414 [31:50<11:06,  6.87s/it][A
 77%|███████▋  | 318/414 [31:55<10:27,  6.54s/it][A
 77%|███████▋  | 319/414 [32:01<10:03,  6.35s/it][A
 77%|███████▋  | 320/414 [32:06<09:17,  5.93s/it][A
 78%|███████▊  | 321/414 [32:13<09:25,  6.08s/it][A
 78%|███████▊  | 322/414 [32:19<09:28,  6.18s/it][A
 78%|███████▊  | 323/414 [32:24<08:42,  5.75s/it][A
 78%|███████▊  | 324/414 [32:28<08:08,  5.43s/it][A
 79%|███████▊  | 325/414 [32:33<07:51,  5.30s/it][A
 79%|███████▊  | 326/414 [32:38<07:33,  5.15s/it][A
 79%|███████▉  | 327/414 [32:45<08:16,  5.71s/it][A
 79%|███████▉  | 328/414 [32:52<08:41,  6.06s/it][A
 79%|███████▉  | 329/414 [32:59<08:49,  6.22s/it][A
 80%|███████▉  | 330/414 [33:07<09:29,  6.78s/it][A
 80%|███████▉  | 331/414 [33:14<09:32,  6.90s/it][A
 80%|████████  | 332/414 [33:21<09:26,  6.90s/it][A
 80%|████████  | 333/414 [33:27<08:59,  6.67s/it][A
 81%|████████  | 334/414 [33:31<07:59,  6.00s/it][A
 81%|████████  | 335/414 [33:36<07:19,  5.56s/it][A
 81%|████████  | 336/414 [33:41<07:04,  5.45s/it][A
 81%|████████▏ | 337/414 [33:46<06:41,  5.22s/it][A
 82%|████████▏ | 338/414 [33:52<06:52,  5.43s/it][A
 82%|████████▏ | 339/414 [33:59<07:31,  6.02s/it][A
 82%|████████▏ | 340/414 [34:07<08:13,  6.67s/it][A
 82%|████████▏ | 341/414 [34:14<08:09,  6.70s/it][A
 83%|████████▎ | 342/414 [34:18<07:08,  5.96s/it][A
 83%|████████▎ | 343/414 [34:25<07:26,  6.29s/it][A
 83%|████████▎ | 344/414 [34:35<08:19,  7.14s/it][A
 83%|████████▎ | 345/414 [34:41<08:02,  6.99s/it][A
 84%|████████▎ | 346/414 [34:46<07:08,  6.30s/it][A
 84%|████████▍ | 347/414 [34:51<06:46,  6.07s/it][A
 84%|████████▍ | 348/414 [34:57<06:34,  5.98s/it][A
 84%|████████▍ | 349/414 [35:02<06:11,  5.72s/it][A
 85%|████████▍ | 350/414 [35:10<06:37,  6.21s/it][A
 85%|████████▍ | 351/414 [35:17<06:52,  6.55s/it][A
 85%|████████▌ | 352/414 [35:22<06:13,  6.02s/it][A
 85%|████████▌ | 353/414 [35:26<05:39,  5.56s/it][A
 86%|████████▌ | 354/414 [35:32<05:30,  5.52s/it][A
 86%|████████▌ | 355/414 [35:37<05:20,  5.44s/it][A
 86%|████████▌ | 356/414 [35:42<05:05,  5.27s/it][A
 86%|████████▌ | 357/414 [35:48<05:13,  5.50s/it][A
 86%|████████▋ | 358/414 [35:55<05:43,  6.13s/it][A
 87%|████████▋ | 359/414 [36:02<05:39,  6.18s/it][A
 87%|████████▋ | 360/414 [36:07<05:11,  5.76s/it][A
 87%|████████▋ | 361/414 [36:11<04:42,  5.34s/it][A
 87%|████████▋ | 362/414 [36:17<04:55,  5.68s/it][A
 88%|████████▊ | 363/414 [36:25<05:18,  6.25s/it][A
 88%|████████▊ | 364/414 [36:30<05:00,  6.01s/it][A
 88%|████████▊ | 365/414 [36:37<04:55,  6.03s/it][A
 88%|████████▊ | 366/414 [36:44<05:06,  6.39s/it][A
 89%|████████▊ | 367/414 [36:50<04:58,  6.35s/it][A
 89%|████████▉ | 368/414 [36:56<04:52,  6.35s/it][A
 89%|████████▉ | 369/414 [37:04<05:02,  6.73s/it][A
 89%|████████▉ | 370/414 [37:11<04:54,  6.69s/it][A
 90%|████████▉ | 371/414 [37:17<04:44,  6.61s/it][A
 90%|████████▉ | 372/414 [37:24<04:41,  6.70s/it][A
 90%|█████████ | 373/414 [37:31<04:45,  6.96s/it][A
 90%|█████████ | 374/414 [37:40<04:54,  7.37s/it][A
 91%|█████████ | 375/414 [37:46<04:32,  6.98s/it][A
 91%|█████████ | 376/414 [37:50<03:52,  6.13s/it][A
 91%|█████████ | 377/414 [37:56<03:44,  6.06s/it][A
 91%|█████████▏| 378/414 [38:03<03:52,  6.46s/it][A
 92%|█████████▏| 379/414 [38:09<03:35,  6.17s/it][A
 92%|█████████▏| 380/414 [38:16<03:40,  6.48s/it][A
 92%|█████████▏| 381/414 [38:27<04:13,  7.69s/it][A
 92%|█████████▏| 382/414 [38:34<04:06,  7.69s/it][A
 93%|█████████▎| 383/414 [38:39<03:30,  6.78s/it][A
 93%|█████████▎| 384/414 [38:45<03:16,  6.56s/it][A
 93%|█████████▎| 385/414 [38:51<03:06,  6.44s/it][A
 93%|█████████▎| 386/414 [38:56<02:47,  5.98s/it][A
 93%|█████████▎| 387/414 [39:01<02:31,  5.61s/it][A
 94%|█████████▎| 388/414 [39:06<02:22,  5.47s/it][A
 94%|█████████▍| 389/414 [39:10<02:09,  5.20s/it][A
 94%|█████████▍| 390/414 [39:15<01:57,  4.89s/it][A
 94%|█████████▍| 391/414 [39:19<01:50,  4.82s/it][A
 95%|█████████▍| 392/414 [39:24<01:46,  4.83s/it][A
 95%|█████████▍| 393/414 [39:31<01:52,  5.37s/it][A
 95%|█████████▌| 394/414 [39:38<01:59,  5.97s/it][A
 95%|█████████▌| 395/414 [39:43<01:48,  5.72s/it][A
 96%|█████████▌| 396/414 [39:50<01:48,  6.01s/it][A
 96%|█████████▌| 397/414 [39:58<01:54,  6.76s/it][A
 96%|█████████▌| 398/414 [40:04<01:44,  6.53s/it][A
 96%|█████████▋| 399/414 [40:08<01:26,  5.74s/it][A
 97%|█████████▋| 400/414 [40:16<01:28,  6.32s/it][A
 97%|█████████▋| 401/414 [40:26<01:37,  7.53s/it][A
 97%|█████████▋| 402/414 [40:33<01:25,  7.16s/it][A
 97%|█████████▋| 403/414 [40:37<01:10,  6.37s/it][A
 98%|█████████▊| 404/414 [40:42<00:57,  5.78s/it][A
 98%|█████████▊| 405/414 [40:46<00:48,  5.38s/it][A
 98%|█████████▊| 406/414 [40:53<00:46,  5.77s/it][A
 98%|█████████▊| 407/414 [41:00<00:44,  6.36s/it][A
 99%|█████████▊| 408/414 [41:06<00:37,  6.19s/it][A
 99%|█████████▉| 409/414 [41:12<00:30,  6.19s/it][A
 99%|█████████▉| 410/414 [41:19<00:24,  6.25s/it][A
 99%|█████████▉| 411/414 [41:23<00:16,  5.66s/it][A
100%|█████████▉| 412/414 [41:30<00:11,  5.90s/it][A
100%|█████████▉| 413/414 [41:36<00:06,  6.20s/it][A
100%|██████████| 414/414 [41:39<00:00,  5.20s/it][A                                                     
                                                 [A{'eval_loss': 0.8720937371253967, 'eval_runtime': 2507.0804, 'eval_samples_per_second': 1.318, 'eval_steps_per_second': 0.165, 'epoch': 0.3}
 30%|███       | 70/233 [7:21:15<7:09:34, 158.12s/it]
100%|██████████| 414/414 [41:40<00:00,  5.20s/it][A
                                                 [A 30%|███       | 71/233 [7:23:19<40:29:59, 900.00s/it] 31%|███       | 72/233 [7:25:17<29:45:35, 665.44s/it] 31%|███▏      | 73/233 [7:27:31<22:29:27, 506.04s/it] 32%|███▏      | 74/233 [7:29:24<17:08:41, 388.19s/it] 32%|███▏      | 75/233 [7:31:52<13:52:21, 316.09s/it] 33%|███▎      | 76/233 [7:33:50<11:11:25, 256.59s/it] 33%|███▎      | 77/233 [7:35:50<9:20:44, 215.67s/it]  33%|███▎      | 78/233 [7:37:52<8:04:42, 187.63s/it] 34%|███▍      | 79/233 [7:39:48<7:06:22, 166.12s/it] 34%|███▍      | 80/233 [7:41:48<6:28:21, 152.30s/it]                                                     {'loss': 0.8895, 'grad_norm': 0.21205613017082214, 'learning_rate': 8.386407858128706e-05, 'epoch': 0.34}
 34%|███▍      | 80/233 [7:41:48<6:28:21, 152.30s/it]
***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****

***** Running Evaluation *****

***** Running Evaluation *****

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1
  Num examples = 3305
  Batch size = 1
  Num examples = 3305
  Num examples = 3305
  Batch size = 1
  Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1
[INFO|trainer.py:4643] 2025-12-05 12:34:17,038 >> 
***** Running Evaluation *****
[INFO|trainer.py:4645] 2025-12-05 12:34:17,038 >>   Num examples = 3305
[INFO|trainer.py:4648] 2025-12-05 12:34:17,038 >>   Batch size = 1

  0%|          | 0/414 [00:00<?, ?it/s][A
  0%|          | 2/414 [00:04<16:31,  2.41s/it][A
  1%|          | 3/414 [00:10<25:32,  3.73s/it][A
  1%|          | 4/414 [00:18<35:40,  5.22s/it][A
  1%|          | 5/414 [00:25<41:23,  6.07s/it][A
  1%|▏         | 6/414 [00:30<37:08,  5.46s/it][A
  2%|▏         | 7/414 [00:34<35:16,  5.20s/it][A
  2%|▏         | 8/414 [00:39<33:48,  5.00s/it][A
  2%|▏         | 9/414 [00:44<35:10,  5.21s/it][A
  2%|▏         | 10/414 [00:52<39:34,  5.88s/it][A
  3%|▎         | 11/414 [00:58<39:09,  5.83s/it][A
  3%|▎         | 12/414 [01:04<40:08,  5.99s/it][A
  3%|▎         | 13/414 [01:12<43:46,  6.55s/it][A
  3%|▎         | 14/414 [01:19<44:57,  6.74s/it][A
  4%|▎         | 15/414 [01:25<44:16,  6.66s/it][A
  4%|▍         | 16/414 [01:32<44:37,  6.73s/it][A
  4%|▍         | 17/414 [01:39<44:35,  6.74s/it][A
  4%|▍         | 18/414 [01:46<44:53,  6.80s/it][A
  5%|▍         | 19/414 [01:54<47:57,  7.29s/it][A
  5%|▍         | 20/414 [02:01<47:02,  7.16s/it][A
  5%|▌         | 21/414 [02:06<42:49,  6.54s/it][A
  5%|▌         | 22/414 [02:12<41:55,  6.42s/it][A
  6%|▌         | 23/414 [02:20<43:51,  6.73s/it][A
  6%|▌         | 24/414 [02:26<42:24,  6.53s/it][A
  6%|▌         | 25/414 [02:33<42:33,  6.57s/it][A
  6%|▋         | 26/414 [02:39<42:40,  6.60s/it][A
  7%|▋         | 27/414 [02:44<38:03,  5.90s/it][A
  7%|▋         | 28/414 [02:49<36:29,  5.67s/it][A
  7%|▋         | 29/414 [02:54<35:40,  5.56s/it][A
  7%|▋         | 30/414 [03:00<36:43,  5.74s/it][A
  7%|▋         | 31/414 [03:06<36:34,  5.73s/it][A
  8%|▊         | 32/414 [03:10<34:09,  5.37s/it][A
  8%|▊         | 33/414 [03:17<36:16,  5.71s/it][A
  8%|▊         | 34/414 [03:23<36:31,  5.77s/it][A
  8%|▊         | 35/414 [03:28<36:13,  5.73s/it][A
  9%|▊         | 36/414 [03:35<37:29,  5.95s/it][A
  9%|▉         | 37/414 [03:41<38:27,  6.12s/it][A
  9%|▉         | 38/414 [03:48<38:45,  6.19s/it][A
  9%|▉         | 39/414 [03:52<34:47,  5.57s/it][A
 10%|▉         | 40/414 [03:58<34:44,  5.57s/it][A
 10%|▉         | 41/414 [04:04<36:36,  5.89s/it][A
 10%|█         | 42/414 [04:10<36:35,  5.90s/it][A
 10%|█         | 43/414 [04:19<42:22,  6.85s/it][A
 11%|█         | 44/414 [04:30<49:23,  8.01s/it][A
 11%|█         | 45/414 [04:37<47:31,  7.73s/it][A
 11%|█         | 46/414 [04:42<41:38,  6.79s/it][A
 11%|█▏        | 47/414 [04:46<36:54,  6.03s/it][A
 12%|█▏        | 48/414 [04:50<32:41,  5.36s/it][A
 12%|█▏        | 49/414 [04:53<29:05,  4.78s/it][A
 12%|█▏        | 50/414 [04:57<27:39,  4.56s/it][A
 12%|█▏        | 51/414 [05:01<26:50,  4.44s/it][A
 13%|█▎        | 52/414 [05:06<26:34,  4.40s/it][A
 13%|█▎        | 53/414 [05:13<31:58,  5.31s/it][A
 13%|█▎        | 54/414 [05:21<37:31,  6.26s/it][A
 13%|█▎        | 55/414 [05:28<38:30,  6.44s/it][A
 14%|█▎        | 56/414 [05:35<38:29,  6.45s/it][A
 14%|█▍        | 57/414 [05:40<35:46,  6.01s/it][A
 14%|█▍        | 58/414 [05:46<35:19,  5.95s/it][A
 14%|█▍        | 59/414 [05:53<37:28,  6.34s/it][A
 14%|█▍        | 60/414 [05:59<36:37,  6.21s/it][A
 15%|█▍        | 61/414 [06:03<33:11,  5.64s/it][A
 15%|█▍        | 62/414 [06:07<30:47,  5.25s/it][A
 15%|█▌        | 63/414 [06:12<30:15,  5.17s/it][A
 15%|█▌        | 64/414 [06:17<29:51,  5.12s/it][A
 16%|█▌        | 65/414 [06:22<29:47,  5.12s/it][A
 16%|█▌        | 66/414 [06:28<30:40,  5.29s/it][A
 16%|█▌        | 67/414 [06:35<33:54,  5.86s/it][A
 16%|█▋        | 68/414 [06:42<36:00,  6.24s/it][A
 17%|█▋        | 69/414 [06:50<37:18,  6.49s/it][A
 17%|█▋        | 70/414 [06:57<38:22,  6.69s/it][A
 17%|█▋        | 71/414 [07:02<35:31,  6.21s/it][A
 17%|█▋        | 72/414 [07:07<33:20,  5.85s/it][A
 18%|█▊        | 73/414 [07:12<31:36,  5.56s/it][A
 18%|█▊        | 74/414 [07:16<29:04,  5.13s/it][A
 18%|█▊        | 75/414 [07:20<27:04,  4.79s/it][A
 18%|█▊        | 76/414 [07:24<26:25,  4.69s/it][A
 19%|█▊        | 77/414 [07:30<28:45,  5.12s/it][A
 19%|█▉        | 78/414 [07:36<30:00,  5.36s/it][A
 19%|█▉        | 79/414 [07:42<30:52,  5.53s/it][A
 19%|█▉        | 80/414 [07:48<30:24,  5.46s/it][A
 20%|█▉        | 81/414 [07:52<29:04,  5.24s/it][A
 20%|█▉        | 82/414 [07:58<29:07,  5.26s/it][A
 20%|██        | 83/414 [08:05<32:41,  5.92s/it][A
 20%|██        | 84/414 [08:13<35:11,  6.40s/it][A
 21%|██        | 85/414 [08:17<32:36,  5.95s/it][A
 21%|██        | 86/414 [08:22<30:45,  5.63s/it][A
 21%|██        | 87/414 [08:27<29:01,  5.33s/it][A
 21%|██▏       | 88/414 [08:32<28:12,  5.19s/it][A
 21%|██▏       | 89/414 [08:38<29:06,  5.37s/it][A
 22%|██▏       | 90/414 [08:45<31:50,  5.90s/it][A
 22%|██▏       | 91/414 [08:50<30:54,  5.74s/it][A
 22%|██▏       | 92/414 [08:56<30:21,  5.66s/it][A
 22%|██▏       | 93/414 [09:01<30:02,  5.62s/it][A
 23%|██▎       | 94/414 [09:06<29:26,  5.52s/it][A
 23%|██▎       | 95/414 [09:12<29:56,  5.63s/it][A
 23%|██▎       | 96/414 [09:18<30:26,  5.74s/it][A
 23%|██▎       | 97/414 [09:24<30:09,  5.71s/it][A
 24%|██▎       | 98/414 [09:28<27:33,  5.23s/it][A
 24%|██▍       | 99/414 [09:33<26:31,  5.05s/it][A
 24%|██▍       | 100/414 [09:39<28:22,  5.42s/it][A
 24%|██▍       | 101/414 [09:47<32:27,  6.22s/it][A
 25%|██▍       | 102/414 [09:54<33:14,  6.39s/it][A
 25%|██▍       | 103/414 [09:59<30:33,  5.90s/it][A
 25%|██▌       | 104/414 [10:03<28:31,  5.52s/it][A
 25%|██▌       | 105/414 [10:09<28:38,  5.56s/it][A
 26%|██▌       | 106/414 [10:16<31:19,  6.10s/it][A
 26%|██▌       | 107/414 [10:22<31:07,  6.08s/it][A
 26%|██▌       | 108/414 [10:27<29:09,  5.72s/it][A
 26%|██▋       | 109/414 [10:32<27:54,  5.49s/it][A
 27%|██▋       | 110/414 [10:38<28:29,  5.62s/it][A
 27%|██▋       | 111/414 [10:44<29:25,  5.83s/it][A
 27%|██▋       | 112/414 [10:49<27:58,  5.56s/it][A
 27%|██▋       | 113/414 [10:56<29:01,  5.78s/it][A
 28%|██▊       | 114/414 [11:03<30:58,  6.19s/it][A
 28%|██▊       | 115/414 [11:09<30:19,  6.09s/it][A
 28%|██▊       | 116/414 [11:15<30:17,  6.10s/it][A
 28%|██▊       | 117/414 [11:23<33:04,  6.68s/it][A
 29%|██▊       | 118/414 [11:32<36:23,  7.38s/it][A
 29%|██▊       | 119/414 [11:41<38:56,  7.92s/it][A
 29%|██▉       | 120/414 [11:49<39:02,  7.97s/it][A
 29%|██▉       | 121/414 [11:57<38:23,  7.86s/it][A
 29%|██▉       | 122/414 [12:04<37:19,  7.67s/it][A
 30%|██▉       | 123/414 [12:10<35:38,  7.35s/it][A
 30%|██▉       | 124/414 [12:18<35:44,  7.40s/it][A
 30%|███       | 125/414 [12:25<35:47,  7.43s/it][A
 30%|███       | 126/414 [12:31<33:12,  6.92s/it][A
 31%|███       | 127/414 [12:38<33:19,  6.97s/it][A
 31%|███       | 128/414 [12:47<35:52,  7.52s/it][A
 31%|███       | 129/414 [12:54<34:40,  7.30s/it][A
 31%|███▏      | 130/414 [13:01<33:56,  7.17s/it][A
 32%|███▏      | 131/414 [13:09<35:38,  7.56s/it][A
 32%|███▏      | 132/414 [13:17<35:30,  7.55s/it][A
 32%|███▏      | 133/414 [13:21<30:48,  6.58s/it][A
 32%|███▏      | 134/414 [13:27<29:48,  6.39s/it][A
 33%|███▎      | 135/414 [13:33<29:48,  6.41s/it][A
 33%|███▎      | 136/414 [13:41<31:36,  6.82s/it][A
 33%|███▎      | 137/414 [13:51<34:56,  7.57s/it][A
 33%|███▎      | 138/414 [13:58<34:11,  7.43s/it][A
 34%|███▎      | 139/414 [14:02<30:23,  6.63s/it][A
 34%|███▍      | 140/414 [14:07<27:40,  6.06s/it][A
 34%|███▍      | 141/414 [14:13<26:51,  5.90s/it][A
 34%|███▍      | 142/414 [14:19<27:45,  6.12s/it][A
 35%|███▍      | 143/414 [14:25<27:40,  6.13s/it][A
 35%|███▍      | 144/414 [14:30<25:19,  5.63s/it][A
 35%|███▌      | 145/414 [14:34<23:34,  5.26s/it][A
 35%|███▌      | 146/414 [14:39<22:06,  4.95s/it][A
 36%|███▌      | 147/414 [14:43<21:52,  4.92s/it][A
 36%|███▌      | 148/414 [14:49<22:30,  5.08s/it][A
 36%|███▌      | 149/414 [14:54<22:07,  5.01s/it][A
 36%|███▌      | 150/414 [14:58<21:31,  4.89s/it][A
 36%|███▋      | 151/414 [15:02<20:25,  4.66s/it][A
 37%|███▋      | 152/414 [15:07<20:47,  4.76s/it][A
 37%|███▋      | 153/414 [15:13<21:33,  4.95s/it][A
 37%|███▋      | 154/414 [15:17<20:32,  4.74s/it][A
 37%|███▋      | 155/414 [15:21<19:52,  4.60s/it][A
 38%|███▊      | 156/414 [15:27<20:44,  4.82s/it][A
 38%|███▊      | 157/414 [15:33<22:37,  5.28s/it][A
 38%|███▊      | 158/414 [15:39<23:59,  5.62s/it][A
 38%|███▊      | 159/414 [15:46<24:50,  5.84s/it][A
 39%|███▊      | 160/414 [15:51<24:30,  5.79s/it][A
 39%|███▉      | 161/414 [15:58<25:42,  6.10s/it][A
 39%|███▉      | 162/414 [16:04<25:39,  6.11s/it][A
 39%|███▉      | 163/414 [16:10<24:59,  5.97s/it][A
 40%|███▉      | 164/414 [16:17<26:21,  6.32s/it][A
 40%|███▉      | 165/414 [16:25<27:42,  6.68s/it][Aslurmstepd: error: *** JOB 1542871 ON kn176 CANCELLED AT 2025-12-05T12:50:52 DUE TO TIME LIMIT ***
