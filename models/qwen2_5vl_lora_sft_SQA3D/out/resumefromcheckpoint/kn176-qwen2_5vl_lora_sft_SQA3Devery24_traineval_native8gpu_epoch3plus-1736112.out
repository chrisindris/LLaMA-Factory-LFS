/project/aip-wangcs/indrisch/LLaMA-Factory /project/aip-wangcs/indrisch/LLaMA-Factory/preliminaries/qwen2_5vl_lora_sft_SQA3D
[INFO|2026-01-02 05:37:04] llamafactory.launcher:143 >> Initializing 8 distributed tasks at: 127.0.0.1:39581
W0102 05:37:06.213000 806312 torch/distributed/run.py:803] 
W0102 05:37:06.213000 806312 torch/distributed/run.py:803] *****************************************
W0102 05:37:06.213000 806312 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0102 05:37:06.213000 806312 torch/distributed/run.py:803] *****************************************
[WARNING|2026-01-02 05:37:16] llamafactory.extras.misc:154 >> Version checking has been disabled, may lead to unexpected behaviors.
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode⚙️  Running in WANDB offline mode


⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
[2026-01-02 05:37:19,913] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-02 05:37:19,913] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-02 05:37:19,913] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-02 05:37:19,913] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-02 05:37:19,913] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-02 05:37:19,913] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-02 05:37:19,913] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-02 05:37:19,914] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2026-01-02 05:37:26,599] [INFO] [comm.py:658:init_distributed] cdb=None
[2026-01-02 05:37:26,599] [INFO] [comm.py:658:init_distributed] cdb=None
[2026-01-02 05:37:26,599] [INFO] [comm.py:658:init_distributed] cdb=None
[2026-01-02 05:37:26,599] [INFO] [comm.py:658:init_distributed] cdb=None
[2026-01-02 05:37:26,599] [INFO] [comm.py:658:init_distributed] cdb=None
[2026-01-02 05:37:26,599] [INFO] [comm.py:658:init_distributed] cdb=None
[2026-01-02 05:37:26,600] [INFO] [comm.py:658:init_distributed] cdb=None
[2026-01-02 05:37:26,600] [INFO] [comm.py:658:init_distributed] cdb=None
[2026-01-02 05:37:26,600] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[W102 05:37:26.724679965 ProcessGroupNCCL.cpp:924] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W102 05:37:26.724679683 ProcessGroupNCCL.cpp:924] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W102 05:37:26.724684277 ProcessGroupNCCL.cpp:924] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W102 05:37:26.724690349 ProcessGroupNCCL.cpp:924] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W102 05:37:26.724701411 ProcessGroupNCCL.cpp:924] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W102 05:37:26.724704643 ProcessGroupNCCL.cpp:924] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W102 05:37:26.724719223 ProcessGroupNCCL.cpp:924] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[W102 05:37:26.727199884 ProcessGroupNCCL.cpp:924] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())
[INFO|2026-01-02 05:37:28] llamafactory.hparams.parser:455 >> Process rank: 6, world size: 8, device: cuda:6, distributed training: True, compute dtype: torch.bfloat16
[INFO|2026-01-02 05:37:28] llamafactory.hparams.parser:455 >> Process rank: 2, world size: 8, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
[INFO|2026-01-02 05:37:28] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.
[INFO|2026-01-02 05:37:28] llamafactory.hparams.parser:455 >> Process rank: 5, world size: 8, device: cuda:5, distributed training: True, compute dtype: torch.bfloat16
[INFO|2026-01-02 05:37:28] llamafactory.hparams.parser:455 >> Process rank: 4, world size: 8, device: cuda:4, distributed training: True, compute dtype: torch.bfloat16
[INFO|2026-01-02 05:37:28] llamafactory.hparams.parser:455 >> Process rank: 0, world size: 8, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|2026-01-02 05:37:28] llamafactory.hparams.parser:455 >> Process rank: 3, world size: 8, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
[INFO|2026-01-02 05:37:28] llamafactory.hparams.parser:455 >> Process rank: 7, world size: 8, device: cuda:7, distributed training: True, compute dtype: torch.bfloat16
[INFO|2026-01-02 05:37:28] llamafactory.hparams.parser:455 >> Process rank: 1, world size: 8, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
[INFO|hub.py:421] 2026-01-02 05:37:28,179 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2026-01-02 05:37:28,196 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2026-01-02 05:37:28,204 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2026-01-02 05:37:28,204 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2026-01-02 05:37:28,205 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2026-01-02 05:37:28,205 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-02 05:37:28,205 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-02 05:37:28,205 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2026-01-02 05:37:28,205 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2026-01-02 05:37:28,511 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|hub.py:421] 2026-01-02 05:37:28,512 >> Offline mode: forcing local_files_only=True
[INFO|hub.py:421] 2026-01-02 05:37:28,512 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:316] 2026-01-02 05:37:28,513 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2026-01-02 05:37:28,515 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:316] 2026-01-02 05:37:28,517 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2026-01-02 05:37:28,519 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:428] 2026-01-02 05:37:28,525 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:1949] 2026-01-02 05:37:28,525 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2026-01-02 05:37:28,530 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2026-01-02 05:37:28,530 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2026-01-02 05:37:28,530 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2026-01-02 05:37:28,530 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-02 05:37:28,530 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-02 05:37:28,530 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2026-01-02 05:37:28,530 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2026-01-02 05:37:28,757 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:660] 2026-01-02 05:37:28,758 >> Offline mode: forcing local_files_only=True
[INFO|video_processing_utils.py:726] 2026-01-02 05:37:28,761 >> loading configuration file video_preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|video_processing_utils.py:770] 2026-01-02 05:37:28,765 >> Video processor Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:928] 2026-01-02 05:37:28,765 >> Offline mode: forcing local_files_only=True
[INFO|processing_utils.py:1116] 2026-01-02 05:37:28,768 >> loading configuration file processor_config.json from cache at None
[INFO|processing_utils.py:1199] 2026-01-02 05:37:29,171 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2_5_VLProcessor"
}

[INFO|2026-01-02 05:37:29] llamafactory.data.loader:143 >> Loading dataset /scratch/indrisch/huggingface/hub/datasets--cvis-tmu--llamafactory-sqa3d-traces-multiimage-vqa/snapshots/ce5c54adc1608d1726730c9ff334e65b6dd70e46/data/...
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once
[rank0]:[W102 05:37:29.880378979 ProcessGroupNCCL.cpp:5072] Guessing device ID based on global rank. This can cause a hang if rank to GPU mapping is heterogeneous. You can specify device_id in init_process_group()
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
training example:
input_ids:
[151644, 8948, 271, 262, 1446, 525, 264, 16585, 11129, 4142, 11528, 17847, 369, 27979, 647, 47522, 389, 264, 3175, 29519, 6109, 6839, 3941, 5248, 5335, 382, 262, 26149, 510, 262, 68252, 1283, 419, 1943, 11, 498, 686, 5258, 451, 6109, 5335, 438, 366, 1805, 29, 9492, 13, 18877, 1105, 438, 2155, 6194, 315, 279, 83490, 6109, 13, 28634, 311, 5335, 553, 220, 16, 5980, 1922, 304, 1973, 315, 11094, 25, 508, 16, 1125, 508, 17, 1125, 4593, 11, 508, 45, 29562, 262, 3596, 35304, 1718, 8575, 38439, 3298, 14017, 510, 262, 7288, 7854, 264, 12966, 220, 18, 35, 10502, 1614, 3941, 6194, 25, 3754, 6171, 3941, 5335, 11, 23583, 8674, 6249, 17040, 11, 323, 990, 5452, 14, 509, 8957, 14688, 21060, 369, 7990, 55916, 624, 262, 7288, 72077, 409, 849, 292, 3793, 26087, 983, 847, 1290, 97089, 17996, 4065, 14, 29898, 484, 32511, 56111, 7612, 1825, 35978, 28455, 448, 5091, 311, 279, 64171, 14, 8092, 13, 1416, 279, 58385, 374, 54761, 11, 1638, 311, 279, 1429, 38219, 8622, 1651, 323, 1977, 773, 624, 262, 7288, 5443, 1172, 9434, 5904, 26, 653, 537, 17023, 63133, 3565, 476, 17188, 389, 9250, 6540, 7797, 6770, 1633, 43898, 3793, 624, 262, 7288, 3197, 5248, 11178, 3000, 11, 8830, 553, 279, 1850, 2432, 311, 27979, 17133, 488, 4274, 1835, 3941, 6194, 13, 1416, 2058, 54761, 1283, 13295, 678, 5335, 11, 1584, 429, 9355, 382, 262, 30990, 320, 327, 32739, 1378, 10010, 11, 304, 419, 1973, 982, 262, 366, 26865, 397, 262, 14822, 14319, 29208, 32711, 304, 220, 18, 4142, 16, 15, 2805, 7354, 13, 356, 632, 5904, 448, 2168, 14937, 320, 68, 1302, 2572, 65750, 18, 5669, 3100, 7579, 18010, 1290, 315, 8718, 26, 508, 16, 5669, 1852, 1894, 11, 42396, 64212, 2823, 63594, 714, 4583, 320, 2640, 37294, 17, 20, 15, 11211, 7241, 5871, 568, 7036, 894, 17796, 8957, 14, 2969, 26745, 487, 323, 1246, 498, 19673, 432, 624, 262, 690, 26865, 397, 262, 366, 9217, 397, 262, 362, 3175, 63594, 4226, 1172, 11, 892, 1231, 387, 510, 262, 7288, 1416, 14016, 25, 3776, 37328, 4226, 11, 4504, 320, 68, 1302, 2572, 1036, 17, 32511, 476, 7414, 14, 2753, 320, 68, 1302, 2572, 1036, 9693, 854, 4292, 262, 7288, 1416, 537, 14016, 25, 362, 11652, 476, 1378, 624, 262, 1416, 4226, 537, 9434, 11, 421, 1052, 374, 902, 2797, 5904, 11, 476, 421, 279, 4226, 374, 35197, 54761, 25, 1036, 33260, 8253, 854, 624, 262, 690, 9217, 1339, 262, 62947, 609, 43797, 50, 510, 262, 7288, 3155, 4183, 13153, 279, 3405, 476, 1140, 5335, 26, 653, 4183, 2550, 4718, 476, 4960, 14158, 624, 262, 7288, 84468, 4285, 1894, 5036, 448, 518, 1429, 825, 22739, 26087, 4145, 3446, 838, 4322, 1574, 58689, 7256, 854, 4292, 262, 7288, 2823, 12966, 3941, 6194, 26, 421, 6194, 28295, 11, 62552, 279, 11299, 15432, 5904, 323, 5185, 432, 304, 366, 26865, 29816, 257, 151645, 198, 151644, 872, 198, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 30, 220, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151645, 198, 151644, 77091, 198, 198, 32462, 432, 11, 1077, 594, 1430, 311, 7071, 700, 1128, 594, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 62410, 13, 5512, 11, 1077, 594, 50087, 279, 14852, 6505, 13, 576, 21921, 374, 4363, 264, 4158, 11, 1378, 1737, 95112, 4982, 13, 2411, 220, 16, 16, 297, 62410, 320, 3481, 7950, 3082, 701, 582, 1184, 311, 1779, 429, 7702, 13, 21131, 518, 279, 5335, 11, 5310, 279, 825, 9027, 279, 21921, 3082, 11, 1052, 594, 264, 17846, 3403, 279, 19309, 11, 714, 3783, 11, 902, 2293, 11489, 11, 279, 3405, 374, 911, 279, 21921, 13, 13824, 11, 7196, 279, 330, 66, 75512, 1, 1588, 374, 279, 5819, 4982, 13, 13824, 11, 902, 11, 279, 19309, 702, 264, 21921, 3685, 30, 13824, 11, 902, 11, 279, 1887, 21921, 374, 279, 4158, 61607, 13, 13824, 11, 304, 279, 2168, 1380, 279, 6249, 4933, 279, 19309, 323, 21921, 11, 279, 1909, 315, 279, 21921, 2293, 11489, 11, 902, 11, 7196, 279, 17846, 30, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 2727, 330, 65, 77832, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 3263, 2055, 220, 16, 16, 297, 62410, 1035, 387, 279, 8416, 2115, 3082, 8674, 311, 279, 19309, 13, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 279, 19309, 3082, 13, 2160, 429, 949, 315, 279, 21921, 6505, 30, 13824, 11, 902, 11, 7196, 279, 21921, 374, 279, 4158, 61607, 1212, 279, 19309, 13, 13824, 11, 902, 11, 279, 1909, 315, 429, 21921, 2293, 11489, 11, 902, 11, 304, 1045, 5335, 11, 1052, 594, 264, 17846, 25911, 476, 21810, 13, 13824, 11, 902, 11, 1077, 594, 1779, 279, 1156, 2421, 5335, 13, 576, 17196, 45766, 374, 33866, 11, 1221, 279, 19309, 3082, 13, 13824, 11, 279, 4396, 4226, 25, 3330, 518, 279, 5335, 11, 279, 1633, 389, 1909, 315, 279, 14852, 21921, 320, 1782, 4158, 26482, 4982, 8, 518, 220, 16, 16, 297, 62410, 374, 264, 17846, 13, 13824, 11, 902, 11, 3783, 21677, 17846, 374, 389, 279, 7002, 3403, 279, 19309, 13, 13824, 11, 7196, 358, 5786, 878, 13, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 594, 3405, 25, 330, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 7521, 2055, 1156, 11, 10542, 279, 21921, 13, 576, 4158, 26482, 4982, 3143, 220, 16, 16, 297, 62410, 320, 3481, 7950, 568, 3555, 594, 389, 1909, 30, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 429, 3082, 13, 13824, 11, 9834, 0, 758, 279, 21844, 476, 21810, 11, 1052, 594, 264, 17846, 13, 2055, 279, 4226, 374, 264, 17846, 4192, 1249, 8253, 1128, 374, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 527, 20666, 11, 23643, 279, 27979, 6789, 323, 5424, 304, 279, 5335, 25, 18611, 16, 13, 95210, 279, 1036, 65, 77832, 21921, 854, 304, 3405, 25, 362, 4158, 11, 1378, 481, 26482, 5819, 4982, 374, 9434, 3143, 279, 1909, 481, 2115, 320, 16, 16, 297, 527, 20666, 8, 3082, 8674, 311, 1008, 37664, 1075, 279, 19309, 323, 17196, 13, 2303, 17, 13, 1374, 19991, 6171, 3403, 419, 21921, 25, 758, 279, 5335, 11, 264, 3070, 73225, 334, 374, 21810, 389, 279, 7002, 3403, 419, 21921, 3082, 13, 79226, 54216, 151645, 198]
inputs:
<|im_start|>system

    You are a careful vision–language assistant for spatial VQA on a single indoor scene shown across multiple images.

    INPUT:
    Immediately after this message, you will receive N scene images as <image> tags. Treat them as different views of the SAME scene. Refer to images by 1-based index in order of appearance: [1], [2], …, [N].

    REASONING PRINCIPLES:
    • Build a consistent 3D mental model across views: track objects across images, infer relative camera pose, and use scale/occlusion/shadows for depth cues.
    • Interpret deictic terms (“to my right/left/in front/behind”) EGOCENTRICALLY with respect to the narrator/agent. If the viewpoint is ambiguous, default to the most informative central view and say so.
    • Use only visible evidence; do not invent unseen details or rely on external knowledge beyond basic object/color terms.
    • When multiple candidates exist, resolve by the best match to spatial phrase + salience across views. If still ambiguous after checking all images, state that clearly.

    OUTPUT (exactly two blocks, in this order):
    <think>
    Step-by-step reasoning in 3–10 short steps. Cite evidence with image indices (e.g., “[3]: light wood desk right of monitor; [1]: same color, confirms”). Be concise but complete (aim ≤250 tokens unless necessary). Note any occlusion/ambiguity and how you resolved it.
    </think>
    <answer>
    A single concise answer only, which may be:
    • If sufficient: One-word answer, Count (e.g., “2”) or Yes/No (e.g., “yes”).
    • If not sufficient: A sentence or two.
    If answer not visible, if there is no clear evidence, or if the answer is genuinely ambiguous: “cannot determine”.
    </answer>

    STYLE & RULES:
    • Do NOT repeat the question or list images; do NOT output JSON or extra sections.
    • Prefer simple color names with at most one modifier (“light/dark/pale/beige”).
    • Be consistent across views; if views disagree, prioritize the clearest evidence and note it in <think>.
    <|im_end|>
<|im_start|>user
What is on top of the bathroom cabinet that is on my 11 o'clock? <|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|im_end|>
<|im_start|>assistant

Got it, let's try to figure out what's on top of the bathroom cabinet at 11 o'clock. First, let's visualize the bathroom setup. The cabinet is likely a white, two-drawer unit. At 11 o'clock (top-left area), we need to check that spot. Looking at the images, especially the one showing the cabinet area, there's a mirror above the sink, but wait, no—wait, the question is about the cabinet. Wait, maybe the "cabinet" here is the storage unit. Wait, no, the sink has a cabinet below? Wait, no, the main cabinet is the white drawers. Wait, in the image where the camera shows the sink and cabinet, the top of the cabinet—wait, no, maybe the mirror? Wait, no, let's check again. Wait, the user says "bathroom cabinet that is on my 11 o'clock". So 11 o'clock would be the upper left area relative to the sink. Looking at the images, there's a mirror mounted on the wall above the sink area. Is that part of the cabinet setup? Wait, no, maybe the cabinet is the white drawers under the sink. Wait, no, the top of that cabinet—wait, no, in some images, there's a mirror reflected or mounted. Wait, no, let's check the first few images. The shower curtain is colorful, then the sink area. Wait, the correct answer: looking at the images, the object on top of the bathroom cabinet (the white drawer unit) at 11 o'clock is a mirror. Wait, no, wait—the mirror is on the wall above the sink. Wait, maybe I misread. Wait, no, let's check again. Wait, the user's question: "What is on top of the bathroom cabinet that is on my 11 o'clock?" So first, identify the cabinet. The white drawer unit near 11 o'clock (top-left). What's on top? Looking at the images, there's a mirror mounted on the wall above that area. Wait, yes! In the reflection or mounted, there's a mirror. So the answer is a mirror.


To determine what is on top of the bathroom cabinet at 11 o’clock, analyze the spatial layout and elements in the images:  

1. Locate the “bathroom cabinet” in question: A white, two - drawer storage unit is visible near the top - left (11 o’clock) area relative to other fixtures like the sink and shower.  
2. Examine objects above this cabinet: In the images, a **mirror** is mounted on the wall above this cabinet area.  


Mirror<|im_end|>

label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 198, 32462, 432, 11, 1077, 594, 1430, 311, 7071, 700, 1128, 594, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 62410, 13, 5512, 11, 1077, 594, 50087, 279, 14852, 6505, 13, 576, 21921, 374, 4363, 264, 4158, 11, 1378, 1737, 95112, 4982, 13, 2411, 220, 16, 16, 297, 62410, 320, 3481, 7950, 3082, 701, 582, 1184, 311, 1779, 429, 7702, 13, 21131, 518, 279, 5335, 11, 5310, 279, 825, 9027, 279, 21921, 3082, 11, 1052, 594, 264, 17846, 3403, 279, 19309, 11, 714, 3783, 11, 902, 2293, 11489, 11, 279, 3405, 374, 911, 279, 21921, 13, 13824, 11, 7196, 279, 330, 66, 75512, 1, 1588, 374, 279, 5819, 4982, 13, 13824, 11, 902, 11, 279, 19309, 702, 264, 21921, 3685, 30, 13824, 11, 902, 11, 279, 1887, 21921, 374, 279, 4158, 61607, 13, 13824, 11, 304, 279, 2168, 1380, 279, 6249, 4933, 279, 19309, 323, 21921, 11, 279, 1909, 315, 279, 21921, 2293, 11489, 11, 902, 11, 7196, 279, 17846, 30, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 2727, 330, 65, 77832, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 3263, 2055, 220, 16, 16, 297, 62410, 1035, 387, 279, 8416, 2115, 3082, 8674, 311, 279, 19309, 13, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 279, 19309, 3082, 13, 2160, 429, 949, 315, 279, 21921, 6505, 30, 13824, 11, 902, 11, 7196, 279, 21921, 374, 279, 4158, 61607, 1212, 279, 19309, 13, 13824, 11, 902, 11, 279, 1909, 315, 429, 21921, 2293, 11489, 11, 902, 11, 304, 1045, 5335, 11, 1052, 594, 264, 17846, 25911, 476, 21810, 13, 13824, 11, 902, 11, 1077, 594, 1779, 279, 1156, 2421, 5335, 13, 576, 17196, 45766, 374, 33866, 11, 1221, 279, 19309, 3082, 13, 13824, 11, 279, 4396, 4226, 25, 3330, 518, 279, 5335, 11, 279, 1633, 389, 1909, 315, 279, 14852, 21921, 320, 1782, 4158, 26482, 4982, 8, 518, 220, 16, 16, 297, 62410, 374, 264, 17846, 13, 13824, 11, 902, 11, 3783, 21677, 17846, 374, 389, 279, 7002, 3403, 279, 19309, 13, 13824, 11, 7196, 358, 5786, 878, 13, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 594, 3405, 25, 330, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 7521, 2055, 1156, 11, 10542, 279, 21921, 13, 576, 4158, 26482, 4982, 3143, 220, 16, 16, 297, 62410, 320, 3481, 7950, 568, 3555, 594, 389, 1909, 30, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 429, 3082, 13, 13824, 11, 9834, 0, 758, 279, 21844, 476, 21810, 11, 1052, 594, 264, 17846, 13, 2055, 279, 4226, 374, 264, 17846, 4192, 1249, 8253, 1128, 374, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 527, 20666, 11, 23643, 279, 27979, 6789, 323, 5424, 304, 279, 5335, 25, 18611, 16, 13, 95210, 279, 1036, 65, 77832, 21921, 854, 304, 3405, 25, 362, 4158, 11, 1378, 481, 26482, 5819, 4982, 374, 9434, 3143, 279, 1909, 481, 2115, 320, 16, 16, 297, 527, 20666, 8, 3082, 8674, 311, 1008, 37664, 1075, 279, 19309, 323, 17196, 13, 2303, 17, 13, 1374, 19991, 6171, 3403, 419, 21921, 25, 758, 279, 5335, 11, 264, 3070, 73225, 334, 374, 21810, 389, 279, 7002, 3403, 419, 21921, 3082, 13, 79226, 54216, 151645, 198]
labels:

Got it, let's try to figure out what's on top of the bathroom cabinet at 11 o'clock. First, let's visualize the bathroom setup. The cabinet is likely a white, two-drawer unit. At 11 o'clock (top-left area), we need to check that spot. Looking at the images, especially the one showing the cabinet area, there's a mirror above the sink, but wait, no—wait, the question is about the cabinet. Wait, maybe the "cabinet" here is the storage unit. Wait, no, the sink has a cabinet below? Wait, no, the main cabinet is the white drawers. Wait, in the image where the camera shows the sink and cabinet, the top of the cabinet—wait, no, maybe the mirror? Wait, no, let's check again. Wait, the user says "bathroom cabinet that is on my 11 o'clock". So 11 o'clock would be the upper left area relative to the sink. Looking at the images, there's a mirror mounted on the wall above the sink area. Is that part of the cabinet setup? Wait, no, maybe the cabinet is the white drawers under the sink. Wait, no, the top of that cabinet—wait, no, in some images, there's a mirror reflected or mounted. Wait, no, let's check the first few images. The shower curtain is colorful, then the sink area. Wait, the correct answer: looking at the images, the object on top of the bathroom cabinet (the white drawer unit) at 11 o'clock is a mirror. Wait, no, wait—the mirror is on the wall above the sink. Wait, maybe I misread. Wait, no, let's check again. Wait, the user's question: "What is on top of the bathroom cabinet that is on my 11 o'clock?" So first, identify the cabinet. The white drawer unit near 11 o'clock (top-left). What's on top? Looking at the images, there's a mirror mounted on the wall above that area. Wait, yes! In the reflection or mounted, there's a mirror. So the answer is a mirror.


To determine what is on top of the bathroom cabinet at 11 o’clock, analyze the spatial layout and elements in the images:  

1. Locate the “bathroom cabinet” in question: A white, two - drawer storage unit is visible near the top - left (11 o’clock) area relative to other fixtures like the sink and shower.  
2. Examine objects above this cabinet: In the images, a **mirror** is mounted on the wall above this cabinet area.  


Mirror<|im_end|>

[INFO|hub.py:421] 2026-01-02 05:37:32,836 >> Offline mode: forcing local_files_only=True
[INFO|configuration_utils.py:765] 2026-01-02 05:37:32,840 >> loading configuration file config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
[INFO|configuration_utils.py:839] 2026-01-02 05:37:32,846 >> Model config Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 128000,
  "max_window_layers": 28,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "_name_or_path": "Qwen/Qwen2.5-VL-7B-Instruct",
    "architectures": [
      "Qwen2_5_VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "dtype": "bfloat16",
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "layer_types": [
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention"
    ],
    "max_position_embeddings": 128000,
    "max_window_layers": 28,
    "model_type": "qwen2_5_vl_text",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "use_cache": true,
    "use_sliding_window": false,
    "vision_token_id": 151654,
    "vocab_size": 152064
  },
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 3584,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

[WARNING|2026-01-02 05:37:32] llamafactory.model.model_utils.attention:148 >> FlashAttention-2 is not installed.
[INFO|2026-01-02 05:37:32] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
[INFO|2026-01-02 05:37:33] llamafactory.model.model_utils.liger_kernel:143 >> Liger kernel has been applied to the model.
[INFO|hub.py:421] 2026-01-02 05:37:33,502 >> Offline mode: forcing local_files_only=True
[WARNING|logging.py:328] 2026-01-02 05:37:33,503 >> `torch_dtype` is deprecated! Use `dtype` instead!
[INFO|hub.py:421] 2026-01-02 05:37:33,503 >> Offline mode: forcing local_files_only=True
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO|modeling_utils.py:4837] 2026-01-02 05:37:33,504 >> Offline mode: forcing local_files_only=True
[INFO|modeling_utils.py:1172] 2026-01-02 05:37:33,508 >> loading weights file model.safetensors from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/model.safetensors.index.json
[INFO|modeling_utils.py:2341] 2026-01-02 05:37:33,517 >> Instantiating Qwen2_5_VLForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:986] 2026-01-02 05:37:33,519 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

[INFO|modeling_utils.py:2341] 2026-01-02 05:37:33,520 >> Instantiating Qwen2_5_VisionTransformerPretrainedModel model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:2341] 2026-01-02 05:37:33,610 >> Instantiating Qwen2_5_VLTextModel model under default dtype torch.bfloat16.
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:20<01:22, 20.56s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:20<01:22, 20.56s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:20<01:22, 20.56s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:20<01:22, 20.58s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:20<01:22, 20.56s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:20<01:22, 20.57s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:20<01:22, 20.57s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:20<01:22, 20.57s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:41<01:01, 20.50s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:41<01:01, 20.50s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:41<01:01, 20.50s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:41<01:01, 20.51s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:41<01:01, 20.51s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:41<01:01, 20.51s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:41<01:01, 20.51s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:41<01:01, 20.51s/it]Loading checkpoint shards:  60%|██████    | 3/5 [01:01<00:40, 20.47s/it]Loading checkpoint shards:  60%|██████    | 3/5 [01:01<00:40, 20.47s/it]Loading checkpoint shards:  60%|██████    | 3/5 [01:01<00:40, 20.47s/it]Loading checkpoint shards:  60%|██████    | 3/5 [01:01<00:40, 20.47s/it]Loading checkpoint shards:  60%|██████    | 3/5 [01:01<00:40, 20.47s/it]Loading checkpoint shards:  60%|██████    | 3/5 [01:01<00:40, 20.47s/it]Loading checkpoint shards:  60%|██████    | 3/5 [01:01<00:40, 20.47s/it]Loading checkpoint shards:  60%|██████    | 3/5 [01:01<00:40, 20.47s/it]Loading checkpoint shards:  80%|████████  | 4/5 [01:22<00:20, 20.54s/it]Loading checkpoint shards:  80%|████████  | 4/5 [01:22<00:20, 20.54s/it]Loading checkpoint shards:  80%|████████  | 4/5 [01:22<00:20, 20.54s/it]Loading checkpoint shards:  80%|████████  | 4/5 [01:22<00:20, 20.54s/it]Loading checkpoint shards:  80%|████████  | 4/5 [01:22<00:20, 20.54s/it]Loading checkpoint shards:  80%|████████  | 4/5 [01:22<00:20, 20.55s/it]Loading checkpoint shards:  80%|████████  | 4/5 [01:22<00:20, 20.54s/it]Loading checkpoint shards:  80%|████████  | 4/5 [01:22<00:20, 20.55s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:27<00:00, 15.06s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:27<00:00, 17.49s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [01:27<00:00, 15.06s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:27<00:00, 17.49s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [01:27<00:00, 15.06s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:27<00:00, 17.49s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [01:27<00:00, 15.06s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:27<00:00, 15.06s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:27<00:00, 17.49s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [01:27<00:00, 17.49s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [01:27<00:00, 15.06s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:27<00:00, 17.49s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [01:27<00:00, 15.06s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:27<00:00, 17.49s/it]
[INFO|configuration_utils.py:941] 2026-01-02 05:39:01,400 >> loading configuration file generation_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/generation_config.json
[INFO|configuration_utils.py:986] 2026-01-02 05:39:01,401 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 1e-06
}

[INFO|dynamic_module_utils.py:423] 2026-01-02 05:39:01,402 >> Could not locate the custom_generate/generate.py inside Qwen/Qwen2.5-VL-7B-Instruct.
Loading checkpoint shards: 100%|██████████| 5/5 [01:27<00:00, 15.06s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:27<00:00, 17.50s/it]
[INFO|2026-01-02 05:39:01] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2026-01-02 05:39:01] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2026-01-02 05:39:01] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.
[INFO|2026-01-02 05:39:01] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA
[INFO|2026-01-02 05:39:02] llamafactory.model.adapter:143 >> Loaded adapter(s): /project/aip-wangcs/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu/checkpoint-233/
[INFO|2026-01-02 05:39:02] llamafactory.model.loader:143 >> trainable params: 20,185,088 || all params: 8,312,351,744 || trainable%: 0.2428
name: base_model.model.model.visual.patch_embed.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.ln_q.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.0.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.0.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.embed_tokens.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.lm_head.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
[INFO|trainer.py:749] 2026-01-02 05:39:02,486 >> Using auto half precision backend
[WARNING|2026-01-02 05:39:02] llamafactory.train.callbacks:154 >> Previous trainer log in this folder will be deleted.
[WARNING|trainer.py:982] 2026-01-02 05:39:02,489 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
[DEBUG|trainer.py:2373] 2026-01-02 05:39:03,053 >> Currently training with a batch size of: 2
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 8. Using DeepSpeed's value.
[2026-01-02 05:39:03,194] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown
[2026-01-02 05:39:03,194] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Currently training with a batch size of: 2
[2026-01-02 05:39:03,963] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Currently training with a batch size of: 2
Currently training with a batch size of: 2
Currently training with a batch size of: 2
Currently training with a batch size of: 2
Currently training with a batch size of: 2
Currently training with a batch size of: 2
[2026-01-02 05:39:04,466] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2026-01-02 05:39:04,467] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2026-01-02 05:39:04,496] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2026-01-02 05:39:04,505] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2026-01-02 05:39:04,508] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2026-01-02 05:39:04,545] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2026-01-02 05:39:05,978] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2026-01-02 05:39:05,986] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2026-01-02 05:39:05,986] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2026-01-02 05:39:06,032] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2026-01-02 05:39:06,032] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2026-01-02 05:39:06,032] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2026-01-02 05:39:06,032] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2026-01-02 05:39:06,033] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2026-01-02 05:39:06,033] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2026-01-02 05:39:06,033] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: True
***** Running training *****
  Num examples = 29,742
  Num Epochs = 3
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Gradient Accumulation steps = 8
  Total optimization steps = 699
  Number of trainable parameters = 20,185,088
***** Running training *****
  Num examples = 29,742
  Num Epochs = 3
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Gradient Accumulation steps = 8
  Total optimization steps = 699
  Number of trainable parameters = 20,185,088
***** Running training *****
  Num examples = 29,742
  Num Epochs = 3
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Gradient Accumulation steps = 8
  Total optimization steps = 699
  Number of trainable parameters = 20,185,088
***** Running training *****
  Num examples = 29,742
  Num Epochs = 3
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Gradient Accumulation steps = 8
  Total optimization steps = 699
***** Running training *****
  Num examples = 29,742
  Num Epochs = 3
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Gradient Accumulation steps = 8
  Total optimization steps = 699
  Number of trainable parameters = 20,185,088
  Number of trainable parameters = 20,185,088
***** Running training *****
  Num examples = 29,742
  Num Epochs = 3
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Gradient Accumulation steps = 8
  Total optimization steps = 699
***** Running training *****
  Num examples = 29,742
  Num Epochs = 3
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Gradient Accumulation steps = 8
  Total optimization steps = 699
  Number of trainable parameters = 20,185,088
  Number of trainable parameters = 20,185,088
[2026-01-02 05:39:06,350] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2026-01-02 05:39:06,351] [INFO] [utils.py:782:see_memory_usage] MA 15.49 GB         Max_MA 15.5 GB         CA 15.51 GB         Max_CA 16 GB 
[2026-01-02 05:39:06,351] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 24.35 GB, percent = 1.2%
[2026-01-02 05:39:06,562] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2026-01-02 05:39:06,562] [INFO] [utils.py:782:see_memory_usage] MA 15.49 GB         Max_MA 15.5 GB         CA 15.51 GB         Max_CA 16 GB 
[2026-01-02 05:39:06,563] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 24.34 GB, percent = 1.2%
[2026-01-02 05:39:06,563] [INFO] [stage_1_and_2.py:550:__init__] optimizer state initialized
[2026-01-02 05:39:06,769] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2026-01-02 05:39:06,770] [INFO] [utils.py:782:see_memory_usage] MA 15.49 GB         Max_MA 15.49 GB         CA 15.51 GB         Max_CA 16 GB 
[2026-01-02 05:39:06,770] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 24.33 GB, percent = 1.2%
[2026-01-02 05:39:06,772] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2026-01-02 05:39:06,772] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2026-01-02 05:39:06,772] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2026-01-02 05:39:06,772] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2026-01-02 05:39:06,778] [INFO] [config.py:1001:print] DeepSpeedEngine configuration:
[2026-01-02 05:39:06,778] [INFO] [config.py:1005:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2026-01-02 05:39:06,778] [INFO] [config.py:1005:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2026-01-02 05:39:06,778] [INFO] [config.py:1005:print]   amp_enabled .................. False
[2026-01-02 05:39:06,778] [INFO] [config.py:1005:print]   amp_params ................... False
[2026-01-02 05:39:06,779] [INFO] [config.py:1005:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2026-01-02 05:39:06,779] [INFO] [config.py:1005:print]   bfloat16_enabled ............. True
[2026-01-02 05:39:06,779] [INFO] [config.py:1005:print]   bfloat16_immediate_grad_update  False
[2026-01-02 05:39:06,779] [INFO] [config.py:1005:print]   checkpoint_parallel_write_pipeline  False
[2026-01-02 05:39:06,779] [INFO] [config.py:1005:print]   checkpoint_tag_validation_enabled  True
[2026-01-02 05:39:06,779] [INFO] [config.py:1005:print]   checkpoint_tag_validation_fail  False
[2026-01-02 05:39:06,779] [INFO] [config.py:1005:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x155245ef1df0>
[2026-01-02 05:39:06,779] [INFO] [config.py:1005:print]   communication_data_type ...... None
[2026-01-02 05:39:06,779] [INFO] [config.py:1005:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2026-01-02 05:39:06,779] [INFO] [config.py:1005:print]   curriculum_enabled_legacy .... False
[2026-01-02 05:39:06,779] [INFO] [config.py:1005:print]   curriculum_params_legacy ..... False
[2026-01-02 05:39:06,779] [INFO] [config.py:1005:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2026-01-02 05:39:06,779] [INFO] [config.py:1005:print]   data_efficiency_enabled ...... False
[2026-01-02 05:39:06,779] [INFO] [config.py:1005:print]   dataloader_drop_last ......... False
[2026-01-02 05:39:06,779] [INFO] [config.py:1005:print]   disable_allgather ............ False
[2026-01-02 05:39:06,779] [INFO] [config.py:1005:print]   dump_state ................... False
[2026-01-02 05:39:06,779] [INFO] [config.py:1005:print]   dynamic_loss_scale_args ...... None
[2026-01-02 05:39:06,779] [INFO] [config.py:1005:print]   eigenvalue_enabled ........... False
[2026-01-02 05:39:06,779] [INFO] [config.py:1005:print]   eigenvalue_gas_boundary_resolution  1
[2026-01-02 05:39:06,779] [INFO] [config.py:1005:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2026-01-02 05:39:06,779] [INFO] [config.py:1005:print]   eigenvalue_layer_num ......... 0
[2026-01-02 05:39:06,779] [INFO] [config.py:1005:print]   eigenvalue_max_iter .......... 100
[2026-01-02 05:39:06,779] [INFO] [config.py:1005:print]   eigenvalue_stability ......... 1e-06
[2026-01-02 05:39:06,779] [INFO] [config.py:1005:print]   eigenvalue_tol ............... 0.01
[2026-01-02 05:39:06,780] [INFO] [config.py:1005:print]   eigenvalue_verbose ........... False
[2026-01-02 05:39:06,780] [INFO] [config.py:1005:print]   elasticity_enabled ........... False
[2026-01-02 05:39:06,780] [INFO] [config.py:1005:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2026-01-02 05:39:06,780] [INFO] [config.py:1005:print]   fp16_auto_cast ............... None
[2026-01-02 05:39:06,780] [INFO] [config.py:1005:print]   fp16_enabled ................. False
[2026-01-02 05:39:06,780] [INFO] [config.py:1005:print]   fp16_master_weights_and_gradients  False
[2026-01-02 05:39:06,780] [INFO] [config.py:1005:print]   global_rank .................. 0
[2026-01-02 05:39:06,780] [INFO] [config.py:1005:print]   grad_accum_dtype ............. None
[2026-01-02 05:39:06,780] [INFO] [config.py:1005:print]   gradient_accumulation_steps .. 8
[2026-01-02 05:39:06,780] [INFO] [config.py:1005:print]   gradient_clipping ............ 1.0
[2026-01-02 05:39:06,780] [INFO] [config.py:1005:print]   gradient_predivide_factor .... 1.0
[2026-01-02 05:39:06,780] [INFO] [config.py:1005:print]   graph_harvesting ............. False
[2026-01-02 05:39:06,780] [INFO] [config.py:1005:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2026-01-02 05:39:06,780] [INFO] [config.py:1005:print]   initial_dynamic_scale ........ 1
[2026-01-02 05:39:06,780] [INFO] [config.py:1005:print]   load_universal_checkpoint .... False
[2026-01-02 05:39:06,780] [INFO] [config.py:1005:print]   loss_scale ................... 1.0
[2026-01-02 05:39:06,780] [INFO] [config.py:1005:print]   memory_breakdown ............. False
[2026-01-02 05:39:06,780] [INFO] [config.py:1005:print]   mics_hierarchial_params_gather  False
[2026-01-02 05:39:06,780] [INFO] [config.py:1005:print]   mics_shard_size .............. -1
[2026-01-02 05:39:06,780] [INFO] [config.py:1005:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2026-01-02 05:39:06,780] [INFO] [config.py:1005:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2026-01-02 05:39:06,780] [INFO] [config.py:1005:print]   optimizer_legacy_fusion ...... False
[2026-01-02 05:39:06,780] [INFO] [config.py:1005:print]   optimizer_name ............... None
[2026-01-02 05:39:06,780] [INFO] [config.py:1005:print]   optimizer_params ............. None
[2026-01-02 05:39:06,780] [INFO] [config.py:1005:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2026-01-02 05:39:06,780] [INFO] [config.py:1005:print]   pld_enabled .................. False
[2026-01-02 05:39:06,780] [INFO] [config.py:1005:print]   pld_params ................... False
[2026-01-02 05:39:06,781] [INFO] [config.py:1005:print]   prescale_gradients ........... False
[2026-01-02 05:39:06,781] [INFO] [config.py:1005:print]   scheduler_name ............... None
[2026-01-02 05:39:06,781] [INFO] [config.py:1005:print]   scheduler_params ............. None
[2026-01-02 05:39:06,781] [INFO] [config.py:1005:print]   seq_parallel_communication_data_type  torch.float32
[2026-01-02 05:39:06,781] [INFO] [config.py:1005:print]   sparse_attention ............. None
[2026-01-02 05:39:06,781] [INFO] [config.py:1005:print]   sparse_gradients_enabled ..... False
[2026-01-02 05:39:06,781] [INFO] [config.py:1005:print]   steps_per_print .............. inf
[2026-01-02 05:39:06,781] [INFO] [config.py:1005:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2026-01-02 05:39:06,781] [INFO] [config.py:1005:print]   timers_config ................ enabled=True synchronized=True
[2026-01-02 05:39:06,781] [INFO] [config.py:1005:print]   train_batch_size ............. 128
[2026-01-02 05:39:06,781] [INFO] [config.py:1005:print]   train_micro_batch_size_per_gpu  2
[2026-01-02 05:39:06,781] [INFO] [config.py:1005:print]   use_data_before_expert_parallel_  False
[2026-01-02 05:39:06,781] [INFO] [config.py:1005:print]   use_node_local_storage ....... False
[2026-01-02 05:39:06,781] [INFO] [config.py:1005:print]   wall_clock_breakdown ......... False
[2026-01-02 05:39:06,781] [INFO] [config.py:1005:print]   weight_quantization_config ... None
[2026-01-02 05:39:06,781] [INFO] [config.py:1005:print]   world_size ................... 8
[2026-01-02 05:39:06,781] [INFO] [config.py:1005:print]   zero_allow_untested_optimizer  True
[2026-01-02 05:39:06,781] [INFO] [config.py:1005:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=True zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2026-01-02 05:39:06,781] [INFO] [config.py:1005:print]   zero_enabled ................. True
[2026-01-02 05:39:06,781] [INFO] [config.py:1005:print]   zero_force_ds_cpu_optimizer .. True
[2026-01-02 05:39:06,781] [INFO] [config.py:1005:print]   zero_optimization_stage ...... 2
[2026-01-02 05:39:06,781] [INFO] [config.py:991:print_user_config]   json = {
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 2, 
    "gradient_accumulation_steps": 8, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": false, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true, 
        "round_robin_gradients": true
    }, 
    "steps_per_print": inf
}
[INFO|trainer.py:2519] 2026-01-02 05:39:06,782 >> ***** Running training *****
[INFO|trainer.py:2520] 2026-01-02 05:39:06,782 >>   Num examples = 29,742
[INFO|trainer.py:2521] 2026-01-02 05:39:06,782 >>   Num Epochs = 3
[INFO|trainer.py:2522] 2026-01-02 05:39:06,782 >>   Instantaneous batch size per device = 2
[INFO|trainer.py:2525] 2026-01-02 05:39:06,782 >>   Total train batch size (w. parallel, distributed & accumulation) = 128
[INFO|trainer.py:2526] 2026-01-02 05:39:06,783 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2527] 2026-01-02 05:39:06,783 >>   Total optimization steps = 699
[INFO|trainer.py:2528] 2026-01-02 05:39:06,787 >>   Number of trainable parameters = 20,185,088
[INFO|integration_utils.py:867] 2026-01-02 05:39:06,791 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: WARNING Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to https://wandb.me/wandb-init.
wandb: Tracking run with wandb version 0.21.2
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /project/aip-wangcs/indrisch/LLaMA-Factory/wandb/wandb/offline-run-20260102_053906-qbwgrwts
  0%|          | 0/699 [00:00<?, ?it/s]  0%|          | 1/699 [03:01<35:15:38, 181.86s/it]  0%|          | 2/699 [07:05<42:13:36, 218.10s/it]  0%|          | 3/699 [09:44<36:59:56, 191.37s/it]  1%|          | 4/699 [12:18<34:02:42, 176.35s/it]  1%|          | 5/699 [15:15<34:05:27, 176.84s/it]  1%|          | 6/699 [17:48<32:25:33, 168.45s/it]  1%|          | 7/699 [20:31<32:05:11, 166.92s/it]  1%|          | 8/699 [23:29<32:41:42, 170.34s/it]  1%|▏         | 9/699 [26:13<32:15:02, 168.26s/it]  1%|▏         | 10/699 [29:50<35:05:12, 183.33s/it]                                                    {'loss': 0.7615, 'grad_norm': 0.30822131037712097, 'learning_rate': 1.2857142857142857e-05, 'epoch': 0.04}
  1%|▏         | 10/699 [29:50<35:05:12, 183.33s/it]  2%|▏         | 11/699 [32:40<34:14:43, 179.19s/it]  2%|▏         | 12/699 [35:29<33:37:42, 176.22s/it]  2%|▏         | 13/699 [38:47<34:49:46, 182.78s/it]  2%|▏         | 14/699 [41:05<32:11:48, 169.21s/it]  2%|▏         | 15/699 [44:07<32:52:49, 173.05s/it]  2%|▏         | 16/699 [47:39<35:04:34, 184.88s/it]  2%|▏         | 17/699 [50:02<32:38:10, 172.27s/it]  3%|▎         | 18/699 [53:01<32:58:16, 174.30s/it]  3%|▎         | 19/699 [55:48<32:30:49, 172.13s/it]  3%|▎         | 20/699 [58:53<33:12:04, 176.03s/it]                                                    {'loss': 0.7456, 'grad_norm': 0.2989386320114136, 'learning_rate': 2.714285714285714e-05, 'epoch': 0.09}
  3%|▎         | 20/699 [58:53<33:12:04, 176.03s/it]  3%|▎         | 21/699 [1:01:20<31:31:10, 167.36s/it]  3%|▎         | 22/699 [1:04:37<33:09:06, 176.29s/it]  3%|▎         | 23/699 [1:07:48<33:53:02, 180.45s/it]  3%|▎         | 24/699 [1:11:02<34:35:46, 184.51s/it]  4%|▎         | 25/699 [1:13:50<33:38:18, 179.67s/it]  4%|▎         | 26/699 [1:17:03<34:19:46, 183.63s/it]  4%|▍         | 27/699 [1:19:40<32:48:20, 175.74s/it]  4%|▍         | 28/699 [1:22:07<31:08:12, 167.05s/it]  4%|▍         | 29/699 [1:25:30<33:06:48, 177.92s/it]  4%|▍         | 30/699 [1:28:46<34:01:49, 183.12s/it]                                                      {'loss': 0.7464, 'grad_norm': 0.34281134605407715, 'learning_rate': 4.1428571428571437e-05, 'epoch': 0.13}
  4%|▍         | 30/699 [1:28:46<34:01:49, 183.12s/it]  4%|▍         | 31/699 [1:31:35<33:14:27, 179.14s/it]  5%|▍         | 32/699 [1:34:24<32:35:10, 175.88s/it]  5%|▍         | 33/699 [1:37:17<32:23:36, 175.10s/it]  5%|▍         | 34/699 [1:40:14<32:27:02, 175.67s/it]  5%|▌         | 35/699 [1:42:39<30:40:53, 166.35s/it]  5%|▌         | 36/699 [1:45:43<31:36:51, 171.66s/it]  5%|▌         | 37/699 [1:48:34<31:34:37, 171.72s/it]  5%|▌         | 38/699 [1:51:04<30:18:23, 165.06s/it]  6%|▌         | 39/699 [1:54:00<30:51:05, 168.28s/it]  6%|▌         | 40/699 [1:56:58<31:19:38, 171.14s/it]                                                      {'loss': 0.7453, 'grad_norm': 0.3675595223903656, 'learning_rate': 5.571428571428572e-05, 'epoch': 0.17}
  6%|▌         | 40/699 [1:56:58<31:19:38, 171.14s/it]  6%|▌         | 41/699 [2:00:20<33:00:58, 180.64s/it]  6%|▌         | 42/699 [2:03:12<32:28:47, 177.97s/it]  6%|▌         | 43/699 [2:06:09<32:22:19, 177.65s/it]  6%|▋         | 44/699 [2:09:23<33:11:51, 182.46s/it]  6%|▋         | 45/699 [2:11:43<30:51:40, 169.88s/it]  7%|▋         | 46/699 [2:14:27<30:30:31, 168.20s/it]  7%|▋         | 47/699 [2:17:33<31:23:02, 173.29s/it]  7%|▋         | 48/699 [2:21:00<33:12:24, 183.63s/it]  7%|▋         | 49/699 [2:24:16<33:48:15, 187.22s/it]  7%|▋         | 50/699 [2:27:15<33:19:37, 184.87s/it]                                                      {'loss': 0.7367, 'grad_norm': 0.37730735540390015, 'learning_rate': 7e-05, 'epoch': 0.22}
  7%|▋         | 50/699 [2:27:15<33:19:37, 184.87s/it]  7%|▋         | 51/699 [2:31:03<35:36:00, 197.78s/it]  7%|▋         | 52/699 [2:34:11<35:01:20, 194.87s/it]  8%|▊         | 53/699 [2:37:04<33:44:42, 188.05s/it]  8%|▊         | 54/699 [2:39:39<31:55:49, 178.22s/it]  8%|▊         | 55/699 [2:42:12<30:33:36, 170.83s/it]  8%|▊         | 56/699 [2:45:35<32:11:35, 180.24s/it]  8%|▊         | 57/699 [2:49:11<34:05:14, 191.14s/it]  8%|▊         | 58/699 [2:52:08<33:15:33, 186.79s/it]  8%|▊         | 59/699 [2:55:23<33:40:10, 189.39s/it]  9%|▊         | 60/699 [2:58:07<32:14:05, 181.60s/it]                                                      {'loss': 0.7519, 'grad_norm': 0.4291645884513855, 'learning_rate': 8.428571428571429e-05, 'epoch': 0.26}
  9%|▊         | 60/699 [2:58:07<32:14:05, 181.60s/it]  9%|▊         | 61/699 [3:01:57<34:45:27, 196.13s/it]  9%|▉         | 62/699 [3:04:38<32:51:06, 185.66s/it]  9%|▉         | 63/699 [3:07:14<31:15:07, 176.90s/it]  9%|▉         | 64/699 [3:10:05<30:51:26, 174.94s/it]  9%|▉         | 65/699 [3:13:59<33:54:58, 192.59s/it]  9%|▉         | 66/699 [3:16:56<33:04:49, 188.14s/it] 10%|▉         | 67/699 [3:19:19<30:39:20, 174.62s/it] 10%|▉         | 68/699 [3:22:31<31:31:13, 179.83s/it] 10%|▉         | 69/699 [3:25:29<31:20:14, 179.07s/it] 10%|█         | 70/699 [3:28:38<31:48:14, 182.03s/it]                                                      {'loss': 0.725, 'grad_norm': 0.39389392733573914, 'learning_rate': 9.857142857142858e-05, 'epoch': 0.3}
 10%|█         | 70/699 [3:28:38<31:48:14, 182.03s/it] 10%|█         | 71/699 [3:31:28<31:09:47, 178.64s/it] 10%|█         | 72/699 [3:34:02<29:49:50, 171.28s/it] 10%|█         | 73/699 [3:37:30<31:39:32, 182.06s/it] 11%|█         | 74/699 [3:39:57<29:46:43, 171.52s/it] 11%|█         | 75/699 [3:43:50<32:56:31, 190.05s/it] 11%|█         | 76/699 [3:46:31<31:22:22, 181.29s/it] 11%|█         | 77/699 [3:49:17<30:31:53, 176.71s/it] 11%|█         | 78/699 [3:52:05<30:02:52, 174.19s/it] 11%|█▏        | 79/699 [3:54:38<28:53:12, 167.73s/it] 11%|█▏        | 80/699 [3:57:20<28:33:49, 166.12s/it]                                                      {'loss': 0.7446, 'grad_norm': 0.44323211908340454, 'learning_rate': 9.994949314197985e-05, 'epoch': 0.34}
 11%|█▏        | 80/699 [3:57:20<28:33:49, 166.12s/it] 12%|█▏        | 81/699 [3:59:35<26:55:39, 156.86s/it] 12%|█▏        | 82/699 [4:02:10<26:46:35, 156.23s/it] 12%|█▏        | 83/699 [4:05:38<29:23:55, 171.81s/it] 12%|█▏        | 84/699 [4:08:30<29:22:15, 171.93s/it] 12%|█▏        | 85/699 [4:10:54<27:52:25, 163.43s/it] 12%|█▏        | 86/699 [4:13:54<28:41:18, 168.48s/it] 12%|█▏        | 87/699 [4:16:49<28:56:39, 170.26s/it] 13%|█▎        | 88/699 [4:19:38<28:52:14, 170.11s/it] 13%|█▎        | 89/699 [4:22:04<27:33:21, 162.63s/it] 13%|█▎        | 90/699 [4:24:53<27:50:56, 164.62s/it]                                                      {'loss': 0.733, 'grad_norm': 0.4867895245552063, 'learning_rate': 9.977503253098892e-05, 'epoch': 0.39}
 13%|█▎        | 90/699 [4:24:53<27:50:56, 164.62s/it] 13%|█▎        | 91/699 [4:27:20<26:56:00, 159.47s/it] 13%|█▎        | 92/699 [4:29:31<25:26:36, 150.90s/it] 13%|█▎        | 93/699 [4:32:00<25:17:29, 150.25s/it] 13%|█▎        | 94/699 [4:34:52<26:19:32, 156.65s/it] 14%|█▎        | 95/699 [4:38:10<28:22:21, 169.11s/it] 14%|█▎        | 96/699 [4:40:21<26:26:04, 157.82s/it] 14%|█▍        | 97/699 [4:43:03<26:34:49, 158.95s/it] 14%|█▍        | 98/699 [4:45:35<26:11:22, 156.88s/it] 14%|█▍        | 99/699 [4:49:05<28:49:52, 172.99s/it] 14%|█▍        | 100/699 [4:52:28<30:14:38, 181.77s/it]                                                       {'loss': 0.7317, 'grad_norm': 0.4429952800273895, 'learning_rate': 9.947642963836852e-05, 'epoch': 0.43}
 14%|█▍        | 100/699 [4:52:28<30:14:38, 181.77s/it] 14%|█▍        | 101/699 [4:55:07<29:04:50, 175.07s/it] 15%|█▍        | 102/699 [4:58:05<29:10:03, 175.88s/it] 15%|█▍        | 103/699 [5:01:16<29:52:46, 180.48s/it] 15%|█▍        | 104/699 [5:04:34<30:42:39, 185.81s/it] 15%|█▌        | 105/699 [5:07:34<30:22:04, 184.05s/it] 15%|█▌        | 106/699 [5:10:34<30:04:53, 182.62s/it] 15%|█▌        | 107/699 [5:12:54<27:56:56, 169.96s/it] 15%|█▌        | 108/699 [5:16:01<28:43:08, 174.94s/it] 16%|█▌        | 109/699 [5:19:28<30:15:48, 184.66s/it] 16%|█▌        | 110/699 [5:22:17<29:26:57, 179.99s/it]                                                       {'loss': 0.7362, 'grad_norm': 0.5530678629875183, 'learning_rate': 9.905442919983266e-05, 'epoch': 0.47}
 16%|█▌        | 110/699 [5:22:17<29:26:57, 179.99s/it] 16%|█▌        | 111/699 [5:26:21<32:32:45, 199.26s/it] 16%|█▌        | 112/699 [5:29:09<30:55:54, 189.70s/it] 16%|█▌        | 113/699 [5:31:52<29:35:49, 181.82s/it] 16%|█▋        | 114/699 [5:34:47<29:11:06, 179.60s/it] 16%|█▋        | 115/699 [5:36:51<26:26:33, 163.00s/it] 17%|█▋        | 116/699 [5:39:10<25:13:42, 155.79s/it] 17%|█▋        | 117/699 [5:42:33<27:29:10, 170.02s/it] 17%|█▋        | 118/699 [5:45:55<29:00:13, 179.71s/it] 17%|█▋        | 119/699 [5:49:07<29:33:26, 183.46s/it] 17%|█▋        | 120/699 [5:52:27<30:17:01, 188.29s/it]                                                       {'loss': 0.7279, 'grad_norm': 0.5109579563140869, 'learning_rate': 9.851008371288106e-05, 'epoch': 0.52}
 17%|█▋        | 120/699 [5:52:27<30:17:01, 188.29s/it] 17%|█▋        | 121/699 [5:55:10<29:00:58, 180.72s/it] 17%|█▋        | 122/699 [5:58:18<29:19:16, 182.94s/it] 18%|█▊        | 123/699 [6:01:44<30:20:41, 189.66s/it] 18%|█▊        | 124/699 [6:04:25<28:57:47, 181.34s/it] 18%|█▊        | 125/699 [6:07:37<29:24:54, 184.48s/it] 18%|█▊        | 126/699 [6:10:15<28:05:43, 176.52s/it] 18%|█▊        | 127/699 [6:12:55<27:14:30, 171.45s/it] 18%|█▊        | 128/699 [6:15:09<25:26:20, 160.39s/it] 18%|█▊        | 129/699 [6:18:14<26:31:49, 167.56s/it] 19%|█▊        | 130/699 [6:21:10<26:54:25, 170.24s/it]                                                       {'loss': 0.7308, 'grad_norm': 0.4139646589756012, 'learning_rate': 9.784475081179962e-05, 'epoch': 0.56}
 19%|█▊        | 130/699 [6:21:10<26:54:25, 170.24s/it] 19%|█▊        | 131/699 [6:24:11<27:20:49, 173.33s/it] 19%|█▉        | 132/699 [6:26:39<26:07:09, 165.84s/it] 19%|█▉        | 133/699 [6:29:17<25:40:37, 163.32s/it] 19%|█▉        | 134/699 [6:32:15<26:19:44, 167.76s/it] 19%|█▉        | 135/699 [6:34:35<24:59:43, 159.55s/it] 19%|█▉        | 136/699 [6:37:54<26:49:03, 171.48s/it] 20%|█▉        | 137/699 [6:41:06<27:41:21, 177.37s/it] 20%|█▉        | 138/699 [6:42:57<24:34:19, 157.68s/it] 20%|█▉        | 139/699 [6:45:54<25:24:36, 163.35s/it] 20%|██        | 140/699 [6:48:16<24:23:02, 157.04s/it]                                                       {'loss': 0.7293, 'grad_norm': 0.5472699403762817, 'learning_rate': 9.706008988162907e-05, 'epoch': 0.6}
 20%|██        | 140/699 [6:48:16<24:23:02, 157.04s/it] 20%|██        | 141/699 [6:51:20<25:36:34, 165.22s/it] 20%|██        | 142/699 [6:54:25<26:26:39, 170.91s/it] 20%|██        | 143/699 [6:57:08<26:01:43, 168.53s/it] 21%|██        | 144/699 [7:00:08<26:31:25, 172.05s/it] 21%|██        | 145/699 [7:03:05<26:43:46, 173.69s/it] 21%|██        | 146/699 [7:05:48<26:09:49, 170.32s/it] 21%|██        | 147/699 [7:08:58<27:02:17, 176.34s/it] 21%|██        | 148/699 [7:12:06<27:30:22, 179.71s/it] 21%|██▏       | 149/699 [7:15:11<27:41:51, 181.29s/it] 21%|██▏       | 150/699 [7:18:08<27:27:59, 180.11s/it]                                                       {'loss': 0.7209, 'grad_norm': 0.4442998766899109, 'learning_rate': 9.615805791954695e-05, 'epoch': 0.65}
 21%|██▏       | 150/699 [7:18:08<27:27:59, 180.11s/it] 22%|██▏       | 151/699 [7:20:42<26:12:49, 172.21s/it] 22%|██▏       | 152/699 [7:23:48<26:47:20, 176.31s/it] 22%|██▏       | 153/699 [7:27:23<28:31:51, 188.12s/it] 22%|██▏       | 154/699 [7:29:52<26:41:24, 176.30s/it] 22%|██▏       | 155/699 [7:33:04<27:20:37, 180.95s/it] 22%|██▏       | 156/699 [7:36:41<28:54:48, 191.69s/it] 22%|██▏       | 157/699 [7:39:46<28:34:59, 189.85s/it] 23%|██▎       | 158/699 [7:42:44<27:58:31, 186.16s/it] 23%|██▎       | 159/699 [7:45:16<26:24:31, 176.06s/it] 23%|██▎       | 160/699 [7:48:18<26:37:53, 177.87s/it]                                                       {'loss': 0.7282, 'grad_norm': 0.45864149928092957, 'learning_rate': 9.514090465398502e-05, 'epoch': 0.69}
 23%|██▎       | 160/699 [7:48:18<26:37:53, 177.87s/it] 23%|██▎       | 161/699 [7:50:57<25:43:29, 172.14s/it] 23%|██▎       | 162/699 [7:53:21<24:23:37, 163.53s/it] 23%|██▎       | 163/699 [7:56:21<25:05:25, 168.52s/it] 23%|██▎       | 164/699 [7:59:21<25:34:42, 172.12s/it] 24%|██▎       | 165/699 [8:02:15<25:37:03, 172.70s/it] 24%|██▎       | 166/699 [8:05:21<26:07:44, 176.48s/it] 24%|██▍       | 167/699 [8:08:06<25:34:43, 173.09s/it] 24%|██▍       | 168/699 [8:10:20<23:49:04, 161.48s/it] 24%|██▍       | 169/699 [8:13:29<24:57:21, 169.51s/it] 24%|██▍       | 170/699 [8:16:05<24:19:57, 165.59s/it]                                                       {'loss': 0.7159, 'grad_norm': 0.49604305624961853, 'learning_rate': 9.401116693365504e-05, 'epoch': 0.73}
 24%|██▍       | 170/699 [8:16:05<24:19:57, 165.59s/it] 24%|██▍       | 171/699 [8:19:08<25:02:28, 170.74s/it] 25%|██▍       | 172/699 [8:22:36<26:38:58, 182.05s/it] 25%|██▍       | 173/699 [8:25:33<26:21:41, 180.42s/it] 25%|██▍       | 174/699 [8:28:25<25:57:34, 178.01s/it] 25%|██▌       | 175/699 [8:31:04<25:03:47, 172.19s/it] 25%|██▌       | 176/699 [8:33:52<24:49:58, 170.93s/it] 25%|██▌       | 177/699 [8:36:48<25:00:28, 172.47s/it] 25%|██▌       | 178/699 [8:40:08<26:11:01, 180.92s/it] 26%|██▌       | 179/699 [8:43:06<25:58:13, 179.79s/it] 26%|██▌       | 180/699 [8:45:46<25:05:00, 173.99s/it]                                                       {'loss': 0.7195, 'grad_norm': 0.471808522939682, 'learning_rate': 9.27716624004773e-05, 'epoch': 0.77}
 26%|██▌       | 180/699 [8:45:46<25:05:00, 173.99s/it] 26%|██▌       | 181/699 [8:48:38<24:55:33, 173.23s/it] 26%|██▌       | 182/699 [8:50:44<22:52:40, 159.31s/it] 26%|██▌       | 183/699 [8:53:40<23:31:15, 164.10s/it] 26%|██▋       | 184/699 [8:56:26<23:34:34, 164.81s/it] 26%|██▋       | 185/699 [8:58:50<22:38:54, 158.63s/it] 27%|██▋       | 186/699 [9:01:40<23:05:45, 162.08s/it] 27%|██▋       | 187/699 [9:04:41<23:51:14, 167.72s/it] 27%|██▋       | 188/699 [9:07:54<24:53:11, 175.33s/it] 27%|██▋       | 189/699 [9:10:37<24:19:09, 171.67s/it] 27%|██▋       | 190/699 [9:13:01<23:04:29, 163.20s/it]                                                       {'loss': 0.7345, 'grad_norm': 0.4587171673774719, 'learning_rate': 9.142548246219212e-05, 'epoch': 0.82}
 27%|██▋       | 190/699 [9:13:01<23:04:29, 163.20s/it] 27%|██▋       | 191/699 [9:16:21<24:35:22, 174.26s/it] 27%|██▋       | 192/699 [9:18:56<23:42:31, 168.35s/it] 28%|██▊       | 193/699 [9:21:57<24:13:29, 172.35s/it] 28%|██▊       | 194/699 [9:24:50<24:11:13, 172.42s/it] 28%|██▊       | 195/699 [9:27:41<24:05:34, 172.09s/it] 28%|██▊       | 196/699 [9:30:52<24:49:36, 177.69s/it] 28%|██▊       | 197/699 [9:33:42<24:27:13, 175.37s/it] 28%|██▊       | 198/699 [9:36:31<24:09:46, 173.63s/it] 28%|██▊       | 199/699 [9:39:30<24:20:15, 175.23s/it] 29%|██▊       | 200/699 [9:42:38<24:47:20, 178.84s/it]                                                       {'loss': 0.7134, 'grad_norm': 0.4670424461364746, 'learning_rate': 8.997598458218068e-05, 'epoch': 0.86}
 29%|██▊       | 200/699 [9:42:38<24:47:20, 178.84s/it]
***** Running Evaluation *****
  Num examples = 3305

***** Running Evaluation *****
  Batch size = 1
  Num examples = 3305

***** Running Evaluation *****
  Batch size = 1
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1
[INFO|trainer.py:4643] 2026-01-02 15:21:46,748 >> 
***** Running Evaluation *****

***** Running Evaluation *****
[INFO|trainer.py:4645] 2026-01-02 15:21:46,748 >>   Num examples = 3305
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****
[INFO|trainer.py:4648] 2026-01-02 15:21:46,748 >>   Batch size = 1
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

  0%|          | 0/414 [00:00<?, ?it/s][A
  0%|          | 2/414 [00:03<12:42,  1.85s/it][A
  1%|          | 3/414 [00:07<19:00,  2.78s/it][A
  1%|          | 4/414 [00:12<24:24,  3.57s/it][A
  1%|          | 5/414 [00:21<37:14,  5.46s/it][A
  1%|▏         | 6/414 [00:25<33:08,  4.87s/it][A
  2%|▏         | 7/414 [00:28<29:38,  4.37s/it][A
  2%|▏         | 8/414 [00:32<29:12,  4.32s/it][A
  2%|▏         | 9/414 [00:36<27:16,  4.04s/it][A
  2%|▏         | 10/414 [00:42<31:41,  4.71s/it][A
  3%|▎         | 11/414 [00:48<33:24,  4.97s/it][A
  3%|▎         | 12/414 [00:51<31:03,  4.64s/it][A
  3%|▎         | 13/414 [00:58<35:29,  5.31s/it][A
  3%|▎         | 14/414 [01:03<34:49,  5.22s/it][A
  4%|▎         | 15/414 [01:09<36:07,  5.43s/it][A
  4%|▍         | 16/414 [01:14<35:31,  5.36s/it][A
  4%|▍         | 17/414 [01:22<39:27,  5.96s/it][A
  4%|▍         | 18/414 [01:26<35:01,  5.31s/it][A
  5%|▍         | 19/414 [01:32<37:41,  5.73s/it][A
  5%|▍         | 20/414 [01:39<40:17,  6.14s/it][A
  5%|▌         | 21/414 [01:44<36:16,  5.54s/it][A
  5%|▌         | 22/414 [01:47<32:26,  4.97s/it][A
  6%|▌         | 23/414 [01:52<32:44,  5.02s/it][A
  6%|▌         | 24/414 [01:58<34:32,  5.31s/it][A
  6%|▌         | 25/414 [02:02<31:37,  4.88s/it][A
  6%|▋         | 26/414 [02:09<35:45,  5.53s/it][A
  7%|▋         | 27/414 [02:12<31:12,  4.84s/it][A
  7%|▋         | 28/414 [02:16<29:25,  4.58s/it][A
  7%|▋         | 29/414 [02:21<28:50,  4.50s/it][A
  7%|▋         | 30/414 [02:26<29:30,  4.61s/it][A
  7%|▋         | 31/414 [02:31<31:40,  4.96s/it][A
  8%|▊         | 32/414 [02:35<28:11,  4.43s/it][A
  8%|▊         | 33/414 [02:39<27:22,  4.31s/it][A
  8%|▊         | 34/414 [02:45<30:42,  4.85s/it][A
  8%|▊         | 35/414 [02:49<30:03,  4.76s/it][A
  9%|▊         | 36/414 [02:55<31:10,  4.95s/it][A
  9%|▉         | 37/414 [02:59<29:30,  4.70s/it][A
  9%|▉         | 38/414 [03:05<31:55,  5.10s/it][A
  9%|▉         | 39/414 [03:08<29:06,  4.66s/it][A
 10%|▉         | 40/414 [03:12<26:44,  4.29s/it][A
 10%|▉         | 41/414 [03:17<28:34,  4.60s/it][A
 10%|█         | 42/414 [03:22<28:05,  4.53s/it][A
 10%|█         | 43/414 [03:27<29:38,  4.79s/it][A
 11%|█         | 44/414 [03:38<41:48,  6.78s/it][A
 11%|█         | 45/414 [03:46<43:19,  7.04s/it][A
 11%|█         | 46/414 [03:50<37:06,  6.05s/it][A
 11%|█▏        | 47/414 [03:53<31:08,  5.09s/it][A
 12%|█▏        | 48/414 [03:56<28:08,  4.61s/it][A
 12%|█▏        | 49/414 [03:59<24:08,  3.97s/it][A
 12%|█▏        | 50/414 [04:01<21:35,  3.56s/it][A
 12%|█▏        | 51/414 [04:04<20:52,  3.45s/it][A
 13%|█▎        | 52/414 [04:07<20:11,  3.35s/it][A
 13%|█▎        | 53/414 [04:12<22:20,  3.71s/it][A
 13%|█▎        | 54/414 [04:21<31:09,  5.19s/it][A
 13%|█▎        | 55/414 [04:26<30:25,  5.09s/it][A
 14%|█▎        | 56/414 [04:32<32:35,  5.46s/it][A
 14%|█▍        | 57/414 [04:36<29:27,  4.95s/it][A
 14%|█▍        | 58/414 [04:40<28:51,  4.86s/it][A
 14%|█▍        | 59/414 [04:47<31:43,  5.36s/it][A
 14%|█▍        | 60/414 [04:52<31:47,  5.39s/it][A
 15%|█▍        | 61/414 [04:56<28:36,  4.86s/it][A
 15%|█▍        | 62/414 [04:59<25:48,  4.40s/it][A
 15%|█▌        | 63/414 [05:03<24:15,  4.15s/it][A
 15%|█▌        | 64/414 [05:07<23:53,  4.09s/it][A
 16%|█▌        | 65/414 [05:11<23:55,  4.11s/it][A
 16%|█▌        | 66/414 [05:16<25:51,  4.46s/it][A
 16%|█▌        | 67/414 [05:21<26:57,  4.66s/it][A
 16%|█▋        | 68/414 [05:27<28:18,  4.91s/it][A
 17%|█▋        | 69/414 [05:32<27:54,  4.85s/it][A
 17%|█▋        | 70/414 [05:38<31:03,  5.42s/it][A
 17%|█▋        | 71/414 [05:42<28:35,  5.00s/it][A
 17%|█▋        | 72/414 [05:46<25:40,  4.50s/it][A
 18%|█▊        | 73/414 [05:50<25:30,  4.49s/it][A
 18%|█▊        | 74/414 [05:53<23:33,  4.16s/it][A
 18%|█▊        | 75/414 [05:57<21:55,  3.88s/it][A
 18%|█▊        | 76/414 [06:00<20:05,  3.57s/it][A
 19%|█▊        | 77/414 [06:04<21:13,  3.78s/it][A
 19%|█▉        | 78/414 [06:10<24:33,  4.38s/it][A
 19%|█▉        | 79/414 [06:13<23:28,  4.20s/it][A
 19%|█▉        | 80/414 [06:19<25:25,  4.57s/it][A
 20%|█▉        | 81/414 [06:22<23:23,  4.21s/it][A
 20%|█▉        | 82/414 [06:26<22:46,  4.12s/it][A
 20%|██        | 83/414 [06:31<24:11,  4.39s/it][A
 20%|██        | 84/414 [06:40<30:50,  5.61s/it][A
 21%|██        | 85/414 [06:43<27:07,  4.95s/it][A
 21%|██        | 86/414 [06:47<25:56,  4.74s/it][A
 21%|██        | 87/414 [06:50<22:49,  4.19s/it][A
 21%|██▏       | 88/414 [06:55<23:05,  4.25s/it][A
 21%|██▏       | 89/414 [06:59<22:39,  4.18s/it][A
 22%|██▏       | 90/414 [07:05<25:52,  4.79s/it][A
 22%|██▏       | 91/414 [07:10<26:06,  4.85s/it][A
 22%|██▏       | 92/414 [07:13<24:10,  4.50s/it][A
 22%|██▏       | 93/414 [07:19<25:03,  4.69s/it][A
 23%|██▎       | 94/414 [07:22<22:50,  4.28s/it][A
 23%|██▎       | 95/414 [07:27<23:23,  4.40s/it][A
 23%|██▎       | 96/414 [07:31<24:04,  4.54s/it][A
 23%|██▎       | 97/414 [07:37<25:08,  4.76s/it][A
 24%|██▎       | 98/414 [07:40<22:44,  4.32s/it][A
 24%|██▍       | 99/414 [07:43<20:36,  3.93s/it][A
 24%|██▍       | 100/414 [07:47<21:23,  4.09s/it][A
 24%|██▍       | 101/414 [07:53<23:34,  4.52s/it][A
 25%|██▍       | 102/414 [08:00<27:52,  5.36s/it][A
 25%|██▍       | 103/414 [08:04<25:13,  4.87s/it][A
 25%|██▌       | 104/414 [08:08<23:20,  4.52s/it][A
 25%|██▌       | 105/414 [08:12<22:38,  4.40s/it][A
 26%|██▌       | 106/414 [08:17<23:53,  4.65s/it][A
 26%|██▌       | 107/414 [08:24<26:37,  5.20s/it][A
 26%|██▌       | 108/414 [08:28<24:51,  4.87s/it][A
 26%|██▋       | 109/414 [08:32<24:38,  4.85s/it][A
 27%|██▋       | 110/414 [08:36<22:46,  4.50s/it][A
 27%|██▋       | 111/414 [08:42<24:23,  4.83s/it][A
 27%|██▋       | 112/414 [08:46<23:26,  4.66s/it][A
 27%|██▋       | 113/414 [08:50<21:48,  4.35s/it][A
 28%|██▊       | 114/414 [08:56<24:45,  4.95s/it][A
 28%|██▊       | 115/414 [09:01<24:47,  4.98s/it][A
 28%|██▊       | 116/414 [09:05<22:40,  4.57s/it][A
 28%|██▊       | 117/414 [09:11<24:57,  5.04s/it][A
 29%|██▊       | 118/414 [09:18<27:26,  5.56s/it][A
 29%|██▊       | 119/414 [09:24<27:54,  5.68s/it][A
 29%|██▉       | 120/414 [09:31<30:43,  6.27s/it][A
 29%|██▉       | 121/414 [09:36<28:14,  5.78s/it][A
 29%|██▉       | 122/414 [09:42<29:12,  6.00s/it][A
 30%|██▉       | 123/414 [09:47<26:44,  5.51s/it][A
 30%|██▉       | 124/414 [09:53<27:13,  5.63s/it][A
 30%|███       | 125/414 [09:58<27:27,  5.70s/it][A
 30%|███       | 126/414 [10:04<27:05,  5.64s/it][A
 31%|███       | 127/414 [10:08<24:26,  5.11s/it][A
 31%|███       | 128/414 [10:18<30:54,  6.49s/it][A
 31%|███       | 129/414 [10:23<29:28,  6.20s/it][A
 31%|███▏      | 130/414 [10:30<29:40,  6.27s/it][A
 32%|███▏      | 131/414 [10:35<28:38,  6.07s/it][A
 32%|███▏      | 132/414 [10:43<31:40,  6.74s/it][A
 32%|███▏      | 133/414 [10:47<27:03,  5.78s/it][A
 32%|███▏      | 134/414 [10:51<25:09,  5.39s/it][A
 33%|███▎      | 135/414 [10:57<25:37,  5.51s/it][A
 33%|███▎      | 136/414 [11:02<24:03,  5.19s/it][A
 33%|███▎      | 137/414 [11:10<28:34,  6.19s/it][A
 33%|███▎      | 138/414 [11:17<29:04,  6.32s/it][A
 34%|███▎      | 139/414 [11:21<25:48,  5.63s/it][A
 34%|███▍      | 140/414 [11:24<22:26,  4.91s/it][A
 34%|███▍      | 141/414 [11:28<21:08,  4.65s/it][A
 34%|███▍      | 142/414 [11:32<19:40,  4.34s/it][A
 35%|███▍      | 143/414 [11:38<22:00,  4.87s/it][A
 35%|███▍      | 144/414 [11:41<20:08,  4.48s/it][A
 35%|███▌      | 145/414 [11:45<19:23,  4.32s/it][A
 35%|███▌      | 146/414 [11:48<17:25,  3.90s/it][A
 36%|███▌      | 147/414 [11:52<17:32,  3.94s/it][A
 36%|███▌      | 148/414 [11:57<18:05,  4.08s/it][A
 36%|███▌      | 149/414 [12:01<18:17,  4.14s/it][A
 36%|███▌      | 150/414 [12:04<17:04,  3.88s/it][A
 36%|███▋      | 151/414 [12:08<16:23,  3.74s/it][A
 37%|███▋      | 152/414 [12:11<15:59,  3.66s/it][A
 37%|███▋      | 153/414 [12:16<17:45,  4.08s/it][A
 37%|███▋      | 154/414 [12:20<16:47,  3.87s/it][A
 37%|███▋      | 155/414 [12:23<15:55,  3.69s/it][A
 38%|███▊      | 156/414 [12:26<15:07,  3.52s/it][A
 38%|███▊      | 157/414 [12:31<17:23,  4.06s/it][A
 38%|███▊      | 158/414 [12:36<18:00,  4.22s/it][A
 38%|███▊      | 159/414 [12:42<20:09,  4.75s/it][A
 39%|███▊      | 160/414 [12:46<19:16,  4.55s/it][A
 39%|███▉      | 161/414 [12:51<19:11,  4.55s/it][A
 39%|███▉      | 162/414 [12:56<20:34,  4.90s/it][A
 39%|███▉      | 163/414 [13:01<20:06,  4.81s/it][A
 40%|███▉      | 164/414 [13:07<21:36,  5.19s/it][A
 40%|███▉      | 165/414 [13:13<23:04,  5.56s/it][A
 40%|████      | 166/414 [13:21<25:02,  6.06s/it][A
 40%|████      | 167/414 [13:27<25:01,  6.08s/it][A
 41%|████      | 168/414 [13:31<22:51,  5.57s/it][A
 41%|████      | 169/414 [13:35<21:16,  5.21s/it][A
 41%|████      | 170/414 [13:39<19:03,  4.68s/it][A
 41%|████▏     | 171/414 [13:43<17:53,  4.42s/it][A
 42%|████▏     | 172/414 [13:46<17:01,  4.22s/it][A
 42%|████▏     | 173/414 [13:52<19:04,  4.75s/it][A
 42%|████▏     | 174/414 [13:56<17:45,  4.44s/it][A
 42%|████▏     | 175/414 [14:02<19:13,  4.83s/it][A
 43%|████▎     | 176/414 [14:06<18:36,  4.69s/it][A
 43%|████▎     | 177/414 [14:10<17:39,  4.47s/it][A
 43%|████▎     | 178/414 [14:16<19:15,  4.90s/it][A
 43%|████▎     | 179/414 [14:20<18:23,  4.70s/it][A
 43%|████▎     | 180/414 [14:24<16:57,  4.35s/it][A
 44%|████▎     | 181/414 [14:29<17:49,  4.59s/it][A
 44%|████▍     | 182/414 [14:33<17:13,  4.45s/it][A
 44%|████▍     | 183/414 [14:38<17:01,  4.42s/it][A
 44%|████▍     | 184/414 [14:43<18:16,  4.77s/it][A
 45%|████▍     | 185/414 [14:47<16:51,  4.42s/it][A
 45%|████▍     | 186/414 [14:51<17:09,  4.51s/it][A
 45%|████▌     | 187/414 [15:00<22:03,  5.83s/it][A
 45%|████▌     | 188/414 [15:10<26:31,  7.04s/it][A
 46%|████▌     | 189/414 [15:15<23:52,  6.37s/it][A
 46%|████▌     | 190/414 [15:25<27:17,  7.31s/it][A
 46%|████▌     | 191/414 [15:29<23:36,  6.35s/it][A
 46%|████▋     | 192/414 [15:34<22:40,  6.13s/it][A
 47%|████▋     | 193/414 [15:38<19:58,  5.42s/it][A
 47%|████▋     | 194/414 [15:42<18:44,  5.11s/it][A
 47%|████▋     | 195/414 [15:47<17:58,  4.92s/it][A
 47%|████▋     | 196/414 [15:50<15:57,  4.39s/it][A
 48%|████▊     | 197/414 [15:55<16:31,  4.57s/it][A
 48%|████▊     | 198/414 [16:01<18:00,  5.00s/it][A
 48%|████▊     | 199/414 [16:05<16:29,  4.60s/it][A
 48%|████▊     | 200/414 [16:09<16:33,  4.64s/it][A
 49%|████▊     | 201/414 [16:14<16:28,  4.64s/it][A
 49%|████▉     | 202/414 [16:20<17:39,  5.00s/it][A
 49%|████▉     | 203/414 [16:24<16:54,  4.81s/it][A
 49%|████▉     | 204/414 [16:28<15:14,  4.35s/it][A
 50%|████▉     | 205/414 [16:31<14:11,  4.07s/it][A
 50%|████▉     | 206/414 [16:35<14:03,  4.06s/it][A
 50%|█████     | 207/414 [16:40<14:41,  4.26s/it][A
 50%|█████     | 208/414 [16:44<14:35,  4.25s/it][A
 50%|█████     | 209/414 [16:50<16:51,  4.93s/it][A
 51%|█████     | 210/414 [16:56<17:04,  5.02s/it][A
 51%|█████     | 211/414 [17:00<16:29,  4.88s/it][A
 51%|█████     | 212/414 [17:04<15:31,  4.61s/it][A
 51%|█████▏    | 213/414 [17:10<16:27,  4.91s/it][A
 52%|█████▏    | 214/414 [17:16<17:09,  5.15s/it][A
 52%|█████▏    | 215/414 [17:20<16:19,  4.92s/it][A
 52%|█████▏    | 216/414 [17:27<18:38,  5.65s/it][A
 52%|█████▏    | 217/414 [17:36<21:40,  6.60s/it][A
 53%|█████▎    | 218/414 [17:40<18:50,  5.77s/it][A
 53%|█████▎    | 219/414 [17:44<17:15,  5.31s/it][A
 53%|█████▎    | 220/414 [17:49<16:24,  5.07s/it][A
 53%|█████▎    | 221/414 [17:53<15:15,  4.74s/it][A
 54%|█████▎    | 222/414 [17:58<15:31,  4.85s/it][A
 54%|█████▍    | 223/414 [18:04<16:35,  5.21s/it][A
 54%|█████▍    | 224/414 [18:09<16:02,  5.07s/it][A
 54%|█████▍    | 225/414 [18:15<17:11,  5.46s/it][A
 55%|█████▍    | 226/414 [18:19<16:14,  5.19s/it][A
 55%|█████▍    | 227/414 [18:27<18:04,  5.80s/it][A
 55%|█████▌    | 228/414 [18:33<18:46,  6.06s/it][A
 55%|█████▌    | 229/414 [18:38<17:08,  5.56s/it][A
 56%|█████▌    | 230/414 [18:44<17:21,  5.66s/it][A
 56%|█████▌    | 231/414 [18:50<18:11,  5.96s/it][A
 56%|█████▌    | 232/414 [18:54<16:11,  5.34s/it][A
 56%|█████▋    | 233/414 [19:00<16:41,  5.53s/it][A
 57%|█████▋    | 234/414 [19:04<15:26,  5.15s/it][A
 57%|█████▋    | 235/414 [19:10<15:35,  5.23s/it][A
 57%|█████▋    | 236/414 [19:15<15:24,  5.19s/it][A
 57%|█████▋    | 237/414 [19:20<15:34,  5.28s/it][A
 57%|█████▋    | 238/414 [19:24<13:45,  4.69s/it][A
 58%|█████▊    | 239/414 [19:28<12:49,  4.40s/it][A
 58%|█████▊    | 240/414 [19:34<14:59,  5.17s/it][A
 58%|█████▊    | 241/414 [19:41<15:41,  5.44s/it][A
 58%|█████▊    | 242/414 [19:44<13:40,  4.77s/it][A
 59%|█████▊    | 243/414 [19:47<12:15,  4.30s/it][A
 59%|█████▉    | 244/414 [19:51<12:14,  4.32s/it][A
 59%|█████▉    | 245/414 [19:57<13:08,  4.67s/it][A
 59%|█████▉    | 246/414 [20:01<12:16,  4.38s/it][A
 60%|█████▉    | 247/414 [20:05<11:56,  4.29s/it][A
 60%|█████▉    | 248/414 [20:09<12:10,  4.40s/it][A
 60%|██████    | 249/414 [20:13<11:29,  4.18s/it][A
 60%|██████    | 250/414 [20:17<11:07,  4.07s/it][A
 61%|██████    | 251/414 [20:20<10:44,  3.95s/it][A
 61%|██████    | 252/414 [20:27<12:29,  4.63s/it][A
 61%|██████    | 253/414 [20:31<12:23,  4.62s/it][A
 61%|██████▏   | 254/414 [20:40<15:42,  5.89s/it][A
 62%|██████▏   | 255/414 [20:44<13:40,  5.16s/it][A
 62%|██████▏   | 256/414 [20:47<12:21,  4.69s/it][A
 62%|██████▏   | 257/414 [20:51<11:48,  4.51s/it][A
 62%|██████▏   | 258/414 [20:55<11:01,  4.24s/it][A
 63%|██████▎   | 259/414 [20:59<10:34,  4.09s/it][A
 63%|██████▎   | 260/414 [21:02<09:55,  3.87s/it][A
 63%|██████▎   | 261/414 [21:07<10:55,  4.28s/it][A
 63%|██████▎   | 262/414 [21:11<10:30,  4.15s/it][A
 64%|██████▎   | 263/414 [21:18<12:49,  5.09s/it][A
 64%|██████▍   | 264/414 [21:24<13:09,  5.27s/it][A
 64%|██████▍   | 265/414 [21:29<13:13,  5.32s/it][A
 64%|██████▍   | 266/414 [21:34<12:22,  5.02s/it][A
 64%|██████▍   | 267/414 [21:39<12:48,  5.23s/it][A
 65%|██████▍   | 268/414 [21:46<13:32,  5.57s/it][A
 65%|██████▍   | 269/414 [21:49<11:32,  4.77s/it][A
 65%|██████▌   | 270/414 [21:52<10:26,  4.35s/it][A
 65%|██████▌   | 271/414 [21:56<09:59,  4.19s/it][A
 66%|██████▌   | 272/414 [22:01<10:35,  4.48s/it][A
 66%|██████▌   | 273/414 [22:06<11:05,  4.72s/it][A
 66%|██████▌   | 274/414 [22:12<11:25,  4.90s/it][A
 66%|██████▋   | 275/414 [22:15<10:34,  4.56s/it][A
 67%|██████▋   | 276/414 [22:20<10:23,  4.52s/it][A
 67%|██████▋   | 277/414 [22:24<09:50,  4.31s/it][A
 67%|██████▋   | 278/414 [22:27<09:20,  4.12s/it][A
 67%|██████▋   | 279/414 [22:31<08:47,  3.91s/it][A
 68%|██████▊   | 280/414 [22:35<09:11,  4.12s/it][A
 68%|██████▊   | 281/414 [22:39<09:06,  4.11s/it][A
 68%|██████▊   | 282/414 [22:44<09:19,  4.24s/it][A
 68%|██████▊   | 283/414 [22:50<10:39,  4.88s/it][A
 69%|██████▊   | 284/414 [22:54<09:50,  4.54s/it][A
 69%|██████▉   | 285/414 [22:58<09:23,  4.37s/it][A
 69%|██████▉   | 286/414 [23:05<10:38,  4.99s/it][A
 69%|██████▉   | 287/414 [23:11<11:28,  5.42s/it][A
 70%|██████▉   | 288/414 [23:16<11:20,  5.40s/it][A
 70%|██████▉   | 289/414 [23:20<10:03,  4.83s/it][A
 70%|███████   | 290/414 [23:23<09:09,  4.43s/it][A
 70%|███████   | 291/414 [23:31<10:51,  5.30s/it][A
 71%|███████   | 292/414 [23:35<09:55,  4.88s/it][A
 71%|███████   | 293/414 [23:40<10:10,  5.05s/it][A
 71%|███████   | 294/414 [23:51<13:37,  6.82s/it][A
 71%|███████▏  | 295/414 [23:55<12:07,  6.11s/it][A
 71%|███████▏  | 296/414 [24:03<13:10,  6.70s/it][A
 72%|███████▏  | 297/414 [24:08<11:52,  6.09s/it][A
 72%|███████▏  | 298/414 [24:13<11:03,  5.72s/it][A
 72%|███████▏  | 299/414 [24:20<11:54,  6.21s/it][A
 72%|███████▏  | 300/414 [24:25<10:42,  5.63s/it][A
 73%|███████▎  | 301/414 [24:30<10:14,  5.44s/it][A
 73%|███████▎  | 302/414 [24:33<09:01,  4.83s/it][A
 73%|███████▎  | 303/414 [24:37<08:16,  4.47s/it][A
 73%|███████▎  | 304/414 [24:42<08:30,  4.64s/it][A
 74%|███████▎  | 305/414 [24:51<10:45,  5.92s/it][A
 74%|███████▍  | 306/414 [24:55<09:55,  5.52s/it][A
 74%|███████▍  | 307/414 [25:00<09:27,  5.30s/it][A
 74%|███████▍  | 308/414 [25:05<09:15,  5.25s/it][A
 75%|███████▍  | 309/414 [25:12<10:16,  5.87s/it][A
 75%|███████▍  | 310/414 [25:16<09:01,  5.20s/it][A
 75%|███████▌  | 311/414 [25:20<08:03,  4.70s/it][A
 75%|███████▌  | 312/414 [25:24<07:41,  4.52s/it][A
 76%|███████▌  | 313/414 [25:28<07:29,  4.45s/it][A
 76%|███████▌  | 314/414 [25:32<07:07,  4.28s/it][A
 76%|███████▌  | 315/414 [25:36<06:45,  4.10s/it][A
 76%|███████▋  | 316/414 [25:42<07:36,  4.66s/it][A
 77%|███████▋  | 317/414 [25:50<09:28,  5.86s/it][A
 77%|███████▋  | 318/414 [25:54<08:31,  5.33s/it][A
 77%|███████▋  | 319/414 [26:00<08:31,  5.39s/it][A
 77%|███████▋  | 320/414 [26:03<07:37,  4.87s/it][A
 78%|███████▊  | 321/414 [26:09<07:46,  5.01s/it][A
 78%|███████▊  | 322/414 [26:15<08:21,  5.45s/it][A
 78%|███████▊  | 323/414 [26:20<07:46,  5.12s/it][A
 78%|███████▊  | 324/414 [26:23<06:53,  4.59s/it][A
 79%|███████▊  | 325/414 [26:27<06:30,  4.39s/it][A
 79%|███████▊  | 326/414 [26:30<06:03,  4.14s/it][A
 79%|███████▉  | 327/414 [26:36<06:26,  4.45s/it][A
 79%|███████▉  | 328/414 [26:43<07:28,  5.21s/it][A
 79%|███████▉  | 329/414 [26:48<07:17,  5.15s/it][A
 80%|███████▉  | 330/414 [26:55<08:05,  5.78s/it][A
 80%|███████▉  | 331/414 [27:01<08:06,  5.86s/it][A
 80%|████████  | 332/414 [27:07<07:59,  5.84s/it][A
 80%|████████  | 333/414 [27:12<07:50,  5.81s/it][A
 81%|████████  | 334/414 [27:17<07:03,  5.30s/it][A
 81%|████████  | 335/414 [27:20<06:08,  4.67s/it][A
 81%|████████  | 336/414 [27:25<06:06,  4.70s/it][A
 81%|████████▏ | 337/414 [27:28<05:35,  4.36s/it][A
 82%|████████▏ | 338/414 [27:31<05:06,  4.04s/it][A
 82%|████████▏ | 339/414 [27:38<05:52,  4.70s/it][A
 82%|████████▏ | 340/414 [27:43<06:12,  5.03s/it][A
 82%|████████▏ | 341/414 [27:51<06:59,  5.74s/it][A
 83%|████████▎ | 342/414 [27:54<06:04,  5.06s/it][A
 83%|████████▎ | 343/414 [27:58<05:39,  4.78s/it][A
 83%|████████▎ | 344/414 [28:07<06:58,  5.98s/it][A
 83%|████████▎ | 345/414 [28:13<06:56,  6.03s/it][A
 84%|████████▎ | 346/414 [28:18<06:12,  5.48s/it][A
 84%|████████▍ | 347/414 [28:21<05:25,  4.86s/it][A
 84%|████████▍ | 348/414 [28:26<05:28,  4.98s/it][A
 84%|████████▍ | 349/414 [28:30<04:52,  4.49s/it][A
 85%|████████▍ | 350/414 [28:35<05:02,  4.73s/it][A
 85%|████████▍ | 351/414 [28:42<05:52,  5.59s/it][A
 85%|████████▌ | 352/414 [28:47<05:20,  5.17s/it][A
 85%|████████▌ | 353/414 [28:50<04:32,  4.47s/it][A
 86%|████████▌ | 354/414 [28:54<04:20,  4.35s/it][A
 86%|████████▌ | 355/414 [28:57<04:06,  4.19s/it][A
 86%|████████▌ | 356/414 [29:01<03:49,  3.96s/it][A
 86%|████████▌ | 357/414 [29:04<03:37,  3.81s/it][A
 86%|████████▋ | 358/414 [29:11<04:24,  4.72s/it][A
 87%|████████▋ | 359/414 [29:18<04:47,  5.23s/it][A
 87%|████████▋ | 360/414 [29:22<04:23,  4.88s/it][A
 87%|████████▋ | 361/414 [29:25<03:58,  4.51s/it][A
 87%|████████▋ | 362/414 [29:29<03:47,  4.37s/it][A
 88%|████████▊ | 363/414 [29:37<04:30,  5.31s/it][A
 88%|████████▊ | 364/414 [29:41<04:10,  5.01s/it][A
 88%|████████▊ | 365/414 [29:46<04:03,  4.97s/it][A
 88%|████████▊ | 366/414 [29:52<04:18,  5.40s/it][A
 89%|████████▊ | 367/414 [29:58<04:16,  5.47s/it][A
 89%|████████▉ | 368/414 [30:02<03:57,  5.16s/it][A
 89%|████████▉ | 369/414 [30:08<03:54,  5.20s/it][A
 89%|████████▉ | 370/414 [30:14<04:01,  5.48s/it][A
 90%|████████▉ | 371/414 [30:19<03:46,  5.27s/it][A
 90%|████████▉ | 372/414 [30:25<03:58,  5.68s/it][A
 90%|█████████ | 373/414 [30:30<03:41,  5.40s/it][A
 90%|█████████ | 374/414 [30:37<03:53,  5.84s/it][A
 91%|█████████ | 375/414 [30:42<03:36,  5.54s/it][A
 91%|█████████ | 376/414 [30:45<03:08,  4.95s/it][A
 91%|█████████ | 377/414 [30:49<02:46,  4.50s/it][A
 91%|█████████▏| 378/414 [30:55<03:03,  5.11s/it][A
 92%|█████████▏| 379/414 [31:01<03:00,  5.15s/it][A
 92%|█████████▏| 380/414 [31:05<02:44,  4.82s/it][A
 92%|█████████▏| 381/414 [31:15<03:34,  6.49s/it][A
 92%|█████████▏| 382/414 [31:23<03:46,  7.08s/it][A
 93%|█████████▎| 383/414 [31:27<03:11,  6.17s/it][A
 93%|█████████▎| 384/414 [31:32<02:47,  5.57s/it][A
 93%|█████████▎| 385/414 [31:37<02:41,  5.55s/it][A
 93%|█████████▎| 386/414 [31:41<02:24,  5.17s/it][A
 93%|█████████▎| 387/414 [31:45<02:10,  4.83s/it][A
 94%|█████████▎| 388/414 [31:50<01:59,  4.60s/it][A
 94%|█████████▍| 389/414 [31:54<01:50,  4.43s/it][A
 94%|█████████▍| 390/414 [31:57<01:37,  4.05s/it][A
 94%|█████████▍| 391/414 [32:00<01:29,  3.89s/it][A
 95%|█████████▍| 392/414 [32:04<01:27,  3.99s/it][A
 95%|█████████▍| 393/414 [32:08<01:21,  3.90s/it][A
 95%|█████████▌| 394/414 [32:14<01:30,  4.52s/it][A
 95%|█████████▌| 395/414 [32:18<01:22,  4.32s/it][A
 96%|█████████▌| 396/414 [32:22<01:17,  4.30s/it][A
 96%|█████████▌| 397/414 [32:30<01:29,  5.24s/it][A
 96%|█████████▌| 398/414 [32:35<01:26,  5.38s/it][A
 96%|█████████▋| 399/414 [32:39<01:11,  4.75s/it][A
 97%|█████████▋| 400/414 [32:44<01:07,  4.83s/it][A
 97%|█████████▋| 401/414 [32:54<01:24,  6.52s/it][A
 97%|█████████▋| 402/414 [33:00<01:16,  6.40s/it][A
 97%|█████████▋| 403/414 [33:04<01:00,  5.53s/it][A
 98%|█████████▊| 404/414 [33:07<00:49,  4.95s/it][A
 98%|█████████▊| 405/414 [33:11<00:40,  4.55s/it][A
 98%|█████████▊| 406/414 [33:15<00:34,  4.34s/it][A
 98%|█████████▊| 407/414 [33:22<00:35,  5.10s/it][A
 99%|█████████▊| 408/414 [33:27<00:31,  5.24s/it][A
 99%|█████████▉| 409/414 [33:31<00:24,  4.87s/it][A
 99%|█████████▉| 410/414 [33:37<00:20,  5.16s/it][A
 99%|█████████▉| 411/414 [33:41<00:14,  4.71s/it][A
100%|█████████▉| 412/414 [33:45<00:08,  4.43s/it][A
100%|█████████▉| 413/414 [33:52<00:05,  5.25s/it][A
100%|██████████| 414/414 [33:55<00:00,  4.55s/it][A                                                       
                                                 [A{'eval_loss': 0.6999348998069763, 'eval_runtime': 2041.3946, 'eval_samples_per_second': 1.619, 'eval_steps_per_second': 0.203, 'epoch': 0.86}
 29%|██▊       | 200/699 [10:16:39<24:47:20, 178.84s/it]
100%|██████████| 414/414 [33:55<00:00,  4.55s/it][A
                                                 [A[INFO|trainer.py:4309] 2026-01-02 15:55:56,402 >> Saving model checkpoint to saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in Qwen/Qwen2.5-VL-7B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2421] 2026-01-02 15:55:56,557 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2026-01-02 15:55:56,560 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2026-01-02 15:55:56,563 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/special_tokens_map.json
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once
[2026-01-02 15:55:57,133] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!
[2026-01-02 15:55:57,164] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/global_step200/mp_rank_00_model_states.pt
[2026-01-02 15:55:57,164] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/global_step200/mp_rank_00_model_states.pt...
[2026-01-02 15:55:57,229] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/global_step200/mp_rank_00_model_states.pt.
[2026-01-02 15:55:57,234] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2026-01-02 15:55:57,234] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/global_step200/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2026-01-02 15:55:57,234] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/global_step200/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2026-01-02 15:55:57,234] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/global_step200/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2026-01-02 15:55:57,234] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/global_step200/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2026-01-02 15:55:57,234] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/global_step200/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2026-01-02 15:55:57,234] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/global_step200/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2026-01-02 15:55:57,234] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/global_step200/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2026-01-02 15:55:57,277] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2026-01-02 15:55:57,303] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/global_step200/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2026-01-02 15:55:57,304] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/global_step200/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt
[2026-01-02 15:55:57,304] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2026-01-02 15:55:57,367] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/global_step200/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2026-01-02 15:55:57,367] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/global_step200/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2026-01-02 15:55:57,368] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2026-01-02 15:55:57,369] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/global_step200/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2026-01-02 15:55:57,370] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/global_step200/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2026-01-02 15:55:57,370] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/global_step200/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2026-01-02 15:55:57,370] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2026-01-02 15:55:57,370] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/global_step200/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2026-01-02 15:55:57,370] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2026-01-02 15:55:57,377] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/global_step200/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2026-01-02 15:55:57,377] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/global_step200/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2026-01-02 15:55:57,377] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2026-01-02 15:55:57,377] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/global_step200/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2026-01-02 15:55:57,378] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/global_step200/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2026-01-02 15:55:57,378] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2026-01-02 15:55:57,380] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/global_step200/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2026-01-02 15:55:57,380] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/global_step200/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt
[2026-01-02 15:55:57,380] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2026-01-02 15:55:57,381] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2026-01-02 15:55:57,381] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[INFO|image_processing_base.py:253] 2026-01-02 15:55:57,402 >> Image processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/preprocessor_config.json
[INFO|tokenization_utils_base.py:2421] 2026-01-02 15:55:57,405 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2026-01-02 15:55:57,408 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2026-01-02 15:55:57,411 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/special_tokens_map.json
[INFO|video_processing_utils.py:600] 2026-01-02 15:55:57,577 >> Video processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/video_preprocessor_config.json
[INFO|processing_utils.py:814] 2026-01-02 15:55:57,579 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-200/chat_template.jinja
 29%|██▉       | 201/699 [10:19:49<109:56:14, 794.73s/it] 29%|██▉       | 202/699 [10:22:21<83:04:01, 601.69s/it]  29%|██▉       | 203/699 [10:24:41<63:49:08, 463.20s/it] 29%|██▉       | 204/699 [10:27:20<51:08:55, 371.99s/it] 29%|██▉       | 205/699 [10:29:49<41:53:15, 305.25s/it] 29%|██▉       | 206/699 [10:33:22<37:59:08, 277.38s/it] 30%|██▉       | 207/699 [10:36:48<34:59:10, 256.00s/it] 30%|██▉       | 208/699 [10:39:46<31:42:48, 232.52s/it] 30%|██▉       | 209/699 [10:42:30<28:50:53, 211.95s/it] 30%|███       | 210/699 [10:45:28<27:25:19, 201.88s/it]                                                        {'loss': 0.7145, 'grad_norm': 0.4262332320213318, 'learning_rate': 8.842678390572557e-05, 'epoch': 0.9}
 30%|███       | 210/699 [10:45:28<27:25:19, 201.88s/it] 30%|███       | 211/699 [10:48:15<25:57:19, 191.48s/it] 30%|███       | 212/699 [10:51:35<26:14:08, 193.94s/it] 30%|███       | 213/699 [10:54:45<26:02:37, 192.92s/it] 31%|███       | 214/699 [10:57:52<25:43:33, 190.96s/it] 31%|███       | 215/699 [11:01:01<25:36:57, 190.53s/it] 31%|███       | 216/699 [11:04:11<25:31:06, 190.20s/it] 31%|███       | 217/699 [11:07:33<25:58:03, 193.95s/it] 31%|███       | 218/699 [11:10:27<25:04:58, 187.73s/it] 31%|███▏      | 219/699 [11:13:26<24:41:14, 185.16s/it] 31%|███▏      | 220/699 [11:16:47<25:16:38, 189.98s/it]                                                        {'loss': 0.7219, 'grad_norm': 0.39374035596847534, 'learning_rate': 8.678174424359485e-05, 'epoch': 0.95}
 31%|███▏      | 220/699 [11:16:47<25:16:38, 189.98s/it] 32%|███▏      | 221/699 [11:19:46<24:46:56, 186.64s/it] 32%|███▏      | 222/699 [11:22:41<24:16:19, 183.18s/it] 32%|███▏      | 223/699 [11:25:48<24:21:03, 184.17s/it] 32%|███▏      | 224/699 [11:28:26<23:17:39, 176.55s/it] 32%|███▏      | 225/699 [11:30:32<21:15:06, 161.41s/it] 32%|███▏      | 226/699 [11:33:08<20:57:43, 159.54s/it] 32%|███▏      | 227/699 [11:36:54<23:32:11, 179.52s/it] 33%|███▎      | 228/699 [11:39:38<22:52:55, 174.89s/it] 33%|███▎      | 229/699 [11:42:19<22:18:35, 170.88s/it] 33%|███▎      | 230/699 [11:45:19<22:36:10, 173.50s/it]                                                        {'loss': 0.7162, 'grad_norm': 0.42338258028030396, 'learning_rate': 8.504496843543813e-05, 'epoch': 0.99}
 33%|███▎      | 230/699 [11:45:19<22:36:10, 173.50s/it] 33%|███▎      | 231/699 [11:48:35<23:25:25, 180.18s/it] 33%|███▎      | 232/699 [11:51:48<23:52:05, 183.99s/it] 33%|███▎      | 233/699 [11:52:23<18:03:47, 139.54s/it] 33%|███▎      | 234/699 [11:54:55<18:29:37, 143.18s/it] 34%|███▎      | 235/699 [11:58:06<20:18:29, 157.56s/it] 34%|███▍      | 236/699 [12:00:52<20:34:57, 160.04s/it] 34%|███▍      | 237/699 [12:04:05<21:49:17, 170.04s/it] 34%|███▍      | 238/699 [12:06:34<20:57:54, 163.72s/it] 34%|███▍      | 239/699 [12:10:13<23:02:20, 180.31s/it] 34%|███▍      | 240/699 [12:12:59<22:25:51, 175.93s/it]                                                        {'loss': 0.6883, 'grad_norm': 0.47141048312187195, 'learning_rate': 8.322078811702823e-05, 'epoch': 1.03}
 34%|███▍      | 240/699 [12:12:59<22:25:51, 175.93s/it] 34%|███▍      | 241/699 [12:15:52<22:15:23, 174.94s/it] 35%|███▍      | 242/699 [12:18:57<22:35:02, 177.90s/it] 35%|███▍      | 243/699 [12:21:24<21:21:54, 168.67s/it] 35%|███▍      | 244/699 [12:24:35<22:10:02, 175.39s/it] 35%|███▌      | 245/699 [12:27:14<21:30:52, 170.60s/it] 35%|███▌      | 246/699 [12:29:51<20:56:51, 166.47s/it] 35%|███▌      | 247/699 [12:32:46<21:13:28, 169.04s/it] 35%|███▌      | 248/699 [12:35:36<21:11:56, 169.22s/it] 36%|███▌      | 249/699 [12:38:31<21:23:23, 171.12s/it] 36%|███▌      | 250/699 [12:42:06<22:59:16, 184.31s/it]                                                        {'loss': 0.708, 'grad_norm': 0.5097454190254211, 'learning_rate': 8.131375291686995e-05, 'epoch': 1.07}
 36%|███▌      | 250/699 [12:42:06<22:59:16, 184.31s/it] 36%|███▌      | 251/699 [12:44:38<21:43:47, 174.62s/it] 36%|███▌      | 252/699 [12:47:46<22:10:51, 178.64s/it] 36%|███▌      | 253/699 [12:50:46<22:09:24, 178.85s/it] 36%|███▋      | 254/699 [12:53:42<22:01:08, 178.13s/it] 36%|███▋      | 255/699 [12:57:43<24:16:35, 196.84s/it] 37%|███▋      | 256/699 [13:00:08<22:18:40, 181.31s/it] 37%|███▋      | 257/699 [13:03:40<23:23:23, 190.51s/it] 37%|███▋      | 258/699 [13:07:16<24:17:07, 198.25s/it] 37%|███▋      | 259/699 [13:10:38<24:22:43, 199.46s/it] 37%|███▋      | 260/699 [13:13:58<24:20:59, 199.68s/it]                                                        {'loss': 0.7044, 'grad_norm': 0.49000415205955505, 'learning_rate': 7.932861910912036e-05, 'epoch': 1.12}
 37%|███▋      | 260/699 [13:13:58<24:20:59, 199.68s/it] 37%|███▋      | 261/699 [13:16:42<22:59:35, 188.98s/it] 37%|███▋      | 262/699 [13:19:13<21:32:59, 177.53s/it] 38%|███▊      | 263/699 [13:22:35<22:21:58, 184.68s/it] 38%|███▊      | 264/699 [13:25:42<22:23:48, 185.35s/it] 38%|███▊      | 265/699 [13:28:04<20:46:57, 172.39s/it] 38%|███▊      | 266/699 [13:30:47<20:23:28, 169.53s/it] 38%|███▊      | 267/699 [13:34:07<21:26:46, 178.72s/it] 38%|███▊      | 268/699 [13:36:34<20:15:24, 169.20s/it] 38%|███▊      | 269/699 [13:39:43<20:55:10, 175.14s/it] 39%|███▊      | 270/699 [13:42:27<20:29:25, 171.95s/it]                                                        {'loss': 0.7051, 'grad_norm': 0.40284037590026855, 'learning_rate': 7.727033775112096e-05, 'epoch': 1.16}
 39%|███▊      | 270/699 [13:42:27<20:29:25, 171.95s/it] 39%|███▉      | 271/699 [13:45:22<20:31:58, 172.71s/it] 39%|███▉      | 272/699 [13:48:25<20:51:13, 175.82s/it] 39%|███▉      | 273/699 [13:51:01<20:06:45, 169.97s/it] 39%|███▉      | 274/699 [13:53:42<19:44:08, 167.17s/it] 39%|███▉      | 275/699 [13:56:44<20:13:39, 171.74s/it] 39%|███▉      | 276/699 [14:00:02<21:06:47, 179.69s/it] 40%|███▉      | 277/699 [14:03:08<21:16:36, 181.51s/it] 40%|███▉      | 278/699 [14:05:55<20:43:25, 177.21s/it] 40%|███▉      | 279/699 [14:09:24<21:46:52, 186.70s/it] 40%|████      | 280/699 [14:12:34<21:51:15, 187.77s/it]                                                        {'loss': 0.6916, 'grad_norm': 0.4469200074672699, 'learning_rate': 7.514404233512725e-05, 'epoch': 1.2}
 40%|████      | 280/699 [14:12:34<21:51:15, 187.77s/it] 40%|████      | 281/699 [14:15:23<21:07:23, 181.92s/it] 40%|████      | 282/699 [14:19:20<22:58:51, 198.40s/it] 40%|████      | 283/699 [14:21:52<21:19:56, 184.61s/it] 41%|████      | 284/699 [14:24:46<20:55:24, 181.51s/it] 41%|████      | 285/699 [14:27:50<20:57:42, 182.28s/it] 41%|████      | 286/699 [14:30:41<20:30:51, 178.82s/it] 41%|████      | 287/699 [14:33:28<20:04:14, 175.38s/it] 41%|████      | 288/699 [14:36:13<19:38:24, 172.03s/it] 41%|████▏     | 289/699 [14:39:03<19:31:37, 171.46s/it] 41%|████▏     | 290/699 [14:42:25<20:31:41, 180.69s/it]                                                        {'loss': 0.6877, 'grad_norm': 0.46001556515693665, 'learning_rate': 7.295503598503366e-05, 'epoch': 1.25}
 41%|████▏     | 290/699 [14:42:25<20:31:41, 180.69s/it] 42%|████▏     | 291/699 [14:45:19<20:14:17, 178.57s/it] 42%|████▏     | 292/699 [14:48:34<20:45:18, 183.58s/it] 42%|████▏     | 293/699 [14:51:50<21:07:32, 187.32s/it] 42%|████▏     | 294/699 [14:54:47<20:43:28, 184.22s/it] 42%|████▏     | 295/699 [14:58:37<22:13:09, 197.99s/it] 42%|████▏     | 296/699 [15:01:11<20:40:55, 184.75s/it] 42%|████▏     | 297/699 [15:04:26<20:58:35, 187.85s/it] 43%|████▎     | 298/699 [15:06:56<19:38:50, 176.39s/it] 43%|████▎     | 299/699 [15:09:52<19:35:04, 176.26s/it] 43%|████▎     | 300/699 [15:12:49<19:34:25, 176.61s/it]                                                        {'loss': 0.6889, 'grad_norm': 0.541511058807373, 'learning_rate': 7.070877823002547e-05, 'epoch': 1.29}
 43%|████▎     | 300/699 [15:12:49<19:34:25, 176.61s/it] 43%|████▎     | 301/699 [15:15:12<18:23:44, 166.39s/it] 43%|████▎     | 302/699 [15:17:37<17:39:09, 160.07s/it] 43%|████▎     | 303/699 [15:20:22<17:46:32, 161.60s/it] 43%|████▎     | 304/699 [15:23:30<18:35:11, 169.40s/it] 44%|████▎     | 305/699 [15:26:12<18:18:07, 167.23s/it] 44%|████▍     | 306/699 [15:28:49<17:55:52, 164.26s/it] 44%|████▍     | 307/699 [15:31:30<17:45:37, 163.11s/it] 44%|████▍     | 308/699 [15:34:38<18:31:46, 170.60s/it] 44%|████▍     | 309/699 [15:37:28<18:29:25, 170.68s/it] 44%|████▍     | 310/699 [15:40:14<18:16:31, 169.13s/it]                                                        {'loss': 0.7024, 'grad_norm': 0.48759961128234863, 'learning_rate': 6.841087138814562e-05, 'epoch': 1.33}
 44%|████▍     | 310/699 [15:40:14<18:16:31, 169.13s/it] 44%|████▍     | 311/699 [15:43:04<18:15:41, 169.44s/it] 45%|████▍     | 312/699 [15:45:18<17:04:51, 158.89s/it] 45%|████▍     | 313/699 [15:47:49<16:45:33, 156.31s/it] 45%|████▍     | 314/699 [15:50:36<17:04:06, 159.60s/it] 45%|████▌     | 315/699 [15:53:25<17:18:47, 162.31s/it] 45%|████▌     | 316/699 [15:55:36<16:16:53, 153.04s/it] 45%|████▌     | 317/699 [15:58:12<16:19:24, 153.83s/it] 45%|████▌     | 318/699 [16:00:24<15:36:13, 147.44s/it] 46%|████▌     | 319/699 [16:03:38<17:02:37, 161.47s/it] 46%|████▌     | 320/699 [16:06:47<17:52:12, 169.74s/it]                                                        {'loss': 0.6907, 'grad_norm': 0.45186111330986023, 'learning_rate': 6.606704659373628e-05, 'epoch': 1.37}
 46%|████▌     | 320/699 [16:06:47<17:52:12, 169.74s/it] 46%|████▌     | 321/699 [16:09:49<18:12:16, 173.38s/it] 46%|████▌     | 322/699 [16:12:28<17:42:09, 169.04s/it] 46%|████▌     | 323/699 [16:15:33<18:08:01, 173.62s/it] 46%|████▋     | 324/699 [16:18:15<17:44:33, 170.33s/it] 46%|████▋     | 325/699 [16:21:03<17:37:18, 169.62s/it] 47%|████▋     | 326/699 [16:24:26<18:35:40, 179.47s/it] 47%|████▋     | 327/699 [16:27:40<19:00:35, 183.97s/it] 47%|████▋     | 328/699 [16:30:43<18:56:03, 183.73s/it] 47%|████▋     | 329/699 [16:33:36<18:32:32, 180.41s/it] 47%|████▋     | 330/699 [16:36:57<19:08:29, 186.75s/it]                                                        {'loss': 0.705, 'grad_norm': 0.5176883935928345, 'learning_rate': 6.368314950360415e-05, 'epoch': 1.42}
 47%|████▋     | 330/699 [16:36:57<19:08:29, 186.75s/it] 47%|████▋     | 331/699 [16:39:49<18:37:03, 182.13s/it] 47%|████▋     | 332/699 [16:43:25<19:37:04, 192.44s/it] 48%|████▊     | 333/699 [16:46:21<19:03:30, 187.46s/it] 48%|████▊     | 334/699 [16:49:48<19:35:01, 193.15s/it] 48%|████▊     | 335/699 [16:52:35<18:44:29, 185.36s/it] 48%|████▊     | 336/699 [16:55:17<18:00:01, 178.52s/it] 48%|████▊     | 337/699 [16:58:08<17:42:47, 176.15s/it] 48%|████▊     | 338/699 [17:00:50<17:13:53, 171.84s/it] 48%|████▊     | 339/699 [17:03:36<17:00:55, 170.16s/it] 49%|████▊     | 340/699 [17:06:09<16:27:08, 164.98s/it]                                                        {'loss': 0.6927, 'grad_norm': 0.4675188660621643, 'learning_rate': 6.126512571755883e-05, 'epoch': 1.46}
 49%|████▊     | 340/699 [17:06:09<16:27:08, 164.98s/it] 49%|████▉     | 341/699 [17:09:04<16:42:53, 168.08s/it] 49%|████▉     | 342/699 [17:11:44<16:24:40, 165.49s/it] 49%|████▉     | 343/699 [17:14:15<15:56:15, 161.17s/it] 49%|████▉     | 344/699 [17:16:49<15:40:39, 158.98s/it] 49%|████▉     | 345/699 [17:20:01<16:36:37, 168.92s/it] 49%|████▉     | 346/699 [17:23:37<17:56:44, 183.02s/it] 50%|████▉     | 347/699 [17:27:00<18:29:04, 189.05s/it] 50%|████▉     | 348/699 [17:30:22<18:48:33, 192.92s/it] 50%|████▉     | 349/699 [17:32:46<17:19:34, 178.21s/it] 50%|█████     | 350/699 [17:35:31<16:53:39, 174.27s/it]                                                        {'loss': 0.6796, 'grad_norm': 0.44313761591911316, 'learning_rate': 5.881900594968667e-05, 'epoch': 1.5}
 50%|█████     | 350/699 [17:35:31<16:53:39, 174.27s/it] 50%|█████     | 351/699 [17:38:14<16:32:08, 171.06s/it] 50%|█████     | 352/699 [17:40:29<15:26:17, 160.16s/it] 51%|█████     | 353/699 [17:43:20<15:42:50, 163.50s/it] 51%|█████     | 354/699 [17:45:57<15:29:15, 161.61s/it] 51%|█████     | 355/699 [17:48:32<15:14:33, 159.52s/it] 51%|█████     | 356/699 [17:51:16<15:19:04, 160.77s/it] 51%|█████     | 357/699 [17:54:58<17:01:01, 179.13s/it] 51%|█████     | 358/699 [17:57:31<16:13:41, 171.32s/it] 51%|█████▏    | 359/699 [18:00:58<17:11:14, 181.98s/it] 52%|█████▏    | 360/699 [18:03:40<16:34:49, 176.07s/it]                                                        {'loss': 0.6884, 'grad_norm': 0.4625360369682312, 'learning_rate': 5.6350890987343944e-05, 'epoch': 1.55}
 52%|█████▏    | 360/699 [18:03:40<16:34:49, 176.07s/it] 52%|█████▏    | 361/699 [18:06:46<16:49:10, 179.14s/it] 52%|█████▏    | 362/699 [18:09:44<16:44:18, 178.81s/it] 52%|█████▏    | 363/699 [18:12:20<16:02:50, 171.94s/it] 52%|█████▏    | 364/699 [18:14:45<15:14:15, 163.75s/it] 52%|█████▏    | 365/699 [18:17:09<14:38:24, 157.80s/it] 52%|█████▏    | 366/699 [18:20:26<15:41:54, 169.71s/it] 53%|█████▎    | 367/699 [18:24:01<16:54:15, 183.30s/it] 53%|█████▎    | 368/699 [18:26:12<15:24:28, 167.58s/it] 53%|█████▎    | 369/699 [18:28:41<14:50:56, 161.99s/it] 53%|█████▎    | 370/699 [18:31:29<14:57:13, 163.63s/it]                                                        {'loss': 0.6997, 'grad_norm': 0.46732425689697266, 'learning_rate': 5.386693647538248e-05, 'epoch': 1.59}
 53%|█████▎    | 370/699 [18:31:29<14:57:13, 163.63s/it] 53%|█████▎    | 371/699 [18:34:23<15:12:49, 166.98s/it] 53%|█████▎    | 372/699 [18:37:35<15:50:15, 174.36s/it] 53%|█████▎    | 373/699 [18:40:34<15:55:32, 175.87s/it] 54%|█████▎    | 374/699 [18:43:32<15:55:27, 176.39s/it] 54%|█████▎    | 375/699 [18:46:28<15:51:53, 176.27s/it] 54%|█████▍    | 376/699 [18:49:18<15:38:21, 174.31s/it] 54%|█████▍    | 377/699 [18:51:39<14:41:38, 164.28s/it] 54%|█████▍    | 378/699 [18:54:16<14:28:02, 162.25s/it] 54%|█████▍    | 379/699 [18:57:14<14:51:05, 167.08s/it] 54%|█████▍    | 380/699 [19:00:05<14:54:00, 168.15s/it]                                                        {'loss': 0.6839, 'grad_norm': 0.44749268889427185, 'learning_rate': 5.137333756355707e-05, 'epoch': 1.63}
 54%|█████▍    | 380/699 [19:00:05<14:54:00, 168.15s/it] 55%|█████▍    | 381/699 [19:02:32<14:18:11, 161.92s/it] 55%|█████▍    | 382/699 [19:05:30<14:40:18, 166.62s/it] 55%|█████▍    | 383/699 [19:07:59<14:10:15, 161.44s/it] 55%|█████▍    | 384/699 [19:10:44<14:12:51, 162.45s/it] 55%|█████▌    | 385/699 [19:13:02<13:31:42, 155.10s/it] 55%|█████▌    | 386/699 [19:15:42<13:36:03, 156.43s/it] 55%|█████▌    | 387/699 [19:18:10<13:20:51, 154.01s/it] 56%|█████▌    | 388/699 [19:21:09<13:56:48, 161.44s/it] 56%|█████▌    | 389/699 [19:23:58<14:06:19, 163.80s/it] 56%|█████▌    | 390/699 [19:27:34<15:24:40, 179.55s/it]                                                        {'loss': 0.6802, 'grad_norm': 0.4614831507205963, 'learning_rate': 4.8876313455404934e-05, 'epoch': 1.68}
 56%|█████▌    | 390/699 [19:27:34<15:24:40, 179.55s/it] 56%|█████▌    | 391/699 [19:30:25<15:08:14, 176.93s/it] 56%|█████▌    | 392/699 [19:33:35<15:25:29, 180.88s/it] 56%|█████▌    | 393/699 [19:37:00<15:58:21, 187.91s/it] 56%|█████▋    | 394/699 [19:40:27<16:24:34, 193.69s/it] 57%|█████▋    | 395/699 [19:43:21<15:51:40, 187.83s/it] 57%|█████▋    | 396/699 [19:47:03<16:40:26, 198.11s/it] 57%|█████▋    | 397/699 [19:50:01<16:07:21, 192.19s/it] 57%|█████▋    | 398/699 [19:52:34<15:03:55, 180.18s/it] 57%|█████▋    | 399/699 [19:55:35<15:03:01, 180.60s/it] 57%|█████▋    | 400/699 [19:58:00<14:06:41, 169.90s/it]                                                        {'loss': 0.6872, 'grad_norm': 0.43416520953178406, 'learning_rate': 4.638209189713351e-05, 'epoch': 1.72}
 57%|█████▋    | 400/699 [19:58:00<14:06:41, 169.90s/it]
***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1
  Num examples = 3305

***** Running Evaluation *****
  Batch size = 1
  Num examples = 3305
  Batch size = 1
[INFO|trainer.py:4643] 2026-01-03 01:37:09,271 >> 
***** Running Evaluation *****
[INFO|trainer.py:4645] 2026-01-03 01:37:09,271 >>   Num examples = 3305
[INFO|trainer.py:4648] 2026-01-03 01:37:09,271 >>   Batch size = 1

  0%|          | 0/414 [00:00<?, ?it/s][A
  0%|          | 2/414 [00:03<12:47,  1.86s/it][A
  1%|          | 3/414 [00:07<19:01,  2.78s/it][A
  1%|          | 4/414 [00:12<24:27,  3.58s/it][A
  1%|          | 5/414 [00:21<37:05,  5.44s/it][A
  1%|▏         | 6/414 [00:25<32:28,  4.78s/it][A
  2%|▏         | 7/414 [00:28<29:10,  4.30s/it][A
  2%|▏         | 8/414 [00:32<29:00,  4.29s/it][A
  2%|▏         | 9/414 [00:36<27:06,  4.02s/it][A
  2%|▏         | 10/414 [00:42<32:11,  4.78s/it][A
  3%|▎         | 11/414 [00:48<33:44,  5.02s/it][A
  3%|▎         | 12/414 [00:51<31:15,  4.67s/it][A
  3%|▎         | 13/414 [00:58<35:03,  5.24s/it][A
  3%|▎         | 14/414 [01:03<35:02,  5.26s/it][A
  4%|▎         | 15/414 [01:09<36:19,  5.46s/it][A
  4%|▍         | 16/414 [01:14<35:11,  5.30s/it][A
  4%|▍         | 17/414 [01:21<38:41,  5.85s/it][A
  4%|▍         | 18/414 [01:25<34:28,  5.22s/it][A
  5%|▍         | 19/414 [01:32<37:52,  5.75s/it][A
  5%|▍         | 20/414 [01:39<40:32,  6.17s/it][A
  5%|▌         | 21/414 [01:44<36:58,  5.64s/it][A
  5%|▌         | 22/414 [01:47<32:55,  5.04s/it][A
  6%|▌         | 23/414 [01:52<33:01,  5.07s/it][A
  6%|▌         | 24/414 [01:58<34:43,  5.34s/it][A
  6%|▌         | 25/414 [02:02<31:46,  4.90s/it][A
  6%|▋         | 26/414 [02:09<35:56,  5.56s/it][A
  7%|▋         | 27/414 [02:13<31:15,  4.85s/it][A
  7%|▋         | 28/414 [02:17<29:32,  4.59s/it][A
  7%|▋         | 29/414 [02:21<29:03,  4.53s/it][A
  7%|▋         | 30/414 [02:26<29:33,  4.62s/it][A
  7%|▋         | 31/414 [02:32<32:10,  5.04s/it][A
  8%|▊         | 32/414 [02:35<28:33,  4.49s/it][A
  8%|▊         | 33/414 [02:39<27:38,  4.35s/it][A
  8%|▊         | 34/414 [02:45<30:50,  4.87s/it][A
  8%|▊         | 35/414 [02:49<29:40,  4.70s/it][A
  9%|▊         | 36/414 [02:55<30:54,  4.91s/it][A
  9%|▉         | 37/414 [02:59<29:37,  4.71s/it][A
  9%|▉         | 38/414 [03:05<31:30,  5.03s/it][A
  9%|▉         | 39/414 [03:08<28:41,  4.59s/it][A
 10%|▉         | 40/414 [03:12<26:26,  4.24s/it][A
 10%|▉         | 41/414 [03:17<28:19,  4.56s/it][A
 10%|█         | 42/414 [03:22<28:25,  4.58s/it][A
 10%|█         | 43/414 [03:27<29:20,  4.74s/it][A
 11%|█         | 44/414 [03:38<41:10,  6.68s/it][A
 11%|█         | 45/414 [03:45<42:22,  6.89s/it][A
 11%|█         | 46/414 [03:49<36:22,  5.93s/it][A
 11%|█▏        | 47/414 [03:52<30:43,  5.02s/it][A
 12%|█▏        | 48/414 [03:56<27:48,  4.56s/it][A
 12%|█▏        | 49/414 [03:58<23:48,  3.91s/it][A
 12%|█▏        | 50/414 [04:01<21:20,  3.52s/it][A
 12%|█▏        | 51/414 [04:04<20:44,  3.43s/it][A
 13%|█▎        | 52/414 [04:07<20:08,  3.34s/it][A
 13%|█▎        | 53/414 [04:11<22:17,  3.70s/it][A
 13%|█▎        | 54/414 [04:20<30:34,  5.09s/it][A
 13%|█▎        | 55/414 [04:25<30:26,  5.09s/it][A
 14%|█▎        | 56/414 [04:31<32:35,  5.46s/it][A
 14%|█▍        | 57/414 [04:35<29:25,  4.94s/it][A
 14%|█▍        | 58/414 [04:40<28:48,  4.86s/it][A
 14%|█▍        | 59/414 [04:46<31:40,  5.35s/it][A
 14%|█▍        | 60/414 [04:52<31:45,  5.38s/it][A
 15%|█▍        | 61/414 [04:55<28:30,  4.85s/it][A
 15%|█▍        | 62/414 [04:58<25:28,  4.34s/it][A
 15%|█▌        | 63/414 [05:02<24:26,  4.18s/it][A
 15%|█▌        | 64/414 [05:06<23:59,  4.11s/it][A
 16%|█▌        | 65/414 [05:10<24:00,  4.13s/it][A
 16%|█▌        | 66/414 [05:15<25:58,  4.48s/it][A
 16%|█▌        | 67/414 [05:21<27:04,  4.68s/it][A
 16%|█▋        | 68/414 [05:26<28:23,  4.92s/it][A
 17%|█▋        | 69/414 [05:31<28:22,  4.93s/it][A
 17%|█▋        | 70/414 [05:38<31:21,  5.47s/it][A
 17%|█▋        | 71/414 [05:42<28:52,  5.05s/it][A
 17%|█▋        | 72/414 [05:45<26:18,  4.62s/it][A
 18%|█▊        | 73/414 [05:50<25:57,  4.57s/it][A
 18%|█▊        | 74/414 [05:53<23:50,  4.21s/it][A
 18%|█▊        | 75/414 [05:56<21:37,  3.83s/it][A
 18%|█▊        | 76/414 [05:59<19:53,  3.53s/it][A
 19%|█▊        | 77/414 [06:03<21:04,  3.75s/it][A
 19%|█▉        | 78/414 [06:09<24:24,  4.36s/it][A
 19%|█▉        | 79/414 [06:13<23:20,  4.18s/it][A
 19%|█▉        | 80/414 [06:18<25:22,  4.56s/it][A
 20%|█▉        | 81/414 [06:22<23:18,  4.20s/it][A
 20%|█▉        | 82/414 [06:26<22:42,  4.10s/it][A
 20%|██        | 83/414 [06:31<24:16,  4.40s/it][A
 20%|██        | 84/414 [06:39<31:18,  5.69s/it][A
 21%|██        | 85/414 [06:43<27:25,  5.00s/it][A
 21%|██        | 86/414 [06:47<26:09,  4.79s/it][A
 21%|██        | 87/414 [06:50<23:04,  4.23s/it][A
 21%|██▏       | 88/414 [06:54<23:14,  4.28s/it][A
 21%|██▏       | 89/414 [06:58<22:50,  4.22s/it][A
 22%|██▏       | 90/414 [07:05<26:00,  4.82s/it][A
 22%|██▏       | 91/414 [07:10<26:35,  4.94s/it][A
 22%|██▏       | 92/414 [07:14<24:31,  4.57s/it][A
 22%|██▏       | 93/414 [07:19<25:18,  4.73s/it][A
 23%|██▎       | 94/414 [07:22<22:58,  4.31s/it][A
 23%|██▎       | 95/414 [07:27<23:28,  4.42s/it][A
 23%|██▎       | 96/414 [07:32<24:20,  4.59s/it][A
 23%|██▎       | 97/414 [07:37<24:57,  4.72s/it][A
 24%|██▎       | 98/414 [07:40<22:39,  4.30s/it][A
 24%|██▍       | 99/414 [07:43<20:30,  3.91s/it][A
 24%|██▍       | 100/414 [07:48<21:21,  4.08s/it][A
 24%|██▍       | 101/414 [07:53<23:59,  4.60s/it][A
 25%|██▍       | 102/414 [08:01<28:09,  5.41s/it][A
 25%|██▍       | 103/414 [08:04<25:26,  4.91s/it][A
 25%|██▌       | 104/414 [08:08<23:27,  4.54s/it][A
 25%|██▌       | 105/414 [08:12<22:46,  4.42s/it][A
 26%|██▌       | 106/414 [08:17<23:55,  4.66s/it][A
 26%|██▌       | 107/414 [08:24<27:03,  5.29s/it][A
 26%|██▌       | 108/414 [08:28<24:49,  4.87s/it][A
 26%|██▋       | 109/414 [08:33<24:34,  4.83s/it][A
 27%|██▋       | 110/414 [08:37<22:50,  4.51s/it][A
 27%|██▋       | 111/414 [08:42<24:30,  4.85s/it][A
 27%|██▋       | 112/414 [08:46<23:31,  4.67s/it][A
 27%|██▋       | 113/414 [08:50<21:49,  4.35s/it][A
 28%|██▊       | 114/414 [08:56<24:41,  4.94s/it][A
 28%|██▊       | 115/414 [09:01<24:49,  4.98s/it][A
 28%|██▊       | 116/414 [09:05<22:41,  4.57s/it][A
 28%|██▊       | 117/414 [09:11<24:55,  5.04s/it][A
 29%|██▊       | 118/414 [09:18<27:24,  5.55s/it][A
 29%|██▊       | 119/414 [09:24<27:52,  5.67s/it][A
 29%|██▉       | 120/414 [09:31<30:16,  6.18s/it][A
 29%|██▉       | 121/414 [09:36<27:55,  5.72s/it][A
 29%|██▉       | 122/414 [09:43<29:24,  6.04s/it][A
 30%|██▉       | 123/414 [09:47<26:46,  5.52s/it][A
 30%|██▉       | 124/414 [09:53<26:50,  5.55s/it][A
 30%|███       | 125/414 [09:58<26:49,  5.57s/it][A
 30%|███       | 126/414 [10:04<26:39,  5.55s/it][A
 31%|███       | 127/414 [10:08<24:04,  5.03s/it][A
 31%|███       | 128/414 [10:17<30:29,  6.40s/it][A
 31%|███       | 129/414 [10:23<29:05,  6.13s/it][A
 31%|███▏      | 130/414 [10:29<29:48,  6.30s/it][A
 32%|███▏      | 131/414 [10:35<28:24,  6.02s/it][A
 32%|███▏      | 132/414 [10:43<31:21,  6.67s/it][A
 32%|███▏      | 133/414 [10:46<26:49,  5.73s/it][A
 32%|███▏      | 134/414 [10:51<24:56,  5.34s/it][A
 33%|███▎      | 135/414 [10:57<25:51,  5.56s/it][A
 33%|███▎      | 136/414 [11:01<24:11,  5.22s/it][A
 33%|███▎      | 137/414 [11:10<28:57,  6.27s/it][A
 33%|███▎      | 138/414 [11:17<29:47,  6.48s/it][A
 34%|███▎      | 139/414 [11:21<26:16,  5.73s/it][A
 34%|███▍      | 140/414 [11:24<22:25,  4.91s/it][A
 34%|███▍      | 141/414 [11:28<21:08,  4.65s/it][A
 34%|███▍      | 142/414 [11:32<19:37,  4.33s/it][A
 35%|███▍      | 143/414 [11:37<21:34,  4.78s/it][A
 35%|███▍      | 144/414 [11:41<19:42,  4.38s/it][A
 35%|███▌      | 145/414 [11:45<18:58,  4.23s/it][A
 35%|███▌      | 146/414 [11:48<17:09,  3.84s/it][A
 36%|███▌      | 147/414 [11:52<16:59,  3.82s/it][A
 36%|███▌      | 148/414 [11:56<17:42,  3.99s/it][A
 36%|███▌      | 149/414 [12:00<17:59,  4.08s/it][A
 36%|███▌      | 150/414 [12:03<16:51,  3.83s/it][A
 36%|███▋      | 151/414 [12:07<16:12,  3.70s/it][A
 37%|███▋      | 152/414 [12:10<15:52,  3.63s/it][A
 37%|███▋      | 153/414 [12:15<17:40,  4.06s/it][A
 37%|███▋      | 154/414 [12:18<16:21,  3.77s/it][A
 37%|███▋      | 155/414 [12:22<15:36,  3.62s/it][A
 38%|███▊      | 156/414 [12:25<14:53,  3.46s/it][A
 38%|███▊      | 157/414 [12:30<17:13,  4.02s/it][A
 38%|███▊      | 158/414 [12:35<17:52,  4.19s/it][A
 38%|███▊      | 159/414 [12:41<20:05,  4.73s/it][A
 39%|███▊      | 160/414 [12:45<19:11,  4.53s/it][A
 39%|███▉      | 161/414 [12:49<19:06,  4.53s/it][A
 39%|███▉      | 162/414 [12:55<20:53,  4.97s/it][A
 39%|███▉      | 163/414 [13:00<20:16,  4.85s/it][A
 40%|███▉      | 164/414 [13:06<21:45,  5.22s/it][A
 40%|███▉      | 165/414 [13:12<23:10,  5.59s/it][A
 40%|████      | 166/414 [13:20<25:08,  6.08s/it][A
 40%|████      | 167/414 [13:26<25:03,  6.09s/it][A
 41%|████      | 168/414 [13:30<22:52,  5.58s/it][A
 41%|████      | 169/414 [13:35<21:28,  5.26s/it][A
 41%|████      | 170/414 [13:38<19:17,  4.74s/it][A
 41%|████▏     | 171/414 [13:42<18:03,  4.46s/it][A
 42%|████▏     | 172/414 [13:46<17:08,  4.25s/it][A
 42%|████▏     | 173/414 [13:52<19:10,  4.77s/it][A
 42%|████▏     | 174/414 [13:55<17:47,  4.45s/it][A
 42%|████▏     | 175/414 [14:01<19:15,  4.83s/it][A
 43%|████▎     | 176/414 [14:06<18:57,  4.78s/it][A
 43%|████▎     | 177/414 [14:10<17:48,  4.51s/it][A
 43%|████▎     | 178/414 [14:15<19:02,  4.84s/it][A
 43%|████▎     | 179/414 [14:19<17:57,  4.58s/it][A
 43%|████▎     | 180/414 [14:23<16:42,  4.28s/it][A
 44%|████▎     | 181/414 [14:28<17:56,  4.62s/it][A
 44%|████▍     | 182/414 [14:32<17:18,  4.48s/it][A
 44%|████▍     | 183/414 [14:37<17:03,  4.43s/it][A
 44%|████▍     | 184/414 [14:42<17:59,  4.69s/it][A
 45%|████▍     | 185/414 [14:46<16:39,  4.36s/it][A
 45%|████▍     | 186/414 [14:50<17:01,  4.48s/it][A
 45%|████▌     | 187/414 [14:59<21:39,  5.72s/it][A
 45%|████▌     | 188/414 [15:09<26:30,  7.04s/it][A
 46%|████▌     | 189/414 [15:14<23:51,  6.36s/it][A
 46%|████▌     | 190/414 [15:23<27:11,  7.28s/it][A
 46%|████▌     | 191/414 [15:27<23:12,  6.25s/it][A
 46%|████▋     | 192/414 [15:33<22:07,  5.98s/it][A
 47%|████▋     | 193/414 [15:36<19:38,  5.33s/it][A
 47%|████▋     | 194/414 [15:41<18:32,  5.06s/it][A
 47%|████▋     | 195/414 [15:45<17:53,  4.90s/it][A
 47%|████▋     | 196/414 [15:48<15:51,  4.37s/it][A
 48%|████▊     | 197/414 [15:53<16:25,  4.54s/it][A
 48%|████▊     | 198/414 [15:59<17:58,  4.99s/it][A
 48%|████▊     | 199/414 [16:03<16:25,  4.59s/it][A
 48%|████▊     | 200/414 [16:08<16:29,  4.63s/it][A
 49%|████▊     | 201/414 [16:12<16:24,  4.62s/it][A
 49%|████▉     | 202/414 [16:18<17:36,  4.98s/it][A
 49%|████▉     | 203/414 [16:23<17:09,  4.88s/it][A
 49%|████▉     | 204/414 [16:26<15:25,  4.41s/it][A
 50%|████▉     | 205/414 [16:30<14:20,  4.12s/it][A
 50%|████▉     | 206/414 [16:34<14:09,  4.09s/it][A
 50%|█████     | 207/414 [16:38<14:45,  4.28s/it][A
 50%|█████     | 208/414 [16:43<14:57,  4.36s/it][A
 50%|█████     | 209/414 [16:49<16:50,  4.93s/it][A
 51%|█████     | 210/414 [16:54<17:03,  5.02s/it][A
 51%|█████     | 211/414 [16:59<16:50,  4.98s/it][A
 51%|█████     | 212/414 [17:03<15:46,  4.69s/it][A
 51%|█████▏    | 213/414 [17:09<16:55,  5.05s/it][A
 52%|█████▏    | 214/414 [17:15<17:27,  5.24s/it][A
 52%|█████▏    | 215/414 [17:19<16:32,  4.98s/it][A
 52%|█████▏    | 216/414 [17:26<18:30,  5.61s/it][A
 52%|█████▏    | 217/414 [17:35<21:34,  6.57s/it][A
 53%|█████▎    | 218/414 [17:39<18:45,  5.74s/it][A
 53%|█████▎    | 219/414 [17:43<17:11,  5.29s/it][A
 53%|█████▎    | 220/414 [17:48<16:21,  5.06s/it][A
 53%|█████▎    | 221/414 [17:52<15:13,  4.74s/it][A
 54%|█████▎    | 222/414 [17:57<15:32,  4.86s/it][A
 54%|█████▍    | 223/414 [18:03<16:36,  5.22s/it][A
 54%|█████▍    | 224/414 [18:08<16:03,  5.07s/it][A
 54%|█████▍    | 225/414 [18:14<17:11,  5.46s/it][A
 55%|█████▍    | 226/414 [18:19<16:16,  5.19s/it][A
 55%|█████▍    | 227/414 [18:26<18:07,  5.81s/it][A
 55%|█████▌    | 228/414 [18:32<18:47,  6.06s/it][A
 55%|█████▌    | 229/414 [18:37<17:12,  5.58s/it][A
 56%|█████▌    | 230/414 [18:43<17:43,  5.78s/it][A
 56%|█████▌    | 231/414 [18:50<18:27,  6.05s/it][A
 56%|█████▌    | 232/414 [18:54<16:22,  5.40s/it][A
 56%|█████▋    | 233/414 [18:59<16:34,  5.49s/it][A
 57%|█████▋    | 234/414 [19:03<15:06,  5.04s/it][A
 57%|█████▋    | 235/414 [19:09<15:19,  5.14s/it][A
 57%|█████▋    | 236/414 [19:14<15:29,  5.22s/it][A
 57%|█████▋    | 237/414 [19:19<15:23,  5.22s/it][A
 57%|█████▋    | 238/414 [19:23<13:38,  4.65s/it][A
 58%|█████▊    | 239/414 [19:26<12:42,  4.36s/it][A
 58%|█████▊    | 240/414 [19:33<14:55,  5.15s/it][A
 58%|█████▊    | 241/414 [19:40<15:45,  5.46s/it][A
 58%|█████▊    | 242/414 [19:43<13:43,  4.79s/it][A
 59%|█████▊    | 243/414 [19:46<12:16,  4.31s/it][A
 59%|█████▉    | 244/414 [19:50<12:14,  4.32s/it][A
 59%|█████▉    | 245/414 [19:56<13:20,  4.74s/it][A
 59%|█████▉    | 246/414 [20:00<12:23,  4.43s/it][A
 60%|█████▉    | 247/414 [20:04<12:02,  4.33s/it][A
 60%|█████▉    | 248/414 [20:08<12:13,  4.42s/it][A
 60%|██████    | 249/414 [20:12<11:31,  4.19s/it][A
 60%|██████    | 250/414 [20:16<11:08,  4.07s/it][A
 61%|██████    | 251/414 [20:20<10:44,  3.95s/it][A
 61%|██████    | 252/414 [20:26<12:29,  4.62s/it][A
 61%|██████    | 253/414 [20:30<12:21,  4.61s/it][A
 61%|██████▏   | 254/414 [20:39<15:26,  5.79s/it][A
 62%|██████▏   | 255/414 [20:43<13:42,  5.17s/it][A
 62%|██████▏   | 256/414 [20:46<12:21,  4.69s/it][A
 62%|██████▏   | 257/414 [20:51<12:01,  4.60s/it][A
 62%|██████▏   | 258/414 [20:54<10:56,  4.21s/it][A
 63%|██████▎   | 259/414 [20:57<10:22,  4.02s/it][A
 63%|██████▎   | 260/414 [21:01<09:47,  3.81s/it][A
 63%|██████▎   | 261/414 [21:06<10:49,  4.25s/it][A
 63%|██████▎   | 262/414 [21:10<10:25,  4.12s/it][A
 64%|██████▎   | 263/414 [21:17<12:58,  5.16s/it][A
 64%|██████▍   | 264/414 [21:23<13:16,  5.31s/it][A
 64%|██████▍   | 265/414 [21:28<13:05,  5.27s/it][A
 64%|██████▍   | 266/414 [21:33<12:16,  4.98s/it][A
 64%|██████▍   | 267/414 [21:38<12:44,  5.20s/it][A
 65%|██████▍   | 268/414 [21:45<13:31,  5.56s/it][A
 65%|██████▍   | 269/414 [21:48<11:30,  4.76s/it][A
 65%|██████▌   | 270/414 [21:51<10:24,  4.34s/it][A
 65%|██████▌   | 271/414 [21:55<09:58,  4.19s/it][A
 66%|██████▌   | 272/414 [22:00<10:33,  4.46s/it][A
 66%|██████▌   | 273/414 [22:05<11:03,  4.71s/it][A
 66%|██████▌   | 274/414 [22:10<11:13,  4.81s/it][A
 66%|██████▋   | 275/414 [22:14<10:25,  4.50s/it][A
 67%|██████▋   | 276/414 [22:19<10:27,  4.55s/it][A
 67%|██████▋   | 277/414 [22:22<09:53,  4.33s/it][A
 67%|██████▋   | 278/414 [22:26<09:20,  4.12s/it][A
 67%|██████▋   | 279/414 [22:29<08:38,  3.84s/it][A
 68%|██████▊   | 280/414 [22:34<09:17,  4.16s/it][A
 68%|██████▊   | 281/414 [22:38<09:15,  4.17s/it][A
 68%|██████▊   | 282/414 [22:43<09:24,  4.28s/it][A
 68%|██████▊   | 283/414 [22:49<10:40,  4.89s/it][A
 69%|██████▊   | 284/414 [22:53<09:49,  4.54s/it][A
 69%|██████▉   | 285/414 [22:57<09:18,  4.33s/it][A
 69%|██████▉   | 286/414 [23:03<10:39,  5.00s/it][A
 69%|██████▉   | 287/414 [23:10<11:18,  5.34s/it][A
 70%|██████▉   | 288/414 [23:15<11:14,  5.35s/it][A
 70%|██████▉   | 289/414 [23:18<09:59,  4.79s/it][A
 70%|███████   | 290/414 [23:22<09:08,  4.42s/it][A
 70%|███████   | 291/414 [23:30<11:02,  5.39s/it][A
 71%|███████   | 292/414 [23:33<09:54,  4.87s/it][A
 71%|███████   | 293/414 [23:39<10:09,  5.04s/it][A
 71%|███████   | 294/414 [23:50<13:37,  6.81s/it][A
 71%|███████▏  | 295/414 [23:54<12:06,  6.10s/it][A
 71%|███████▏  | 296/414 [24:02<13:10,  6.70s/it][A
 72%|███████▏  | 297/414 [24:07<11:52,  6.09s/it][A
 72%|███████▏  | 298/414 [24:12<11:03,  5.72s/it][A
 72%|███████▏  | 299/414 [24:19<11:44,  6.13s/it][A
 72%|███████▏  | 300/414 [24:23<10:25,  5.48s/it][A
 73%|███████▎  | 301/414 [24:28<10:01,  5.32s/it][A
 73%|███████▎  | 302/414 [24:31<08:51,  4.74s/it][A
 73%|███████▎  | 303/414 [24:35<08:18,  4.50s/it][A
 73%|███████▎  | 304/414 [24:40<08:32,  4.66s/it][A
 74%|███████▎  | 305/414 [24:49<10:33,  5.82s/it][A
 74%|███████▍  | 306/414 [24:53<09:46,  5.44s/it][A
 74%|███████▍  | 307/414 [24:58<09:19,  5.23s/it][A
 74%|███████▍  | 308/414 [25:03<09:18,  5.27s/it][A
 75%|███████▍  | 309/414 [25:10<10:09,  5.81s/it][A
 75%|███████▍  | 310/414 [25:14<08:56,  5.16s/it][A
 75%|███████▌  | 311/414 [25:17<08:01,  4.67s/it][A
 75%|███████▌  | 312/414 [25:22<07:38,  4.50s/it][A
 76%|███████▌  | 313/414 [25:26<07:27,  4.43s/it][A
 76%|███████▌  | 314/414 [25:30<07:06,  4.27s/it][A
 76%|███████▌  | 315/414 [25:33<06:45,  4.09s/it][A
 76%|███████▋  | 316/414 [25:39<07:37,  4.66s/it][A
 77%|███████▋  | 317/414 [25:48<09:33,  5.91s/it][A
 77%|███████▋  | 318/414 [25:52<08:34,  5.36s/it][A
 77%|███████▋  | 319/414 [25:58<08:26,  5.33s/it][A
 77%|███████▋  | 320/414 [26:01<07:32,  4.82s/it][A
 78%|███████▊  | 321/414 [26:07<07:43,  4.98s/it][A
 78%|███████▊  | 322/414 [26:13<08:11,  5.34s/it][A
 78%|███████▊  | 323/414 [26:17<07:37,  5.03s/it][A
 78%|███████▊  | 324/414 [26:20<06:47,  4.53s/it][A
 79%|███████▊  | 325/414 [26:25<06:34,  4.43s/it][A
 79%|███████▊  | 326/414 [26:28<06:06,  4.17s/it][A
 79%|███████▉  | 327/414 [26:33<06:27,  4.45s/it][A
 79%|███████▉  | 328/414 [26:40<07:28,  5.22s/it][A
 79%|███████▉  | 329/414 [26:45<07:10,  5.06s/it][A
 80%|███████▉  | 330/414 [26:52<08:01,  5.73s/it][A
 80%|███████▉  | 331/414 [26:58<08:02,  5.82s/it][A
 80%|████████  | 332/414 [27:04<07:56,  5.81s/it][A
 80%|████████  | 333/414 [27:10<07:48,  5.79s/it][A
 81%|████████  | 334/414 [27:14<07:09,  5.37s/it][A
 81%|████████  | 335/414 [27:17<06:12,  4.72s/it][A
 81%|████████  | 336/414 [27:22<06:09,  4.73s/it][A
 81%|████████▏ | 337/414 [27:26<05:37,  4.39s/it][A
 82%|████████▏ | 338/414 [27:29<05:08,  4.05s/it][A
 82%|████████▏ | 339/414 [27:35<05:53,  4.72s/it][A
 82%|████████▏ | 340/414 [27:41<06:13,  5.05s/it][A
 82%|████████▏ | 341/414 [27:49<07:00,  5.76s/it][A
 83%|████████▎ | 342/414 [27:52<06:04,  5.07s/it][A
 83%|████████▎ | 343/414 [27:56<05:40,  4.80s/it][A
 83%|████████▎ | 344/414 [28:05<06:59,  5.99s/it][A
 83%|████████▎ | 345/414 [28:11<06:55,  6.03s/it][A
 84%|████████▎ | 346/414 [28:15<06:11,  5.47s/it][A
 84%|████████▍ | 347/414 [28:19<05:24,  4.85s/it][A
 84%|████████▍ | 348/414 [28:24<05:22,  4.89s/it][A
 84%|████████▍ | 349/414 [28:27<04:46,  4.41s/it][A
 85%|████████▍ | 350/414 [28:32<04:53,  4.58s/it][A
 85%|████████▍ | 351/414 [28:40<05:50,  5.57s/it][A
 85%|████████▌ | 352/414 [28:44<05:19,  5.15s/it][A
 85%|████████▌ | 353/414 [28:47<04:32,  4.47s/it][A
 86%|████████▌ | 354/414 [28:51<04:25,  4.42s/it][A
 86%|████████▌ | 355/414 [28:55<04:09,  4.24s/it][A
 86%|████████▌ | 356/414 [28:58<03:51,  4.00s/it][A
 86%|████████▌ | 357/414 [29:02<03:38,  3.84s/it][A
 86%|████████▋ | 358/414 [29:09<04:26,  4.76s/it][A
 87%|████████▋ | 359/414 [29:15<04:49,  5.26s/it][A
 87%|████████▋ | 360/414 [29:19<04:23,  4.89s/it][A
 87%|████████▋ | 361/414 [29:23<03:58,  4.51s/it][A
 87%|████████▋ | 362/414 [29:27<03:47,  4.38s/it][A
 88%|████████▊ | 363/414 [29:35<04:35,  5.41s/it][A
 88%|████████▊ | 364/414 [29:39<04:13,  5.07s/it][A
 88%|████████▊ | 365/414 [29:44<04:06,  5.03s/it][A
 88%|████████▊ | 366/414 [29:50<04:17,  5.36s/it][A
 89%|████████▊ | 367/414 [29:56<04:19,  5.53s/it][A
 89%|████████▉ | 368/414 [30:00<03:55,  5.13s/it][A
 89%|████████▉ | 369/414 [30:06<03:56,  5.26s/it][A
 89%|████████▉ | 370/414 [30:12<04:02,  5.52s/it][A
 90%|████████▉ | 371/414 [30:17<03:51,  5.38s/it][A
 90%|████████▉ | 372/414 [30:24<04:01,  5.76s/it][A
 90%|█████████ | 373/414 [30:28<03:43,  5.45s/it][A
 90%|█████████ | 374/414 [30:35<03:55,  5.88s/it][A
 91%|█████████ | 375/414 [30:40<03:37,  5.57s/it][A
 91%|█████████ | 376/414 [30:44<03:09,  4.98s/it][A
 91%|█████████ | 377/414 [30:47<02:47,  4.53s/it][A
 91%|█████████▏| 378/414 [30:54<03:08,  5.24s/it][A
 92%|█████████▏| 379/414 [30:59<03:03,  5.24s/it][A
 92%|█████████▏| 380/414 [31:03<02:46,  4.89s/it][A
 92%|█████████▏| 381/414 [31:14<03:37,  6.59s/it][A
 92%|█████████▏| 382/414 [31:22<03:49,  7.16s/it][A
 93%|█████████▎| 383/414 [31:26<03:12,  6.21s/it][A
 93%|█████████▎| 384/414 [31:30<02:47,  5.59s/it][A
 93%|█████████▎| 385/414 [31:36<02:41,  5.56s/it][A
 93%|█████████▎| 386/414 [31:40<02:24,  5.17s/it][A
 93%|█████████▎| 387/414 [31:44<02:10,  4.83s/it][A
 94%|█████████▎| 388/414 [31:48<01:57,  4.51s/it][A
 94%|█████████▍| 389/414 [31:52<01:51,  4.45s/it][A
 94%|█████████▍| 390/414 [31:55<01:37,  4.06s/it][A
 94%|█████████▍| 391/414 [31:59<01:29,  3.89s/it][A
 95%|█████████▍| 392/414 [32:03<01:27,  3.97s/it][A
 95%|█████████▍| 393/414 [32:07<01:21,  3.89s/it][A
 95%|█████████▌| 394/414 [32:13<01:30,  4.51s/it][A
 95%|█████████▌| 395/414 [32:17<01:21,  4.30s/it][A
 96%|█████████▌| 396/414 [32:21<01:17,  4.29s/it][A
 96%|█████████▌| 397/414 [32:28<01:27,  5.15s/it][A
 96%|█████████▌| 398/414 [32:34<01:24,  5.31s/it][A
 96%|█████████▋| 399/414 [32:37<01:10,  4.71s/it][A
 97%|█████████▋| 400/414 [32:42<01:07,  4.81s/it][A
 97%|█████████▋| 401/414 [32:52<01:24,  6.49s/it][A
 97%|█████████▋| 402/414 [32:59<01:17,  6.45s/it][A
 97%|█████████▋| 403/414 [33:02<01:00,  5.49s/it][A
 98%|█████████▊| 404/414 [33:06<00:49,  4.92s/it][A
 98%|█████████▊| 405/414 [33:09<00:40,  4.54s/it][A
 98%|█████████▊| 406/414 [33:13<00:34,  4.35s/it][A
 98%|█████████▊| 407/414 [33:20<00:35,  5.10s/it][A
 99%|█████████▊| 408/414 [33:26<00:31,  5.23s/it][A
 99%|█████████▉| 409/414 [33:30<00:24,  4.85s/it][A
 99%|█████████▉| 410/414 [33:35<00:20,  5.14s/it][A
 99%|█████████▉| 411/414 [33:39<00:13,  4.63s/it][A
100%|█████████▉| 412/414 [33:43<00:08,  4.38s/it][A
100%|█████████▉| 413/414 [33:50<00:05,  5.28s/it][A
100%|██████████| 414/414 [33:53<00:00,  4.58s/it][A                                                        
                                                 [A{'eval_loss': 0.6789088845252991, 'eval_runtime': 2039.6002, 'eval_samples_per_second': 1.62, 'eval_steps_per_second': 0.203, 'epoch': 1.72}
 57%|█████▋    | 400/699 [20:32:00<14:06:41, 169.90s/it]
100%|██████████| 414/414 [33:53<00:00,  4.58s/it][A
                                                 [A[INFO|trainer.py:4309] 2026-01-03 02:11:17,038 >> Saving model checkpoint to saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in Qwen/Qwen2.5-VL-7B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2421] 2026-01-03 02:11:17,181 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2026-01-03 02:11:17,183 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2026-01-03 02:11:17,185 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/special_tokens_map.json
/project/6110552/indrisch/venv_llamafactory_cu126/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once
[2026-01-03 02:11:17,775] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step400 is about to be saved!
[2026-01-03 02:11:17,808] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/global_step400/mp_rank_00_model_states.pt
[2026-01-03 02:11:17,808] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/global_step400/mp_rank_00_model_states.pt...
[2026-01-03 02:11:17,887] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/global_step400/mp_rank_00_model_states.pt.
[2026-01-03 02:11:17,892] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2026-01-03 02:11:17,892] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/global_step400/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2026-01-03 02:11:17,892] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/global_step400/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2026-01-03 02:11:17,892] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/global_step400/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2026-01-03 02:11:17,892] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/global_step400/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2026-01-03 02:11:17,892] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/global_step400/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2026-01-03 02:11:17,892] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/global_step400/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2026-01-03 02:11:17,892] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/global_step400/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2026-01-03 02:11:17,998] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2026-01-03 02:11:18,006] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/global_step400/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2026-01-03 02:11:18,007] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/global_step400/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2026-01-03 02:11:18,007] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2026-01-03 02:11:18,035] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/global_step400/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2026-01-03 02:11:18,035] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/global_step400/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2026-01-03 02:11:18,035] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2026-01-03 02:11:18,035] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/global_step400/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2026-01-03 02:11:18,035] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/global_step400/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2026-01-03 02:11:18,035] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2026-01-03 02:11:18,035] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/global_step400/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2026-01-03 02:11:18,036] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/global_step400/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2026-01-03 02:11:18,036] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2026-01-03 02:11:18,040] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/global_step400/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2026-01-03 02:11:18,040] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/global_step400/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt
[2026-01-03 02:11:18,041] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2026-01-03 02:11:18,043] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/global_step400/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2026-01-03 02:11:18,043] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/global_step400/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2026-01-03 02:11:18,043] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2026-01-03 02:11:18,043] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2026-01-03 02:11:18,044] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2026-01-03 02:11:18,047] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/global_step400/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2026-01-03 02:11:18,047] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/global_step400/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt
[2026-01-03 02:11:18,047] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[INFO|image_processing_base.py:253] 2026-01-03 02:11:18,071 >> Image processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/preprocessor_config.json
[INFO|tokenization_utils_base.py:2421] 2026-01-03 02:11:18,074 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2026-01-03 02:11:18,076 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2026-01-03 02:11:18,079 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/special_tokens_map.json
[INFO|video_processing_utils.py:600] 2026-01-03 02:11:18,264 >> Video processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/video_preprocessor_config.json
[INFO|processing_utils.py:814] 2026-01-03 02:11:18,266 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval_native8gpu_epoch3plus/checkpoint-400/chat_template.jinja
 57%|█████▋    | 401/699 [20:35:49<66:10:52, 799.51s/it] 58%|█████▊    | 402/699 [20:39:27<51:34:20, 625.12s/it] 58%|█████▊    | 403/699 [20:42:01<39:46:50, 483.82s/it] 58%|█████▊    | 404/699 [20:44:44<31:45:13, 387.50s/it] 58%|█████▊    | 405/699 [20:47:43<26:32:46, 325.06s/it] 58%|█████▊    | 406/699 [20:51:23<23:53:36, 293.57s/it] 58%|█████▊    | 407/699 [20:54:57<21:52:47, 269.75s/it] 58%|█████▊    | 408/699 [20:57:37<19:07:38, 236.63s/it] 59%|█████▊    | 409/699 [21:00:01<16:49:14, 208.81s/it] 59%|█████▊    | 410/699 [21:02:44<15:39:22, 195.02s/it]                                                        {'loss': 0.6812, 'grad_norm': 0.44385990500450134, 'learning_rate': 4.389689364520219e-05, 'epoch': 1.76}
 59%|█████▊    | 410/699 [21:02:44<15:39:22, 195.02s/it] 59%|█████▉    | 411/699 [21:05:27<14:50:06, 185.44s/it] 59%|█████▉    | 412/699 [21:08:36<14:52:32, 186.60s/it] 59%|█████▉    | 413/699 [21:10:54<13:39:59, 172.03s/it] 59%|█████▉    | 414/699 [21:14:32<14:43:10, 185.93s/it] 59%|█████▉    | 415/699 [21:17:17<14:10:16, 179.64s/it] 60%|█████▉    | 416/699 [21:20:12<13:59:36, 178.01s/it] 60%|█████▉    | 417/699 [21:23:29<14:23:43, 183.77s/it] 60%|█████▉    | 418/699 [21:26:27<14:12:42, 182.07s/it] 60%|█████▉    | 419/699 [21:30:10<15:07:01, 194.36s/it] 60%|██████    | 420/699 [21:33:36<15:19:46, 197.80s/it]                                                        {'loss': 0.6832, 'grad_norm': 0.4715149700641632, 'learning_rate': 4.142691695133698e-05, 'epoch': 1.8}
 60%|██████    | 420/699 [21:33:36<15:19:46, 197.80s/it] 60%|██████    | 421/699 [21:36:00<14:02:24, 181.81s/it] 60%|██████    | 422/699 [21:39:13<14:14:35, 185.11s/it] 61%|██████    | 423/699 [21:42:09<13:58:28, 182.28s/it] 61%|██████    | 424/699 [21:45:18<14:05:45, 184.53s/it] 61%|██████    | 425/699 [21:48:07<13:40:35, 179.69s/it] 61%|██████    | 426/699 [21:50:31<12:49:22, 169.09s/it] 61%|██████    | 427/699 [21:52:54<12:10:31, 161.15s/it] 61%|██████    | 428/699 [21:55:46<12:22:11, 164.32s/it] 61%|██████▏   | 429/699 [21:59:04<13:04:58, 174.44s/it] 62%|██████▏   | 430/699 [22:02:40<13:58:03, 186.93s/it]                                                        {'loss': 0.672, 'grad_norm': 0.42143094539642334, 'learning_rate': 3.8978322103673397e-05, 'epoch': 1.85}
 62%|██████▏   | 430/699 [22:02:40<13:58:03, 186.93s/it] 62%|██████▏   | 431/699 [22:05:28<13:29:29, 181.23s/it] 62%|██████▏   | 432/699 [22:08:19<13:13:40, 178.36s/it] 62%|██████▏   | 433/699 [22:11:40<13:39:49, 184.92s/it] 62%|██████▏   | 434/699 [22:14:31<13:18:27, 180.78s/it] 62%|██████▏   | 435/699 [22:16:52<12:22:49, 168.83s/it] 62%|██████▏   | 436/699 [22:19:09<11:39:22, 159.55s/it] 63%|██████▎   | 437/699 [22:21:19<10:57:20, 150.54s/it] 63%|██████▎   | 438/699 [22:24:40<12:00:28, 165.63s/it] 63%|██████▎   | 439/699 [22:27:50<12:29:36, 172.99s/it] 63%|██████▎   | 440/699 [22:30:49<12:34:02, 174.68s/it]                                                        {'loss': 0.6832, 'grad_norm': 0.42355185747146606, 'learning_rate': 3.655721606258334e-05, 'epoch': 1.89}
 63%|██████▎   | 440/699 [22:30:49<12:34:02, 174.68s/it] 63%|██████▎   | 441/699 [22:33:41<12:28:27, 174.06s/it] 63%|██████▎   | 442/699 [22:36:28<12:15:58, 171.82s/it] 63%|██████▎   | 443/699 [22:39:35<12:32:56, 176.47s/it] 64%|██████▎   | 444/699 [22:42:34<12:32:40, 177.10s/it] 64%|██████▎   | 445/699 [22:45:12<12:05:32, 171.39s/it] 64%|██████▍   | 446/699 [22:48:02<12:01:07, 171.02s/it] 64%|██████▍   | 447/699 [22:50:21<11:17:30, 161.31s/it] 64%|██████▍   | 448/699 [22:54:02<12:30:24, 179.38s/it] 64%|██████▍   | 449/699 [22:56:44<12:05:28, 174.11s/it] 64%|██████▍   | 450/699 [23:00:01<12:31:42, 181.13s/it]                                                        {'loss': 0.6747, 'grad_norm': 0.4644629657268524, 'learning_rate': 3.416963722950472e-05, 'epoch': 1.93}
 64%|██████▍   | 450/699 [23:00:01<12:31:42, 181.13s/it] 65%|██████▍   | 451/699 [23:02:36<11:55:47, 173.18s/it] 65%|██████▍   | 452/699 [23:05:03<11:20:53, 165.40s/it] 65%|██████▍   | 453/699 [23:07:57<11:28:32, 167.94s/it] 65%|██████▍   | 454/699 [23:10:26<11:02:00, 162.12s/it] 65%|██████▌   | 455/699 [23:13:49<11:49:29, 174.46s/it] 65%|██████▌   | 456/699 [23:16:50<11:54:17, 176.37s/it] 65%|██████▌   | 457/699 [23:19:53<11:59:41, 178.44s/it] 66%|██████▌   | 458/699 [23:22:36<11:37:31, 173.66s/it] 66%|██████▌   | 459/699 [23:25:33<11:39:15, 174.81s/it] 66%|██████▌   | 460/699 [23:28:20<11:26:40, 172.39s/it]                                                        {'loss': 0.6766, 'grad_norm': 0.45688948035240173, 'learning_rate': 3.1821540386761894e-05, 'epoch': 1.98}
 66%|██████▌   | 460/699 [23:28:20<11:26:40, 172.39s/it] 66%|██████▌   | 461/699 [23:31:21<11:34:30, 175.09s/it] 66%|██████▌   | 462/699 [23:33:58<11:10:19, 169.70s/it] 66%|██████▌   | 463/699 [23:36:22<10:37:08, 161.98s/it] 66%|██████▋   | 464/699 [23:39:11<10:42:46, 164.11s/it] 67%|██████▋   | 465/699 [23:41:33<10:13:12, 157.24s/it] 67%|██████▋   | 466/699 [23:42:42<8:28:01, 130.82s/it]  67%|██████▋   | 467/699 [23:45:43<9:23:59, 145.86s/it] 67%|██████▋   | 468/699 [23:48:01<9:12:51, 143.60s/it] 67%|██████▋   | 469/699 [23:50:49<9:38:26, 150.90s/it] 67%|██████▋   | 470/699 [23:53:26<9:42:37, 152.65s/it]                                                       {'loss': 0.6864, 'grad_norm': 0.4774983823299408, 'learning_rate': 2.9518781845937626e-05, 'epoch': 2.02}
 67%|██████▋   | 470/699 [23:53:26<9:42:37, 152.65s/it] 67%|██████▋   | 471/699 [23:56:12<9:55:45, 156.78s/it]slurmstepd: error: *** JOB 1736112 ON kn176 CANCELLED AT 2026-01-03T05:36:54 DUE TO TIME LIMIT ***
