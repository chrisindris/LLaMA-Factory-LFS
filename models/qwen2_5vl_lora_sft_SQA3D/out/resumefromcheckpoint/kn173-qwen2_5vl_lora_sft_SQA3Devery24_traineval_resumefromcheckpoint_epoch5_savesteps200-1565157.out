
==========
== CUDA ==
==========

CUDA Version 12.4.1

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

[INFO|2025-12-08 11:07:23] llamafactory.launcher:143 >> Initializing 4 distributed tasks at: 127.0.0.1:43065
W1208 11:07:24.788000 481903 site-packages/torch/distributed/run.py:792] 
W1208 11:07:24.788000 481903 site-packages/torch/distributed/run.py:792] *****************************************
W1208 11:07:24.788000 481903 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1208 11:07:24.788000 481903 site-packages/torch/distributed/run.py:792] *****************************************
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
[2025-12-08 11:07:34,927] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-08 11:07:34,927] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-08 11:07:34,937] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-08 11:07:34,937] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2025-12-08 11:07:41,755] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-08 11:07:42,069] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-08 11:07:42,069] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-12-08 11:07:42,097] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-08 11:07:42,169] [INFO] [comm.py:669:init_distributed] cdb=None
[INFO|2025-12-08 11:07:42] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.
[INFO|2025-12-08 11:07:42] llamafactory.hparams.parser:423 >> Process rank: 0, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-12-08 11:07:42] llamafactory.hparams.parser:423 >> Process rank: 2, world size: 4, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
[INFO|hub.py:421] 2025-12-08 11:07:42,391 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2025-12-08 11:07:42,480 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2025-12-08 11:07:42,490 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-12-08 11:07:42,490 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-12-08 11:07:42,490 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-12-08 11:07:42,490 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-08 11:07:42,490 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-08 11:07:42,490 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-12-08 11:07:42,491 >> loading file chat_template.jinja from cache at None
[INFO|2025-12-08 11:07:42] llamafactory.hparams.parser:423 >> Process rank: 1, world size: 4, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-12-08 11:07:42] llamafactory.hparams.parser:423 >> Process rank: 3, world size: 4, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2364] 2025-12-08 11:07:42,883 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|hub.py:421] 2025-12-08 11:07:42,883 >> Offline mode: forcing local_files_only=True
[INFO|hub.py:421] 2025-12-08 11:07:42,884 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:316] 2025-12-08 11:07:42,886 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2025-12-08 11:07:42,888 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:316] 2025-12-08 11:07:42,901 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2025-12-08 11:07:42,903 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:428] 2025-12-08 11:07:42,942 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:1949] 2025-12-08 11:07:42,942 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2025-12-08 11:07:42,949 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-12-08 11:07:42,949 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-12-08 11:07:42,949 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-12-08 11:07:42,949 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-08 11:07:42,949 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-08 11:07:42,949 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-12-08 11:07:42,949 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-12-08 11:07:43,175 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:660] 2025-12-08 11:07:43,186 >> Offline mode: forcing local_files_only=True
[INFO|video_processing_utils.py:726] 2025-12-08 11:07:43,188 >> loading configuration file video_preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|video_processing_utils.py:770] 2025-12-08 11:07:43,231 >> Video processor Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:928] 2025-12-08 11:07:43,231 >> Offline mode: forcing local_files_only=True
[INFO|processing_utils.py:1116] 2025-12-08 11:07:43,238 >> loading configuration file processor_config.json from cache at None
[INFO|processing_utils.py:1199] 2025-12-08 11:07:43,613 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2_5_VLProcessor"
}

[INFO|2025-12-08 11:07:43] llamafactory.data.loader:143 >> Loading dataset /scratch/indrisch/huggingface/hub/datasets--cvis-tmu--llamafactory-sqa3d-traces-multiimage-vqa/snapshots/ce5c54adc1608d1726730c9ff334e65b6dd70e46/data/...
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
[rank2]:[W1208 11:07:43.419845758 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W1208 11:07:43.468417583 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W1208 11:07:43.482801973 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank0]:[W1208 11:07:44.026210618 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
training example:
input_ids:
[151644, 8948, 271, 262, 1446, 525, 264, 16585, 11129, 4142, 11528, 17847, 369, 27979, 647, 47522, 389, 264, 3175, 29519, 6109, 6839, 3941, 5248, 5335, 382, 262, 26149, 510, 262, 68252, 1283, 419, 1943, 11, 498, 686, 5258, 451, 6109, 5335, 438, 366, 1805, 29, 9492, 13, 18877, 1105, 438, 2155, 6194, 315, 279, 83490, 6109, 13, 28634, 311, 5335, 553, 220, 16, 5980, 1922, 304, 1973, 315, 11094, 25, 508, 16, 1125, 508, 17, 1125, 4593, 11, 508, 45, 29562, 262, 3596, 35304, 1718, 8575, 38439, 3298, 14017, 510, 262, 7288, 7854, 264, 12966, 220, 18, 35, 10502, 1614, 3941, 6194, 25, 3754, 6171, 3941, 5335, 11, 23583, 8674, 6249, 17040, 11, 323, 990, 5452, 14, 509, 8957, 14688, 21060, 369, 7990, 55916, 624, 262, 7288, 72077, 409, 849, 292, 3793, 26087, 983, 847, 1290, 97089, 17996, 4065, 14, 29898, 484, 32511, 56111, 7612, 1825, 35978, 28455, 448, 5091, 311, 279, 64171, 14, 8092, 13, 1416, 279, 58385, 374, 54761, 11, 1638, 311, 279, 1429, 38219, 8622, 1651, 323, 1977, 773, 624, 262, 7288, 5443, 1172, 9434, 5904, 26, 653, 537, 17023, 63133, 3565, 476, 17188, 389, 9250, 6540, 7797, 6770, 1633, 43898, 3793, 624, 262, 7288, 3197, 5248, 11178, 3000, 11, 8830, 553, 279, 1850, 2432, 311, 27979, 17133, 488, 4274, 1835, 3941, 6194, 13, 1416, 2058, 54761, 1283, 13295, 678, 5335, 11, 1584, 429, 9355, 382, 262, 30990, 320, 327, 32739, 1378, 10010, 11, 304, 419, 1973, 982, 262, 366, 26865, 397, 262, 14822, 14319, 29208, 32711, 304, 220, 18, 4142, 16, 15, 2805, 7354, 13, 356, 632, 5904, 448, 2168, 14937, 320, 68, 1302, 2572, 65750, 18, 5669, 3100, 7579, 18010, 1290, 315, 8718, 26, 508, 16, 5669, 1852, 1894, 11, 42396, 64212, 2823, 63594, 714, 4583, 320, 2640, 37294, 17, 20, 15, 11211, 7241, 5871, 568, 7036, 894, 17796, 8957, 14, 2969, 26745, 487, 323, 1246, 498, 19673, 432, 624, 262, 690, 26865, 397, 262, 366, 9217, 397, 262, 362, 3175, 63594, 4226, 1172, 11, 892, 1231, 387, 510, 262, 7288, 1416, 14016, 25, 3776, 37328, 4226, 11, 4504, 320, 68, 1302, 2572, 1036, 17, 32511, 476, 7414, 14, 2753, 320, 68, 1302, 2572, 1036, 9693, 854, 4292, 262, 7288, 1416, 537, 14016, 25, 362, 11652, 476, 1378, 624, 262, 1416, 4226, 537, 9434, 11, 421, 1052, 374, 902, 2797, 5904, 11, 476, 421, 279, 4226, 374, 35197, 54761, 25, 1036, 33260, 8253, 854, 624, 262, 690, 9217, 1339, 262, 62947, 609, 43797, 50, 510, 262, 7288, 3155, 4183, 13153, 279, 3405, 476, 1140, 5335, 26, 653, 4183, 2550, 4718, 476, 4960, 14158, 624, 262, 7288, 84468, 4285, 1894, 5036, 448, 518, 1429, 825, 22739, 26087, 4145, 3446, 838, 4322, 1574, 58689, 7256, 854, 4292, 262, 7288, 2823, 12966, 3941, 6194, 26, 421, 6194, 28295, 11, 62552, 279, 11299, 15432, 5904, 323, 5185, 432, 304, 366, 26865, 29816, 257, 151645, 198, 151644, 872, 198, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 30, 220, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151645, 198, 151644, 77091, 198, 198, 32462, 432, 11, 1077, 594, 1430, 311, 7071, 700, 1128, 594, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 62410, 13, 5512, 11, 1077, 594, 50087, 279, 14852, 6505, 13, 576, 21921, 374, 4363, 264, 4158, 11, 1378, 1737, 95112, 4982, 13, 2411, 220, 16, 16, 297, 62410, 320, 3481, 7950, 3082, 701, 582, 1184, 311, 1779, 429, 7702, 13, 21131, 518, 279, 5335, 11, 5310, 279, 825, 9027, 279, 21921, 3082, 11, 1052, 594, 264, 17846, 3403, 279, 19309, 11, 714, 3783, 11, 902, 2293, 11489, 11, 279, 3405, 374, 911, 279, 21921, 13, 13824, 11, 7196, 279, 330, 66, 75512, 1, 1588, 374, 279, 5819, 4982, 13, 13824, 11, 902, 11, 279, 19309, 702, 264, 21921, 3685, 30, 13824, 11, 902, 11, 279, 1887, 21921, 374, 279, 4158, 61607, 13, 13824, 11, 304, 279, 2168, 1380, 279, 6249, 4933, 279, 19309, 323, 21921, 11, 279, 1909, 315, 279, 21921, 2293, 11489, 11, 902, 11, 7196, 279, 17846, 30, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 2727, 330, 65, 77832, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 3263, 2055, 220, 16, 16, 297, 62410, 1035, 387, 279, 8416, 2115, 3082, 8674, 311, 279, 19309, 13, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 279, 19309, 3082, 13, 2160, 429, 949, 315, 279, 21921, 6505, 30, 13824, 11, 902, 11, 7196, 279, 21921, 374, 279, 4158, 61607, 1212, 279, 19309, 13, 13824, 11, 902, 11, 279, 1909, 315, 429, 21921, 2293, 11489, 11, 902, 11, 304, 1045, 5335, 11, 1052, 594, 264, 17846, 25911, 476, 21810, 13, 13824, 11, 902, 11, 1077, 594, 1779, 279, 1156, 2421, 5335, 13, 576, 17196, 45766, 374, 33866, 11, 1221, 279, 19309, 3082, 13, 13824, 11, 279, 4396, 4226, 25, 3330, 518, 279, 5335, 11, 279, 1633, 389, 1909, 315, 279, 14852, 21921, 320, 1782, 4158, 26482, 4982, 8, 518, 220, 16, 16, 297, 62410, 374, 264, 17846, 13, 13824, 11, 902, 11, 3783, 21677, 17846, 374, 389, 279, 7002, 3403, 279, 19309, 13, 13824, 11, 7196, 358, 5786, 878, 13, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 594, 3405, 25, 330, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 7521, 2055, 1156, 11, 10542, 279, 21921, 13, 576, 4158, 26482, 4982, 3143, 220, 16, 16, 297, 62410, 320, 3481, 7950, 568, 3555, 594, 389, 1909, 30, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 429, 3082, 13, 13824, 11, 9834, 0, 758, 279, 21844, 476, 21810, 11, 1052, 594, 264, 17846, 13, 2055, 279, 4226, 374, 264, 17846, 4192, 1249, 8253, 1128, 374, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 527, 20666, 11, 23643, 279, 27979, 6789, 323, 5424, 304, 279, 5335, 25, 18611, 16, 13, 95210, 279, 1036, 65, 77832, 21921, 854, 304, 3405, 25, 362, 4158, 11, 1378, 481, 26482, 5819, 4982, 374, 9434, 3143, 279, 1909, 481, 2115, 320, 16, 16, 297, 527, 20666, 8, 3082, 8674, 311, 1008, 37664, 1075, 279, 19309, 323, 17196, 13, 2303, 17, 13, 1374, 19991, 6171, 3403, 419, 21921, 25, 758, 279, 5335, 11, 264, 3070, 73225, 334, 374, 21810, 389, 279, 7002, 3403, 419, 21921, 3082, 13, 79226, 54216, 151645, 198]
inputs:
<|im_start|>system

    You are a careful vision–language assistant for spatial VQA on a single indoor scene shown across multiple images.

    INPUT:
    Immediately after this message, you will receive N scene images as <image> tags. Treat them as different views of the SAME scene. Refer to images by 1-based index in order of appearance: [1], [2], …, [N].

    REASONING PRINCIPLES:
    • Build a consistent 3D mental model across views: track objects across images, infer relative camera pose, and use scale/occlusion/shadows for depth cues.
    • Interpret deictic terms (“to my right/left/in front/behind”) EGOCENTRICALLY with respect to the narrator/agent. If the viewpoint is ambiguous, default to the most informative central view and say so.
    • Use only visible evidence; do not invent unseen details or rely on external knowledge beyond basic object/color terms.
    • When multiple candidates exist, resolve by the best match to spatial phrase + salience across views. If still ambiguous after checking all images, state that clearly.

    OUTPUT (exactly two blocks, in this order):
    <think>
    Step-by-step reasoning in 3–10 short steps. Cite evidence with image indices (e.g., “[3]: light wood desk right of monitor; [1]: same color, confirms”). Be concise but complete (aim ≤250 tokens unless necessary). Note any occlusion/ambiguity and how you resolved it.
    </think>
    <answer>
    A single concise answer only, which may be:
    • If sufficient: One-word answer, Count (e.g., “2”) or Yes/No (e.g., “yes”).
    • If not sufficient: A sentence or two.
    If answer not visible, if there is no clear evidence, or if the answer is genuinely ambiguous: “cannot determine”.
    </answer>

    STYLE & RULES:
    • Do NOT repeat the question or list images; do NOT output JSON or extra sections.
    • Prefer simple color names with at most one modifier (“light/dark/pale/beige”).
    • Be consistent across views; if views disagree, prioritize the clearest evidence and note it in <think>.
    <|im_end|>
<|im_start|>user
What is on top of the bathroom cabinet that is on my 11 o'clock? <|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|im_end|>
<|im_start|>assistant

Got it, let's try to figure out what's on top of the bathroom cabinet at 11 o'clock. First, let's visualize the bathroom setup. The cabinet is likely a white, two-drawer unit. At 11 o'clock (top-left area), we need to check that spot. Looking at the images, especially the one showing the cabinet area, there's a mirror above the sink, but wait, no—wait, the question is about the cabinet. Wait, maybe the "cabinet" here is the storage unit. Wait, no, the sink has a cabinet below? Wait, no, the main cabinet is the white drawers. Wait, in the image where the camera shows the sink and cabinet, the top of the cabinet—wait, no, maybe the mirror? Wait, no, let's check again. Wait, the user says "bathroom cabinet that is on my 11 o'clock". So 11 o'clock would be the upper left area relative to the sink. Looking at the images, there's a mirror mounted on the wall above the sink area. Is that part of the cabinet setup? Wait, no, maybe the cabinet is the white drawers under the sink. Wait, no, the top of that cabinet—wait, no, in some images, there's a mirror reflected or mounted. Wait, no, let's check the first few images. The shower curtain is colorful, then the sink area. Wait, the correct answer: looking at the images, the object on top of the bathroom cabinet (the white drawer unit) at 11 o'clock is a mirror. Wait, no, wait—the mirror is on the wall above the sink. Wait, maybe I misread. Wait, no, let's check again. Wait, the user's question: "What is on top of the bathroom cabinet that is on my 11 o'clock?" So first, identify the cabinet. The white drawer unit near 11 o'clock (top-left). What's on top? Looking at the images, there's a mirror mounted on the wall above that area. Wait, yes! In the reflection or mounted, there's a mirror. So the answer is a mirror.


To determine what is on top of the bathroom cabinet at 11 o’clock, analyze the spatial layout and elements in the images:  

1. Locate the “bathroom cabinet” in question: A white, two - drawer storage unit is visible near the top - left (11 o’clock) area relative to other fixtures like the sink and shower.  
2. Examine objects above this cabinet: In the images, a **mirror** is mounted on the wall above this cabinet area.  


Mirror<|im_end|>

label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 198, 32462, 432, 11, 1077, 594, 1430, 311, 7071, 700, 1128, 594, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 62410, 13, 5512, 11, 1077, 594, 50087, 279, 14852, 6505, 13, 576, 21921, 374, 4363, 264, 4158, 11, 1378, 1737, 95112, 4982, 13, 2411, 220, 16, 16, 297, 62410, 320, 3481, 7950, 3082, 701, 582, 1184, 311, 1779, 429, 7702, 13, 21131, 518, 279, 5335, 11, 5310, 279, 825, 9027, 279, 21921, 3082, 11, 1052, 594, 264, 17846, 3403, 279, 19309, 11, 714, 3783, 11, 902, 2293, 11489, 11, 279, 3405, 374, 911, 279, 21921, 13, 13824, 11, 7196, 279, 330, 66, 75512, 1, 1588, 374, 279, 5819, 4982, 13, 13824, 11, 902, 11, 279, 19309, 702, 264, 21921, 3685, 30, 13824, 11, 902, 11, 279, 1887, 21921, 374, 279, 4158, 61607, 13, 13824, 11, 304, 279, 2168, 1380, 279, 6249, 4933, 279, 19309, 323, 21921, 11, 279, 1909, 315, 279, 21921, 2293, 11489, 11, 902, 11, 7196, 279, 17846, 30, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 2727, 330, 65, 77832, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 3263, 2055, 220, 16, 16, 297, 62410, 1035, 387, 279, 8416, 2115, 3082, 8674, 311, 279, 19309, 13, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 279, 19309, 3082, 13, 2160, 429, 949, 315, 279, 21921, 6505, 30, 13824, 11, 902, 11, 7196, 279, 21921, 374, 279, 4158, 61607, 1212, 279, 19309, 13, 13824, 11, 902, 11, 279, 1909, 315, 429, 21921, 2293, 11489, 11, 902, 11, 304, 1045, 5335, 11, 1052, 594, 264, 17846, 25911, 476, 21810, 13, 13824, 11, 902, 11, 1077, 594, 1779, 279, 1156, 2421, 5335, 13, 576, 17196, 45766, 374, 33866, 11, 1221, 279, 19309, 3082, 13, 13824, 11, 279, 4396, 4226, 25, 3330, 518, 279, 5335, 11, 279, 1633, 389, 1909, 315, 279, 14852, 21921, 320, 1782, 4158, 26482, 4982, 8, 518, 220, 16, 16, 297, 62410, 374, 264, 17846, 13, 13824, 11, 902, 11, 3783, 21677, 17846, 374, 389, 279, 7002, 3403, 279, 19309, 13, 13824, 11, 7196, 358, 5786, 878, 13, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 594, 3405, 25, 330, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 7521, 2055, 1156, 11, 10542, 279, 21921, 13, 576, 4158, 26482, 4982, 3143, 220, 16, 16, 297, 62410, 320, 3481, 7950, 568, 3555, 594, 389, 1909, 30, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 429, 3082, 13, 13824, 11, 9834, 0, 758, 279, 21844, 476, 21810, 11, 1052, 594, 264, 17846, 13, 2055, 279, 4226, 374, 264, 17846, 4192, 1249, 8253, 1128, 374, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 527, 20666, 11, 23643, 279, 27979, 6789, 323, 5424, 304, 279, 5335, 25, 18611, 16, 13, 95210, 279, 1036, 65, 77832, 21921, 854, 304, 3405, 25, 362, 4158, 11, 1378, 481, 26482, 5819, 4982, 374, 9434, 3143, 279, 1909, 481, 2115, 320, 16, 16, 297, 527, 20666, 8, 3082, 8674, 311, 1008, 37664, 1075, 279, 19309, 323, 17196, 13, 2303, 17, 13, 1374, 19991, 6171, 3403, 419, 21921, 25, 758, 279, 5335, 11, 264, 3070, 73225, 334, 374, 21810, 389, 279, 7002, 3403, 419, 21921, 3082, 13, 79226, 54216, 151645, 198]
labels:

Got it, let's try to figure out what's on top of the bathroom cabinet at 11 o'clock. First, let's visualize the bathroom setup. The cabinet is likely a white, two-drawer unit. At 11 o'clock (top-left area), we need to check that spot. Looking at the images, especially the one showing the cabinet area, there's a mirror above the sink, but wait, no—wait, the question is about the cabinet. Wait, maybe the "cabinet" here is the storage unit. Wait, no, the sink has a cabinet below? Wait, no, the main cabinet is the white drawers. Wait, in the image where the camera shows the sink and cabinet, the top of the cabinet—wait, no, maybe the mirror? Wait, no, let's check again. Wait, the user says "bathroom cabinet that is on my 11 o'clock". So 11 o'clock would be the upper left area relative to the sink. Looking at the images, there's a mirror mounted on the wall above the sink area. Is that part of the cabinet setup? Wait, no, maybe the cabinet is the white drawers under the sink. Wait, no, the top of that cabinet—wait, no, in some images, there's a mirror reflected or mounted. Wait, no, let's check the first few images. The shower curtain is colorful, then the sink area. Wait, the correct answer: looking at the images, the object on top of the bathroom cabinet (the white drawer unit) at 11 o'clock is a mirror. Wait, no, wait—the mirror is on the wall above the sink. Wait, maybe I misread. Wait, no, let's check again. Wait, the user's question: "What is on top of the bathroom cabinet that is on my 11 o'clock?" So first, identify the cabinet. The white drawer unit near 11 o'clock (top-left). What's on top? Looking at the images, there's a mirror mounted on the wall above that area. Wait, yes! In the reflection or mounted, there's a mirror. So the answer is a mirror.


To determine what is on top of the bathroom cabinet at 11 o’clock, analyze the spatial layout and elements in the images:  

1. Locate the “bathroom cabinet” in question: A white, two - drawer storage unit is visible near the top - left (11 o’clock) area relative to other fixtures like the sink and shower.  
2. Examine objects above this cabinet: In the images, a **mirror** is mounted on the wall above this cabinet area.  


Mirror<|im_end|>

[INFO|hub.py:421] 2025-12-08 11:07:47,373 >> Offline mode: forcing local_files_only=True
[INFO|configuration_utils.py:765] 2025-12-08 11:07:47,376 >> loading configuration file config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
[INFO|configuration_utils.py:839] 2025-12-08 11:07:47,379 >> Model config Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 128000,
  "max_window_layers": 28,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "_name_or_path": "Qwen/Qwen2.5-VL-7B-Instruct",
    "architectures": [
      "Qwen2_5_VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "dtype": "bfloat16",
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "layer_types": [
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention"
    ],
    "max_position_embeddings": 128000,
    "max_window_layers": 28,
    "model_type": "qwen2_5_vl_text",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "use_cache": true,
    "use_sliding_window": false,
    "vision_token_id": 151654,
    "vocab_size": 152064
  },
  "tie_word_embeddings": false,
  "transformers_version": "4.57.0",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 3584,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

[INFO|2025-12-08 11:07:47] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
[INFO|2025-12-08 11:07:47] llamafactory.model.model_utils.liger_kernel:143 >> Liger kernel has been applied to the model.
[INFO|hub.py:421] 2025-12-08 11:07:47,833 >> Offline mode: forcing local_files_only=True
[WARNING|logging.py:328] 2025-12-08 11:07:47,835 >> `torch_dtype` is deprecated! Use `dtype` instead!
[INFO|hub.py:421] 2025-12-08 11:07:47,835 >> Offline mode: forcing local_files_only=True
[INFO|modeling_utils.py:4840] 2025-12-08 11:07:47,836 >> Offline mode: forcing local_files_only=True
[INFO|modeling_utils.py:1176] 2025-12-08 11:07:47,840 >> loading weights file model.safetensors from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/model.safetensors.index.json
[INFO|modeling_utils.py:2345] 2025-12-08 11:07:47,845 >> Instantiating Qwen2_5_VLForConditionalGeneration model under default dtype torch.bfloat16.
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO|configuration_utils.py:986] 2025-12-08 11:07:47,865 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

[INFO|modeling_utils.py:2345] 2025-12-08 11:07:47,877 >> Instantiating Qwen2_5_VisionTransformerPretrainedModel model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:2345] 2025-12-08 11:07:47,977 >> Instantiating Qwen2_5_VLTextModel model under default dtype torch.bfloat16.
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:05<00:22,  5.69s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:05<00:23,  5.81s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:05<00:22,  5.69s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:05<00:22,  5.70s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:24<00:39, 13.17s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:24<00:39, 13.12s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:24<00:39, 13.14s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:24<00:39, 13.14s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:41<00:29, 14.94s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:41<00:29, 14.95s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:41<00:29, 14.98s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:41<00:29, 14.95s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:59<00:16, 16.25s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:59<00:16, 16.27s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:59<00:16, 16.26s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:59<00:16, 16.28s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:02<00:00, 11.61s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:02<00:00, 12.56s/it]
[INFO|configuration_utils.py:941] 2025-12-08 11:08:50,952 >> loading configuration file generation_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/generation_config.json
[INFO|configuration_utils.py:986] 2025-12-08 11:08:50,953 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 1e-06
}

[INFO|dynamic_module_utils.py:423] 2025-12-08 11:08:50,954 >> Could not locate the custom_generate/generate.py inside Qwen/Qwen2.5-VL-7B-Instruct.
Loading checkpoint shards: 100%|██████████| 5/5 [01:02<00:00, 11.61s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:02<00:00, 11.62s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:02<00:00, 11.61s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:02<00:00, 12.56s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [01:02<00:00, 12.58s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [01:02<00:00, 12.56s/it]
[INFO|2025-12-08 11:08:50] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2025-12-08 11:08:50] llamafactory.model.model_utils.attention:143 >> Using FlashAttention-2 for faster training and inference.
[INFO|2025-12-08 11:08:50] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.
[INFO|2025-12-08 11:08:50] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA
[INFO|2025-12-08 11:08:51] llamafactory.model.adapter:143 >> Loaded adapter(s): /project/aip-wangcs/indrisch/LLaMA-Factory/saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-465/
[INFO|2025-12-08 11:08:51] llamafactory.model.loader:143 >> trainable params: 20,185,088 || all params: 8,312,351,744 || trainable%: 0.2428
name: base_model.model.model.visual.patch_embed.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.ln_q.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.0.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.0.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.embed_tokens.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.lm_head.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
[INFO|trainer.py:749] 2025-12-08 11:08:51,883 >> Using auto half precision backend
[WARNING|2025-12-08 11:08:51] llamafactory.train.callbacks:154 >> Previous trainer log in this folder will be deleted.
[WARNING|trainer.py:982] 2025-12-08 11:08:51,887 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
[DEBUG|trainer.py:2373] 2025-12-08 11:08:52,167 >> Currently training with a batch size of: 2
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 8. Using DeepSpeed's value.
[INFO|deepspeed.py:383] 2025-12-08 11:08:52,182 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Currently training with a batch size of: 2
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Currently training with a batch size of: 2
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Currently training with a batch size of: 2
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Using /tmp/.cache/torch_extensions as PyTorch extensions root...
Creating extension directory /tmp/.cache/torch_extensions/cpu_adam...
Detected CUDA files, patching ldflags
Emitting ninja build file /tmp/.cache/torch_extensions/cpu_adam/build.ninja...
/opt/conda/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module cpu_adam...
Using envvar MAX_JOBS (16) as the number of workers...
Using /tmp/.cache/torch_extensions as PyTorch extensions root...
Using /tmp/.cache/torch_extensions as PyTorch extensions root...
Using /tmp/.cache/torch_extensions as PyTorch extensions root...
[1/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.11/site-packages/torch/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o 
[2/3] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.11/site-packages/torch/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o 
[3/3] c++ cpu_adam.o cpu_adam_impl.o -shared -lcurand -L/usr/local/cuda/lib64 -L/opt/conda/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so
Loading extension module cpu_adam...
Time to load cpu_adam op: 29.124128103256226 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-12-08 11:09:22,532] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.9, git-hash=unknown, git-branch=unknown
[2025-12-08 11:09:22,532] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
Loading extension module cpu_adam...
Time to load cpu_adam op: 28.772715091705322 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-12-08 11:09:22,535] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
Loading extension module cpu_adam...
Time to load cpu_adam op: 28.61211895942688 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-12-08 11:09:22,572] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
Loading extension module cpu_adam...
Time to load cpu_adam op: 28.767183303833008 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-12-08 11:09:22,626] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
[2025-12-08 11:09:24,088] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-12-08 11:09:24,093] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-12-08 11:09:24,093] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-12-08 11:09:24,120] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2025-12-08 11:09:24,121] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2025-12-08 11:09:24,121] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-12-08 11:09:24,121] [INFO] [stage_1_and_2.py:150:__init__] Reduce bucket size 500000000
[2025-12-08 11:09:24,121] [INFO] [stage_1_and_2.py:151:__init__] Allgather bucket size 500000000
[2025-12-08 11:09:24,121] [INFO] [stage_1_and_2.py:152:__init__] CPU Offload: True
[2025-12-08 11:09:24,121] [INFO] [stage_1_and_2.py:153:__init__] Round robin gradient partitioning: True
***** Running training *****
  Num examples = 29,742
  Num Epochs = 2
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 8
  Total optimization steps = 930
***** Running training *****
  Num examples = 29,742
  Num Epochs = 2
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 8
  Total optimization steps = 930
***** Running training *****
  Num examples = 29,742
  Num Epochs = 2
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 8
  Total optimization steps = 930
  Number of trainable parameters = 20,185,088
  Number of trainable parameters = 20,185,088
  Number of trainable parameters = 20,185,088
[2025-12-08 11:09:24,488] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-12-08 11:09:24,489] [INFO] [utils.py:782:see_memory_usage] MA 15.48 GB         Max_MA 15.48 GB         CA 15.49 GB         Max_CA 15 GB 
[2025-12-08 11:09:24,489] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 41.34 GB, percent = 2.1%
[2025-12-08 11:09:24,729] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-12-08 11:09:24,729] [INFO] [utils.py:782:see_memory_usage] MA 15.48 GB         Max_MA 15.48 GB         CA 15.49 GB         Max_CA 15 GB 
[2025-12-08 11:09:24,729] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 41.36 GB, percent = 2.1%
[2025-12-08 11:09:24,729] [INFO] [stage_1_and_2.py:557:__init__] optimizer state initialized
[2025-12-08 11:09:24,947] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-12-08 11:09:24,947] [INFO] [utils.py:782:see_memory_usage] MA 15.48 GB         Max_MA 15.48 GB         CA 15.49 GB         Max_CA 15 GB 
[2025-12-08 11:09:24,947] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 41.35 GB, percent = 2.1%
[2025-12-08 11:09:24,950] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-12-08 11:09:24,950] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-12-08 11:09:24,950] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-12-08 11:09:24,950] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2025-12-08 11:09:24,954] [INFO] [config.py:1003:print] DeepSpeedEngine configuration:
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   amp_enabled .................. False
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   amp_params ................... False
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   bfloat16_enabled ............. True
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   bfloat16_immediate_grad_update  True
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   checkpoint_parallel_write_pipeline  False
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   checkpoint_tag_validation_enabled  True
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   checkpoint_tag_validation_fail  False
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x155221b4de10>
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   communication_data_type ...... None
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   curriculum_enabled_legacy .... False
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   curriculum_params_legacy ..... False
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   data_efficiency_enabled ...... False
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   dataloader_drop_last ......... False
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   disable_allgather ............ False
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   dump_state ................... False
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   dynamic_loss_scale_args ...... None
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   eigenvalue_enabled ........... False
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   eigenvalue_gas_boundary_resolution  1
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   eigenvalue_layer_num ......... 0
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   eigenvalue_max_iter .......... 100
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   eigenvalue_stability ......... 1e-06
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   eigenvalue_tol ............... 0.01
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   eigenvalue_verbose ........... False
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   elasticity_enabled ........... False
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   fp16_auto_cast ............... None
[2025-12-08 11:09:24,955] [INFO] [config.py:1007:print]   fp16_enabled ................. False
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   fp16_master_weights_and_gradients  False
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   global_rank .................. 0
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   grad_accum_dtype ............. None
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   gradient_accumulation_steps .. 8
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   gradient_clipping ............ 1.0
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   gradient_predivide_factor .... 1.0
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   graph_harvesting ............. False
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   initial_dynamic_scale ........ 1
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   load_universal_checkpoint .... False
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   loss_scale ................... 1.0
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   memory_breakdown ............. False
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   mics_hierarchial_params_gather  False
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   mics_shard_size .............. -1
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   optimizer_legacy_fusion ...... False
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   optimizer_name ............... None
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   optimizer_params ............. None
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   pld_enabled .................. False
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   pld_params ................... False
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   prescale_gradients ........... False
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   scheduler_name ............... None
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   scheduler_params ............. None
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   seq_parallel_communication_data_type  torch.float32
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   sparse_attention ............. None
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   sparse_gradients_enabled ..... False
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   steps_per_print .............. inf
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   timers_config ................ enabled=True synchronized=True
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   train_batch_size ............. 64
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   train_micro_batch_size_per_gpu  2
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   use_data_before_expert_parallel_  False
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   use_node_local_storage ....... False
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   wall_clock_breakdown ......... False
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   weight_quantization_config ... None
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   world_size ................... 4
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   zero_allow_untested_optimizer  True
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=True zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   zero_enabled ................. True
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   zero_force_ds_cpu_optimizer .. True
[2025-12-08 11:09:24,956] [INFO] [config.py:1007:print]   zero_optimization_stage ...... 2
[2025-12-08 11:09:24,956] [INFO] [config.py:993:print_user_config]   json = {
    "train_batch_size": 64, 
    "train_micro_batch_size_per_gpu": 2, 
    "gradient_accumulation_steps": 8, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": false, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true, 
        "round_robin_gradients": true
    }, 
    "steps_per_print": inf
}
[INFO|trainer.py:2519] 2025-12-08 11:09:24,958 >> ***** Running training *****
[INFO|trainer.py:2520] 2025-12-08 11:09:24,958 >>   Num examples = 29,742
[INFO|trainer.py:2521] 2025-12-08 11:09:24,958 >>   Num Epochs = 2
[INFO|trainer.py:2522] 2025-12-08 11:09:24,958 >>   Instantaneous batch size per device = 2
[INFO|trainer.py:2525] 2025-12-08 11:09:24,958 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2526] 2025-12-08 11:09:24,958 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2527] 2025-12-08 11:09:24,958 >>   Total optimization steps = 930
[INFO|trainer.py:2528] 2025-12-08 11:09:24,961 >>   Number of trainable parameters = 20,185,088
[INFO|integration_utils.py:867] 2025-12-08 11:09:24,964 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Tracking run with wandb version 0.23.0
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /project/aip-wangcs/indrisch/LLaMA-Factory/wandb/wandb/offline-run-20251208_110925-bqpfe4gz
  0%|          | 0/930 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /tmp/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /tmp/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /tmp/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /tmp/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
  0%|          | 1/930 [01:51<28:51:52, 111.85s/it]  0%|          | 2/930 [03:22<25:36:07, 99.32s/it]   0%|          | 3/930 [05:27<28:35:07, 111.01s/it]  0%|          | 4/930 [07:15<28:17:35, 110.00s/it]  1%|          | 5/930 [09:03<28:04:52, 109.29s/it]  1%|          | 6/930 [10:36<26:35:00, 103.57s/it]  1%|          | 7/930 [12:01<24:59:40, 97.49s/it]   1%|          | 8/930 [13:46<25:36:52, 100.01s/it]  1%|          | 9/930 [15:41<26:44:34, 104.53s/it]  1%|          | 10/930 [17:16<25:59:24, 101.70s/it]                                                    {'loss': 0.72, 'grad_norm': 0.22806836664676666, 'learning_rate': 9.67741935483871e-06, 'epoch': 0.02}
  1%|          | 10/930 [17:16<25:59:24, 101.70s/it]  1%|          | 11/930 [18:48<25:11:39, 98.69s/it]   1%|▏         | 12/930 [20:25<25:03:29, 98.27s/it]  1%|▏         | 13/930 [22:18<26:11:07, 102.80s/it]  2%|▏         | 14/930 [23:51<25:21:22, 99.65s/it]   2%|▏         | 15/930 [25:44<26:24:02, 103.87s/it]  2%|▏         | 16/930 [27:07<24:42:47, 97.34s/it]   2%|▏         | 17/930 [28:39<24:20:38, 95.99s/it]  2%|▏         | 18/930 [30:20<24:39:36, 97.34s/it]  2%|▏         | 19/930 [32:17<26:08:23, 103.30s/it]  2%|▏         | 20/930 [34:07<26:35:50, 105.22s/it]                                                    {'loss': 0.7308, 'grad_norm': 0.26126617193222046, 'learning_rate': 2.0430107526881722e-05, 'epoch': 0.04}
  2%|▏         | 20/930 [34:07<26:35:50, 105.22s/it]  2%|▏         | 21/930 [35:49<26:21:43, 104.40s/it]  2%|▏         | 22/930 [37:26<25:43:08, 101.97s/it]  2%|▏         | 23/930 [39:07<25:40:49, 101.93s/it]  3%|▎         | 24/930 [40:40<24:57:06, 99.15s/it]   3%|▎         | 25/930 [42:34<26:01:19, 103.51s/it]  3%|▎         | 26/930 [44:21<26:18:00, 104.74s/it]  3%|▎         | 27/930 [45:47<24:48:36, 98.91s/it]   3%|▎         | 28/930 [47:25<24:42:37, 98.62s/it]  3%|▎         | 29/930 [49:13<25:25:00, 101.55s/it]  3%|▎         | 30/930 [51:00<25:48:52, 103.26s/it]                                                    {'loss': 0.7101, 'grad_norm': 0.25080791115760803, 'learning_rate': 3.118279569892473e-05, 'epoch': 0.06}
  3%|▎         | 30/930 [51:00<25:48:52, 103.26s/it]  3%|▎         | 31/930 [52:59<26:56:43, 107.90s/it]  3%|▎         | 32/930 [54:42<26:33:49, 106.49s/it]  4%|▎         | 33/930 [56:18<25:44:58, 103.34s/it]  4%|▎         | 34/930 [57:48<24:43:50, 99.36s/it]   4%|▍         | 35/930 [59:29<24:47:02, 99.69s/it]  4%|▍         | 36/930 [1:01:06<24:36:43, 99.11s/it]  4%|▍         | 37/930 [1:02:54<25:11:36, 101.56s/it]  4%|▍         | 38/930 [1:04:39<25:27:28, 102.74s/it]  4%|▍         | 39/930 [1:06:26<25:45:22, 104.07s/it]  4%|▍         | 40/930 [1:08:09<25:36:10, 103.56s/it]                                                      {'loss': 0.6996, 'grad_norm': 0.2831973135471344, 'learning_rate': 4.1935483870967746e-05, 'epoch': 0.09}
  4%|▍         | 40/930 [1:08:09<25:36:10, 103.56s/it]  4%|▍         | 41/930 [1:09:49<25:20:02, 102.59s/it]  5%|▍         | 42/930 [1:11:20<24:25:47, 99.04s/it]   5%|▍         | 43/930 [1:12:59<24:25:15, 99.12s/it]  5%|▍         | 44/930 [1:14:54<25:31:30, 103.71s/it]  5%|▍         | 45/930 [1:16:43<25:54:24, 105.38s/it]  5%|▍         | 46/930 [1:18:24<25:32:37, 104.02s/it]  5%|▌         | 47/930 [1:20:12<25:49:12, 105.27s/it]  5%|▌         | 48/930 [1:22:02<26:08:53, 106.73s/it]  5%|▌         | 49/930 [1:23:31<24:50:06, 101.48s/it]  5%|▌         | 50/930 [1:25:26<25:45:42, 105.39s/it]                                                      {'loss': 0.7092, 'grad_norm': 0.27276116609573364, 'learning_rate': 5.268817204301075e-05, 'epoch': 0.11}
  5%|▌         | 50/930 [1:25:26<25:45:42, 105.39s/it]  5%|▌         | 51/930 [1:27:29<27:03:01, 110.79s/it]  6%|▌         | 52/930 [1:29:19<26:59:03, 110.64s/it]  6%|▌         | 53/930 [1:30:57<26:00:12, 106.74s/it]  6%|▌         | 54/930 [1:32:33<25:10:21, 103.45s/it]  6%|▌         | 55/930 [1:33:59<23:52:01, 98.20s/it]   6%|▌         | 56/930 [1:35:43<24:15:57, 99.95s/it]  6%|▌         | 57/930 [1:37:40<25:28:37, 105.06s/it]  6%|▌         | 58/930 [1:39:26<25:29:41, 105.25s/it]  6%|▋         | 59/930 [1:41:18<25:57:05, 107.26s/it]  6%|▋         | 60/930 [1:42:51<24:53:58, 103.03s/it]                                                      {'loss': 0.7098, 'grad_norm': 0.28840216994285583, 'learning_rate': 6.344086021505376e-05, 'epoch': 0.13}
  6%|▋         | 60/930 [1:42:51<24:53:58, 103.03s/it]  7%|▋         | 61/930 [1:44:25<24:13:48, 100.38s/it]  7%|▋         | 62/930 [1:46:02<24:00:11, 99.55s/it]   7%|▋         | 63/930 [1:47:41<23:52:11, 99.11s/it]  7%|▋         | 64/930 [1:49:10<23:06:29, 96.06s/it]  7%|▋         | 65/930 [1:50:55<23:46:19, 98.94s/it]  7%|▋         | 66/930 [1:52:36<23:54:56, 99.65s/it]  7%|▋         | 67/930 [1:54:15<23:47:18, 99.23s/it]  7%|▋         | 68/930 [1:55:59<24:06:28, 100.68s/it]  7%|▋         | 69/930 [1:57:38<23:56:41, 100.12s/it]  8%|▊         | 70/930 [1:59:05<23:01:44, 96.40s/it]                                                      {'loss': 0.7084, 'grad_norm': 0.3115297853946686, 'learning_rate': 7.419354838709677e-05, 'epoch': 0.15}
  8%|▊         | 70/930 [1:59:05<23:01:44, 96.40s/it]  8%|▊         | 71/930 [2:00:57<24:05:23, 100.96s/it]  8%|▊         | 72/930 [2:02:25<23:06:49, 96.98s/it]   8%|▊         | 73/930 [2:04:04<23:15:19, 97.69s/it]  8%|▊         | 74/930 [2:05:44<23:23:24, 98.37s/it]  8%|▊         | 75/930 [2:07:13<22:40:53, 95.50s/it]  8%|▊         | 76/930 [2:08:55<23:06:17, 97.40s/it]  8%|▊         | 77/930 [2:10:37<23:26:18, 98.92s/it]  8%|▊         | 78/930 [2:12:17<23:28:22, 99.18s/it]  8%|▊         | 79/930 [2:13:57<23:32:03, 99.56s/it]  9%|▊         | 80/930 [2:15:52<24:35:44, 104.17s/it]                                                      {'loss': 0.7264, 'grad_norm': 0.2873910367488861, 'learning_rate': 8.494623655913979e-05, 'epoch': 0.17}
  9%|▊         | 80/930 [2:15:52<24:35:44, 104.17s/it]  9%|▊         | 81/930 [2:17:34<24:22:01, 103.32s/it]  9%|▉         | 82/930 [2:19:27<25:04:00, 106.42s/it]  9%|▉         | 83/930 [2:21:12<24:57:21, 106.07s/it]  9%|▉         | 84/930 [2:22:48<24:12:00, 102.98s/it]  9%|▉         | 85/930 [2:24:04<22:13:42, 94.70s/it]   9%|▉         | 86/930 [2:26:01<23:48:15, 101.54s/it]  9%|▉         | 87/930 [2:27:49<24:12:11, 103.36s/it]  9%|▉         | 88/930 [2:29:32<24:12:09, 103.48s/it] 10%|▉         | 89/930 [2:31:01<23:09:28, 99.13s/it]  10%|▉         | 90/930 [2:32:28<22:13:06, 95.22s/it]                                                     {'loss': 0.7128, 'grad_norm': 0.25397437810897827, 'learning_rate': 9.56989247311828e-05, 'epoch': 0.19}
 10%|▉         | 90/930 [2:32:28<22:13:06, 95.22s/it] 10%|▉         | 91/930 [2:34:05<22:21:05, 95.91s/it] 10%|▉         | 92/930 [2:35:46<22:40:15, 97.39s/it] 10%|█         | 93/930 [2:37:49<24:24:45, 105.00s/it] 10%|█         | 94/930 [2:39:18<23:17:27, 100.30s/it] 10%|█         | 95/930 [2:41:06<23:48:20, 102.64s/it] 10%|█         | 96/930 [2:43:01<24:39:19, 106.43s/it] 10%|█         | 97/930 [2:44:35<23:43:18, 102.52s/it] 11%|█         | 98/930 [2:46:20<23:52:55, 103.34s/it] 11%|█         | 99/930 [2:48:23<25:13:50, 109.30s/it] 11%|█         | 100/930 [2:49:58<24:13:46, 105.09s/it]                                                       {'loss': 0.7167, 'grad_norm': 0.2759314179420471, 'learning_rate': 9.998732135085665e-05, 'epoch': 0.22}
 11%|█         | 100/930 [2:49:58<24:13:46, 105.09s/it] 11%|█         | 101/930 [2:52:03<25:33:03, 110.96s/it] 11%|█         | 102/930 [2:53:49<25:11:17, 109.51s/it] 11%|█         | 103/930 [2:55:43<25:27:43, 110.84s/it] 11%|█         | 104/930 [2:57:18<24:19:28, 106.02s/it] 11%|█▏        | 105/930 [2:58:52<23:29:04, 102.48s/it] 11%|█▏        | 106/930 [3:00:40<23:49:21, 104.08s/it] 12%|█▏        | 107/930 [3:02:13<23:03:05, 100.83s/it] 12%|█▏        | 108/930 [3:03:43<22:17:53, 97.66s/it]  12%|█▏        | 109/930 [3:05:17<22:00:39, 96.52s/it] 12%|█▏        | 110/930 [3:06:54<21:57:54, 96.43s/it]                                                      {'loss': 0.7424, 'grad_norm': 0.24786153435707092, 'learning_rate': 9.990986400130607e-05, 'epoch': 0.24}
 12%|█▏        | 110/930 [3:06:54<21:57:54, 96.43s/it] 12%|█▏        | 111/930 [3:08:45<22:56:07, 100.82s/it] 12%|█▏        | 112/930 [3:10:22<22:41:33, 99.87s/it]  12%|█▏        | 113/930 [3:12:19<23:49:07, 104.95s/it] 12%|█▏        | 114/930 [3:14:05<23:52:00, 105.29s/it] 12%|█▏        | 115/930 [3:15:54<24:03:27, 106.27s/it] 12%|█▏        | 116/930 [3:17:37<23:49:03, 105.34s/it] 13%|█▎        | 117/930 [3:19:31<24:22:31, 107.93s/it] 13%|█▎        | 118/930 [3:21:16<24:08:13, 107.01s/it] 13%|█▎        | 119/930 [3:23:03<24:09:21, 107.23s/it] 13%|█▎        | 120/930 [3:24:31<22:48:29, 101.37s/it]                                                       {'loss': 0.7378, 'grad_norm': 0.26143890619277954, 'learning_rate': 9.976210197283718e-05, 'epoch': 0.26}
 13%|█▎        | 120/930 [3:24:31<22:48:29, 101.37s/it] 13%|█▎        | 121/930 [3:26:28<23:50:11, 106.07s/it] 13%|█▎        | 122/930 [3:28:49<26:09:43, 116.56s/it] 13%|█▎        | 123/930 [3:30:13<23:53:52, 106.61s/it] 13%|█▎        | 124/930 [3:32:00<23:56:43, 106.95s/it] 13%|█▎        | 125/930 [3:33:30<22:45:48, 101.80s/it] 14%|█▎        | 126/930 [3:35:04<22:10:39, 99.30s/it]  14%|█▎        | 127/930 [3:36:46<22:21:30, 100.24s/it] 14%|█▍        | 128/930 [3:38:26<22:16:53, 100.02s/it] 14%|█▍        | 129/930 [3:40:28<23:45:22, 106.77s/it] 14%|█▍        | 130/930 [3:42:20<24:02:11, 108.16s/it]                                                       {'loss': 0.7121, 'grad_norm': 0.2566812336444855, 'learning_rate': 9.954424340791196e-05, 'epoch': 0.28}
 14%|█▍        | 130/930 [3:42:20<24:02:11, 108.16s/it] 14%|█▍        | 131/930 [3:43:56<23:13:19, 104.63s/it] 14%|█▍        | 132/930 [3:45:42<23:18:21, 105.14s/it] 14%|█▍        | 133/930 [3:47:13<22:20:25, 100.91s/it] 14%|█▍        | 134/930 [3:48:49<21:57:37, 99.32s/it]  15%|█▍        | 135/930 [3:50:20<21:23:45, 96.89s/it] 15%|█▍        | 136/930 [3:52:12<22:20:05, 101.27s/it] 15%|█▍        | 137/930 [3:53:43<21:37:32, 98.17s/it]  15%|█▍        | 138/930 [3:55:36<22:36:58, 102.80s/it] 15%|█▍        | 139/930 [3:57:14<22:15:15, 101.28s/it] 15%|█▌        | 140/930 [3:58:58<22:25:32, 102.19s/it]                                                       {'loss': 0.7119, 'grad_norm': 0.2494976669549942, 'learning_rate': 9.925659518928315e-05, 'epoch': 0.3}
 15%|█▌        | 140/930 [3:58:58<22:25:32, 102.19s/it] 15%|█▌        | 141/930 [4:00:31<21:46:51, 99.38s/it]  15%|█▌        | 142/930 [4:02:17<22:09:35, 101.24s/it] 15%|█▌        | 143/930 [4:03:50<21:38:08, 98.97s/it]  15%|█▌        | 144/930 [4:05:28<21:29:57, 98.47s/it] 16%|█▌        | 145/930 [4:07:09<21:40:58, 99.44s/it] 16%|█▌        | 146/930 [4:08:59<22:17:51, 102.39s/it] 16%|█▌        | 147/930 [4:10:48<22:45:32, 104.64s/it] 16%|█▌        | 148/930 [4:12:19<21:49:06, 100.44s/it] 16%|█▌        | 149/930 [4:14:14<22:43:44, 104.77s/it] 16%|█▌        | 150/930 [4:15:54<22:22:22, 103.26s/it]                                                       {'loss': 0.7298, 'grad_norm': 0.2524902820587158, 'learning_rate': 9.889956250770932e-05, 'epoch': 0.32}
 16%|█▌        | 150/930 [4:15:54<22:22:22, 103.26s/it] 16%|█▌        | 151/930 [4:17:33<22:04:09, 101.99s/it] 16%|█▋        | 152/930 [4:19:04<21:20:45, 98.77s/it]  16%|█▋        | 153/930 [4:20:46<21:31:30, 99.73s/it] 17%|█▋        | 154/930 [4:22:28<21:39:30, 100.48s/it] 17%|█▋        | 155/930 [4:24:00<21:03:09, 97.79s/it]  17%|█▋        | 156/930 [4:25:47<21:38:07, 100.63s/it] 17%|█▋        | 157/930 [4:27:08<20:21:51, 94.84s/it]  17%|█▋        | 158/930 [4:28:55<21:06:45, 98.45s/it] 17%|█▋        | 159/930 [4:30:39<21:24:17, 99.94s/it] 17%|█▋        | 160/930 [4:32:13<21:01:58, 98.34s/it]                                                      {'loss': 0.733, 'grad_norm': 0.2716614305973053, 'learning_rate': 9.847364829118963e-05, 'epoch': 0.34}
 17%|█▋        | 160/930 [4:32:13<21:01:58, 98.34s/it] 17%|█▋        | 161/930 [4:33:31<19:42:50, 92.29s/it] 17%|█▋        | 162/930 [4:34:59<19:24:01, 90.94s/it] 18%|█▊        | 163/930 [4:36:32<19:30:49, 91.59s/it] 18%|█▊        | 164/930 [4:38:15<20:10:30, 94.82s/it] 18%|█▊        | 165/930 [4:40:02<20:55:51, 98.50s/it] 18%|█▊        | 166/930 [4:42:05<22:28:48, 105.93s/it] 18%|█▊        | 167/930 [4:43:51<22:26:53, 105.92s/it] 18%|█▊        | 168/930 [4:45:15<21:01:17, 99.31s/it]  18%|█▊        | 169/930 [4:46:41<20:11:18, 95.50s/it] 18%|█▊        | 170/930 [4:48:11<19:47:33, 93.75s/it]                                                      {'loss': 0.7156, 'grad_norm': 0.26536691188812256, 'learning_rate': 9.797945249652295e-05, 'epoch': 0.37}
 18%|█▊        | 170/930 [4:48:11<19:47:33, 93.75s/it] 18%|█▊        | 171/930 [4:49:58<20:36:24, 97.74s/it] 18%|█▊        | 172/930 [4:51:46<21:14:32, 100.89s/it] 19%|█▊        | 173/930 [4:53:19<20:41:11, 98.38s/it]  19%|█▊        | 174/930 [4:54:59<20:46:55, 98.96s/it] 19%|█▉        | 175/930 [4:56:30<20:16:22, 96.67s/it] 19%|█▉        | 176/930 [4:58:15<20:42:54, 98.91s/it] 19%|█▉        | 177/930 [4:59:45<20:09:47, 96.40s/it] 19%|█▉        | 178/930 [5:01:15<19:43:12, 94.40s/it] 19%|█▉        | 179/930 [5:02:42<19:15:37, 92.33s/it] 19%|█▉        | 180/930 [5:04:22<19:42:11, 94.58s/it]                                                      {'loss': 0.7339, 'grad_norm': 0.2579261064529419, 'learning_rate': 9.741767126418897e-05, 'epoch': 0.39}
 19%|█▉        | 180/930 [5:04:22<19:42:11, 94.58s/it] 19%|█▉        | 181/930 [5:05:50<19:16:42, 92.66s/it] 20%|█▉        | 182/930 [5:07:29<19:37:57, 94.49s/it] 20%|█▉        | 183/930 [5:08:53<18:55:06, 91.17s/it] 20%|█▉        | 184/930 [5:10:25<19:00:11, 91.70s/it] 20%|█▉        | 185/930 [5:11:58<19:01:15, 91.91s/it] 20%|██        | 186/930 [5:13:35<19:19:11, 93.48s/it] 20%|██        | 187/930 [5:15:20<20:01:37, 97.04s/it] 20%|██        | 188/930 [5:16:54<19:46:17, 95.93s/it] 20%|██        | 189/930 [5:18:53<21:12:04, 103.00s/it] 20%|██        | 190/930 [5:20:31<20:52:11, 101.53s/it]                                                       {'loss': 0.7335, 'grad_norm': 0.27098366618156433, 'learning_rate': 9.67890959377418e-05, 'epoch': 0.41}
 20%|██        | 190/930 [5:20:31<20:52:11, 101.53s/it] 21%|██        | 191/930 [5:22:04<20:19:29, 99.01s/it]  21%|██        | 192/930 [5:23:27<19:15:44, 93.96s/it] 21%|██        | 193/930 [5:25:14<20:01:48, 97.84s/it] 21%|██        | 194/930 [5:26:43<19:30:24, 95.41s/it] 21%|██        | 195/930 [5:28:21<19:36:12, 96.02s/it] 21%|██        | 196/930 [5:30:02<19:52:22, 97.47s/it] 21%|██        | 197/930 [5:31:53<20:42:44, 101.73s/it] 21%|██▏       | 198/930 [5:33:43<21:12:10, 104.28s/it] 21%|██▏       | 199/930 [5:35:46<22:18:00, 109.82s/it] 22%|██▏       | 200/930 [5:37:26<21:38:55, 106.76s/it]                                                       {'loss': 0.7176, 'grad_norm': 0.25566044449806213, 'learning_rate': 9.60946119490972e-05, 'epoch': 0.43}
 22%|██▏       | 200/930 [5:37:26<21:38:55, 106.76s/it]
***** Running Evaluation *****

***** Running Evaluation *****
[INFO|trainer.py:4643] 2025-12-08 16:46:55,000 >> 
***** Running Evaluation *****
  Num examples = 3305
  Num examples = 3305
  Batch size = 1
  Batch size = 1
[INFO|trainer.py:4645] 2025-12-08 16:46:55,000 >>   Num examples = 3305
[INFO|trainer.py:4648] 2025-12-08 16:46:55,001 >>   Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

  0%|          | 0/827 [00:00<?, ?it/s][A
  0%|          | 2/827 [00:04<29:36,  2.15s/it][A
  0%|          | 3/827 [00:07<32:49,  2.39s/it][A
  0%|          | 4/827 [00:10<38:05,  2.78s/it][A
  1%|          | 5/827 [00:14<43:39,  3.19s/it][A
  1%|          | 6/827 [00:18<46:23,  3.39s/it][A
  1%|          | 7/827 [00:22<50:07,  3.67s/it][A
  1%|          | 8/827 [00:28<1:01:20,  4.49s/it][A
  1%|          | 9/827 [00:36<1:14:18,  5.45s/it][A
  1%|          | 10/827 [00:43<1:22:31,  6.06s/it][A
  1%|▏         | 11/827 [00:47<1:12:18,  5.32s/it][A
  1%|▏         | 12/827 [00:50<1:02:54,  4.63s/it][A
  2%|▏         | 13/827 [00:53<57:09,  4.21s/it]  [A
  2%|▏         | 14/827 [00:57<56:05,  4.14s/it][A
  2%|▏         | 15/827 [01:02<57:28,  4.25s/it][A
  2%|▏         | 16/827 [01:05<55:06,  4.08s/it][A
  2%|▏         | 17/827 [01:09<51:38,  3.83s/it][A
  2%|▏         | 18/827 [01:12<50:30,  3.75s/it][A
  2%|▏         | 19/827 [01:17<56:08,  4.17s/it][A
  2%|▏         | 20/827 [01:23<1:02:02,  4.61s/it][A
  3%|▎         | 21/827 [01:27<1:00:42,  4.52s/it][A
  3%|▎         | 22/827 [01:32<1:01:26,  4.58s/it][A
  3%|▎         | 23/827 [01:35<55:28,  4.14s/it]  [A
  3%|▎         | 24/827 [01:39<55:43,  4.16s/it][A
  3%|▎         | 25/827 [01:45<1:00:00,  4.49s/it][A
  3%|▎         | 26/827 [01:50<1:03:22,  4.75s/it][A
  3%|▎         | 27/827 [01:55<1:02:55,  4.72s/it][A
  3%|▎         | 28/827 [01:58<56:23,  4.23s/it]  [A
  4%|▎         | 29/827 [02:03<1:00:27,  4.55s/it][A
  4%|▎         | 30/827 [02:08<1:02:40,  4.72s/it][A
  4%|▎         | 31/827 [02:12<59:40,  4.50s/it]  [A
  4%|▍         | 32/827 [02:17<59:55,  4.52s/it][A
  4%|▍         | 33/827 [02:22<1:02:58,  4.76s/it][A
  4%|▍         | 34/827 [02:25<57:43,  4.37s/it]  [A
  4%|▍         | 35/827 [02:29<53:48,  4.08s/it][A
  4%|▍         | 36/827 [02:32<48:49,  3.70s/it][A
  4%|▍         | 37/827 [02:37<55:37,  4.23s/it][A
  5%|▍         | 38/827 [02:42<58:53,  4.48s/it][A
  5%|▍         | 39/827 [02:47<1:00:50,  4.63s/it][A
  5%|▍         | 40/827 [02:52<1:02:23,  4.76s/it][A
  5%|▍         | 41/827 [02:56<57:05,  4.36s/it]  [A
  5%|▌         | 42/827 [02:59<55:00,  4.20s/it][A
  5%|▌         | 43/827 [03:03<53:05,  4.06s/it][A
  5%|▌         | 44/827 [03:07<51:43,  3.96s/it][A
  5%|▌         | 45/827 [03:11<53:59,  4.14s/it][A
  6%|▌         | 46/827 [03:17<1:00:07,  4.62s/it][A
  6%|▌         | 47/827 [03:22<1:00:46,  4.67s/it][A
  6%|▌         | 48/827 [03:26<56:48,  4.38s/it]  [A
  6%|▌         | 49/827 [03:29<53:53,  4.16s/it][A
  6%|▌         | 50/827 [03:32<49:22,  3.81s/it][A
  6%|▌         | 51/827 [03:38<56:42,  4.38s/it][A
  6%|▋         | 52/827 [03:44<1:02:47,  4.86s/it][A
  6%|▋         | 53/827 [03:47<54:58,  4.26s/it]  [A
  7%|▋         | 54/827 [03:50<51:04,  3.96s/it][A
  7%|▋         | 55/827 [03:53<47:33,  3.70s/it][A
  7%|▋         | 56/827 [03:57<47:10,  3.67s/it][A
  7%|▋         | 57/827 [04:01<47:38,  3.71s/it][A
  7%|▋         | 58/827 [04:04<46:05,  3.60s/it][A
  7%|▋         | 59/827 [04:07<45:31,  3.56s/it][A
  7%|▋         | 60/827 [04:12<50:06,  3.92s/it][A
  7%|▋         | 61/827 [04:17<54:28,  4.27s/it][A
  7%|▋         | 62/827 [04:20<49:53,  3.91s/it][A
  8%|▊         | 63/827 [04:23<46:44,  3.67s/it][A
  8%|▊         | 64/827 [04:27<44:49,  3.52s/it][A
  8%|▊         | 65/827 [04:30<44:28,  3.50s/it][A
  8%|▊         | 66/827 [04:36<51:56,  4.10s/it][A
  8%|▊         | 67/827 [04:40<54:52,  4.33s/it][A
  8%|▊         | 68/827 [04:43<48:44,  3.85s/it][A
  8%|▊         | 69/827 [04:47<47:16,  3.74s/it][A
  8%|▊         | 70/827 [04:50<43:58,  3.49s/it][A
  9%|▊         | 71/827 [04:54<48:18,  3.83s/it][A
  9%|▊         | 72/827 [04:59<52:02,  4.14s/it][A
  9%|▉         | 73/827 [05:03<50:17,  4.00s/it][A
  9%|▉         | 74/827 [05:09<57:53,  4.61s/it][A
  9%|▉         | 75/827 [05:14<59:30,  4.75s/it][A
  9%|▉         | 76/827 [05:17<53:56,  4.31s/it][A
  9%|▉         | 77/827 [05:20<49:30,  3.96s/it][A
  9%|▉         | 78/827 [05:23<46:13,  3.70s/it][A
 10%|▉         | 79/827 [05:27<44:02,  3.53s/it][A
 10%|▉         | 80/827 [05:31<48:32,  3.90s/it][A
 10%|▉         | 81/827 [05:36<51:29,  4.14s/it][A
 10%|▉         | 82/827 [05:40<51:57,  4.18s/it][A
 10%|█         | 83/827 [05:45<53:13,  4.29s/it][A
 10%|█         | 84/827 [05:49<53:50,  4.35s/it][A
 10%|█         | 85/827 [05:53<52:12,  4.22s/it][A
 10%|█         | 86/827 [05:56<47:54,  3.88s/it][A
 11%|█         | 87/827 [06:04<1:00:17,  4.89s/it][A
 11%|█         | 88/827 [06:14<1:20:58,  6.57s/it][A
 11%|█         | 89/827 [06:22<1:25:43,  6.97s/it][A
 11%|█         | 90/827 [06:27<1:19:31,  6.47s/it][A
 11%|█         | 91/827 [06:31<1:09:35,  5.67s/it][A
 11%|█         | 92/827 [06:35<1:03:36,  5.19s/it][A
 11%|█         | 93/827 [06:38<55:15,  4.52s/it]  [A
 11%|█▏        | 94/827 [06:41<50:43,  4.15s/it][A
 11%|█▏        | 95/827 [06:45<47:46,  3.92s/it][A
 12%|█▏        | 96/827 [06:48<43:34,  3.58s/it][A
 12%|█▏        | 97/827 [06:50<39:49,  3.27s/it][A
 12%|█▏        | 98/827 [06:53<37:48,  3.11s/it][A
 12%|█▏        | 99/827 [06:56<36:22,  3.00s/it][A
 12%|█▏        | 100/827 [06:58<34:06,  2.82s/it][A
 12%|█▏        | 101/827 [07:02<37:47,  3.12s/it][A
 12%|█▏        | 102/827 [07:05<38:17,  3.17s/it][A
 12%|█▏        | 103/827 [07:08<37:03,  3.07s/it][A
 13%|█▎        | 104/827 [07:11<37:29,  3.11s/it][A
 13%|█▎        | 105/827 [07:15<38:52,  3.23s/it][A
 13%|█▎        | 106/827 [07:22<52:31,  4.37s/it][A
 13%|█▎        | 107/827 [07:28<59:59,  5.00s/it][A
 13%|█▎        | 108/827 [07:33<1:00:19,  5.03s/it][A
 13%|█▎        | 109/827 [07:37<55:33,  4.64s/it]  [A
 13%|█▎        | 110/827 [07:41<52:30,  4.39s/it][A
 13%|█▎        | 111/827 [07:46<56:21,  4.72s/it][A
 14%|█▎        | 112/827 [07:52<1:00:58,  5.12s/it][A
 14%|█▎        | 113/827 [07:56<57:26,  4.83s/it]  [A
 14%|█▍        | 114/827 [08:00<52:52,  4.45s/it][A
 14%|█▍        | 115/827 [08:04<50:02,  4.22s/it][A
 14%|█▍        | 116/827 [08:08<49:02,  4.14s/it][A
 14%|█▍        | 117/827 [08:13<51:58,  4.39s/it][A
 14%|█▍        | 118/827 [08:18<56:54,  4.82s/it][A
 14%|█▍        | 119/827 [08:24<59:16,  5.02s/it][A
 15%|█▍        | 120/827 [08:29<57:50,  4.91s/it][A
 15%|█▍        | 121/827 [08:32<52:06,  4.43s/it][A
 15%|█▍        | 122/827 [08:35<48:00,  4.09s/it][A
 15%|█▍        | 123/827 [08:38<43:40,  3.72s/it][A
 15%|█▍        | 124/827 [08:42<43:20,  3.70s/it][A
 15%|█▌        | 125/827 [08:46<44:50,  3.83s/it][A
 15%|█▌        | 126/827 [08:49<43:39,  3.74s/it][A
 15%|█▌        | 127/827 [08:53<42:51,  3.67s/it][A
 15%|█▌        | 128/827 [08:57<43:33,  3.74s/it][A
 16%|█▌        | 129/827 [09:00<41:09,  3.54s/it][A
 16%|█▌        | 130/827 [09:04<42:21,  3.65s/it][A
 16%|█▌        | 131/827 [09:08<44:33,  3.84s/it][A
 16%|█▌        | 132/827 [09:13<49:26,  4.27s/it][A
 16%|█▌        | 133/827 [09:18<51:24,  4.44s/it][A
 16%|█▌        | 134/827 [09:22<48:31,  4.20s/it][A
 16%|█▋        | 135/827 [09:27<53:07,  4.61s/it][A
 16%|█▋        | 136/827 [09:34<58:50,  5.11s/it][A
 17%|█▋        | 137/827 [09:37<53:14,  4.63s/it][A
 17%|█▋        | 138/827 [09:44<59:46,  5.20s/it][A
 17%|█▋        | 139/827 [09:51<1:06:03,  5.76s/it][A
 17%|█▋        | 140/827 [09:54<58:19,  5.09s/it]  [A
 17%|█▋        | 141/827 [09:58<54:48,  4.79s/it][A
 17%|█▋        | 142/827 [10:02<50:37,  4.43s/it][A
 17%|█▋        | 143/827 [10:06<47:53,  4.20s/it][A
 17%|█▋        | 144/827 [10:09<43:55,  3.86s/it][A
 18%|█▊        | 145/827 [10:13<44:42,  3.93s/it][A
 18%|█▊        | 146/827 [10:17<45:18,  3.99s/it][A
 18%|█▊        | 147/827 [10:20<43:43,  3.86s/it][A
 18%|█▊        | 148/827 [10:24<42:19,  3.74s/it][A
 18%|█▊        | 149/827 [10:27<41:17,  3.65s/it][A
 18%|█▊        | 150/827 [10:30<38:13,  3.39s/it][A
 18%|█▊        | 151/827 [10:33<35:45,  3.17s/it][A
 18%|█▊        | 152/827 [10:36<36:39,  3.26s/it][A
 19%|█▊        | 153/827 [10:40<36:51,  3.28s/it][A
 19%|█▊        | 154/827 [10:44<39:46,  3.55s/it][A
 19%|█▊        | 155/827 [10:49<46:25,  4.14s/it][A
 19%|█▉        | 156/827 [10:54<48:46,  4.36s/it][A
 19%|█▉        | 157/827 [10:58<46:03,  4.13s/it][A
 19%|█▉        | 158/827 [11:01<42:41,  3.83s/it][A
 19%|█▉        | 159/827 [11:06<46:01,  4.13s/it][A
 19%|█▉        | 160/827 [11:10<46:18,  4.17s/it][A
 19%|█▉        | 161/827 [11:13<41:35,  3.75s/it][A
 20%|█▉        | 162/827 [11:17<42:58,  3.88s/it][A
 20%|█▉        | 163/827 [11:21<43:25,  3.92s/it][A
 20%|█▉        | 164/827 [11:25<42:45,  3.87s/it][A
 20%|█▉        | 165/827 [11:28<41:04,  3.72s/it][A
 20%|██        | 166/827 [11:35<50:24,  4.58s/it][A
 20%|██        | 167/827 [11:44<1:07:12,  6.11s/it][A
 20%|██        | 168/827 [11:50<1:04:30,  5.87s/it][A
 20%|██        | 169/827 [11:53<56:23,  5.14s/it]  [A
 21%|██        | 170/827 [11:58<53:52,  4.92s/it][A
 21%|██        | 171/827 [12:01<50:28,  4.62s/it][A
 21%|██        | 172/827 [12:04<44:36,  4.09s/it][A
 21%|██        | 173/827 [12:07<41:09,  3.78s/it][A
 21%|██        | 174/827 [12:11<39:49,  3.66s/it][A
 21%|██        | 175/827 [12:15<40:42,  3.75s/it][A
 21%|██▏       | 176/827 [12:18<38:00,  3.50s/it][A
 21%|██▏       | 177/827 [12:20<35:24,  3.27s/it][A
 22%|██▏       | 178/827 [12:25<39:58,  3.70s/it][A
 22%|██▏       | 179/827 [12:31<46:35,  4.31s/it][A
 22%|██▏       | 180/827 [12:35<45:39,  4.23s/it][A
 22%|██▏       | 181/827 [12:39<45:15,  4.20s/it][A
 22%|██▏       | 182/827 [12:44<47:22,  4.41s/it][A
 22%|██▏       | 183/827 [12:47<43:27,  4.05s/it][A
 22%|██▏       | 184/827 [12:52<45:43,  4.27s/it][A
 22%|██▏       | 185/827 [12:57<48:12,  4.50s/it][A
 22%|██▏       | 186/827 [13:00<43:55,  4.11s/it][A
 23%|██▎       | 187/827 [13:03<40:33,  3.80s/it][A
 23%|██▎       | 188/827 [13:07<41:57,  3.94s/it][A
 23%|██▎       | 189/827 [13:12<45:24,  4.27s/it][A
 23%|██▎       | 190/827 [13:16<44:12,  4.16s/it][A
 23%|██▎       | 191/827 [13:21<44:08,  4.16s/it][A
 23%|██▎       | 192/827 [13:26<46:39,  4.41s/it][A
 23%|██▎       | 193/827 [13:29<45:06,  4.27s/it][A
 23%|██▎       | 194/827 [13:33<41:21,  3.92s/it][A
 24%|██▎       | 195/827 [13:36<39:00,  3.70s/it][A
 24%|██▎       | 196/827 [13:39<36:03,  3.43s/it][A
 24%|██▍       | 197/827 [13:42<34:32,  3.29s/it][A
 24%|██▍       | 198/827 [13:45<35:36,  3.40s/it][A
 24%|██▍       | 199/827 [13:49<36:00,  3.44s/it][A
 24%|██▍       | 200/827 [13:53<39:34,  3.79s/it][A
 24%|██▍       | 201/827 [13:58<43:53,  4.21s/it][A
 24%|██▍       | 202/827 [14:04<47:30,  4.56s/it][A
 25%|██▍       | 203/827 [14:09<50:15,  4.83s/it][A
 25%|██▍       | 204/827 [14:15<53:29,  5.15s/it][A
 25%|██▍       | 205/827 [14:19<48:35,  4.69s/it][A
 25%|██▍       | 206/827 [14:22<44:24,  4.29s/it][A
 25%|██▌       | 207/827 [14:26<41:48,  4.05s/it][A
 25%|██▌       | 208/827 [14:29<40:07,  3.89s/it][A
 25%|██▌       | 209/827 [14:33<38:24,  3.73s/it][A
 25%|██▌       | 210/827 [14:36<37:33,  3.65s/it][A
 26%|██▌       | 211/827 [14:41<42:13,  4.11s/it][A
 26%|██▌       | 212/827 [14:46<45:22,  4.43s/it][A
 26%|██▌       | 213/827 [14:52<49:00,  4.79s/it][A
 26%|██▌       | 214/827 [14:56<45:14,  4.43s/it][A
 26%|██▌       | 215/827 [14:59<41:29,  4.07s/it][A
 26%|██▌       | 216/827 [15:03<40:44,  4.00s/it][A
 26%|██▌       | 217/827 [15:07<41:21,  4.07s/it][A
 26%|██▋       | 218/827 [15:11<41:01,  4.04s/it][A
 26%|██▋       | 219/827 [15:14<37:59,  3.75s/it][A
 27%|██▋       | 220/827 [15:19<41:41,  4.12s/it][A
 27%|██▋       | 221/827 [15:25<46:11,  4.57s/it][A
 27%|██▋       | 222/827 [15:28<42:18,  4.20s/it][A
 27%|██▋       | 223/827 [15:31<39:29,  3.92s/it][A
 27%|██▋       | 224/827 [15:34<37:27,  3.73s/it][A
 27%|██▋       | 225/827 [15:38<35:48,  3.57s/it][A
 27%|██▋       | 226/827 [15:40<33:03,  3.30s/it][A
 27%|██▋       | 227/827 [15:45<38:01,  3.80s/it][A
 28%|██▊       | 228/827 [15:51<42:22,  4.24s/it][A
 28%|██▊       | 229/827 [15:54<40:39,  4.08s/it][A
 28%|██▊       | 230/827 [15:58<40:40,  4.09s/it][A
 28%|██▊       | 231/827 [16:02<38:44,  3.90s/it][A
 28%|██▊       | 232/827 [16:05<37:16,  3.76s/it][A
 28%|██▊       | 233/827 [16:10<41:19,  4.17s/it][A
 28%|██▊       | 234/827 [16:16<46:37,  4.72s/it][A
 28%|██▊       | 235/827 [16:21<47:16,  4.79s/it][A
 29%|██▊       | 236/827 [16:25<45:21,  4.60s/it][A
 29%|██▊       | 237/827 [16:32<50:21,  5.12s/it][A
 29%|██▉       | 238/827 [16:38<51:54,  5.29s/it][A
 29%|██▉       | 239/827 [16:43<52:21,  5.34s/it][A
 29%|██▉       | 240/827 [16:49<53:21,  5.45s/it][A
 29%|██▉       | 241/827 [16:55<54:47,  5.61s/it][A
 29%|██▉       | 242/827 [16:59<51:19,  5.26s/it][A
 29%|██▉       | 243/827 [17:04<50:23,  5.18s/it][A
 30%|██▉       | 244/827 [17:10<52:19,  5.39s/it][A
 30%|██▉       | 245/827 [17:14<47:40,  4.91s/it][A
 30%|██▉       | 246/827 [17:18<44:10,  4.56s/it][A
 30%|██▉       | 247/827 [17:22<45:01,  4.66s/it][A
 30%|██▉       | 248/827 [17:29<51:56,  5.38s/it][A
 30%|███       | 249/827 [17:34<49:54,  5.18s/it][A
 30%|███       | 250/827 [17:37<42:14,  4.39s/it][A
 30%|███       | 251/827 [17:41<42:06,  4.39s/it][A
 30%|███       | 252/827 [17:46<42:49,  4.47s/it][A
 31%|███       | 253/827 [17:49<38:21,  4.01s/it][A
 31%|███       | 254/827 [17:55<44:07,  4.62s/it][A
 31%|███       | 255/827 [18:02<50:14,  5.27s/it][A
 31%|███       | 256/827 [18:08<53:05,  5.58s/it][A
 31%|███       | 257/827 [18:12<49:47,  5.24s/it][A
 31%|███       | 258/827 [18:17<47:22,  5.00s/it][A
 31%|███▏      | 259/827 [18:23<51:14,  5.41s/it][A
 31%|███▏      | 260/827 [18:29<53:30,  5.66s/it][A
 32%|███▏      | 261/827 [18:33<48:24,  5.13s/it][A
 32%|███▏      | 262/827 [18:40<52:19,  5.56s/it][A
 32%|███▏      | 263/827 [18:46<54:35,  5.81s/it][A
 32%|███▏      | 264/827 [18:49<46:10,  4.92s/it][A
 32%|███▏      | 265/827 [18:52<40:26,  4.32s/it][A
 32%|███▏      | 266/827 [18:55<37:26,  4.00s/it][A
 32%|███▏      | 267/827 [18:59<35:31,  3.81s/it][A
 32%|███▏      | 268/827 [19:02<34:23,  3.69s/it][A
 33%|███▎      | 269/827 [19:07<37:21,  4.02s/it][A
 33%|███▎      | 270/827 [19:12<39:47,  4.29s/it][A
 33%|███▎      | 271/827 [19:15<37:21,  4.03s/it][A
 33%|███▎      | 272/827 [19:19<35:34,  3.85s/it][A
 33%|███▎      | 273/827 [19:25<43:01,  4.66s/it][A
 33%|███▎      | 274/827 [19:34<54:54,  5.96s/it][A
 33%|███▎      | 275/827 [19:39<53:07,  5.78s/it][A
 33%|███▎      | 276/827 [19:44<48:30,  5.28s/it][A
 33%|███▎      | 277/827 [19:47<44:27,  4.85s/it][A
 34%|███▎      | 278/827 [19:52<42:36,  4.66s/it][A
 34%|███▎      | 279/827 [19:55<39:16,  4.30s/it][A
 34%|███▍      | 280/827 [19:58<35:56,  3.94s/it][A
 34%|███▍      | 281/827 [20:02<36:43,  4.04s/it][A
 34%|███▍      | 282/827 [20:06<35:36,  3.92s/it][A
 34%|███▍      | 283/827 [20:10<34:51,  3.84s/it][A
 34%|███▍      | 284/827 [20:14<35:14,  3.89s/it][A
 34%|███▍      | 285/827 [20:19<38:00,  4.21s/it][A
 35%|███▍      | 286/827 [20:24<40:08,  4.45s/it][A
 35%|███▍      | 287/827 [20:26<35:15,  3.92s/it][A
 35%|███▍      | 288/827 [20:30<33:06,  3.69s/it][A
 35%|███▍      | 289/827 [20:33<32:11,  3.59s/it][A
 35%|███▌      | 290/827 [20:36<31:26,  3.51s/it][A
 35%|███▌      | 291/827 [20:39<29:47,  3.34s/it][A
 35%|███▌      | 292/827 [20:43<30:29,  3.42s/it][A
 35%|███▌      | 293/827 [20:46<29:28,  3.31s/it][A
 36%|███▌      | 294/827 [20:50<30:36,  3.45s/it][A
 36%|███▌      | 295/827 [20:53<31:10,  3.52s/it][A
 36%|███▌      | 296/827 [20:58<33:51,  3.83s/it][A
 36%|███▌      | 297/827 [21:02<34:22,  3.89s/it][A
 36%|███▌      | 298/827 [21:05<32:19,  3.67s/it][A
 36%|███▌      | 299/827 [21:08<31:43,  3.60s/it][A
 36%|███▋      | 300/827 [21:12<30:14,  3.44s/it][A
 36%|███▋      | 301/827 [21:15<30:00,  3.42s/it][A
 37%|███▋      | 302/827 [21:19<30:27,  3.48s/it][A
 37%|███▋      | 303/827 [21:21<27:43,  3.18s/it][A
 37%|███▋      | 304/827 [21:25<28:37,  3.28s/it][A
 37%|███▋      | 305/827 [21:29<32:25,  3.73s/it][A
 37%|███▋      | 306/827 [21:33<33:25,  3.85s/it][A
 37%|███▋      | 307/827 [21:37<32:13,  3.72s/it][A
 37%|███▋      | 308/827 [21:40<30:43,  3.55s/it][A
 37%|███▋      | 309/827 [21:44<30:50,  3.57s/it][A
 37%|███▋      | 310/827 [21:46<28:39,  3.33s/it][A
 38%|███▊      | 311/827 [21:49<27:29,  3.20s/it][A
 38%|███▊      | 312/827 [21:53<29:37,  3.45s/it][A
 38%|███▊      | 313/827 [21:58<31:44,  3.71s/it][A
 38%|███▊      | 314/827 [22:01<32:03,  3.75s/it][A
 38%|███▊      | 315/827 [22:06<34:22,  4.03s/it][A
 38%|███▊      | 316/827 [22:11<36:57,  4.34s/it][A
 38%|███▊      | 317/827 [22:16<39:05,  4.60s/it][A
 38%|███▊      | 318/827 [22:21<38:37,  4.55s/it][A
 39%|███▊      | 319/827 [22:25<37:24,  4.42s/it][A
 39%|███▊      | 320/827 [22:29<35:21,  4.18s/it][A
 39%|███▉      | 321/827 [22:34<37:23,  4.43s/it][A
 39%|███▉      | 322/827 [22:37<35:30,  4.22s/it][A
 39%|███▉      | 323/827 [22:42<35:56,  4.28s/it][A
 39%|███▉      | 324/827 [22:47<37:19,  4.45s/it][A
 39%|███▉      | 325/827 [22:50<34:26,  4.12s/it][A
 39%|███▉      | 326/827 [22:54<34:17,  4.11s/it][A
 40%|███▉      | 327/827 [22:59<36:01,  4.32s/it][A
 40%|███▉      | 328/827 [23:04<37:04,  4.46s/it][A
 40%|███▉      | 329/827 [23:10<42:13,  5.09s/it][A
 40%|███▉      | 330/827 [23:15<40:41,  4.91s/it][A
 40%|████      | 331/827 [23:20<40:23,  4.89s/it][A
 40%|████      | 332/827 [23:24<39:42,  4.81s/it][A
 40%|████      | 333/827 [23:29<40:56,  4.97s/it][A
 40%|████      | 334/827 [23:34<39:13,  4.77s/it][A
 41%|████      | 335/827 [23:37<36:02,  4.40s/it][A
 41%|████      | 336/827 [23:41<33:02,  4.04s/it][A
 41%|████      | 337/827 [23:44<31:06,  3.81s/it][A
 41%|████      | 338/827 [23:47<29:52,  3.67s/it][A
 41%|████      | 339/827 [23:50<28:40,  3.53s/it][A
 41%|████      | 340/827 [23:54<29:32,  3.64s/it][A
 41%|████      | 341/827 [23:58<30:48,  3.80s/it][A
 41%|████▏     | 342/827 [24:02<29:34,  3.66s/it][A
 41%|████▏     | 343/827 [24:05<28:46,  3.57s/it][A
 42%|████▏     | 344/827 [24:10<31:24,  3.90s/it][A
 42%|████▏     | 345/827 [24:15<34:45,  4.33s/it][A
 42%|████▏     | 346/827 [24:19<34:15,  4.27s/it][A
 42%|████▏     | 347/827 [24:23<33:24,  4.18s/it][A
 42%|████▏     | 348/827 [24:26<30:22,  3.80s/it][A
 42%|████▏     | 349/827 [24:31<33:02,  4.15s/it][A
 42%|████▏     | 350/827 [24:36<34:41,  4.36s/it][A
 42%|████▏     | 351/827 [24:39<32:34,  4.11s/it][A
 43%|████▎     | 352/827 [24:44<32:58,  4.17s/it][A
 43%|████▎     | 353/827 [24:47<31:31,  3.99s/it][A
 43%|████▎     | 354/827 [24:51<30:33,  3.88s/it][A
 43%|████▎     | 355/827 [24:56<33:23,  4.25s/it][A
 43%|████▎     | 356/827 [25:01<35:27,  4.52s/it][A
 43%|████▎     | 357/827 [25:05<33:13,  4.24s/it][A
 43%|████▎     | 358/827 [25:08<30:03,  3.85s/it][A
 43%|████▎     | 359/827 [25:12<29:57,  3.84s/it][A
 44%|████▎     | 360/827 [25:17<32:53,  4.23s/it][A
 44%|████▎     | 361/827 [25:21<32:13,  4.15s/it][A
 44%|████▍     | 362/827 [25:23<28:57,  3.74s/it][A
 44%|████▍     | 363/827 [25:28<30:48,  3.98s/it][A
 44%|████▍     | 364/827 [25:32<30:20,  3.93s/it][A
 44%|████▍     | 365/827 [25:35<29:36,  3.85s/it][A
 44%|████▍     | 366/827 [25:41<34:35,  4.50s/it][A
 44%|████▍     | 367/827 [25:46<34:48,  4.54s/it][A
 44%|████▍     | 368/827 [25:49<30:56,  4.05s/it][A
 45%|████▍     | 369/827 [25:53<30:19,  3.97s/it][A
 45%|████▍     | 370/827 [25:57<29:51,  3.92s/it][A
 45%|████▍     | 371/827 [26:01<31:17,  4.12s/it][A
 45%|████▍     | 372/827 [26:05<30:53,  4.07s/it][A
 45%|████▌     | 373/827 [26:10<33:30,  4.43s/it][A
 45%|████▌     | 374/827 [26:18<39:59,  5.30s/it][A
 45%|████▌     | 375/827 [26:27<49:14,  6.54s/it][A
 45%|████▌     | 376/827 [26:35<50:59,  6.78s/it][A
 46%|████▌     | 377/827 [26:39<46:21,  6.18s/it][A
 46%|████▌     | 378/827 [26:46<47:41,  6.37s/it][A
 46%|████▌     | 379/827 [26:56<54:55,  7.36s/it][A
 46%|████▌     | 380/827 [27:02<51:57,  6.97s/it][A
 46%|████▌     | 381/827 [27:06<45:16,  6.09s/it][A
 46%|████▌     | 382/827 [27:11<42:51,  5.78s/it][A
 46%|████▋     | 383/827 [27:15<39:13,  5.30s/it][A
 46%|████▋     | 384/827 [27:19<36:11,  4.90s/it][A
 47%|████▋     | 385/827 [27:22<32:45,  4.45s/it][A
 47%|████▋     | 386/827 [27:26<30:48,  4.19s/it][A
 47%|████▋     | 387/827 [27:30<30:29,  4.16s/it][A
 47%|████▋     | 388/827 [27:35<31:39,  4.33s/it][A
 47%|████▋     | 389/827 [27:39<30:24,  4.17s/it][A
 47%|████▋     | 390/827 [27:42<29:01,  3.98s/it][A
 47%|████▋     | 391/827 [27:46<27:52,  3.84s/it][A
 47%|████▋     | 392/827 [27:50<28:08,  3.88s/it][A
 48%|████▊     | 393/827 [27:54<29:33,  4.09s/it][A
 48%|████▊     | 394/827 [28:00<32:40,  4.53s/it][A
 48%|████▊     | 395/827 [28:04<32:55,  4.57s/it][A
 48%|████▊     | 396/827 [28:09<33:02,  4.60s/it][A
 48%|████▊     | 397/827 [28:12<29:49,  4.16s/it][A
 48%|████▊     | 398/827 [28:16<28:15,  3.95s/it][A
 48%|████▊     | 399/827 [28:19<26:57,  3.78s/it][A
 48%|████▊     | 400/827 [28:24<28:40,  4.03s/it][A
 48%|████▊     | 401/827 [28:29<30:42,  4.33s/it][A
 49%|████▊     | 402/827 [28:33<31:09,  4.40s/it][A
 49%|████▊     | 403/827 [28:38<31:59,  4.53s/it][A
 49%|████▉     | 404/827 [28:41<28:11,  4.00s/it][A
 49%|████▉     | 405/827 [28:44<26:57,  3.83s/it][A
 49%|████▉     | 406/827 [28:48<25:52,  3.69s/it][A
 49%|████▉     | 407/827 [28:51<24:10,  3.45s/it][A
 49%|████▉     | 408/827 [28:54<23:39,  3.39s/it][A
 49%|████▉     | 409/827 [28:57<23:08,  3.32s/it][A
 50%|████▉     | 410/827 [29:00<22:05,  3.18s/it][A
 50%|████▉     | 411/827 [29:03<22:14,  3.21s/it][A
 50%|████▉     | 412/827 [29:06<22:17,  3.22s/it][A
 50%|████▉     | 413/827 [29:11<25:50,  3.74s/it][A
 50%|█████     | 414/827 [29:17<28:41,  4.17s/it][A
 50%|█████     | 415/827 [29:19<26:05,  3.80s/it][A
 50%|█████     | 416/827 [29:24<28:05,  4.10s/it][A
 50%|█████     | 417/827 [29:29<30:05,  4.40s/it][A
 51%|█████     | 418/827 [29:35<32:18,  4.74s/it][A
 51%|█████     | 419/827 [29:39<31:37,  4.65s/it][A
 51%|█████     | 420/827 [29:44<30:38,  4.52s/it][A
 51%|█████     | 421/827 [29:47<29:06,  4.30s/it][A
 51%|█████     | 422/827 [29:51<26:57,  3.99s/it][A
 51%|█████     | 423/827 [29:54<25:14,  3.75s/it][A
 51%|█████▏    | 424/827 [29:58<25:28,  3.79s/it][A
 51%|█████▏    | 425/827 [30:03<27:58,  4.17s/it][A
 52%|█████▏    | 426/827 [30:08<29:43,  4.45s/it][A
 52%|█████▏    | 427/827 [30:13<31:29,  4.72s/it][A
 52%|█████▏    | 428/827 [30:18<31:42,  4.77s/it][A
 52%|█████▏    | 429/827 [30:22<29:35,  4.46s/it][A
 52%|█████▏    | 430/827 [30:26<28:43,  4.34s/it][A
 52%|█████▏    | 431/827 [30:30<29:00,  4.40s/it][A
 52%|█████▏    | 432/827 [30:36<32:10,  4.89s/it][A
 52%|█████▏    | 433/827 [30:43<35:06,  5.35s/it][A
 52%|█████▏    | 434/827 [30:48<34:57,  5.34s/it][A
 53%|█████▎    | 435/827 [30:51<30:44,  4.71s/it][A
 53%|█████▎    | 436/827 [30:56<30:10,  4.63s/it][A
 53%|█████▎    | 437/827 [31:01<30:12,  4.65s/it][A
 53%|█████▎    | 438/827 [31:04<28:08,  4.34s/it][A
 53%|█████▎    | 439/827 [31:08<27:12,  4.21s/it][A
 53%|█████▎    | 440/827 [31:12<26:49,  4.16s/it][A
 53%|█████▎    | 441/827 [31:15<24:25,  3.80s/it][A
 53%|█████▎    | 442/827 [31:18<23:17,  3.63s/it][A
 54%|█████▎    | 443/827 [31:21<22:11,  3.47s/it][A
 54%|█████▎    | 444/827 [31:26<23:41,  3.71s/it][A
 54%|█████▍    | 445/827 [31:31<27:28,  4.32s/it][A
 54%|█████▍    | 446/827 [31:35<25:54,  4.08s/it][A
 54%|█████▍    | 447/827 [31:40<27:06,  4.28s/it][A
 54%|█████▍    | 448/827 [31:45<29:18,  4.64s/it][A
 54%|█████▍    | 449/827 [31:53<35:27,  5.63s/it][A
 54%|█████▍    | 450/827 [31:59<35:10,  5.60s/it][A
 55%|█████▍    | 451/827 [32:03<31:53,  5.09s/it][A
 55%|█████▍    | 452/827 [32:08<33:07,  5.30s/it][A
 55%|█████▍    | 453/827 [32:14<33:40,  5.40s/it][A
 55%|█████▍    | 454/827 [32:17<30:03,  4.84s/it][A
 55%|█████▌    | 455/827 [32:22<29:21,  4.73s/it][A
 55%|█████▌    | 456/827 [32:28<31:55,  5.16s/it][A
 55%|█████▌    | 457/827 [32:33<30:58,  5.02s/it][A
 55%|█████▌    | 458/827 [32:37<29:04,  4.73s/it][A
 56%|█████▌    | 459/827 [32:42<28:50,  4.70s/it][A
 56%|█████▌    | 460/827 [32:48<31:34,  5.16s/it][A
 56%|█████▌    | 461/827 [32:56<36:35,  6.00s/it][A
 56%|█████▌    | 462/827 [33:00<33:48,  5.56s/it][A
 56%|█████▌    | 463/827 [33:03<29:31,  4.87s/it][A
 56%|█████▌    | 464/827 [33:09<29:59,  4.96s/it][A
 56%|█████▌    | 465/827 [33:13<28:38,  4.75s/it][A
 56%|█████▋    | 466/827 [33:17<26:54,  4.47s/it][A
 56%|█████▋    | 467/827 [33:21<26:12,  4.37s/it][A
 57%|█████▋    | 468/827 [33:26<26:42,  4.46s/it][A
 57%|█████▋    | 469/827 [33:30<27:24,  4.59s/it][A
 57%|█████▋    | 470/827 [33:34<25:29,  4.28s/it][A
 57%|█████▋    | 471/827 [33:39<26:08,  4.41s/it][A
 57%|█████▋    | 472/827 [33:45<30:19,  5.13s/it][A
 57%|█████▋    | 473/827 [33:50<28:51,  4.89s/it][A
 57%|█████▋    | 474/827 [33:54<27:16,  4.64s/it][A
 57%|█████▋    | 475/827 [33:57<24:30,  4.18s/it][A
 58%|█████▊    | 476/827 [34:00<21:32,  3.68s/it][A
 58%|█████▊    | 477/827 [34:02<19:35,  3.36s/it][A
 58%|█████▊    | 478/827 [34:06<21:13,  3.65s/it][A
 58%|█████▊    | 479/827 [34:11<23:36,  4.07s/it][A
 58%|█████▊    | 480/827 [34:16<24:50,  4.29s/it][A
 58%|█████▊    | 481/827 [34:21<25:46,  4.47s/it][A
 58%|█████▊    | 482/827 [34:24<23:00,  4.00s/it][A
 58%|█████▊    | 483/827 [34:27<20:15,  3.53s/it][A
 59%|█████▊    | 484/827 [34:30<20:02,  3.51s/it][A
 59%|█████▊    | 485/827 [34:33<19:08,  3.36s/it][A
 59%|█████▉    | 486/827 [34:37<19:26,  3.42s/it][A
 59%|█████▉    | 487/827 [34:40<20:01,  3.53s/it][A
 59%|█████▉    | 488/827 [34:46<22:46,  4.03s/it][A
 59%|█████▉    | 489/827 [34:51<24:46,  4.40s/it][A
 59%|█████▉    | 490/827 [34:56<25:22,  4.52s/it][A
 59%|█████▉    | 491/827 [34:59<23:42,  4.23s/it][A
 59%|█████▉    | 492/827 [35:03<22:39,  4.06s/it][A
 60%|█████▉    | 493/827 [35:07<21:59,  3.95s/it][A
 60%|█████▉    | 494/827 [35:10<21:52,  3.94s/it][A
 60%|█████▉    | 495/827 [35:14<21:51,  3.95s/it][A
 60%|█████▉    | 496/827 [35:18<21:51,  3.96s/it][A
 60%|██████    | 497/827 [35:21<19:54,  3.62s/it][A
 60%|██████    | 498/827 [35:25<19:28,  3.55s/it][A
 60%|██████    | 499/827 [35:29<21:17,  3.89s/it][A
 60%|██████    | 500/827 [35:33<20:09,  3.70s/it][A
 61%|██████    | 501/827 [35:35<18:47,  3.46s/it][A
 61%|██████    | 502/827 [35:40<21:09,  3.91s/it][A
 61%|██████    | 503/827 [35:45<22:25,  4.15s/it][A
 61%|██████    | 504/827 [35:49<21:19,  3.96s/it][A
 61%|██████    | 505/827 [35:53<21:13,  3.95s/it][A
 61%|██████    | 506/827 [35:57<21:06,  3.94s/it][A
 61%|██████▏   | 507/827 [36:02<24:13,  4.54s/it][A
 61%|██████▏   | 508/827 [36:09<27:01,  5.08s/it][A
 62%|██████▏   | 509/827 [36:12<23:36,  4.45s/it][A
 62%|██████▏   | 510/827 [36:16<22:34,  4.27s/it][A
 62%|██████▏   | 511/827 [36:19<20:55,  3.97s/it][A
 62%|██████▏   | 512/827 [36:22<19:16,  3.67s/it][A
 62%|██████▏   | 513/827 [36:26<19:41,  3.76s/it][A
 62%|██████▏   | 514/827 [36:30<19:33,  3.75s/it][A
 62%|██████▏   | 515/827 [36:33<19:11,  3.69s/it][A
 62%|██████▏   | 516/827 [36:37<19:51,  3.83s/it][A
 63%|██████▎   | 517/827 [36:41<19:22,  3.75s/it][A
 63%|██████▎   | 518/827 [36:44<18:14,  3.54s/it][A
 63%|██████▎   | 519/827 [36:47<17:24,  3.39s/it][A
 63%|██████▎   | 520/827 [36:51<17:42,  3.46s/it][A
 63%|██████▎   | 521/827 [36:55<19:14,  3.77s/it][A
 63%|██████▎   | 522/827 [37:01<21:46,  4.28s/it][A
 63%|██████▎   | 523/827 [37:04<19:51,  3.92s/it][A
 63%|██████▎   | 524/827 [37:08<20:19,  4.03s/it][A
 63%|██████▎   | 525/827 [37:14<23:37,  4.69s/it][A
 64%|██████▎   | 526/827 [37:20<25:07,  5.01s/it][A
 64%|██████▎   | 527/827 [37:24<24:08,  4.83s/it][A
 64%|██████▍   | 528/827 [37:29<24:12,  4.86s/it][A
 64%|██████▍   | 529/827 [37:35<24:57,  5.03s/it][A
 64%|██████▍   | 530/827 [37:39<24:21,  4.92s/it][A
 64%|██████▍   | 531/827 [37:42<21:44,  4.41s/it][A
 64%|██████▍   | 532/827 [37:46<20:26,  4.16s/it][A
 64%|██████▍   | 533/827 [37:52<22:29,  4.59s/it][A
 65%|██████▍   | 534/827 [37:57<23:43,  4.86s/it][A
 65%|██████▍   | 535/827 [38:02<24:06,  4.95s/it][A
 65%|██████▍   | 536/827 [38:06<22:00,  4.54s/it][A
 65%|██████▍   | 537/827 [38:08<19:04,  3.95s/it][A
 65%|██████▌   | 538/827 [38:12<18:04,  3.75s/it][A
 65%|██████▌   | 539/827 [38:15<17:17,  3.60s/it][A
 65%|██████▌   | 540/827 [38:19<17:18,  3.62s/it][A
 65%|██████▌   | 541/827 [38:23<17:51,  3.75s/it][A
 66%|██████▌   | 542/827 [38:27<18:28,  3.89s/it][A
 66%|██████▌   | 543/827 [38:32<19:45,  4.18s/it][A
 66%|██████▌   | 544/827 [38:37<21:09,  4.49s/it][A
 66%|██████▌   | 545/827 [38:41<20:52,  4.44s/it][A
 66%|██████▌   | 546/827 [38:46<20:58,  4.48s/it][A
 66%|██████▌   | 547/827 [38:50<20:44,  4.44s/it][A
 66%|██████▋   | 548/827 [38:53<18:22,  3.95s/it][A
 66%|██████▋   | 549/827 [38:56<17:27,  3.77s/it][A
 67%|██████▋   | 550/827 [39:00<17:07,  3.71s/it][A
 67%|██████▋   | 551/827 [39:04<17:46,  3.86s/it][A
 67%|██████▋   | 552/827 [39:08<17:35,  3.84s/it][A
 67%|██████▋   | 553/827 [39:12<17:21,  3.80s/it][A
 67%|██████▋   | 554/827 [39:14<15:52,  3.49s/it][A
 67%|██████▋   | 555/827 [39:18<15:35,  3.44s/it][A
 67%|██████▋   | 556/827 [39:21<15:21,  3.40s/it][A
 67%|██████▋   | 557/827 [39:24<14:30,  3.23s/it][A
 67%|██████▋   | 558/827 [39:28<15:54,  3.55s/it][A
 68%|██████▊   | 559/827 [39:32<16:23,  3.67s/it][A
 68%|██████▊   | 560/827 [39:36<15:53,  3.57s/it][A
 68%|██████▊   | 561/827 [39:39<16:02,  3.62s/it][A
 68%|██████▊   | 562/827 [39:43<16:29,  3.73s/it][A
 68%|██████▊   | 563/827 [39:47<16:27,  3.74s/it][A
 68%|██████▊   | 564/827 [39:51<16:13,  3.70s/it][A
 68%|██████▊   | 565/827 [39:56<18:01,  4.13s/it][A
 68%|██████▊   | 566/827 [40:00<18:41,  4.30s/it][A
 69%|██████▊   | 567/827 [40:04<17:23,  4.01s/it][A
 69%|██████▊   | 568/827 [40:08<17:01,  3.95s/it][A
 69%|██████▉   | 569/827 [40:11<16:20,  3.80s/it][A
 69%|██████▉   | 570/827 [40:15<16:43,  3.91s/it][A
 69%|██████▉   | 571/827 [40:21<18:36,  4.36s/it][A
 69%|██████▉   | 572/827 [40:26<20:06,  4.73s/it][A
 69%|██████▉   | 573/827 [40:32<21:07,  4.99s/it][A
 69%|██████▉   | 574/827 [40:36<20:17,  4.81s/it][A
 70%|██████▉   | 575/827 [40:42<21:07,  5.03s/it][A
 70%|██████▉   | 576/827 [40:46<19:50,  4.74s/it][A
 70%|██████▉   | 577/827 [40:49<18:18,  4.40s/it][A
 70%|██████▉   | 578/827 [40:53<17:39,  4.26s/it][A
 70%|███████   | 579/827 [40:57<16:34,  4.01s/it][A
 70%|███████   | 580/827 [41:02<18:16,  4.44s/it][A
 70%|███████   | 581/827 [41:09<21:30,  5.25s/it][A
 70%|███████   | 582/827 [41:14<20:37,  5.05s/it][A
 70%|███████   | 583/827 [41:17<18:33,  4.56s/it][A
 71%|███████   | 584/827 [41:21<16:50,  4.16s/it][A
 71%|███████   | 585/827 [41:24<15:26,  3.83s/it][A
 71%|███████   | 586/827 [41:28<15:53,  3.96s/it][A
 71%|███████   | 587/827 [41:36<20:44,  5.18s/it][A
 71%|███████   | 588/827 [41:43<23:06,  5.80s/it][A
 71%|███████   | 589/827 [41:48<22:00,  5.55s/it][A
 71%|███████▏  | 590/827 [41:55<23:09,  5.86s/it][A
 71%|███████▏  | 591/827 [42:02<24:23,  6.20s/it][A
 72%|███████▏  | 592/827 [42:05<20:59,  5.36s/it][A
 72%|███████▏  | 593/827 [42:09<19:02,  4.88s/it][A
 72%|███████▏  | 594/827 [42:12<16:56,  4.36s/it][A
 72%|███████▏  | 595/827 [42:16<16:06,  4.17s/it][A
 72%|███████▏  | 596/827 [42:23<19:43,  5.12s/it][A
 72%|███████▏  | 597/827 [42:29<21:01,  5.49s/it][A
 72%|███████▏  | 598/827 [42:33<18:16,  4.79s/it][A
 72%|███████▏  | 599/827 [42:36<16:53,  4.45s/it][A
 73%|███████▎  | 600/827 [42:41<17:06,  4.52s/it][A
 73%|███████▎  | 601/827 [42:45<16:44,  4.45s/it][A
 73%|███████▎  | 602/827 [42:49<16:04,  4.29s/it][A
 73%|███████▎  | 603/827 [42:52<14:58,  4.01s/it][A
 73%|███████▎  | 604/827 [42:56<14:38,  3.94s/it][A
 73%|███████▎  | 605/827 [43:00<14:09,  3.83s/it][A
 73%|███████▎  | 606/827 [43:04<14:06,  3.83s/it][A
 73%|███████▎  | 607/827 [43:08<14:23,  3.92s/it][A
 74%|███████▎  | 608/827 [43:11<13:53,  3.80s/it][A
 74%|███████▎  | 609/827 [43:17<16:13,  4.46s/it][A
 74%|███████▍  | 610/827 [43:24<18:20,  5.07s/it][A
 74%|███████▍  | 611/827 [43:28<17:07,  4.76s/it][A
 74%|███████▍  | 612/827 [43:32<16:45,  4.68s/it][A
 74%|███████▍  | 613/827 [43:37<17:01,  4.77s/it][A
 74%|███████▍  | 614/827 [43:42<16:25,  4.63s/it][A
 74%|███████▍  | 615/827 [43:46<15:34,  4.41s/it][A
 74%|███████▍  | 616/827 [43:49<14:17,  4.06s/it][A
 75%|███████▍  | 617/827 [43:54<15:47,  4.51s/it][A
 75%|███████▍  | 618/827 [44:01<18:17,  5.25s/it][A
 75%|███████▍  | 619/827 [44:04<15:52,  4.58s/it][A
 75%|███████▍  | 620/827 [44:08<15:09,  4.39s/it][A
 75%|███████▌  | 621/827 [44:11<13:44,  4.00s/it][A
 75%|███████▌  | 622/827 [44:14<12:38,  3.70s/it][A
 75%|███████▌  | 623/827 [44:18<12:29,  3.67s/it][A
 75%|███████▌  | 624/827 [44:23<13:43,  4.06s/it][A
 76%|███████▌  | 625/827 [44:26<13:05,  3.89s/it][A
 76%|███████▌  | 626/827 [44:30<13:02,  3.89s/it][A
 76%|███████▌  | 627/827 [44:34<12:44,  3.82s/it][A
 76%|███████▌  | 628/827 [44:37<11:51,  3.58s/it][A
 76%|███████▌  | 629/827 [44:40<11:39,  3.53s/it][A
 76%|███████▌  | 630/827 [44:44<11:17,  3.44s/it][A
 76%|███████▋  | 631/827 [44:49<13:21,  4.09s/it][A
 76%|███████▋  | 632/827 [44:54<14:25,  4.44s/it][A
 77%|███████▋  | 633/827 [45:00<15:33,  4.81s/it][A
 77%|███████▋  | 634/827 [45:07<17:05,  5.31s/it][A
 77%|███████▋  | 635/827 [45:10<14:52,  4.65s/it][A
 77%|███████▋  | 636/827 [45:14<14:27,  4.54s/it][A
 77%|███████▋  | 637/827 [45:20<15:15,  4.82s/it][A
 77%|███████▋  | 638/827 [45:24<15:10,  4.82s/it][A
 77%|███████▋  | 639/827 [45:28<13:35,  4.34s/it][A
 77%|███████▋  | 640/827 [45:31<12:36,  4.04s/it][A
 78%|███████▊  | 641/827 [45:35<12:49,  4.14s/it][A
 78%|███████▊  | 642/827 [45:39<12:26,  4.04s/it][A
 78%|███████▊  | 643/827 [45:44<13:15,  4.32s/it][A
 78%|███████▊  | 644/827 [45:49<13:27,  4.41s/it][A
 78%|███████▊  | 645/827 [45:52<12:30,  4.12s/it][A
 78%|███████▊  | 646/827 [45:55<11:27,  3.80s/it][A
 78%|███████▊  | 647/827 [45:59<11:12,  3.73s/it][A
 78%|███████▊  | 648/827 [46:03<11:34,  3.88s/it][A
 78%|███████▊  | 649/827 [46:07<11:24,  3.85s/it][A
 79%|███████▊  | 650/827 [46:10<10:56,  3.71s/it][A
 79%|███████▊  | 651/827 [46:14<10:38,  3.63s/it][A
 79%|███████▉  | 652/827 [46:17<10:22,  3.56s/it][A
 79%|███████▉  | 653/827 [46:21<10:49,  3.73s/it][A
 79%|███████▉  | 654/827 [46:27<12:36,  4.37s/it][A
 79%|███████▉  | 655/827 [46:33<13:33,  4.73s/it][A
 79%|███████▉  | 656/827 [46:35<11:58,  4.20s/it][A
 79%|███████▉  | 657/827 [46:39<11:04,  3.91s/it][A
 80%|███████▉  | 658/827 [46:44<11:59,  4.25s/it][A
 80%|███████▉  | 659/827 [46:52<15:20,  5.48s/it][A
 80%|███████▉  | 660/827 [46:58<15:14,  5.48s/it][A
 80%|███████▉  | 661/827 [47:03<15:28,  5.60s/it][A
 80%|████████  | 662/827 [47:08<14:12,  5.16s/it][A
 80%|████████  | 663/827 [47:12<13:37,  4.99s/it][A
 80%|████████  | 664/827 [47:17<13:48,  5.08s/it][A
 80%|████████  | 665/827 [47:22<13:02,  4.83s/it][A
 81%|████████  | 666/827 [47:26<12:25,  4.63s/it][A
 81%|████████  | 667/827 [47:29<11:24,  4.28s/it][A
 81%|████████  | 668/827 [47:33<11:09,  4.21s/it][A
 81%|████████  | 669/827 [47:36<10:03,  3.82s/it][A
 81%|████████  | 670/827 [47:40<09:31,  3.64s/it][A
 81%|████████  | 671/827 [47:44<09:43,  3.74s/it][A
 81%|████████▏ | 672/827 [47:47<09:28,  3.67s/it][A
 81%|████████▏ | 673/827 [47:50<09:08,  3.56s/it][A
 81%|████████▏ | 674/827 [47:54<08:56,  3.51s/it][A
 82%|████████▏ | 675/827 [47:58<09:13,  3.64s/it][A
 82%|████████▏ | 676/827 [48:03<10:16,  4.08s/it][A
 82%|████████▏ | 677/827 [48:08<11:12,  4.48s/it][A
 82%|████████▏ | 678/827 [48:13<11:42,  4.72s/it][A
 82%|████████▏ | 679/827 [48:18<11:32,  4.68s/it][A
 82%|████████▏ | 680/827 [48:21<10:20,  4.22s/it][A
 82%|████████▏ | 681/827 [48:27<11:14,  4.62s/it][A
 82%|████████▏ | 682/827 [48:32<11:27,  4.74s/it][A
 83%|████████▎ | 683/827 [48:35<10:04,  4.20s/it][A
 83%|████████▎ | 684/827 [48:38<09:14,  3.88s/it][A
 83%|████████▎ | 685/827 [48:41<08:30,  3.60s/it][A
 83%|████████▎ | 686/827 [48:47<10:21,  4.41s/it][A
 83%|████████▎ | 687/827 [48:54<12:16,  5.26s/it][A
 83%|████████▎ | 688/827 [49:00<12:18,  5.31s/it][A
 83%|████████▎ | 689/827 [49:05<11:56,  5.19s/it][A
 83%|████████▎ | 690/827 [49:08<10:33,  4.62s/it][A
 84%|████████▎ | 691/827 [49:12<09:44,  4.30s/it][A
 84%|████████▎ | 692/827 [49:16<09:40,  4.30s/it][A
 84%|████████▍ | 693/827 [49:19<09:06,  4.08s/it][A
 84%|████████▍ | 694/827 [49:23<08:37,  3.89s/it][A
 84%|████████▍ | 695/827 [49:27<08:57,  4.07s/it][A
 84%|████████▍ | 696/827 [49:32<09:24,  4.31s/it][A
 84%|████████▍ | 697/827 [49:36<08:57,  4.13s/it][A
 84%|████████▍ | 698/827 [49:39<07:54,  3.68s/it][A
 85%|████████▍ | 699/827 [49:42<07:37,  3.57s/it][A
 85%|████████▍ | 700/827 [49:49<09:42,  4.58s/it][A
 85%|████████▍ | 701/827 [49:55<10:52,  5.18s/it][A
 85%|████████▍ | 702/827 [50:00<10:31,  5.05s/it][A
 85%|████████▌ | 703/827 [50:04<09:43,  4.71s/it][A
 85%|████████▌ | 704/827 [50:07<08:46,  4.28s/it][A
 85%|████████▌ | 705/827 [50:10<07:49,  3.85s/it][A
 85%|████████▌ | 706/827 [50:13<07:14,  3.59s/it][A
 85%|████████▌ | 707/827 [50:17<07:30,  3.76s/it][A
 86%|████████▌ | 708/827 [50:21<07:14,  3.65s/it][A
 86%|████████▌ | 709/827 [50:25<07:22,  3.75s/it][A
 86%|████████▌ | 710/827 [50:28<07:19,  3.76s/it][A
 86%|████████▌ | 711/827 [50:33<07:27,  3.86s/it][A
 86%|████████▌ | 712/827 [50:36<06:59,  3.65s/it][A
 86%|████████▌ | 713/827 [50:39<06:52,  3.62s/it][A
 86%|████████▋ | 714/827 [50:44<07:15,  3.85s/it][A
 86%|████████▋ | 715/827 [50:48<07:30,  4.03s/it][A
 87%|████████▋ | 716/827 [50:52<07:31,  4.07s/it][A
 87%|████████▋ | 717/827 [50:58<08:08,  4.45s/it][A
 87%|████████▋ | 718/827 [51:03<08:27,  4.65s/it][A
 87%|████████▋ | 719/827 [51:06<07:45,  4.31s/it][A
 87%|████████▋ | 720/827 [51:10<07:22,  4.14s/it][A
 87%|████████▋ | 721/827 [51:13<06:48,  3.86s/it][A
 87%|████████▋ | 722/827 [51:17<06:45,  3.86s/it][A
 87%|████████▋ | 723/827 [51:21<06:35,  3.80s/it][A
 88%|████████▊ | 724/827 [51:23<05:55,  3.45s/it][A
 88%|████████▊ | 725/827 [51:29<06:55,  4.07s/it][A
 88%|████████▊ | 726/827 [51:34<07:36,  4.52s/it][A
 88%|████████▊ | 727/827 [51:39<07:21,  4.42s/it][A
 88%|████████▊ | 728/827 [51:42<06:41,  4.05s/it][A
 88%|████████▊ | 729/827 [51:45<06:02,  3.70s/it][A
 88%|████████▊ | 730/827 [51:49<06:09,  3.81s/it][A
 88%|████████▊ | 731/827 [51:54<06:47,  4.25s/it][A
 89%|████████▊ | 732/827 [51:58<06:49,  4.31s/it][A
 89%|████████▊ | 733/827 [52:03<06:47,  4.33s/it][A
 89%|████████▉ | 734/827 [52:06<05:57,  3.85s/it][A
 89%|████████▉ | 735/827 [52:10<06:02,  3.94s/it][A
 89%|████████▉ | 736/827 [52:16<06:50,  4.51s/it][A
 89%|████████▉ | 737/827 [52:21<06:58,  4.65s/it][A
 89%|████████▉ | 738/827 [52:24<06:17,  4.24s/it][A
 89%|████████▉ | 739/827 [52:29<06:39,  4.54s/it][A
 89%|████████▉ | 740/827 [52:35<07:10,  4.94s/it][A
 90%|████████▉ | 741/827 [52:39<06:45,  4.72s/it][A
 90%|████████▉ | 742/827 [52:43<06:32,  4.62s/it][A
 90%|████████▉ | 743/827 [52:49<06:57,  4.97s/it][A
 90%|████████▉ | 744/827 [52:53<06:31,  4.72s/it][A
 90%|█████████ | 745/827 [52:57<06:09,  4.51s/it][A
 90%|█████████ | 746/827 [53:04<06:42,  4.97s/it][A
 90%|█████████ | 747/827 [53:09<06:44,  5.06s/it][A
 90%|█████████ | 748/827 [53:13<06:15,  4.75s/it][A
 91%|█████████ | 749/827 [53:18<06:11,  4.76s/it][A
 91%|█████████ | 750/827 [53:22<05:57,  4.64s/it][A
 91%|█████████ | 751/827 [53:25<05:23,  4.25s/it][A
 91%|█████████ | 752/827 [53:28<04:52,  3.90s/it][A
 91%|█████████ | 753/827 [53:31<04:26,  3.60s/it][A
 91%|█████████ | 754/827 [53:34<04:12,  3.46s/it][A
 91%|█████████▏| 755/827 [53:39<04:37,  3.86s/it][A
 91%|█████████▏| 756/827 [53:44<05:01,  4.25s/it][A
 92%|█████████▏| 757/827 [53:49<04:59,  4.29s/it][A
 92%|█████████▏| 758/827 [53:53<04:56,  4.30s/it][A
 92%|█████████▏| 759/827 [53:56<04:28,  3.95s/it][A
 92%|█████████▏| 760/827 [54:02<05:07,  4.59s/it][A
 92%|█████████▏| 761/827 [54:09<05:49,  5.29s/it][A
 92%|█████████▏| 762/827 [54:16<06:11,  5.72s/it][A
 92%|█████████▏| 763/827 [54:22<06:14,  5.86s/it][A
 92%|█████████▏| 764/827 [54:26<05:27,  5.20s/it][A
 93%|█████████▎| 765/827 [54:29<04:43,  4.58s/it][A
 93%|█████████▎| 766/827 [54:32<04:10,  4.11s/it][A
 93%|█████████▎| 767/827 [54:36<03:57,  3.96s/it][A
 93%|█████████▎| 768/827 [54:40<03:58,  4.05s/it][A
 93%|█████████▎| 769/827 [54:44<04:04,  4.21s/it][A
 93%|█████████▎| 770/827 [54:48<03:47,  3.99s/it][A
 93%|█████████▎| 771/827 [54:52<03:41,  3.96s/it][A
 93%|█████████▎| 772/827 [54:56<03:35,  3.93s/it][A
 93%|█████████▎| 773/827 [55:00<03:40,  4.07s/it][A
 94%|█████████▎| 774/827 [55:03<03:24,  3.85s/it][A
 94%|█████████▎| 775/827 [55:07<03:23,  3.92s/it][A
 94%|█████████▍| 776/827 [55:11<03:17,  3.88s/it][A
 94%|█████████▍| 777/827 [55:15<03:06,  3.72s/it][A
 94%|█████████▍| 778/827 [55:18<03:01,  3.71s/it][A
 94%|█████████▍| 779/827 [55:21<02:50,  3.55s/it][A
 94%|█████████▍| 780/827 [55:25<02:42,  3.45s/it][A
 94%|█████████▍| 781/827 [55:28<02:33,  3.35s/it][A
 95%|█████████▍| 782/827 [55:31<02:29,  3.31s/it][A
 95%|█████████▍| 783/827 [55:34<02:28,  3.37s/it][A
 95%|█████████▍| 784/827 [55:38<02:26,  3.42s/it][A
 95%|█████████▍| 785/827 [55:42<02:29,  3.55s/it][A
 95%|█████████▌| 786/827 [55:47<02:41,  3.94s/it][A
 95%|█████████▌| 787/827 [55:52<02:56,  4.41s/it][A
 95%|█████████▌| 788/827 [55:56<02:48,  4.33s/it][A
 95%|█████████▌| 789/827 [56:01<02:50,  4.48s/it][A
 96%|█████████▌| 790/827 [56:05<02:36,  4.23s/it][A
 96%|█████████▌| 791/827 [56:08<02:19,  3.88s/it][A
 96%|█████████▌| 792/827 [56:11<02:10,  3.72s/it][A
 96%|█████████▌| 793/827 [56:17<02:22,  4.18s/it][A
 96%|█████████▌| 794/827 [56:22<02:29,  4.54s/it][A
 96%|█████████▌| 795/827 [56:28<02:43,  5.12s/it][A
 96%|█████████▋| 796/827 [56:34<02:38,  5.13s/it][A
 96%|█████████▋| 797/827 [56:37<02:16,  4.56s/it][A
 96%|█████████▋| 798/827 [56:40<01:57,  4.05s/it][A
 97%|█████████▋| 799/827 [56:42<01:42,  3.65s/it][A
 97%|█████████▋| 800/827 [56:50<02:09,  4.79s/it][A
 97%|█████████▋| 801/827 [56:57<02:23,  5.51s/it][A
 97%|█████████▋| 802/827 [57:02<02:11,  5.26s/it][A
 97%|█████████▋| 803/827 [57:08<02:10,  5.45s/it][A
 97%|█████████▋| 804/827 [57:13<02:02,  5.33s/it][A
 97%|█████████▋| 805/827 [57:16<01:43,  4.69s/it][A
 97%|█████████▋| 806/827 [57:19<01:27,  4.15s/it][A
 98%|█████████▊| 807/827 [57:22<01:19,  3.98s/it][A
 98%|█████████▊| 808/827 [57:26<01:15,  3.96s/it][A
 98%|█████████▊| 809/827 [57:30<01:08,  3.79s/it][A
 98%|█████████▊| 810/827 [57:33<01:04,  3.77s/it][A
 98%|█████████▊| 811/827 [57:37<01:00,  3.76s/it][A
 98%|█████████▊| 812/827 [57:41<00:59,  3.96s/it][A
 98%|█████████▊| 813/827 [57:47<01:01,  4.38s/it][A
 98%|█████████▊| 814/827 [57:52<01:01,  4.72s/it][A
 99%|█████████▊| 815/827 [57:57<00:55,  4.59s/it][A
 99%|█████████▊| 816/827 [58:01<00:48,  4.42s/it][A
 99%|█████████▉| 817/827 [58:04<00:41,  4.12s/it][A
 99%|█████████▉| 818/827 [58:10<00:41,  4.64s/it][A
 99%|█████████▉| 819/827 [58:15<00:38,  4.79s/it][A
 99%|█████████▉| 820/827 [58:20<00:33,  4.76s/it][A
 99%|█████████▉| 821/827 [58:24<00:27,  4.55s/it][A
 99%|█████████▉| 822/827 [58:28<00:21,  4.34s/it][A
100%|█████████▉| 823/827 [58:30<00:15,  3.89s/it][A
100%|█████████▉| 824/827 [58:34<00:11,  3.69s/it][A
100%|█████████▉| 825/827 [58:39<00:08,  4.19s/it][A
100%|█████████▉| 826/827 [58:45<00:04,  4.68s/it][A
100%|██████████| 827/827 [58:47<00:00,  4.00s/it][A                                                       
                                                 [A{'eval_loss': 0.7407565116882324, 'eval_runtime': 3533.9556, 'eval_samples_per_second': 0.935, 'eval_steps_per_second': 0.234, 'epoch': 0.43}
 22%|██▏       | 200/930 [6:36:20<21:38:55, 106.76s/it]
100%|██████████| 827/827 [58:48<00:00,  4.00s/it][A
                                                 [A[INFO|trainer.py:4309] 2025-12-08 17:45:57,214 >> Saving model checkpoint to saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-200
/opt/conda/lib/python3.11/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in Qwen/Qwen2.5-VL-7B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2421] 2025-12-08 17:45:57,301 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-200/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-08 17:45:57,303 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-08 17:45:57,305 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-200/special_tokens_map.json
[2025-12-08 17:45:58,037] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!
[2025-12-08 17:45:58,059] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/mp_rank_00_model_states.pt
[2025-12-08 17:45:58,059] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/mp_rank_00_model_states.pt...
[2025-12-08 17:45:58,123] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/mp_rank_00_model_states.pt.
[2025-12-08 17:45:58,126] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-12-08 17:45:58,126] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2025-12-08 17:45:58,126] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2025-12-08 17:45:58,126] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2025-12-08 17:45:58,219] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2025-12-08 17:45:58,219] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2025-12-08 17:45:58,219] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2025-12-08 17:45:58,228] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2025-12-08 17:45:58,229] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2025-12-08 17:45:58,229] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2025-12-08 17:45:58,231] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-12-08 17:45:58,235] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-12-08 17:45:58,236] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2025-12-08 17:45:58,306] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2025-12-08 17:45:58,306] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2025-12-08 17:45:58,306] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[INFO|image_processing_base.py:253] 2025-12-08 17:45:58,319 >> Image processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-200/preprocessor_config.json
[INFO|tokenization_utils_base.py:2421] 2025-12-08 17:45:58,321 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-200/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-08 17:45:58,324 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-08 17:45:58,326 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-200/special_tokens_map.json
[INFO|video_processing_utils.py:600] 2025-12-08 17:45:58,479 >> Video processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-200/video_preprocessor_config.json
[INFO|processing_utils.py:814] 2025-12-08 17:45:58,481 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-200/chat_template.jinja
 22%|██▏       | 201/930 [6:37:55<235:36:08, 1163.47s/it] 22%|██▏       | 202/930 [6:39:30<170:27:33, 842.93s/it]  22%|██▏       | 203/930 [6:40:59<124:34:20, 616.86s/it] 22%|██▏       | 204/930 [6:42:48<93:40:40, 464.52s/it]  22%|██▏       | 205/930 [6:44:41<72:18:14, 359.03s/it] 22%|██▏       | 206/930 [6:46:22<56:38:23, 281.63s/it] 22%|██▏       | 207/930 [6:48:15<46:21:23, 230.82s/it] 22%|██▏       | 208/930 [6:49:58<38:38:50, 192.70s/it] 22%|██▏       | 209/930 [6:51:36<32:54:15, 164.29s/it] 23%|██▎       | 210/930 [6:53:21<29:15:47, 146.32s/it]                                                       {'loss': 0.7245, 'grad_norm': 0.2707437574863434, 'learning_rate': 9.533519757128426e-05, 'epoch': 0.45}
 23%|██▎       | 210/930 [6:53:21<29:15:47, 146.32s/it] 23%|██▎       | 211/930 [6:55:04<26:39:54, 133.51s/it] 23%|██▎       | 212/930 [6:56:49<24:54:26, 124.88s/it] 23%|██▎       | 213/930 [6:58:17<22:38:32, 113.68s/it] 23%|██▎       | 214/930 [6:59:41<20:51:11, 104.85s/it] 23%|██▎       | 215/930 [7:01:18<20:20:32, 102.42s/it] 23%|██▎       | 216/930 [7:03:11<20:58:32, 105.76s/it] 23%|██▎       | 217/930 [7:05:08<21:36:05, 109.07s/it] 23%|██▎       | 218/930 [7:06:43<20:44:20, 104.86s/it] 24%|██▎       | 219/930 [7:08:12<19:44:51, 99.99s/it]  24%|██▎       | 220/930 [7:09:53<19:49:50, 100.55s/it]                                                       {'loss': 0.7351, 'grad_norm': 0.271445631980896, 'learning_rate': 9.451192254041758e-05, 'epoch': 0.47}
 24%|██▎       | 220/930 [7:09:53<19:49:50, 100.55s/it] 24%|██▍       | 221/930 [7:11:44<20:22:13, 103.43s/it] 24%|██▍       | 222/930 [7:13:55<21:59:15, 111.80s/it] 24%|██▍       | 223/930 [7:15:26<20:43:16, 105.51s/it] 24%|██▍       | 224/930 [7:17:12<20:44:17, 105.75s/it] 24%|██▍       | 225/930 [7:18:46<20:00:18, 102.15s/it] 24%|██▍       | 226/930 [7:20:21<19:35:17, 100.17s/it] 24%|██▍       | 227/930 [7:22:17<20:29:18, 104.92s/it] 25%|██▍       | 228/930 [7:24:01<20:24:39, 104.67s/it] 25%|██▍       | 229/930 [7:25:32<19:31:40, 100.29s/it] 25%|██▍       | 230/930 [7:26:52<18:21:51, 94.44s/it]                                                       {'loss': 0.7214, 'grad_norm': 0.2580903172492981, 'learning_rate': 9.362594654883185e-05, 'epoch': 0.49}
 25%|██▍       | 230/930 [7:26:52<18:21:51, 94.44s/it] 25%|██▍       | 231/930 [7:28:10<17:21:21, 89.39s/it] 25%|██▍       | 232/930 [7:29:48<17:48:21, 91.84s/it] 25%|██▌       | 233/930 [7:31:40<19:00:29, 98.18s/it] 25%|██▌       | 234/930 [7:33:27<19:27:17, 100.63s/it] 25%|██▌       | 235/930 [7:35:10<19:35:58, 101.52s/it] 25%|██▌       | 236/930 [7:36:58<19:54:05, 103.24s/it] 25%|██▌       | 237/930 [7:38:46<20:08:37, 104.64s/it] 26%|██▌       | 238/930 [7:40:22<19:38:37, 102.19s/it] 26%|██▌       | 239/930 [7:41:58<19:17:01, 100.46s/it] 26%|██▌       | 240/930 [7:43:59<20:24:08, 106.45s/it]                                                       {'loss': 0.7261, 'grad_norm': 0.3023596405982971, 'learning_rate': 9.267851761150093e-05, 'epoch': 0.52}
 26%|██▌       | 240/930 [7:43:59<20:24:08, 106.45s/it] 26%|██▌       | 241/930 [7:45:42<20:10:36, 105.42s/it] 26%|██▌       | 242/930 [7:47:16<19:31:23, 102.16s/it] 26%|██▌       | 243/930 [7:49:16<20:28:24, 107.29s/it] 26%|██▌       | 244/930 [7:51:00<20:17:25, 106.48s/it] 26%|██▋       | 245/930 [7:52:46<20:12:40, 106.22s/it] 26%|██▋       | 246/930 [7:54:31<20:08:24, 106.00s/it] 27%|██▋       | 247/930 [7:56:02<19:13:42, 101.35s/it] 27%|██▋       | 248/930 [7:57:41<19:03:03, 100.56s/it] 27%|██▋       | 249/930 [7:59:37<19:54:31, 105.24s/it] 27%|██▋       | 250/930 [8:01:22<19:51:43, 105.15s/it]                                                       {'loss': 0.7244, 'grad_norm': 0.22445327043533325, 'learning_rate': 9.167097030804288e-05, 'epoch': 0.54}
 27%|██▋       | 250/930 [8:01:22<19:51:43, 105.15s/it] 27%|██▋       | 251/930 [8:03:06<19:45:30, 104.76s/it] 27%|██▋       | 252/930 [8:04:35<18:50:39, 100.06s/it] 27%|██▋       | 253/930 [8:06:02<18:04:26, 96.11s/it]  27%|██▋       | 254/930 [8:07:39<18:05:37, 96.36s/it] 27%|██▋       | 255/930 [8:09:06<17:33:55, 93.68s/it] 28%|██▊       | 256/930 [8:10:30<16:58:33, 90.67s/it] 28%|██▊       | 257/930 [8:12:13<17:41:07, 94.60s/it] 28%|██▊       | 258/930 [8:14:12<18:58:43, 101.67s/it] 28%|██▊       | 259/930 [8:15:51<18:49:00, 100.95s/it] 28%|██▊       | 260/930 [8:17:40<19:14:49, 103.42s/it]                                                       {'loss': 0.7353, 'grad_norm': 0.2471494823694229, 'learning_rate': 9.060472390278718e-05, 'epoch': 0.56}
 28%|██▊       | 260/930 [8:17:40<19:14:49, 103.42s/it] 28%|██▊       | 261/930 [8:19:31<19:39:01, 105.74s/it] 28%|██▊       | 262/930 [8:21:10<19:14:56, 103.74s/it] 28%|██▊       | 263/930 [8:22:53<19:09:45, 103.43s/it] 28%|██▊       | 264/930 [8:24:11<17:42:49, 95.75s/it]  28%|██▊       | 265/930 [8:25:47<17:43:55, 95.99s/it] 29%|██▊       | 266/930 [8:27:34<18:17:50, 99.20s/it] 29%|██▊       | 267/930 [8:29:16<18:24:22, 99.94s/it] 29%|██▉       | 268/930 [8:30:59<18:34:37, 101.02s/it] 29%|██▉       | 269/930 [8:32:35<18:15:14, 99.42s/it]  29%|██▉       | 270/930 [8:34:03<17:37:40, 96.15s/it]                                                      {'loss': 0.7233, 'grad_norm': 0.2981410622596741, 'learning_rate': 8.94812803455521e-05, 'epoch': 0.58}
 29%|██▉       | 270/930 [8:34:03<17:37:40, 96.15s/it] 29%|██▉       | 271/930 [8:35:56<18:28:50, 100.96s/it] 29%|██▉       | 272/930 [8:37:54<19:25:50, 106.31s/it] 29%|██▉       | 273/930 [8:39:28<18:40:52, 102.36s/it] 29%|██▉       | 274/930 [8:41:15<18:54:45, 103.79s/it] 30%|██▉       | 275/930 [8:42:35<17:36:01, 96.73s/it]  30%|██▉       | 276/930 [8:43:56<16:44:37, 92.17s/it] 30%|██▉       | 277/930 [8:45:26<16:35:17, 91.45s/it] 30%|██▉       | 278/930 [8:47:19<17:44:00, 97.92s/it] 30%|███       | 279/930 [8:48:52<17:25:00, 96.31s/it] 30%|███       | 280/930 [8:50:17<16:46:23, 92.90s/it]                                                      {'loss': 0.7373, 'grad_norm': 0.2760569453239441, 'learning_rate': 8.83022221559489e-05, 'epoch': 0.6}
 30%|███       | 280/930 [8:50:17<16:46:23, 92.90s/it] 30%|███       | 281/930 [8:52:03<17:29:14, 97.00s/it] 30%|███       | 282/930 [8:53:57<18:22:46, 102.11s/it] 30%|███       | 283/930 [8:55:39<18:19:18, 101.95s/it] 31%|███       | 284/930 [8:57:35<19:02:55, 106.15s/it] 31%|███       | 285/930 [8:59:29<19:25:18, 108.40s/it] 31%|███       | 286/930 [9:00:53<18:07:46, 101.35s/it] 31%|███       | 287/930 [9:02:34<18:04:45, 101.22s/it] 31%|███       | 288/930 [9:04:27<18:39:18, 104.61s/it] 31%|███       | 289/930 [9:06:01<18:02:53, 101.36s/it] 31%|███       | 290/930 [9:07:43<18:05:26, 101.76s/it]                                                       {'loss': 0.7215, 'grad_norm': 0.24977006018161774, 'learning_rate': 8.706921019419236e-05, 'epoch': 0.62}
 31%|███       | 290/930 [9:07:43<18:05:26, 101.76s/it] 31%|███▏      | 291/930 [9:09:03<16:51:44, 95.00s/it]  31%|███▏      | 292/930 [9:10:52<17:35:36, 99.27s/it] 32%|███▏      | 293/930 [9:12:39<17:58:46, 101.61s/it] 32%|███▏      | 294/930 [9:14:26<18:13:05, 103.12s/it] 32%|███▏      | 295/930 [9:16:03<17:52:18, 101.32s/it] 32%|███▏      | 296/930 [9:17:49<18:07:09, 102.89s/it] 32%|███▏      | 297/930 [9:19:47<18:53:22, 107.43s/it] 32%|███▏      | 298/930 [9:21:20<18:05:06, 103.02s/it] 32%|███▏      | 299/930 [9:23:03<18:02:23, 102.92s/it] 32%|███▏      | 300/930 [9:24:49<18:10:48, 103.89s/it]                                                       {'loss': 0.7238, 'grad_norm': 0.26210853457450867, 'learning_rate': 8.578398132155845e-05, 'epoch': 0.65}
 32%|███▏      | 300/930 [9:24:49<18:10:48, 103.89s/it] 32%|███▏      | 301/930 [9:26:18<17:24:35, 99.64s/it]  32%|███▏      | 302/930 [9:27:56<17:15:00, 98.89s/it] 33%|███▎      | 303/930 [9:29:48<17:57:11, 103.08s/it] 33%|███▎      | 304/930 [9:31:25<17:33:54, 101.01s/it] 33%|███▎      | 305/930 [9:33:29<18:43:48, 107.89s/it] 33%|███▎      | 306/930 [9:35:16<18:39:35, 107.65s/it] 33%|███▎      | 307/930 [9:36:39<17:21:47, 100.33s/it] 33%|███▎      | 308/930 [9:38:18<17:15:14, 99.86s/it]  33%|███▎      | 309/930 [9:40:02<17:28:03, 101.26s/it] 33%|███▎      | 310/930 [9:42:03<18:25:39, 107.00s/it]                                                       {'loss': 0.7392, 'grad_norm': 0.23182083666324615, 'learning_rate': 8.444834595378434e-05, 'epoch': 0.67}
 33%|███▎      | 310/930 [9:42:03<18:25:39, 107.00s/it] 33%|███▎      | 311/930 [9:44:00<18:54:35, 109.98s/it] 34%|███▎      | 312/930 [9:45:48<18:48:42, 109.58s/it] 34%|███▎      | 313/930 [9:47:33<18:30:53, 108.03s/it] 34%|███▍      | 314/930 [9:49:24<18:38:47, 108.97s/it] 34%|███▍      | 315/930 [9:51:03<18:06:51, 106.03s/it] 34%|███▍      | 316/930 [9:52:47<17:59:15, 105.47s/it] 34%|███▍      | 317/930 [9:54:18<17:13:09, 101.12s/it] 34%|███▍      | 318/930 [9:55:58<17:06:45, 100.66s/it] 34%|███▍      | 319/930 [9:57:48<17:35:25, 103.64s/it] 34%|███▍      | 320/930 [9:59:29<17:23:53, 102.68s/it]                                                       {'loss': 0.7235, 'grad_norm': 0.3070104718208313, 'learning_rate': 8.306418551085706e-05, 'epoch': 0.69}
 34%|███▍      | 320/930 [9:59:29<17:23:53, 102.68s/it] 35%|███▍      | 321/930 [10:01:01<16:50:39, 99.57s/it] 35%|███▍      | 322/930 [10:02:47<17:07:41, 101.42s/it] 35%|███▍      | 323/930 [10:04:20<16:41:47, 99.02s/it]  35%|███▍      | 324/930 [10:05:53<16:21:12, 97.15s/it] 35%|███▍      | 325/930 [10:07:36<16:37:44, 98.95s/it] 35%|███▌      | 326/930 [10:09:18<16:43:58, 99.73s/it] 35%|███▌      | 327/930 [10:10:57<16:39:52, 99.49s/it] 35%|███▌      | 328/930 [10:12:39<16:46:59, 100.36s/it] 35%|███▌      | 329/930 [10:14:20<16:46:53, 100.52s/it] 35%|███▌      | 330/930 [10:15:58<16:39:09, 99.92s/it]                                                        {'loss': 0.718, 'grad_norm': 0.24300529062747955, 'learning_rate': 8.163344976678342e-05, 'epoch': 0.71}
 35%|███▌      | 330/930 [10:15:58<16:39:09, 99.92s/it] 36%|███▌      | 331/930 [10:17:41<16:44:37, 100.63s/it] 36%|███▌      | 332/930 [10:19:29<17:05:08, 102.86s/it] 36%|███▌      | 333/930 [10:21:07<16:49:41, 101.48s/it] 36%|███▌      | 334/930 [10:22:41<16:25:25, 99.20s/it]  36%|███▌      | 335/930 [10:24:06<15:41:43, 94.96s/it] 36%|███▌      | 336/930 [10:25:35<15:22:18, 93.16s/it] 36%|███▌      | 337/930 [10:27:29<16:23:59, 99.56s/it] 36%|███▋      | 338/930 [10:29:07<16:17:39, 99.09s/it] 36%|███▋      | 339/930 [10:30:44<16:07:21, 98.21s/it] 37%|███▋      | 340/930 [10:32:21<16:03:35, 97.99s/it]                                                       {'loss': 0.727, 'grad_norm': 0.2501356899738312, 'learning_rate': 8.015815410307398e-05, 'epoch': 0.73}
 37%|███▋      | 340/930 [10:32:21<16:03:35, 97.99s/it] 37%|███▋      | 341/930 [10:34:04<16:17:51, 99.61s/it] 37%|███▋      | 342/930 [10:35:49<16:29:16, 100.95s/it] 37%|███▋      | 343/930 [10:37:23<16:09:39, 99.11s/it]  37%|███▋      | 344/930 [10:39:11<16:33:29, 101.72s/it] 37%|███▋      | 345/930 [10:40:52<16:30:05, 101.55s/it] 37%|███▋      | 346/930 [10:42:32<16:23:25, 101.04s/it] 37%|███▋      | 347/930 [10:44:09<16:08:59, 99.73s/it]  37%|███▋      | 348/930 [10:45:57<16:31:40, 102.23s/it] 38%|███▊      | 349/930 [10:47:28<15:56:15, 98.75s/it]  38%|███▊      | 350/930 [10:49:07<15:56:50, 98.98s/it]                                                       {'loss': 0.7192, 'grad_norm': 0.24036923050880432, 'learning_rate': 7.864037666981037e-05, 'epoch': 0.75}
 38%|███▊      | 350/930 [10:49:07<15:56:50, 98.98s/it] 38%|███▊      | 351/930 [10:51:01<16:37:40, 103.39s/it] 38%|███▊      | 352/930 [10:52:30<15:55:00, 99.14s/it]  38%|███▊      | 353/930 [10:54:14<16:08:21, 100.70s/it] 38%|███▊      | 354/930 [10:55:49<15:49:22, 98.89s/it]  38%|███▊      | 355/930 [10:57:31<15:58:11, 99.99s/it] 38%|███▊      | 356/930 [10:59:23<16:28:18, 103.31s/it] 38%|███▊      | 357/930 [11:01:11<16:42:26, 104.97s/it] 38%|███▊      | 358/930 [11:02:33<15:34:18, 98.00s/it]  39%|███▊      | 359/930 [11:04:02<15:06:04, 95.21s/it] 39%|███▊      | 360/930 [11:05:42<15:18:55, 96.73s/it]                                                       {'loss': 0.7343, 'grad_norm': 0.2899198532104492, 'learning_rate': 7.708225545829446e-05, 'epoch': 0.77}
 39%|███▊      | 360/930 [11:05:42<15:18:55, 96.73s/it] 39%|███▉      | 361/930 [11:07:26<15:39:05, 99.03s/it] 39%|███▉      | 362/930 [11:09:00<15:21:06, 97.30s/it] 39%|███▉      | 363/930 [11:10:20<14:32:22, 92.31s/it] 39%|███▉      | 364/930 [11:11:46<14:12:35, 90.38s/it] 39%|███▉      | 365/930 [11:13:25<14:35:44, 93.00s/it] 39%|███▉      | 366/930 [11:15:12<15:11:23, 96.96s/it] 39%|███▉      | 367/930 [11:16:57<15:32:49, 99.41s/it] 40%|███▉      | 368/930 [11:18:31<15:16:47, 97.88s/it] 40%|███▉      | 369/930 [11:19:58<14:45:41, 94.73s/it] 40%|███▉      | 370/930 [11:21:42<15:07:39, 97.25s/it]                                                       {'loss': 0.7387, 'grad_norm': 0.23061959445476532, 'learning_rate': 7.548598528940353e-05, 'epoch': 0.8}
 40%|███▉      | 370/930 [11:21:42<15:07:39, 97.25s/it] 40%|███▉      | 371/930 [11:23:22<15:14:12, 98.13s/it] 40%|████      | 372/930 [11:24:53<14:53:16, 96.05s/it] 40%|████      | 373/930 [11:26:26<14:44:26, 95.27s/it] 40%|████      | 374/930 [11:28:17<15:24:49, 99.80s/it] 40%|████      | 375/930 [11:30:02<15:38:57, 101.51s/it] 40%|████      | 376/930 [11:31:46<15:44:46, 102.32s/it] 41%|████      | 377/930 [11:33:27<15:38:19, 101.81s/it] 41%|████      | 378/930 [11:35:07<15:30:07, 101.10s/it] 41%|████      | 379/930 [11:36:43<15:16:32, 99.81s/it]  41%|████      | 380/930 [11:38:14<14:49:56, 97.08s/it]                                                       {'loss': 0.7449, 'grad_norm': 0.22765184938907623, 'learning_rate': 7.385381472189321e-05, 'epoch': 0.82}
 41%|████      | 380/930 [11:38:14<14:49:56, 97.08s/it] 41%|████      | 381/930 [11:39:51<14:49:02, 97.16s/it] 41%|████      | 382/930 [11:41:45<15:32:32, 102.10s/it] 41%|████      | 383/930 [11:43:30<15:38:05, 102.90s/it] 41%|████▏     | 384/930 [11:45:08<15:23:03, 101.43s/it] 41%|████▏     | 385/930 [11:46:54<15:35:14, 102.96s/it] 42%|████▏     | 386/930 [11:48:31<15:16:48, 101.12s/it] 42%|████▏     | 387/930 [11:50:00<14:41:24, 97.39s/it]  42%|████▏     | 388/930 [11:51:53<15:22:44, 102.15s/it] 42%|████▏     | 389/930 [11:53:32<15:12:38, 101.22s/it] 42%|████▏     | 390/930 [11:55:13<15:10:47, 101.20s/it]                                                        {'loss': 0.7288, 'grad_norm': 0.2208685427904129, 'learning_rate': 7.218804288500343e-05, 'epoch': 0.84}
 42%|████▏     | 390/930 [11:55:13<15:10:47, 101.20s/it] 42%|████▏     | 391/930 [11:57:14<16:01:33, 107.04s/it] 42%|████▏     | 392/930 [11:58:50<15:29:44, 103.69s/it] 42%|████▏     | 393/930 [12:00:36<15:35:44, 104.55s/it] 42%|████▏     | 394/930 [12:02:18<15:27:10, 103.79s/it] 42%|████▏     | 395/930 [12:04:02<15:25:49, 103.83s/it] 43%|████▎     | 396/930 [12:05:43<15:15:28, 102.86s/it] 43%|████▎     | 397/930 [12:07:35<15:39:01, 105.71s/it] 43%|████▎     | 398/930 [12:09:11<15:10:03, 102.64s/it] 43%|████▎     | 399/930 [12:11:06<15:42:47, 106.53s/it] 43%|████▎     | 400/930 [12:12:40<15:06:23, 102.61s/it]                                                        {'loss': 0.7126, 'grad_norm': 0.2418779879808426, 'learning_rate': 7.049101623982937e-05, 'epoch': 0.86}
 43%|████▎     | 400/930 [12:12:40<15:06:23, 102.61s/it]
***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1
[INFO|trainer.py:4643] 2025-12-08 23:22:09,024 >> 
***** Running Evaluation *****
[INFO|trainer.py:4645] 2025-12-08 23:22:09,024 >>   Num examples = 3305
[INFO|trainer.py:4648] 2025-12-08 23:22:09,024 >>   Batch size = 1

  0%|          | 0/827 [00:00<?, ?it/s][A
  0%|          | 2/827 [00:04<29:21,  2.14s/it][A
  0%|          | 3/827 [00:06<32:25,  2.36s/it][A
  0%|          | 4/827 [00:10<37:16,  2.72s/it][A
  1%|          | 5/827 [00:14<42:44,  3.12s/it][A
  1%|          | 6/827 [00:17<45:53,  3.35s/it][A
  1%|          | 7/827 [00:22<49:42,  3.64s/it][A
  1%|          | 8/827 [00:28<1:01:08,  4.48s/it][A
  1%|          | 9/827 [00:36<1:14:25,  5.46s/it][A
  1%|          | 10/827 [00:43<1:21:48,  6.01s/it][A
  1%|▏         | 11/827 [00:47<1:11:53,  5.29s/it][A
  1%|▏         | 12/827 [00:50<1:02:43,  4.62s/it][A
  2%|▏         | 13/827 [00:53<57:11,  4.22s/it]  [A
  2%|▏         | 14/827 [00:57<56:19,  4.16s/it][A
  2%|▏         | 15/827 [01:01<57:46,  4.27s/it][A
  2%|▏         | 16/827 [01:05<55:24,  4.10s/it][A
  2%|▏         | 17/827 [01:08<51:57,  3.85s/it][A
  2%|▏         | 18/827 [01:12<50:49,  3.77s/it][A
  2%|▏         | 19/827 [01:17<56:56,  4.23s/it][A
  2%|▏         | 20/827 [01:23<1:02:42,  4.66s/it][A
  3%|▎         | 21/827 [01:27<1:01:36,  4.59s/it][A
  3%|▎         | 22/827 [01:32<1:02:00,  4.62s/it][A
  3%|▎         | 23/827 [01:35<55:52,  4.17s/it]  [A
  3%|▎         | 24/827 [01:39<55:34,  4.15s/it][A
  3%|▎         | 25/827 [01:45<1:00:05,  4.50s/it][A
  3%|▎         | 26/827 [01:50<1:03:26,  4.75s/it][A
  3%|▎         | 27/827 [01:55<1:02:36,  4.70s/it][A
  3%|▎         | 28/827 [01:58<56:24,  4.24s/it]  [A
  4%|▎         | 29/827 [02:03<1:00:23,  4.54s/it][A
  4%|▎         | 30/827 [02:08<1:02:33,  4.71s/it][A
  4%|▎         | 31/827 [02:12<59:29,  4.48s/it]  [A
  4%|▍         | 32/827 [02:17<59:53,  4.52s/it][A
  4%|▍         | 33/827 [02:22<1:03:10,  4.77s/it][A
  4%|▍         | 34/827 [02:25<57:52,  4.38s/it]  [A
  4%|▍         | 35/827 [02:29<54:02,  4.09s/it][A
  4%|▍         | 36/827 [02:32<48:37,  3.69s/it][A
  4%|▍         | 37/827 [02:37<55:39,  4.23s/it][A
  5%|▍         | 38/827 [02:42<59:00,  4.49s/it][A
  5%|▍         | 39/827 [02:47<1:00:26,  4.60s/it][A
  5%|▍         | 40/827 [02:52<1:02:04,  4.73s/it][A
  5%|▍         | 41/827 [02:56<56:53,  4.34s/it]  [A
  5%|▌         | 42/827 [02:59<55:00,  4.20s/it][A
  5%|▌         | 43/827 [03:03<53:15,  4.08s/it][A
  5%|▌         | 44/827 [03:07<51:58,  3.98s/it][A
  5%|▌         | 45/827 [03:12<54:23,  4.17s/it][A
  6%|▌         | 46/827 [03:17<1:00:37,  4.66s/it][A
  6%|▌         | 47/827 [03:22<1:01:09,  4.70s/it][A
  6%|▌         | 48/827 [03:26<57:16,  4.41s/it]  [A
  6%|▌         | 49/827 [03:30<54:06,  4.17s/it][A
  6%|▌         | 50/827 [03:33<49:36,  3.83s/it][A
  6%|▌         | 51/827 [03:38<56:17,  4.35s/it][A
  6%|▋         | 52/827 [03:44<1:01:57,  4.80s/it][A
  6%|▋         | 53/827 [03:47<54:26,  4.22s/it]  [A
  7%|▋         | 54/827 [03:50<50:46,  3.94s/it][A
  7%|▋         | 55/827 [03:53<47:05,  3.66s/it][A
  7%|▋         | 56/827 [03:57<46:10,  3.59s/it][A
  7%|▋         | 57/827 [04:00<46:41,  3.64s/it][A
  7%|▋         | 58/827 [04:04<45:29,  3.55s/it][A
  7%|▋         | 59/827 [04:07<45:11,  3.53s/it][A
  7%|▋         | 60/827 [04:12<50:10,  3.92s/it][A
  7%|▋         | 61/827 [04:17<54:37,  4.28s/it][A
  7%|▋         | 62/827 [04:20<49:32,  3.89s/it][A
  8%|▊         | 63/827 [04:23<46:29,  3.65s/it][A
  8%|▊         | 64/827 [04:26<44:38,  3.51s/it][A
  8%|▊         | 65/827 [04:30<44:25,  3.50s/it][A
  8%|▊         | 66/827 [04:35<52:05,  4.11s/it][A
  8%|▊         | 67/827 [04:40<55:08,  4.35s/it][A
  8%|▊         | 68/827 [04:43<48:53,  3.87s/it][A
  8%|▊         | 69/827 [04:46<47:28,  3.76s/it][A
  8%|▊         | 70/827 [04:49<44:21,  3.52s/it][A
  9%|▊         | 71/827 [04:54<48:43,  3.87s/it][A
  9%|▊         | 72/827 [04:59<52:25,  4.17s/it][A
  9%|▉         | 73/827 [05:03<50:50,  4.05s/it][A
  9%|▉         | 74/827 [05:09<58:29,  4.66s/it][A
  9%|▉         | 75/827 [05:14<1:00:04,  4.79s/it][A
  9%|▉         | 76/827 [05:17<53:51,  4.30s/it]  [A
  9%|▉         | 77/827 [05:20<49:11,  3.93s/it][A
  9%|▉         | 78/827 [05:23<46:02,  3.69s/it][A
 10%|▉         | 79/827 [05:26<43:38,  3.50s/it][A
 10%|▉         | 80/827 [05:31<48:34,  3.90s/it][A
 10%|▉         | 81/827 [05:36<51:34,  4.15s/it][A
 10%|▉         | 82/827 [05:40<51:37,  4.16s/it][A
 10%|█         | 83/827 [05:45<52:26,  4.23s/it][A
 10%|█         | 84/827 [05:49<53:56,  4.36s/it][A
 10%|█         | 85/827 [05:53<52:12,  4.22s/it][A
 10%|█         | 86/827 [05:56<47:56,  3.88s/it][A
 11%|█         | 87/827 [06:03<59:31,  4.83s/it][A
 11%|█         | 88/827 [06:13<1:19:35,  6.46s/it][A
 11%|█         | 89/827 [06:21<1:24:31,  6.87s/it][A
 11%|█         | 90/827 [06:27<1:18:18,  6.38s/it][A
 11%|█         | 91/827 [06:30<1:08:19,  5.57s/it][A
 11%|█         | 92/827 [06:34<1:02:44,  5.12s/it][A
 11%|█         | 93/827 [06:37<54:44,  4.47s/it]  [A
 11%|█▏        | 94/827 [06:41<50:24,  4.13s/it][A
 11%|█▏        | 95/827 [06:44<47:35,  3.90s/it][A
 12%|█▏        | 96/827 [06:47<43:35,  3.58s/it][A
 12%|█▏        | 97/827 [06:49<39:34,  3.25s/it][A
 12%|█▏        | 98/827 [06:52<37:38,  3.10s/it][A
 12%|█▏        | 99/827 [06:55<36:15,  2.99s/it][A
 12%|█▏        | 100/827 [06:57<33:52,  2.80s/it][A
 12%|█▏        | 101/827 [07:01<37:13,  3.08s/it][A
 12%|█▏        | 102/827 [07:04<37:34,  3.11s/it][A
 12%|█▏        | 103/827 [07:07<36:37,  3.04s/it][A
 13%|█▎        | 104/827 [07:10<36:42,  3.05s/it][A
 13%|█▎        | 105/827 [07:13<38:30,  3.20s/it][A
 13%|█▎        | 106/827 [07:21<52:51,  4.40s/it][A
 13%|█▎        | 107/827 [07:27<59:57,  5.00s/it][A
 13%|█▎        | 108/827 [07:32<59:49,  4.99s/it][A
 13%|█▎        | 109/827 [07:36<55:21,  4.63s/it][A
 13%|█▎        | 110/827 [07:40<52:28,  4.39s/it][A
 13%|█▎        | 111/827 [07:45<56:30,  4.73s/it][A
 14%|█▎        | 112/827 [07:51<1:01:13,  5.14s/it][A
 14%|█▎        | 113/827 [07:55<57:38,  4.84s/it]  [A
 14%|█▍        | 114/827 [07:59<53:06,  4.47s/it][A
 14%|█▍        | 115/827 [08:03<49:39,  4.18s/it][A
 14%|█▍        | 116/827 [08:06<48:18,  4.08s/it][A
 14%|█▍        | 117/827 [08:11<50:45,  4.29s/it][A
 14%|█▍        | 118/827 [08:17<56:07,  4.75s/it][A
 14%|█▍        | 119/827 [08:23<58:47,  4.98s/it][A
 15%|█▍        | 120/827 [08:27<57:10,  4.85s/it][A
 15%|█▍        | 121/827 [08:30<51:19,  4.36s/it][A
 15%|█▍        | 122/827 [08:34<47:38,  4.06s/it][A
 15%|█▍        | 123/827 [08:36<43:16,  3.69s/it][A
 15%|█▍        | 124/827 [08:40<43:14,  3.69s/it][A
 15%|█▌        | 125/827 [08:44<44:58,  3.84s/it][A
 15%|█▌        | 126/827 [08:48<43:44,  3.74s/it][A
 15%|█▌        | 127/827 [08:51<42:59,  3.68s/it][A
 15%|█▌        | 128/827 [08:55<43:38,  3.75s/it][A
 16%|█▌        | 129/827 [08:58<41:05,  3.53s/it][A
 16%|█▌        | 130/827 [09:02<42:11,  3.63s/it][A
 16%|█▌        | 131/827 [09:06<44:26,  3.83s/it][A
 16%|█▌        | 132/827 [09:12<48:52,  4.22s/it][A
 16%|█▌        | 133/827 [09:16<50:34,  4.37s/it][A
 16%|█▌        | 134/827 [09:20<47:42,  4.13s/it][A
 16%|█▋        | 135/827 [09:25<52:04,  4.51s/it][A
 16%|█▋        | 136/827 [09:31<57:44,  5.01s/it][A
 17%|█▋        | 137/827 [09:35<52:24,  4.56s/it][A
 17%|█▋        | 138/827 [09:42<59:10,  5.15s/it][A
 17%|█▋        | 139/827 [09:49<1:05:42,  5.73s/it][A
 17%|█▋        | 140/827 [09:52<58:15,  5.09s/it]  [A
 17%|█▋        | 141/827 [09:56<54:44,  4.79s/it][A
 17%|█▋        | 142/827 [10:00<50:17,  4.40s/it][A
 17%|█▋        | 143/827 [10:03<47:38,  4.18s/it][A
 17%|█▋        | 144/827 [10:06<43:40,  3.84s/it][A
 18%|█▊        | 145/827 [10:11<44:36,  3.92s/it][A
 18%|█▊        | 146/827 [10:15<45:04,  3.97s/it][A
 18%|█▊        | 147/827 [10:18<43:54,  3.87s/it][A
 18%|█▊        | 148/827 [10:22<42:30,  3.76s/it][A
 18%|█▊        | 149/827 [10:25<41:30,  3.67s/it][A
 18%|█▊        | 150/827 [10:28<38:27,  3.41s/it][A
 18%|█▊        | 151/827 [10:31<35:41,  3.17s/it][A
 18%|█▊        | 152/827 [10:34<36:44,  3.27s/it][A
 19%|█▊        | 153/827 [10:38<37:06,  3.30s/it][A
 19%|█▊        | 154/827 [10:42<40:01,  3.57s/it][A
 19%|█▊        | 155/827 [10:47<46:23,  4.14s/it][A
 19%|█▉        | 156/827 [10:52<48:41,  4.35s/it][A
 19%|█▉        | 157/827 [10:56<45:54,  4.11s/it][A
 19%|█▉        | 158/827 [10:59<42:41,  3.83s/it][A
 19%|█▉        | 159/827 [11:04<46:24,  4.17s/it][A
 19%|█▉        | 160/827 [11:08<46:40,  4.20s/it][A
 19%|█▉        | 161/827 [11:11<41:52,  3.77s/it][A
 20%|█▉        | 162/827 [11:15<43:26,  3.92s/it][A
 20%|█▉        | 163/827 [11:19<43:50,  3.96s/it][A
 20%|█▉        | 164/827 [11:23<42:38,  3.86s/it][A
 20%|█▉        | 165/827 [11:26<41:01,  3.72s/it][A
 20%|██        | 166/827 [11:33<50:08,  4.55s/it][A
 20%|██        | 167/827 [11:42<1:06:16,  6.02s/it][A
 20%|██        | 168/827 [11:47<1:03:36,  5.79s/it][A
 20%|██        | 169/827 [11:51<55:54,  5.10s/it]  [A
 21%|██        | 170/827 [11:55<53:46,  4.91s/it][A
 21%|██        | 171/827 [11:59<50:33,  4.62s/it][A
 21%|██        | 172/827 [12:02<44:24,  4.07s/it][A
 21%|██        | 173/827 [12:05<40:37,  3.73s/it][A
 21%|██        | 174/827 [12:08<39:03,  3.59s/it][A
 21%|██        | 175/827 [12:12<39:51,  3.67s/it][A
 21%|██▏       | 176/827 [12:15<36:59,  3.41s/it][A
 21%|██▏       | 177/827 [12:18<34:50,  3.22s/it][A
 22%|██▏       | 178/827 [12:22<39:23,  3.64s/it][A
 22%|██▏       | 179/827 [12:28<45:59,  4.26s/it][A
 22%|██▏       | 180/827 [12:32<45:25,  4.21s/it][A
 22%|██▏       | 181/827 [12:36<45:08,  4.19s/it][A
 22%|██▏       | 182/827 [12:41<47:24,  4.41s/it][A
 22%|██▏       | 183/827 [12:44<43:36,  4.06s/it][A
 22%|██▏       | 184/827 [12:49<46:27,  4.33s/it][A
 22%|██▏       | 185/827 [12:54<48:52,  4.57s/it][A
 22%|██▏       | 186/827 [12:58<44:30,  4.17s/it][A
 23%|██▎       | 187/827 [13:01<41:09,  3.86s/it][A
 23%|██▎       | 188/827 [13:05<42:28,  3.99s/it][A
 23%|██▎       | 189/827 [13:10<45:57,  4.32s/it][A
 23%|██▎       | 190/827 [13:14<44:48,  4.22s/it][A
 23%|██▎       | 191/827 [13:18<44:38,  4.21s/it][A
 23%|██▎       | 192/827 [13:23<47:16,  4.47s/it][A
 23%|██▎       | 193/827 [13:27<45:40,  4.32s/it][A
 23%|██▎       | 194/827 [13:31<41:51,  3.97s/it][A
 24%|██▎       | 195/827 [13:34<39:23,  3.74s/it][A
 24%|██▎       | 196/827 [13:37<36:05,  3.43s/it][A
 24%|██▍       | 197/827 [13:39<34:24,  3.28s/it][A
 24%|██▍       | 198/827 [13:43<35:37,  3.40s/it][A
 24%|██▍       | 199/827 [13:47<36:08,  3.45s/it][A
 24%|██▍       | 200/827 [13:51<40:05,  3.84s/it][A
 24%|██▍       | 201/827 [13:57<44:28,  4.26s/it][A
 24%|██▍       | 202/827 [14:02<48:01,  4.61s/it][A
 25%|██▍       | 203/827 [14:08<50:55,  4.90s/it][A
 25%|██▍       | 204/827 [14:14<53:53,  5.19s/it][A
 25%|██▍       | 205/827 [14:17<49:02,  4.73s/it][A
 25%|██▍       | 206/827 [14:21<44:29,  4.30s/it][A
 25%|██▌       | 207/827 [14:24<41:54,  4.06s/it][A
 25%|██▌       | 208/827 [14:27<39:53,  3.87s/it][A
 25%|██▌       | 209/827 [14:31<37:59,  3.69s/it][A
 25%|██▌       | 210/827 [14:34<37:16,  3.62s/it][A
 26%|██▌       | 211/827 [14:39<41:42,  4.06s/it][A
 26%|██▌       | 212/827 [14:44<45:02,  4.39s/it][A
 26%|██▌       | 213/827 [14:50<48:57,  4.78s/it][A
 26%|██▌       | 214/827 [14:54<45:14,  4.43s/it][A
 26%|██▌       | 215/827 [14:57<41:28,  4.07s/it][A
 26%|██▌       | 216/827 [15:01<40:48,  4.01s/it][A
 26%|██▌       | 217/827 [15:05<41:01,  4.04s/it][A
 26%|██▋       | 218/827 [15:09<40:39,  4.00s/it][A
 26%|██▋       | 219/827 [15:12<37:43,  3.72s/it][A
 27%|██▋       | 220/827 [15:17<41:24,  4.09s/it][A
 27%|██▋       | 221/827 [15:22<45:43,  4.53s/it][A
 27%|██▋       | 222/827 [15:26<42:03,  4.17s/it][A
 27%|██▋       | 223/827 [15:29<39:25,  3.92s/it][A
 27%|██▋       | 224/827 [15:32<37:31,  3.73s/it][A
 27%|██▋       | 225/827 [15:36<35:54,  3.58s/it][A
 27%|██▋       | 226/827 [15:38<33:13,  3.32s/it][A
 27%|██▋       | 227/827 [15:43<38:31,  3.85s/it][A
 28%|██▊       | 228/827 [15:49<42:41,  4.28s/it][A
 28%|██▊       | 229/827 [15:52<41:01,  4.12s/it][A
 28%|██▊       | 230/827 [15:57<41:02,  4.13s/it][A
 28%|██▊       | 231/827 [16:00<39:07,  3.94s/it][A
 28%|██▊       | 232/827 [16:04<37:35,  3.79s/it][A
 28%|██▊       | 233/827 [16:09<41:47,  4.22s/it][A
 28%|██▊       | 234/827 [16:15<47:04,  4.76s/it][A
 28%|██▊       | 235/827 [16:20<47:42,  4.83s/it][A
 29%|██▊       | 236/827 [16:24<45:43,  4.64s/it][A
 29%|██▊       | 237/827 [16:30<50:55,  5.18s/it][A
 29%|██▉       | 238/827 [16:36<52:18,  5.33s/it][A
 29%|██▉       | 239/827 [16:42<52:55,  5.40s/it][A
 29%|██▉       | 240/827 [16:47<53:44,  5.49s/it][A
 29%|██▉       | 241/827 [16:53<55:14,  5.66s/it][A
 29%|██▉       | 242/827 [16:58<51:06,  5.24s/it][A
 29%|██▉       | 243/827 [17:03<50:31,  5.19s/it][A
 30%|██▉       | 244/827 [17:09<52:34,  5.41s/it][A
 30%|██▉       | 245/827 [17:13<47:58,  4.95s/it][A
 30%|██▉       | 246/827 [17:16<44:28,  4.59s/it][A
 30%|██▉       | 247/827 [17:21<45:14,  4.68s/it][A
 30%|██▉       | 248/827 [17:28<52:09,  5.41s/it][A
 30%|███       | 249/827 [17:33<49:57,  5.19s/it][A
 30%|███       | 250/827 [17:35<42:14,  4.39s/it][A
 30%|███       | 251/827 [17:40<42:06,  4.39s/it][A
 30%|███       | 252/827 [17:45<42:48,  4.47s/it][A
 31%|███       | 253/827 [17:47<38:22,  4.01s/it][A
 31%|███       | 254/827 [17:54<44:18,  4.64s/it][A
 31%|███       | 255/827 [18:00<49:53,  5.23s/it][A
 31%|███       | 256/827 [18:07<53:00,  5.57s/it][A
 31%|███       | 257/827 [18:11<49:47,  5.24s/it][A
 31%|███       | 258/827 [18:15<47:22,  5.00s/it][A
 31%|███▏      | 259/827 [18:22<51:15,  5.42s/it][A
 31%|███▏      | 260/827 [18:28<53:29,  5.66s/it][A
 32%|███▏      | 261/827 [18:32<48:26,  5.14s/it][A
 32%|███▏      | 262/827 [18:39<52:24,  5.57s/it][A
 32%|███▏      | 263/827 [18:45<54:44,  5.82s/it][A
 32%|███▏      | 264/827 [18:48<46:18,  4.94s/it][A
 32%|███▏      | 265/827 [18:51<40:32,  4.33s/it][A
 32%|███▏      | 266/827 [18:54<37:38,  4.03s/it][A
 32%|███▏      | 267/827 [18:57<35:49,  3.84s/it][A
 32%|███▏      | 268/827 [19:01<34:49,  3.74s/it][A
 33%|███▎      | 269/827 [19:06<37:50,  4.07s/it][A
 33%|███▎      | 270/827 [19:11<40:15,  4.34s/it][A
 33%|███▎      | 271/827 [19:14<37:52,  4.09s/it][A
 33%|███▎      | 272/827 [19:18<36:02,  3.90s/it][A
 33%|███▎      | 273/827 [19:24<43:05,  4.67s/it][A
 33%|███▎      | 274/827 [19:33<54:42,  5.94s/it][A
 33%|███▎      | 275/827 [19:38<52:46,  5.74s/it][A
 33%|███▎      | 276/827 [19:43<48:26,  5.27s/it][A
 33%|███▎      | 277/827 [19:46<44:24,  4.84s/it][A
 34%|███▎      | 278/827 [19:51<42:21,  4.63s/it][A
 34%|███▎      | 279/827 [19:54<39:12,  4.29s/it][A
 34%|███▍      | 280/827 [19:57<35:49,  3.93s/it][A
 34%|███▍      | 281/827 [20:01<36:38,  4.03s/it][A
 34%|███▍      | 282/827 [20:05<35:36,  3.92s/it][A
 34%|███▍      | 283/827 [20:09<34:58,  3.86s/it][A
 34%|███▍      | 284/827 [20:13<35:12,  3.89s/it][A
 34%|███▍      | 285/827 [20:18<38:01,  4.21s/it][A
 35%|███▍      | 286/827 [20:23<40:05,  4.45s/it][A
 35%|███▍      | 287/827 [20:25<35:16,  3.92s/it][A
 35%|███▍      | 288/827 [20:29<33:12,  3.70s/it][A
 35%|███▍      | 289/827 [20:32<32:18,  3.60s/it][A
 35%|███▌      | 290/827 [20:35<31:27,  3.51s/it][A
 35%|███▌      | 291/827 [20:38<29:37,  3.32s/it][A
 35%|███▌      | 292/827 [20:42<30:18,  3.40s/it][A
 35%|███▌      | 293/827 [20:45<29:27,  3.31s/it][A
 36%|███▌      | 294/827 [20:49<30:38,  3.45s/it][A
 36%|███▌      | 295/827 [20:52<31:10,  3.52s/it][A
 36%|███▌      | 296/827 [20:57<34:03,  3.85s/it][A
 36%|███▌      | 297/827 [21:01<34:33,  3.91s/it][A
 36%|███▌      | 298/827 [21:04<32:19,  3.67s/it][A
 36%|███▌      | 299/827 [21:07<31:49,  3.62s/it][A
 36%|███▋      | 300/827 [21:11<30:22,  3.46s/it][A
 36%|███▋      | 301/827 [21:14<29:42,  3.39s/it][A
 37%|███▋      | 302/827 [21:17<29:59,  3.43s/it][A
 37%|███▋      | 303/827 [21:20<27:28,  3.15s/it][A
 37%|███▋      | 304/827 [21:23<28:28,  3.27s/it][A
 37%|███▋      | 305/827 [21:28<32:30,  3.74s/it][A
 37%|███▋      | 306/827 [21:32<33:32,  3.86s/it][A
 37%|███▋      | 307/827 [21:36<32:21,  3.73s/it][A
 37%|███▋      | 308/827 [21:39<30:54,  3.57s/it][A
 37%|███▋      | 309/827 [21:43<31:01,  3.59s/it][A
 37%|███▋      | 310/827 [21:45<28:53,  3.35s/it][A
 38%|███▊      | 311/827 [21:48<27:39,  3.22s/it][A
 38%|███▊      | 312/827 [21:52<29:12,  3.40s/it][A
 38%|███▊      | 313/827 [21:56<31:20,  3.66s/it][A
 38%|███▊      | 314/827 [22:00<31:42,  3.71s/it][A
 38%|███▊      | 315/827 [22:05<34:09,  4.00s/it][A
 38%|███▊      | 316/827 [22:10<36:58,  4.34s/it][A
 38%|███▊      | 317/827 [22:15<39:09,  4.61s/it][A
 38%|███▊      | 318/827 [22:20<38:58,  4.59s/it][A
 39%|███▊      | 319/827 [22:24<37:38,  4.45s/it][A
 39%|███▊      | 320/827 [22:28<35:34,  4.21s/it][A
 39%|███▉      | 321/827 [22:32<37:09,  4.41s/it][A
 39%|███▉      | 322/827 [22:36<35:24,  4.21s/it][A
 39%|███▉      | 323/827 [22:41<36:16,  4.32s/it][A
 39%|███▉      | 324/827 [22:46<37:35,  4.48s/it][A
 39%|███▉      | 325/827 [22:49<34:37,  4.14s/it][A
 39%|███▉      | 326/827 [22:53<34:27,  4.13s/it][A
 40%|███▉      | 327/827 [22:58<36:12,  4.35s/it][A
 40%|███▉      | 328/827 [23:03<37:06,  4.46s/it][A
 40%|███▉      | 329/827 [23:09<42:31,  5.12s/it][A
 40%|███▉      | 330/827 [23:14<41:01,  4.95s/it][A
 40%|████      | 331/827 [23:19<40:36,  4.91s/it][A
 40%|████      | 332/827 [23:23<39:51,  4.83s/it][A
 40%|████      | 333/827 [23:29<41:04,  4.99s/it][A
 40%|████      | 334/827 [23:33<39:37,  4.82s/it][A
 41%|████      | 335/827 [23:37<36:23,  4.44s/it][A
 41%|████      | 336/827 [23:40<33:14,  4.06s/it][A
 41%|████      | 337/827 [23:43<31:13,  3.82s/it][A
 41%|████      | 338/827 [23:46<29:57,  3.68s/it][A
 41%|████      | 339/827 [23:50<28:46,  3.54s/it][A
 41%|████      | 340/827 [23:53<29:16,  3.61s/it][A
 41%|████      | 341/827 [23:58<30:25,  3.76s/it][A
 41%|████▏     | 342/827 [24:01<29:09,  3.61s/it][A
 41%|████▏     | 343/827 [24:04<28:26,  3.53s/it][A
 42%|████▏     | 344/827 [24:09<31:08,  3.87s/it][A
 42%|████▏     | 345/827 [24:14<34:19,  4.27s/it][A
 42%|████▏     | 346/827 [24:18<33:40,  4.20s/it][A
 42%|████▏     | 347/827 [24:22<33:04,  4.13s/it][A
 42%|████▏     | 348/827 [24:25<30:08,  3.78s/it][A
 42%|████▏     | 349/827 [24:30<32:38,  4.10s/it][A
 42%|████▏     | 350/827 [24:35<34:13,  4.31s/it][A
 42%|████▏     | 351/827 [24:38<32:19,  4.07s/it][A
 43%|████▎     | 352/827 [24:42<32:48,  4.14s/it][A
 43%|████▎     | 353/827 [24:46<31:33,  3.99s/it][A
 43%|████▎     | 354/827 [24:50<30:12,  3.83s/it][A
 43%|████▎     | 355/827 [24:55<33:08,  4.21s/it][A
 43%|████▎     | 356/827 [25:00<35:12,  4.49s/it][A
 43%|████▎     | 357/827 [25:03<33:06,  4.23s/it][A
 43%|████▎     | 358/827 [25:06<30:02,  3.84s/it][A
 43%|████▎     | 359/827 [25:10<29:39,  3.80s/it][A
 44%|████▎     | 360/827 [25:15<32:44,  4.21s/it][A
 44%|████▎     | 361/827 [25:19<32:07,  4.14s/it][A
 44%|████▍     | 362/827 [25:22<28:55,  3.73s/it][A
 44%|████▍     | 363/827 [25:27<30:53,  4.00s/it][A
 44%|████▍     | 364/827 [25:30<30:26,  3.95s/it][A
 44%|████▍     | 365/827 [25:34<29:19,  3.81s/it][A
 44%|████▍     | 366/827 [25:40<34:13,  4.45s/it][A
 44%|████▍     | 367/827 [25:44<34:26,  4.49s/it][A
 44%|████▍     | 368/827 [25:47<30:39,  4.01s/it][A
 45%|████▍     | 369/827 [25:51<30:11,  3.95s/it][A
 45%|████▍     | 370/827 [25:55<29:45,  3.91s/it][A
 45%|████▍     | 371/827 [26:00<31:14,  4.11s/it][A
 45%|████▍     | 372/827 [26:04<30:52,  4.07s/it][A
 45%|████▌     | 373/827 [26:09<32:56,  4.35s/it][A
 45%|████▌     | 374/827 [26:16<39:15,  5.20s/it][A
 45%|████▌     | 375/827 [26:25<48:24,  6.43s/it][A
 45%|████▌     | 376/827 [26:32<50:13,  6.68s/it][A
 46%|████▌     | 377/827 [26:37<45:51,  6.11s/it][A
 46%|████▌     | 378/827 [26:44<47:32,  6.35s/it][A
 46%|████▌     | 379/827 [26:54<54:48,  7.34s/it][A
 46%|████▌     | 380/827 [27:00<51:53,  6.96s/it][A
 46%|████▌     | 381/827 [27:04<44:51,  6.04s/it][A
 46%|████▌     | 382/827 [27:09<42:29,  5.73s/it][A
 46%|████▋     | 383/827 [27:13<39:00,  5.27s/it][A
 46%|████▋     | 384/827 [27:17<35:42,  4.84s/it][A
 47%|████▋     | 385/827 [27:20<32:14,  4.38s/it][A
 47%|████▋     | 386/827 [27:23<30:03,  4.09s/it][A
 47%|████▋     | 387/827 [27:27<29:42,  4.05s/it][A
 47%|████▋     | 388/827 [27:32<30:40,  4.19s/it][A
 47%|████▋     | 389/827 [27:36<29:33,  4.05s/it][A
 47%|████▋     | 390/827 [27:39<28:27,  3.91s/it][A
 47%|████▋     | 391/827 [27:43<27:35,  3.80s/it][A
 47%|████▋     | 392/827 [27:47<27:50,  3.84s/it][A
 48%|████▊     | 393/827 [27:51<29:25,  4.07s/it][A
 48%|████▊     | 394/827 [27:57<32:50,  4.55s/it][A
 48%|████▊     | 395/827 [28:02<33:04,  4.59s/it][A
 48%|████▊     | 396/827 [28:06<33:11,  4.62s/it][A
 48%|████▊     | 397/827 [28:10<30:17,  4.23s/it][A
 48%|████▊     | 398/827 [28:13<28:38,  4.01s/it][A
 48%|████▊     | 399/827 [28:16<27:19,  3.83s/it][A
 48%|████▊     | 400/827 [28:21<29:03,  4.08s/it][A
 48%|████▊     | 401/827 [28:26<31:02,  4.37s/it][A
 49%|████▊     | 402/827 [28:31<31:32,  4.45s/it][A
 49%|████▊     | 403/827 [28:36<32:19,  4.57s/it][A
 49%|████▉     | 404/827 [28:38<28:22,  4.03s/it][A
 49%|████▉     | 405/827 [28:42<27:10,  3.86s/it][A
 49%|████▉     | 406/827 [28:45<25:57,  3.70s/it][A
 49%|████▉     | 407/827 [28:48<24:05,  3.44s/it][A
 49%|████▉     | 408/827 [28:51<23:37,  3.38s/it][A
 49%|████▉     | 409/827 [28:55<23:08,  3.32s/it][A
 50%|████▉     | 410/827 [28:57<21:45,  3.13s/it][A
 50%|████▉     | 411/827 [29:00<22:03,  3.18s/it][A
 50%|████▉     | 412/827 [29:04<22:00,  3.18s/it][A
 50%|████▉     | 413/827 [29:09<25:28,  3.69s/it][A
 50%|█████     | 414/827 [29:14<28:26,  4.13s/it][A
 50%|█████     | 415/827 [29:17<25:51,  3.77s/it][A
 50%|█████     | 416/827 [29:21<27:57,  4.08s/it][A
 50%|█████     | 417/827 [29:27<30:08,  4.41s/it][A
 51%|█████     | 418/827 [29:32<32:21,  4.75s/it][A
 51%|█████     | 419/827 [29:37<31:43,  4.66s/it][A
 51%|█████     | 420/827 [29:41<30:46,  4.54s/it][A
 51%|█████     | 421/827 [29:45<29:07,  4.30s/it][A
 51%|█████     | 422/827 [29:48<26:52,  3.98s/it][A
 51%|█████     | 423/827 [29:51<25:06,  3.73s/it][A
 51%|█████▏    | 424/827 [29:55<25:20,  3.77s/it][A
 51%|█████▏    | 425/827 [30:00<27:43,  4.14s/it][A
 52%|█████▏    | 426/827 [30:05<29:27,  4.41s/it][A
 52%|█████▏    | 427/827 [30:10<31:10,  4.68s/it][A
 52%|█████▏    | 428/827 [30:15<31:22,  4.72s/it][A
 52%|█████▏    | 429/827 [30:19<29:22,  4.43s/it][A
 52%|█████▏    | 430/827 [30:23<28:46,  4.35s/it][A
 52%|█████▏    | 431/827 [30:27<28:48,  4.36s/it][A
 52%|█████▏    | 432/827 [30:34<32:17,  4.91s/it][A
 52%|█████▏    | 433/827 [30:40<35:18,  5.38s/it][A
 52%|█████▏    | 434/827 [30:45<34:56,  5.34s/it][A
 53%|█████▎    | 435/827 [30:48<30:47,  4.71s/it][A
 53%|█████▎    | 436/827 [30:53<30:24,  4.67s/it][A
 53%|█████▎    | 437/827 [30:58<30:34,  4.70s/it][A
 53%|█████▎    | 438/827 [31:01<28:27,  4.39s/it][A
 53%|█████▎    | 439/827 [31:05<27:16,  4.22s/it][A
 53%|█████▎    | 440/827 [31:09<26:47,  4.15s/it][A
 53%|█████▎    | 441/827 [31:12<24:28,  3.80s/it][A
 53%|█████▎    | 442/827 [31:16<23:27,  3.66s/it][A
 54%|█████▎    | 443/827 [31:19<22:24,  3.50s/it][A
 54%|█████▎    | 444/827 [31:23<24:06,  3.78s/it][A
 54%|█████▍    | 445/827 [31:29<27:45,  4.36s/it][A
 54%|█████▍    | 446/827 [31:33<26:16,  4.14s/it][A
 54%|█████▍    | 447/827 [31:37<27:21,  4.32s/it][A
 54%|█████▍    | 448/827 [31:43<29:48,  4.72s/it][A
 54%|█████▍    | 449/827 [31:51<35:58,  5.71s/it][A
 54%|█████▍    | 450/827 [31:56<35:37,  5.67s/it][A
 55%|█████▍    | 451/827 [32:00<32:16,  5.15s/it][A
 55%|█████▍    | 452/827 [32:06<33:12,  5.31s/it][A
 55%|█████▍    | 453/827 [32:12<33:34,  5.39s/it][A
 55%|█████▍    | 454/827 [32:15<30:06,  4.84s/it][A
 55%|█████▌    | 455/827 [32:20<29:37,  4.78s/it][A
 55%|█████▌    | 456/827 [32:26<32:17,  5.22s/it][A
 55%|█████▌    | 457/827 [32:31<31:18,  5.08s/it][A
 55%|█████▌    | 458/827 [32:35<29:22,  4.78s/it][A
 56%|█████▌    | 459/827 [32:40<28:53,  4.71s/it][A
 56%|█████▌    | 460/827 [32:46<31:30,  5.15s/it][A
 56%|█████▌    | 461/827 [32:54<36:37,  6.01s/it][A
 56%|█████▌    | 462/827 [32:58<33:43,  5.54s/it][A
 56%|█████▌    | 463/827 [33:01<29:24,  4.85s/it][A
 56%|█████▌    | 464/827 [33:07<29:54,  4.94s/it][A
 56%|█████▌    | 465/827 [33:11<28:35,  4.74s/it][A
 56%|█████▋    | 466/827 [33:15<26:36,  4.42s/it][A
 56%|█████▋    | 467/827 [33:19<26:07,  4.35s/it][A
 57%|█████▋    | 468/827 [33:23<26:41,  4.46s/it][A
 57%|█████▋    | 469/827 [33:28<27:28,  4.60s/it][A
 57%|█████▋    | 470/827 [33:32<25:20,  4.26s/it][A
 57%|█████▋    | 471/827 [33:37<26:12,  4.42s/it][A
 57%|█████▋    | 472/827 [33:43<30:27,  5.15s/it][A
 57%|█████▋    | 473/827 [33:48<28:59,  4.91s/it][A
 57%|█████▋    | 474/827 [33:52<27:29,  4.67s/it][A
 57%|█████▋    | 475/827 [33:55<24:29,  4.18s/it][A
 58%|█████▊    | 476/827 [33:57<21:30,  3.68s/it][A
 58%|█████▊    | 477/827 [34:00<19:34,  3.36s/it][A
 58%|█████▊    | 478/827 [34:04<21:08,  3.64s/it][A
 58%|█████▊    | 479/827 [34:09<23:31,  4.05s/it][A
 58%|█████▊    | 480/827 [34:14<24:42,  4.27s/it][A
 58%|█████▊    | 481/827 [34:19<25:37,  4.44s/it][A
 58%|█████▊    | 482/827 [34:22<22:55,  3.99s/it][A
 58%|█████▊    | 483/827 [34:24<20:09,  3.52s/it][A
 59%|█████▊    | 484/827 [34:28<19:38,  3.43s/it][A
 59%|█████▊    | 485/827 [34:31<18:47,  3.30s/it][A
 59%|█████▉    | 486/827 [34:34<19:06,  3.36s/it][A
 59%|█████▉    | 487/827 [34:38<20:05,  3.55s/it][A
 59%|█████▉    | 488/827 [34:43<23:03,  4.08s/it][A
 59%|█████▉    | 489/827 [34:48<24:32,  4.36s/it][A
 59%|█████▉    | 490/827 [34:53<24:55,  4.44s/it][A
 59%|█████▉    | 491/827 [34:57<23:32,  4.20s/it][A
 59%|█████▉    | 492/827 [35:00<22:36,  4.05s/it][A
 60%|█████▉    | 493/827 [35:04<22:04,  3.97s/it][A
 60%|█████▉    | 494/827 [35:08<21:42,  3.91s/it][A
 60%|█████▉    | 495/827 [35:12<21:51,  3.95s/it][A
 60%|█████▉    | 496/827 [35:16<21:59,  3.98s/it][A
 60%|██████    | 497/827 [35:19<19:57,  3.63s/it][A
 60%|██████    | 498/827 [35:22<19:32,  3.56s/it][A
 60%|██████    | 499/827 [35:27<21:28,  3.93s/it][A
 60%|██████    | 500/827 [35:30<20:23,  3.74s/it][A
 61%|██████    | 501/827 [35:33<19:03,  3.51s/it][A
 61%|██████    | 502/827 [35:38<21:16,  3.93s/it][A
 61%|██████    | 503/827 [35:43<22:33,  4.18s/it][A
 61%|██████    | 504/827 [35:46<21:10,  3.93s/it][A
 61%|██████    | 505/827 [35:50<21:05,  3.93s/it][A
 61%|██████    | 506/827 [35:54<20:58,  3.92s/it][A
 61%|██████▏   | 507/827 [36:00<23:53,  4.48s/it][A
 61%|██████▏   | 508/827 [36:06<26:38,  5.01s/it][A
 62%|██████▏   | 509/827 [36:09<23:28,  4.43s/it][A
 62%|██████▏   | 510/827 [36:13<22:34,  4.27s/it][A
 62%|██████▏   | 511/827 [36:16<20:42,  3.93s/it][A
 62%|██████▏   | 512/827 [36:19<19:08,  3.65s/it][A
 62%|██████▏   | 513/827 [36:23<19:31,  3.73s/it][A
 62%|██████▏   | 514/827 [36:27<19:19,  3.70s/it][A
 62%|██████▏   | 515/827 [36:30<19:01,  3.66s/it][A
 62%|██████▏   | 516/827 [36:34<19:35,  3.78s/it][A
 63%|██████▎   | 517/827 [36:38<19:00,  3.68s/it][A
 63%|██████▎   | 518/827 [36:41<17:59,  3.49s/it][A
 63%|██████▎   | 519/827 [36:44<17:14,  3.36s/it][A
 63%|██████▎   | 520/827 [36:47<17:24,  3.40s/it][A
 63%|██████▎   | 521/827 [36:52<18:50,  3.69s/it][A
 63%|██████▎   | 522/827 [36:57<21:29,  4.23s/it][A
 63%|██████▎   | 523/827 [37:00<19:31,  3.85s/it][A
 63%|██████▎   | 524/827 [37:05<20:07,  3.98s/it][A
 63%|██████▎   | 525/827 [37:11<23:25,  4.65s/it][A
 64%|██████▎   | 526/827 [37:17<25:00,  4.98s/it][A
 64%|██████▎   | 527/827 [37:21<24:10,  4.83s/it][A
 64%|██████▍   | 528/827 [37:26<24:03,  4.83s/it][A
 64%|██████▍   | 529/827 [37:31<24:50,  5.00s/it][A
 64%|██████▍   | 530/827 [37:36<24:11,  4.89s/it][A
 64%|██████▍   | 531/827 [37:39<21:28,  4.35s/it][A
 64%|██████▍   | 532/827 [37:43<20:15,  4.12s/it][A
 64%|██████▍   | 533/827 [37:48<22:26,  4.58s/it][A
 65%|██████▍   | 534/827 [37:54<23:49,  4.88s/it][A
 65%|██████▍   | 535/827 [37:59<24:15,  4.99s/it][A
 65%|██████▍   | 536/827 [38:03<22:10,  4.57s/it][A
 65%|██████▍   | 537/827 [38:05<19:11,  3.97s/it][A
 65%|██████▌   | 538/827 [38:08<17:59,  3.74s/it][A
 65%|██████▌   | 539/827 [38:12<17:07,  3.57s/it][A
 65%|██████▌   | 540/827 [38:15<17:11,  3.59s/it][A
 65%|██████▌   | 541/827 [38:19<17:52,  3.75s/it][A
 66%|██████▌   | 542/827 [38:23<18:15,  3.84s/it][A
 66%|██████▌   | 543/827 [38:28<19:24,  4.10s/it][A
 66%|██████▌   | 544/827 [38:33<20:39,  4.38s/it][A
 66%|██████▌   | 545/827 [38:37<20:21,  4.33s/it][A
 66%|██████▌   | 546/827 [38:42<20:34,  4.39s/it][A
 66%|██████▌   | 547/827 [38:46<20:31,  4.40s/it][A
 66%|██████▋   | 548/827 [38:49<18:15,  3.93s/it][A
 66%|██████▋   | 549/827 [38:52<17:08,  3.70s/it][A
 67%|██████▋   | 550/827 [38:56<16:49,  3.65s/it][A
 67%|██████▋   | 551/827 [39:00<17:28,  3.80s/it][A
 67%|██████▋   | 552/827 [39:04<17:24,  3.80s/it][A
 67%|██████▋   | 553/827 [39:07<17:10,  3.76s/it][A
 67%|██████▋   | 554/827 [39:10<15:41,  3.45s/it][A
 67%|██████▋   | 555/827 [39:14<15:35,  3.44s/it][A
 67%|██████▋   | 556/827 [39:17<15:24,  3.41s/it][A
 67%|██████▋   | 557/827 [39:20<14:36,  3.25s/it][A
 67%|██████▋   | 558/827 [39:24<16:17,  3.64s/it][A
 68%|██████▊   | 559/827 [39:28<16:45,  3.75s/it][A
 68%|██████▊   | 560/827 [39:32<16:12,  3.64s/it][A
 68%|██████▊   | 561/827 [39:35<16:14,  3.66s/it][A
 68%|██████▊   | 562/827 [39:39<16:26,  3.72s/it][A
 68%|██████▊   | 563/827 [39:43<16:16,  3.70s/it][A
 68%|██████▊   | 564/827 [39:46<15:58,  3.65s/it][A
 68%|██████▊   | 565/827 [39:52<17:47,  4.08s/it][A
 68%|██████▊   | 566/827 [39:56<18:26,  4.24s/it][A
 69%|██████▊   | 567/827 [39:59<17:08,  3.96s/it][A
 69%|██████▊   | 568/827 [40:03<16:57,  3.93s/it][A
 69%|██████▉   | 569/827 [40:07<16:23,  3.81s/it][A
 69%|██████▉   | 570/827 [40:11<16:39,  3.89s/it][A
 69%|██████▉   | 571/827 [40:16<18:23,  4.31s/it][A
 69%|██████▉   | 572/827 [40:22<19:49,  4.66s/it][A
 69%|██████▉   | 573/827 [40:27<20:56,  4.95s/it][A
 69%|██████▉   | 574/827 [40:32<20:14,  4.80s/it][A
 70%|██████▉   | 575/827 [40:37<21:12,  5.05s/it][A
 70%|██████▉   | 576/827 [40:42<20:01,  4.79s/it][A
 70%|██████▉   | 577/827 [40:45<18:29,  4.44s/it][A
 70%|██████▉   | 578/827 [40:49<17:48,  4.29s/it][A
 70%|███████   | 579/827 [40:53<16:40,  4.03s/it][A
 70%|███████   | 580/827 [40:58<18:09,  4.41s/it][A
 70%|███████   | 581/827 [41:05<21:20,  5.21s/it][A
 70%|███████   | 582/827 [41:09<20:20,  4.98s/it][A
 70%|███████   | 583/827 [41:13<18:22,  4.52s/it][A
 71%|███████   | 584/827 [41:16<16:46,  4.14s/it][A
 71%|███████   | 585/827 [41:19<15:24,  3.82s/it][A
 71%|███████   | 586/827 [41:23<15:52,  3.95s/it][A
 71%|███████   | 587/827 [41:31<20:39,  5.16s/it][A
 71%|███████   | 588/827 [41:39<23:07,  5.81s/it][A
 71%|███████   | 589/827 [41:44<22:09,  5.59s/it][A
 71%|███████▏  | 590/827 [41:50<23:15,  5.89s/it][A
 71%|███████▏  | 591/827 [41:57<24:29,  6.22s/it][A
 72%|███████▏  | 592/827 [42:01<21:06,  5.39s/it][A
 72%|███████▏  | 593/827 [42:05<19:14,  4.93s/it][A
 72%|███████▏  | 594/827 [42:08<17:08,  4.42s/it][A
 72%|███████▏  | 595/827 [42:12<16:23,  4.24s/it][A
 72%|███████▏  | 596/827 [42:19<20:02,  5.21s/it][A
 72%|███████▏  | 597/827 [42:26<21:19,  5.56s/it][A
 72%|███████▏  | 598/827 [42:29<18:31,  4.85s/it][A
 72%|███████▏  | 599/827 [42:32<16:54,  4.45s/it][A
 73%|███████▎  | 600/827 [42:37<16:55,  4.47s/it][A
 73%|███████▎  | 601/827 [42:41<16:50,  4.47s/it][A
 73%|███████▎  | 602/827 [42:45<16:10,  4.32s/it][A
 73%|███████▎  | 603/827 [42:49<15:07,  4.05s/it][A
 73%|███████▎  | 604/827 [42:53<14:45,  3.97s/it][A
 73%|███████▎  | 605/827 [42:56<14:15,  3.85s/it][A
 73%|███████▎  | 606/827 [43:00<14:17,  3.88s/it][A
 73%|███████▎  | 607/827 [43:04<14:33,  3.97s/it][A
 74%|███████▎  | 608/827 [43:08<14:03,  3.85s/it][A
 74%|███████▎  | 609/827 [43:14<16:26,  4.52s/it][A
 74%|███████▍  | 610/827 [43:20<18:38,  5.15s/it][A
 74%|███████▍  | 611/827 [43:24<17:11,  4.77s/it][A
 74%|███████▍  | 612/827 [43:29<16:36,  4.64s/it][A
 74%|███████▍  | 613/827 [43:34<16:43,  4.69s/it][A
 74%|███████▍  | 614/827 [43:38<16:04,  4.53s/it][A
 74%|███████▍  | 615/827 [43:42<15:20,  4.34s/it][A
 74%|███████▍  | 616/827 [43:45<14:08,  4.02s/it][A
 75%|███████▍  | 617/827 [43:50<15:39,  4.47s/it][A
 75%|███████▍  | 618/827 [43:57<18:08,  5.21s/it][A
 75%|███████▍  | 619/827 [44:00<15:43,  4.53s/it][A
 75%|███████▍  | 620/827 [44:04<15:01,  4.36s/it][A
 75%|███████▌  | 621/827 [44:07<13:28,  3.92s/it][A
 75%|███████▌  | 622/827 [44:10<12:20,  3.61s/it][A
 75%|███████▌  | 623/827 [44:14<12:22,  3.64s/it][A
 75%|███████▌  | 624/827 [44:19<13:38,  4.03s/it][A
 76%|███████▌  | 625/827 [44:22<13:01,  3.87s/it][A
 76%|███████▌  | 626/827 [44:26<12:47,  3.82s/it][A
 76%|███████▌  | 627/827 [44:29<12:28,  3.74s/it][A
 76%|███████▌  | 628/827 [44:32<11:39,  3.51s/it][A
 76%|███████▌  | 629/827 [44:36<11:27,  3.47s/it][A
 76%|███████▌  | 630/827 [44:39<11:08,  3.39s/it][A
 76%|███████▋  | 631/827 [44:45<13:16,  4.06s/it][A
 76%|███████▋  | 632/827 [44:50<14:18,  4.40s/it][A
 77%|███████▋  | 633/827 [44:55<15:28,  4.79s/it][A
 77%|███████▋  | 634/827 [45:02<17:02,  5.30s/it][A
 77%|███████▋  | 635/827 [45:05<14:46,  4.62s/it][A
 77%|███████▋  | 636/827 [45:09<14:25,  4.53s/it][A
 77%|███████▋  | 637/827 [45:15<15:02,  4.75s/it][A
 77%|███████▋  | 638/827 [45:19<14:57,  4.75s/it][A
 77%|███████▋  | 639/827 [45:23<13:28,  4.30s/it][A
 77%|███████▋  | 640/827 [45:26<12:21,  3.97s/it][A
 78%|███████▊  | 641/827 [45:30<12:34,  4.06s/it][A
 78%|███████▊  | 642/827 [45:34<12:18,  3.99s/it][A
 78%|███████▊  | 643/827 [45:39<13:06,  4.27s/it][A
 78%|███████▊  | 644/827 [45:43<13:20,  4.38s/it][A
 78%|███████▊  | 645/827 [45:47<12:26,  4.10s/it][A
 78%|███████▊  | 646/827 [45:50<11:23,  3.78s/it][A
 78%|███████▊  | 647/827 [45:53<11:07,  3.71s/it][A
 78%|███████▊  | 648/827 [45:58<11:31,  3.86s/it][A
 78%|███████▊  | 649/827 [46:01<11:22,  3.84s/it][A
 79%|███████▊  | 650/827 [46:05<10:48,  3.66s/it][A
 79%|███████▊  | 651/827 [46:08<10:27,  3.57s/it][A
 79%|███████▉  | 652/827 [46:11<10:16,  3.52s/it][A
 79%|███████▉  | 653/827 [46:15<10:30,  3.62s/it][A
 79%|███████▉  | 654/827 [46:21<12:16,  4.26s/it][A
 79%|███████▉  | 655/827 [46:27<13:19,  4.65s/it][A
 79%|███████▉  | 656/827 [46:30<11:45,  4.12s/it][A
 79%|███████▉  | 657/827 [46:33<10:49,  3.82s/it][A
 80%|███████▉  | 658/827 [46:38<11:47,  4.18s/it][A
 80%|███████▉  | 659/827 [46:46<15:10,  5.42s/it][A
 80%|███████▉  | 660/827 [46:51<15:08,  5.44s/it][A
 80%|███████▉  | 661/827 [46:57<15:18,  5.53s/it][A
 80%|████████  | 662/827 [47:01<14:04,  5.12s/it][A
 80%|████████  | 663/827 [47:06<13:28,  4.93s/it][A
 80%|████████  | 664/827 [47:11<13:37,  5.02s/it][A
 80%|████████  | 665/827 [47:15<12:52,  4.77s/it][A
 81%|████████  | 666/827 [47:19<12:19,  4.59s/it][A
 81%|████████  | 667/827 [47:23<11:19,  4.25s/it][A
 81%|████████  | 668/827 [47:27<11:02,  4.17s/it][A
 81%|████████  | 669/827 [47:30<10:00,  3.80s/it][A
 81%|████████  | 670/827 [47:33<09:28,  3.62s/it][A
 81%|████████  | 671/827 [47:37<09:39,  3.71s/it][A
 81%|████████▏ | 672/827 [47:40<09:27,  3.66s/it][A
 81%|████████▏ | 673/827 [47:44<09:03,  3.53s/it][A
 81%|████████▏ | 674/827 [47:47<08:55,  3.50s/it][A
 82%|████████▏ | 675/827 [47:51<09:15,  3.65s/it][A
 82%|████████▏ | 676/827 [47:56<10:12,  4.06s/it][A
 82%|████████▏ | 677/827 [48:01<11:04,  4.43s/it][A
 82%|████████▏ | 678/827 [48:07<11:32,  4.65s/it][A
 82%|████████▏ | 679/827 [48:11<11:23,  4.62s/it][A
 82%|████████▏ | 680/827 [48:14<10:15,  4.19s/it][A
 82%|████████▏ | 681/827 [48:20<11:12,  4.60s/it][A
 82%|████████▏ | 682/827 [48:25<11:26,  4.73s/it][A
 83%|████████▎ | 683/827 [48:28<10:03,  4.19s/it][A
 83%|████████▎ | 684/827 [48:31<09:11,  3.86s/it][A
 83%|████████▎ | 685/827 [48:34<08:25,  3.56s/it][A
 83%|████████▎ | 686/827 [48:40<10:21,  4.41s/it][A
 83%|████████▎ | 687/827 [48:47<12:17,  5.27s/it][A
 83%|████████▎ | 688/827 [48:53<12:18,  5.31s/it][A
 83%|████████▎ | 689/827 [48:58<11:57,  5.20s/it][A
 83%|████████▎ | 690/827 [49:01<10:31,  4.61s/it][A
 84%|████████▎ | 691/827 [49:05<09:39,  4.26s/it][A
 84%|████████▎ | 692/827 [49:09<09:31,  4.23s/it][A
 84%|████████▍ | 693/827 [49:12<08:56,  4.00s/it][A
 84%|████████▍ | 694/827 [49:15<08:25,  3.80s/it][A
 84%|████████▍ | 695/827 [49:20<08:48,  4.01s/it][A
 84%|████████▍ | 696/827 [49:25<09:17,  4.25s/it][A
 84%|████████▍ | 697/827 [49:29<08:53,  4.11s/it][A
 84%|████████▍ | 698/827 [49:31<07:51,  3.66s/it][A
 85%|████████▍ | 699/827 [49:34<07:34,  3.55s/it][A
 85%|████████▍ | 700/827 [49:41<09:38,  4.56s/it][A
 85%|████████▍ | 701/827 [49:48<10:48,  5.15s/it][A
 85%|████████▍ | 702/827 [49:53<10:27,  5.02s/it][A
 85%|████████▌ | 703/827 [49:57<09:42,  4.70s/it][A
 85%|████████▌ | 704/827 [50:00<08:42,  4.25s/it][A
 85%|████████▌ | 705/827 [50:03<07:43,  3.80s/it][A
 85%|████████▌ | 706/827 [50:06<07:10,  3.56s/it][A
 85%|████████▌ | 707/827 [50:10<07:31,  3.76s/it][A
 86%|████████▌ | 708/827 [50:13<07:14,  3.65s/it][A
 86%|████████▌ | 709/827 [50:17<07:21,  3.74s/it][A
 86%|████████▌ | 710/827 [50:21<07:19,  3.76s/it][A
 86%|████████▌ | 711/827 [50:25<07:18,  3.78s/it][A
 86%|████████▌ | 712/827 [50:28<06:52,  3.59s/it][A
 86%|████████▌ | 713/827 [50:31<06:46,  3.57s/it][A
 86%|████████▋ | 714/827 [50:36<07:10,  3.81s/it][A
 86%|████████▋ | 715/827 [50:40<07:24,  3.97s/it][A
 87%|████████▋ | 716/827 [50:44<07:25,  4.01s/it][A
 87%|████████▋ | 717/827 [50:49<08:01,  4.38s/it][A
 87%|████████▋ | 718/827 [50:55<08:22,  4.61s/it][A
 87%|████████▋ | 719/827 [50:58<07:43,  4.29s/it][A
 87%|████████▋ | 720/827 [51:02<07:22,  4.13s/it][A
 87%|████████▋ | 721/827 [51:05<06:48,  3.85s/it][A
 87%|████████▋ | 722/827 [51:09<06:45,  3.86s/it][A
 87%|████████▋ | 723/827 [51:13<06:34,  3.80s/it][A
 88%|████████▊ | 724/827 [51:15<05:55,  3.45s/it][A
 88%|████████▊ | 725/827 [51:21<06:52,  4.04s/it][A
 88%|████████▊ | 726/827 [51:26<07:32,  4.48s/it][A
 88%|████████▊ | 727/827 [51:30<07:20,  4.40s/it][A
 88%|████████▊ | 728/827 [51:34<06:39,  4.04s/it][A
 88%|████████▊ | 729/827 [51:36<06:01,  3.68s/it][A
 88%|████████▊ | 730/827 [51:41<06:11,  3.83s/it][A
 88%|████████▊ | 731/827 [51:46<06:49,  4.27s/it][A
 89%|████████▊ | 732/827 [51:50<06:50,  4.32s/it][A
 89%|████████▊ | 733/827 [51:55<06:48,  4.34s/it][A
 89%|████████▉ | 734/827 [51:58<05:59,  3.87s/it][A
 89%|████████▉ | 735/827 [52:02<06:04,  3.96s/it][A
 89%|████████▉ | 736/827 [52:08<06:50,  4.51s/it][A
 89%|████████▉ | 737/827 [52:12<06:58,  4.65s/it][A
 89%|████████▉ | 738/827 [52:16<06:17,  4.24s/it][A
 89%|████████▉ | 739/827 [52:21<06:44,  4.59s/it][A
 89%|████████▉ | 740/827 [52:27<07:13,  4.98s/it][A
 90%|████████▉ | 741/827 [52:31<06:44,  4.70s/it][A
 90%|████████▉ | 742/827 [52:35<06:31,  4.60s/it][A
 90%|████████▉ | 743/827 [52:41<06:56,  4.96s/it][A
 90%|████████▉ | 744/827 [52:45<06:28,  4.68s/it][A
 90%|█████████ | 745/827 [52:49<06:07,  4.49s/it][A
 90%|█████████ | 746/827 [52:55<06:39,  4.93s/it][A
 90%|█████████ | 747/827 [53:00<06:40,  5.01s/it][A
 90%|█████████ | 748/827 [53:04<06:11,  4.71s/it][A
 91%|█████████ | 749/827 [53:09<06:05,  4.69s/it][A
 91%|█████████ | 750/827 [53:14<05:53,  4.60s/it][A
 91%|█████████ | 751/827 [53:17<05:20,  4.22s/it][A
 91%|█████████ | 752/827 [53:20<04:50,  3.87s/it][A
 91%|█████████ | 753/827 [53:23<04:25,  3.58s/it][A
 91%|█████████ | 754/827 [53:26<04:11,  3.44s/it][A
 91%|█████████▏| 755/827 [53:31<04:37,  3.85s/it][A
 91%|█████████▏| 756/827 [53:36<04:59,  4.22s/it][A
 92%|█████████▏| 757/827 [53:40<04:58,  4.26s/it][A
 92%|█████████▏| 758/827 [53:44<04:54,  4.28s/it][A
 92%|█████████▏| 759/827 [53:48<04:26,  3.93s/it][A
 92%|█████████▏| 760/827 [53:54<05:07,  4.58s/it][A
 92%|█████████▏| 761/827 [54:01<05:48,  5.27s/it][A
 92%|█████████▏| 762/827 [54:07<06:07,  5.66s/it][A
 92%|█████████▏| 763/827 [54:13<06:11,  5.80s/it][A
 92%|█████████▏| 764/827 [54:17<05:25,  5.17s/it][A
 93%|█████████▎| 765/827 [54:20<04:40,  4.53s/it][A
 93%|█████████▎| 766/827 [54:23<04:08,  4.08s/it][A
 93%|█████████▎| 767/827 [54:27<03:56,  3.95s/it][A
 93%|█████████▎| 768/827 [54:31<03:57,  4.02s/it][A
 93%|█████████▎| 769/827 [54:35<04:01,  4.17s/it][A
 93%|█████████▎| 770/827 [54:39<03:45,  3.95s/it][A
 93%|█████████▎| 771/827 [54:43<03:39,  3.92s/it][A
 93%|█████████▎| 772/827 [54:47<03:34,  3.90s/it][A
 93%|█████████▎| 773/827 [54:51<03:38,  4.05s/it][A
 94%|█████████▎| 774/827 [54:54<03:23,  3.83s/it][A
 94%|█████████▎| 775/827 [54:58<03:23,  3.91s/it][A
 94%|█████████▍| 776/827 [55:02<03:15,  3.83s/it][A
 94%|█████████▍| 777/827 [55:05<03:03,  3.67s/it][A
 94%|█████████▍| 778/827 [55:09<03:00,  3.68s/it][A
 94%|█████████▍| 779/827 [55:12<02:49,  3.53s/it][A
 94%|█████████▍| 780/827 [55:15<02:41,  3.44s/it][A
 94%|█████████▍| 781/827 [55:19<02:33,  3.34s/it][A
 95%|█████████▍| 782/827 [55:22<02:27,  3.29s/it][A
 95%|█████████▍| 783/827 [55:25<02:27,  3.36s/it][A
 95%|█████████▍| 784/827 [55:29<02:26,  3.40s/it][A
 95%|█████████▍| 785/827 [55:33<02:28,  3.53s/it][A
 95%|█████████▌| 786/827 [55:37<02:38,  3.87s/it][A
 95%|█████████▌| 787/827 [55:43<02:53,  4.34s/it][A
 95%|█████████▌| 788/827 [55:47<02:46,  4.27s/it][A
 95%|█████████▌| 789/827 [55:52<02:47,  4.42s/it][A
 96%|█████████▌| 790/827 [55:55<02:34,  4.19s/it][A
 96%|█████████▌| 791/827 [55:58<02:18,  3.86s/it][A
 96%|█████████▌| 792/827 [56:02<02:10,  3.72s/it][A
 96%|█████████▌| 793/827 [56:07<02:20,  4.15s/it][A
 96%|█████████▌| 794/827 [56:12<02:28,  4.49s/it][A
 96%|█████████▌| 795/827 [56:19<02:42,  5.08s/it][A
 96%|█████████▋| 796/827 [56:24<02:38,  5.10s/it][A
 96%|█████████▋| 797/827 [56:27<02:15,  4.53s/it][A
 96%|█████████▋| 798/827 [56:30<01:56,  4.03s/it][A
 97%|█████████▋| 799/827 [56:32<01:41,  3.61s/it][A
 97%|█████████▋| 800/827 [56:40<02:07,  4.71s/it][A
 97%|█████████▋| 801/827 [56:47<02:21,  5.45s/it][A
 97%|█████████▋| 802/827 [56:51<02:10,  5.21s/it][A
 97%|█████████▋| 803/827 [56:58<02:11,  5.48s/it][A
 97%|█████████▋| 804/827 [57:03<02:03,  5.35s/it][A
 97%|█████████▋| 805/827 [57:06<01:43,  4.72s/it][A
 97%|█████████▋| 806/827 [57:09<01:27,  4.18s/it][A
 98%|█████████▊| 807/827 [57:12<01:20,  4.01s/it][A
 98%|█████████▊| 808/827 [57:16<01:14,  3.94s/it][A
 98%|█████████▊| 809/827 [57:20<01:07,  3.76s/it][A
 98%|█████████▊| 810/827 [57:23<01:03,  3.75s/it][A
 98%|█████████▊| 811/827 [57:27<00:59,  3.72s/it][A
 98%|█████████▊| 812/827 [57:31<00:58,  3.93s/it][A
 98%|█████████▊| 813/827 [57:37<01:00,  4.35s/it][A
 98%|█████████▊| 814/827 [57:42<01:01,  4.74s/it][A
 99%|█████████▊| 815/827 [57:46<00:54,  4.56s/it][A
 99%|█████████▊| 816/827 [57:50<00:48,  4.38s/it][A
 99%|█████████▉| 817/827 [57:54<00:40,  4.05s/it][A
 99%|█████████▉| 818/827 [57:59<00:41,  4.57s/it][A
 99%|█████████▉| 819/827 [58:05<00:37,  4.71s/it][A
 99%|█████████▉| 820/827 [58:09<00:32,  4.71s/it][A
 99%|█████████▉| 821/827 [58:13<00:26,  4.50s/it][A
 99%|█████████▉| 822/827 [58:17<00:21,  4.31s/it][A
100%|█████████▉| 823/827 [58:20<00:15,  3.85s/it][A
100%|█████████▉| 824/827 [58:23<00:10,  3.66s/it][A
100%|█████████▉| 825/827 [58:29<00:08,  4.22s/it][A
100%|█████████▉| 826/827 [58:34<00:04,  4.72s/it][A
100%|██████████| 827/827 [58:37<00:00,  4.03s/it][A                                                        
                                                 [A{'eval_loss': 0.7102150917053223, 'eval_runtime': 3523.491, 'eval_samples_per_second': 0.938, 'eval_steps_per_second': 0.235, 'epoch': 0.86}
 43%|████▎     | 400/930 [13:11:23<15:06:23, 102.61s/it]
100%|██████████| 827/827 [58:37<00:00,  4.03s/it][A
                                                 [A[INFO|trainer.py:4309] 2025-12-09 00:21:02,923 >> Saving model checkpoint to saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-400
/opt/conda/lib/python3.11/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in Qwen/Qwen2.5-VL-7B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2421] 2025-12-09 00:21:03,006 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-400/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-09 00:21:03,008 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-09 00:21:03,010 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-400/special_tokens_map.json
[2025-12-09 00:21:03,831] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step400 is about to be saved!
[2025-12-09 00:21:03,853] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/mp_rank_00_model_states.pt
[2025-12-09 00:21:03,853] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/mp_rank_00_model_states.pt...
[2025-12-09 00:21:04,015] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/mp_rank_00_model_states.pt.
[2025-12-09 00:21:04,020] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-12-09 00:21:04,020] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2025-12-09 00:21:04,020] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2025-12-09 00:21:04,020] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2025-12-09 00:21:04,103] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2025-12-09 00:21:04,103] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2025-12-09 00:21:04,103] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2025-12-09 00:21:04,117] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-12-09 00:21:04,121] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-12-09 00:21:04,121] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2025-12-09 00:21:04,123] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2025-12-09 00:21:04,124] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2025-12-09 00:21:04,124] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2025-12-09 00:21:04,125] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2025-12-09 00:21:04,125] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2025-12-09 00:21:04,125] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[INFO|image_processing_base.py:253] 2025-12-09 00:21:04,138 >> Image processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-400/preprocessor_config.json
[INFO|tokenization_utils_base.py:2421] 2025-12-09 00:21:04,140 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-400/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-09 00:21:04,142 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-09 00:21:04,145 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-400/special_tokens_map.json
[INFO|video_processing_utils.py:600] 2025-12-09 00:21:04,290 >> Video processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-400/video_preprocessor_config.json
[INFO|processing_utils.py:814] 2025-12-09 00:21:04,292 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-400/chat_template.jinja
 43%|████▎     | 401/930 [13:13:13<170:43:16, 1161.81s/it] 43%|████▎     | 402/930 [13:15:01<124:02:19, 845.72s/it]  43%|████▎     | 403/930 [13:16:39<90:57:17, 621.32s/it]  43%|████▎     | 404/930 [13:18:12<67:38:24, 462.94s/it] 44%|████▎     | 405/930 [13:19:45<51:17:48, 351.75s/it] 44%|████▎     | 406/930 [13:21:07<39:24:48, 270.78s/it] 44%|████▍     | 407/930 [13:22:59<32:26:20, 223.29s/it] 44%|████▍     | 408/930 [13:24:28<26:31:00, 182.88s/it] 44%|████▍     | 409/930 [13:26:07<22:51:43, 157.97s/it] 44%|████▍     | 410/930 [13:27:43<20:06:45, 139.24s/it]                                                        {'loss': 0.7137, 'grad_norm': 0.26575490832328796, 'learning_rate': 6.876512527401897e-05, 'epoch': 0.88}
 44%|████▍     | 410/930 [13:27:43<20:06:45, 139.24s/it] 44%|████▍     | 411/930 [13:29:46<19:22:51, 134.43s/it] 44%|████▍     | 412/930 [13:31:23<17:42:08, 123.03s/it] 44%|████▍     | 413/930 [13:33:19<17:23:27, 121.10s/it] 45%|████▍     | 414/930 [13:35:09<16:53:22, 117.83s/it] 45%|████▍     | 415/930 [13:37:02<16:38:57, 116.38s/it] 45%|████▍     | 416/930 [13:38:47<16:06:08, 112.78s/it] 45%|████▍     | 417/930 [13:40:39<16:02:44, 112.60s/it] 45%|████▍     | 418/930 [13:42:10<15:06:29, 106.23s/it] 45%|████▌     | 419/930 [13:43:58<15:07:34, 106.57s/it] 45%|████▌     | 420/930 [13:45:39<14:51:13, 104.85s/it]                                                        {'loss': 0.7276, 'grad_norm': 0.19804580509662628, 'learning_rate': 6.701280113445324e-05, 'epoch': 0.9}
 45%|████▌     | 420/930 [13:45:39<14:51:13, 104.85s/it] 45%|████▌     | 421/930 [13:47:07<14:07:18, 99.88s/it]  45%|████▌     | 422/930 [13:48:56<14:28:41, 102.60s/it] 45%|████▌     | 423/930 [13:50:33<14:12:08, 100.85s/it] 46%|████▌     | 424/930 [13:52:25<14:39:50, 104.33s/it] 46%|████▌     | 425/930 [13:54:10<14:39:57, 104.55s/it] 46%|████▌     | 426/930 [13:55:54<14:36:54, 104.39s/it] 46%|████▌     | 427/930 [13:57:28<14:08:26, 101.21s/it] 46%|████▌     | 428/930 [13:59:15<14:21:50, 103.01s/it] 46%|████▌     | 429/930 [14:01:14<15:00:28, 107.84s/it] 46%|████▌     | 430/930 [14:02:51<14:32:19, 104.68s/it]                                                        {'loss': 0.7288, 'grad_norm': 0.25570687651634216, 'learning_rate': 6.523651220265268e-05, 'epoch': 0.93}
 46%|████▌     | 430/930 [14:02:51<14:32:19, 104.68s/it] 46%|████▋     | 431/930 [14:04:36<14:31:22, 104.77s/it] 46%|████▋     | 432/930 [14:06:19<14:23:40, 104.06s/it] 47%|████▋     | 433/930 [14:07:48<13:45:20, 99.64s/it]  47%|████▋     | 434/930 [14:09:44<14:24:59, 104.64s/it] 47%|████▋     | 435/930 [14:11:44<14:58:55, 108.96s/it] 47%|████▋     | 436/930 [14:13:08<13:57:12, 101.69s/it] 47%|████▋     | 437/930 [14:14:48<13:50:22, 101.06s/it] 47%|████▋     | 438/930 [14:16:25<13:40:01, 100.00s/it] 47%|████▋     | 439/930 [14:18:01<13:26:20, 98.54s/it]  47%|████▋     | 440/930 [14:19:56<14:06:19, 103.63s/it]                                                        {'loss': 0.7329, 'grad_norm': 0.20003923773765564, 'learning_rate': 6.343876061773385e-05, 'epoch': 0.95}
 47%|████▋     | 440/930 [14:19:56<14:06:19, 103.63s/it] 47%|████▋     | 441/930 [14:21:37<13:57:47, 102.80s/it] 48%|████▊     | 442/930 [14:23:15<13:44:58, 101.43s/it] 48%|████▊     | 443/930 [14:24:51<13:30:29, 99.86s/it]  48%|████▊     | 444/930 [14:26:30<13:25:33, 99.45s/it] 48%|████▊     | 445/930 [14:28:07<13:19:28, 98.90s/it] 48%|████▊     | 446/930 [14:29:54<13:37:09, 101.30s/it] 48%|████▊     | 447/930 [14:31:32<13:27:35, 100.32s/it] 48%|████▊     | 448/930 [14:33:17<13:35:43, 101.54s/it] 48%|████▊     | 449/930 [14:34:45<13:03:02, 97.68s/it]  48%|████▊     | 450/930 [14:36:08<12:25:15, 93.16s/it]                                                       {'loss': 0.728, 'grad_norm': 0.21206864714622498, 'learning_rate': 6.162207875181354e-05, 'epoch': 0.97}
 48%|████▊     | 450/930 [14:36:08<12:25:15, 93.16s/it] 48%|████▊     | 451/930 [14:37:46<12:35:13, 94.60s/it] 49%|████▊     | 452/930 [14:39:21<12:33:58, 94.64s/it] 49%|████▊     | 453/930 [14:41:06<12:56:57, 97.73s/it] 49%|████▉     | 454/930 [14:43:12<14:02:27, 106.19s/it] 49%|████▉     | 455/930 [14:44:41<13:21:46, 101.28s/it] 49%|████▉     | 456/930 [14:46:25<13:24:45, 101.87s/it] 49%|████▉     | 457/930 [14:47:56<12:57:01, 98.57s/it]  49%|████▉     | 458/930 [14:49:36<12:59:28, 99.09s/it] 49%|████▉     | 459/930 [14:51:03<12:28:37, 95.37s/it] 49%|████▉     | 460/930 [14:52:59<13:15:35, 101.56s/it]                                                        {'loss': 0.7215, 'grad_norm': 0.2277304083108902, 'learning_rate': 5.978902564282616e-05, 'epoch': 0.99}
 49%|████▉     | 460/930 [14:52:59<13:15:35, 101.56s/it] 50%|████▉     | 461/930 [14:54:31<12:52:38, 98.85s/it]  50%|████▉     | 462/930 [14:56:26<13:27:53, 103.57s/it] 50%|████▉     | 463/930 [14:58:13<13:34:00, 104.58s/it] 50%|████▉     | 464/930 [15:00:03<13:45:49, 106.33s/it] 50%|█████     | 465/930 [15:00:59<11:46:45, 91.19s/it]  50%|█████     | 466/930 [15:02:34<11:54:08, 92.35s/it] 50%|█████     | 467/930 [15:04:14<12:11:36, 94.81s/it] 50%|█████     | 468/930 [15:06:05<12:45:45, 99.45s/it] 50%|█████     | 469/930 [15:07:56<13:10:36, 102.90s/it] 51%|█████     | 470/930 [15:09:26<12:39:44, 99.10s/it]                                                        {'loss': 0.7018, 'grad_norm': 0.2113838493824005, 'learning_rate': 5.794218338977854e-05, 'epoch': 1.01}
 51%|█████     | 470/930 [15:09:26<12:39:44, 99.10s/it] 51%|█████     | 471/930 [15:11:09<12:47:57, 100.39s/it] 51%|█████     | 472/930 [15:12:54<12:55:21, 101.58s/it] 51%|█████     | 473/930 [15:14:38<13:00:29, 102.47s/it] 51%|█████     | 474/930 [15:15:57<12:03:48, 95.24s/it]  51%|█████     | 475/930 [15:17:49<12:40:17, 100.26s/it] 51%|█████     | 476/930 [15:19:48<13:21:27, 105.92s/it] 51%|█████▏    | 477/930 [15:21:38<13:30:10, 107.31s/it] 51%|█████▏    | 478/930 [15:23:23<13:22:25, 106.52s/it] 52%|█████▏    | 479/930 [15:24:55<12:48:17, 102.21s/it] 52%|█████▏    | 480/930 [15:26:38<12:47:37, 102.35s/it]                                                        {'loss': 0.6845, 'grad_norm': 0.18511570990085602, 'learning_rate': 5.6084153515520134e-05, 'epoch': 1.03}
 52%|█████▏    | 480/930 [15:26:38<12:47:37, 102.35s/it] 52%|█████▏    | 481/930 [15:28:03<12:06:44, 97.12s/it]  52%|█████▏    | 482/930 [15:29:47<12:20:26, 99.17s/it] 52%|█████▏    | 483/930 [15:31:20<12:04:57, 97.31s/it] 52%|█████▏    | 484/930 [15:32:49<11:45:20, 94.89s/it] 52%|█████▏    | 485/930 [15:34:28<11:54:07, 96.29s/it] 52%|█████▏    | 486/930 [15:36:20<12:26:31, 100.88s/it] 52%|█████▏    | 487/930 [15:38:14<12:53:45, 104.80s/it] 52%|█████▏    | 488/930 [15:39:52<12:36:37, 102.71s/it] 53%|█████▎    | 489/930 [15:41:32<12:29:30, 101.97s/it] 53%|█████▎    | 490/930 [15:43:13<12:24:38, 101.54s/it]                                                        {'loss': 0.7225, 'grad_norm': 0.21811512112617493, 'learning_rate': 5.4217553302152237e-05, 'epoch': 1.05}
 53%|█████▎    | 490/930 [15:43:13<12:24:38, 101.54s/it] 53%|█████▎    | 491/930 [15:44:47<12:07:17, 99.40s/it]  53%|█████▎    | 492/930 [15:46:30<12:14:26, 100.61s/it] 53%|█████▎    | 493/930 [15:48:16<12:24:23, 102.21s/it] 53%|█████▎    | 494/930 [15:50:06<12:39:57, 104.58s/it] 53%|█████▎    | 495/930 [15:51:49<12:32:58, 103.86s/it] 53%|█████▎    | 496/930 [15:53:35<12:36:04, 104.53s/it] 53%|█████▎    | 497/930 [15:55:21<12:37:30, 104.97s/it] 54%|█████▎    | 498/930 [15:57:01<12:26:46, 103.72s/it] 54%|█████▎    | 499/930 [15:58:59<12:54:10, 107.77s/it] 54%|█████▍    | 500/930 [16:00:39<12:35:31, 105.42s/it]                                                        {'loss': 0.7039, 'grad_norm': 0.2307167500257492, 'learning_rate': 5.23450121042383e-05, 'epoch': 1.08}
 54%|█████▍    | 500/930 [16:00:39<12:35:31, 105.42s/it] 54%|█████▍    | 501/930 [16:02:09<12:02:18, 101.02s/it] 54%|█████▍    | 502/930 [16:03:50<12:00:37, 101.02s/it] 54%|█████▍    | 503/930 [16:05:30<11:55:01, 100.47s/it] 54%|█████▍    | 504/930 [16:06:58<11:26:59, 96.76s/it]  54%|█████▍    | 505/930 [16:08:56<12:11:48, 103.31s/it] 54%|█████▍    | 506/930 [16:10:57<12:47:58, 108.68s/it] 55%|█████▍    | 507/930 [16:12:38<12:28:19, 106.15s/it] 55%|█████▍    | 508/930 [16:14:26<12:30:43, 106.74s/it] 55%|█████▍    | 509/930 [16:16:24<12:52:57, 110.16s/it] 55%|█████▍    | 510/930 [16:17:58<12:16:27, 105.21s/it]                                                        {'loss': 0.7073, 'grad_norm': 0.18468739092350006, 'learning_rate': 5.046916764500824e-05, 'epoch': 1.1}
 55%|█████▍    | 510/930 [16:17:58<12:16:27, 105.21s/it] 55%|█████▍    | 511/930 [16:19:32<11:51:59, 101.96s/it] 55%|█████▌    | 512/930 [16:21:25<12:13:05, 105.23s/it] 55%|█████▌    | 513/930 [16:23:24<12:40:20, 109.40s/it] 55%|█████▌    | 514/930 [16:25:32<13:16:15, 114.84s/it] 55%|█████▌    | 515/930 [16:27:11<12:41:35, 110.11s/it] 55%|█████▌    | 516/930 [16:29:04<12:46:05, 111.03s/it] 56%|█████▌    | 517/930 [16:30:40<12:12:37, 106.44s/it] 56%|█████▌    | 518/930 [16:32:08<11:32:55, 100.91s/it] 56%|█████▌    | 519/930 [16:34:01<11:57:30, 104.75s/it] 56%|█████▌    | 520/930 [16:35:45<11:53:46, 104.46s/it]                                                        {'loss': 0.712, 'grad_norm': 0.22259995341300964, 'learning_rate': 4.859266230077474e-05, 'epoch': 1.12}
 56%|█████▌    | 520/930 [16:35:45<11:53:46, 104.46s/it] 56%|█████▌    | 521/930 [16:37:18<11:28:52, 101.06s/it] 56%|█████▌    | 522/930 [16:38:45<10:59:13, 96.94s/it]  56%|█████▌    | 523/930 [16:40:28<11:09:29, 98.70s/it] 56%|█████▋    | 524/930 [16:42:13<11:21:01, 100.64s/it] 56%|█████▋    | 525/930 [16:44:14<11:58:44, 106.48s/it] 57%|█████▋    | 526/930 [16:46:01<11:58:43, 106.74s/it] 57%|█████▋    | 527/930 [16:47:43<11:47:05, 105.27s/it] 57%|█████▋    | 528/930 [16:49:20<11:28:38, 102.78s/it] 57%|█████▋    | 529/930 [16:50:53<11:07:36, 99.89s/it]  57%|█████▋    | 530/930 [16:52:29<10:58:06, 98.72s/it]                                                       {'loss': 0.7009, 'grad_norm': 0.21913620829582214, 'learning_rate': 4.671813937879494e-05, 'epoch': 1.14}
 57%|█████▋    | 530/930 [16:52:29<10:58:06, 98.72s/it] 57%|█████▋    | 531/930 [16:54:10<11:02:08, 99.57s/it] 57%|█████▋    | 532/930 [16:56:07<11:33:36, 104.56s/it] 57%|█████▋    | 533/930 [16:57:53<11:36:04, 105.20s/it] 57%|█████▋    | 534/930 [16:59:29<11:15:15, 102.31s/it] 58%|█████▊    | 535/930 [17:01:04<10:58:31, 100.03s/it] 58%|█████▊    | 536/930 [17:03:11<11:49:52, 108.10s/it] 58%|█████▊    | 537/930 [17:04:48<11:26:51, 104.86s/it] 58%|█████▊    | 538/930 [17:06:28<11:15:03, 103.33s/it] 58%|█████▊    | 539/930 [17:07:52<10:36:24, 97.66s/it]  58%|█████▊    | 540/930 [17:09:45<11:05:12, 102.34s/it]                                                        {'loss': 0.7025, 'grad_norm': 0.21413300931453705, 'learning_rate': 4.4848239393820564e-05, 'epoch': 1.16}
 58%|█████▊    | 540/930 [17:09:45<11:05:12, 102.34s/it] 58%|█████▊    | 541/930 [17:11:24<10:56:21, 101.24s/it] 58%|█████▊    | 542/930 [17:13:07<10:57:45, 101.72s/it] 58%|█████▊    | 543/930 [17:14:38<10:35:46, 98.57s/it]  58%|█████▊    | 544/930 [17:16:07<10:16:04, 95.76s/it] 59%|█████▊    | 545/930 [17:17:47<10:22:16, 96.98s/it] 59%|█████▊    | 546/930 [17:19:20<10:13:33, 95.87s/it] 59%|█████▉    | 547/930 [17:21:04<10:27:16, 98.27s/it] 59%|█████▉    | 548/930 [17:22:44<10:28:05, 98.65s/it] 59%|█████▉    | 549/930 [17:24:29<10:38:49, 100.60s/it] 59%|█████▉    | 550/930 [17:26:01<10:21:05, 98.07s/it]                                                        {'loss': 0.7001, 'grad_norm': 0.24860665202140808, 'learning_rate': 4.2985596348582016e-05, 'epoch': 1.18}
 59%|█████▉    | 550/930 [17:26:01<10:21:05, 98.07s/it] 59%|█████▉    | 551/930 [17:28:14<11:26:12, 108.63s/it] 59%|█████▉    | 552/930 [17:30:15<11:47:55, 112.37s/it] 59%|█████▉    | 553/930 [17:32:01<11:33:44, 110.41s/it] 60%|█████▉    | 554/930 [17:33:45<11:19:58, 108.51s/it] 60%|█████▉    | 555/930 [17:35:21<10:54:27, 104.71s/it] 60%|█████▉    | 556/930 [17:37:14<11:08:04, 107.18s/it] 60%|█████▉    | 557/930 [17:38:59<11:01:44, 106.45s/it] 60%|██████    | 558/930 [17:40:36<10:43:28, 103.79s/it] 60%|██████    | 559/930 [17:42:23<10:46:29, 104.55s/it] 60%|██████    | 560/930 [17:44:00<10:30:19, 102.22s/it]                                                        {'loss': 0.6888, 'grad_norm': 0.2186179757118225, 'learning_rate': 4.11328340234453e-05, 'epoch': 1.2}
 60%|██████    | 560/930 [17:44:00<10:30:19, 102.22s/it] 60%|██████    | 561/930 [17:45:46<10:36:13, 103.45s/it] 60%|██████    | 562/930 [17:47:41<10:55:14, 106.83s/it] 61%|██████    | 563/930 [17:49:45<11:25:44, 112.11s/it] 61%|██████    | 564/930 [17:51:31<11:12:05, 110.18s/it] 61%|██████    | 565/930 [17:53:04<10:38:51, 105.02s/it] 61%|██████    | 566/930 [17:54:36<10:14:46, 101.34s/it] 61%|██████    | 567/930 [17:56:06<9:51:47, 97.82s/it]   61%|██████    | 568/930 [17:57:44<9:49:47, 97.76s/it] 61%|██████    | 569/930 [17:59:32<10:07:42, 101.00s/it] 61%|██████▏   | 570/930 [18:01:17<10:12:22, 102.06s/it]                                                        {'loss': 0.6842, 'grad_norm': 0.2035905122756958, 'learning_rate': 3.9292562280468445e-05, 'epoch': 1.23}
 61%|██████▏   | 570/930 [18:01:17<10:12:22, 102.06s/it] 61%|██████▏   | 571/930 [18:02:58<10:08:24, 101.68s/it] 62%|██████▏   | 572/930 [18:04:43<10:12:50, 102.71s/it] 62%|██████▏   | 573/930 [18:06:14<9:50:00, 99.16s/it]   62%|██████▏   | 574/930 [18:07:47<9:39:03, 97.59s/it] 62%|██████▏   | 575/930 [18:09:24<9:35:33, 97.28s/it] 62%|██████▏   | 576/930 [18:11:08<9:45:20, 99.21s/it] 62%|██████▏   | 577/930 [18:12:40<9:32:03, 97.23s/it] 62%|██████▏   | 578/930 [18:14:29<9:49:54, 100.55s/it] 62%|██████▏   | 579/930 [18:16:13<9:54:21, 101.60s/it] 62%|██████▏   | 580/930 [18:17:49<9:44:17, 100.16s/it]                                                       {'loss': 0.7006, 'grad_norm': 0.22576186060905457, 'learning_rate': 3.746737338706397e-05, 'epoch': 1.25}
 62%|██████▏   | 580/930 [18:17:49<9:44:17, 100.16s/it] 62%|██████▏   | 581/930 [18:19:34<9:50:00, 101.43s/it] 63%|██████▎   | 582/930 [18:21:33<10:18:29, 106.64s/it] 63%|██████▎   | 583/930 [18:23:20<10:18:27, 106.94s/it] 63%|██████▎   | 584/930 [18:25:01<10:05:54, 105.07s/it] 63%|██████▎   | 585/930 [18:26:55<10:19:44, 107.78s/it] 63%|██████▎   | 586/930 [18:28:46<10:23:14, 108.71s/it] 63%|██████▎   | 587/930 [18:30:18<9:53:28, 103.81s/it]  63%|██████▎   | 588/930 [18:32:12<10:08:26, 106.75s/it] 63%|██████▎   | 589/930 [18:34:08<10:22:15, 109.49s/it] 63%|██████▎   | 590/930 [18:35:39<9:49:10, 103.97s/it]                                                        {'loss': 0.6924, 'grad_norm': 0.22421567142009735, 'learning_rate': 3.5659838364445505e-05, 'epoch': 1.27}
 63%|██████▎   | 590/930 [18:35:39<9:49:10, 103.97s/it] 64%|██████▎   | 591/930 [18:37:15<9:33:35, 101.52s/it] 64%|██████▎   | 592/930 [18:39:05<9:47:17, 104.25s/it] 64%|██████▍   | 593/930 [18:40:51<9:48:01, 104.69s/it] 64%|██████▍   | 594/930 [18:42:29<9:34:05, 102.52s/it] 64%|██████▍   | 595/930 [18:44:08<9:27:30, 101.64s/it] 64%|██████▍   | 596/930 [18:46:01<9:45:06, 105.11s/it] 64%|██████▍   | 597/930 [18:47:34<9:23:24, 101.51s/it] 64%|██████▍   | 598/930 [18:49:19<9:26:00, 102.29s/it] 64%|██████▍   | 599/930 [18:50:58<9:19:40, 101.45s/it] 65%|██████▍   | 600/930 [18:52:17<8:41:34, 94.83s/it] 
***** Running Evaluation *****

***** Running Evaluation *****

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1
  Num examples = 3305
  Num examples = 3305
  Batch size = 1
  Batch size = 1
                                                      {'loss': 0.6942, 'grad_norm': 0.21488508582115173, 'learning_rate': 3.3872503366002536e-05, 'epoch': 1.29}
 65%|██████▍   | 600/930 [18:52:17<8:41:34, 94.83s/it][INFO|trainer.py:4643] 2025-12-09 06:01:46,675 >> 
***** Running Evaluation *****
[INFO|trainer.py:4645] 2025-12-09 06:01:46,675 >>   Num examples = 3305
[INFO|trainer.py:4648] 2025-12-09 06:01:46,675 >>   Batch size = 1

  0%|          | 0/827 [00:00<?, ?it/s][A
  0%|          | 2/827 [00:04<29:24,  2.14s/it][A
  0%|          | 3/827 [00:06<32:15,  2.35s/it][A
  0%|          | 4/827 [00:10<37:00,  2.70s/it][A
  1%|          | 5/827 [00:14<42:26,  3.10s/it][A
  1%|          | 6/827 [00:17<45:29,  3.33s/it][A
  1%|          | 7/827 [00:22<49:36,  3.63s/it][A
  1%|          | 8/827 [00:28<1:00:57,  4.47s/it][A
  1%|          | 9/827 [00:36<1:14:20,  5.45s/it][A
  1%|          | 10/827 [00:43<1:21:42,  6.00s/it][A
  1%|▏         | 11/827 [00:46<1:11:41,  5.27s/it][A
  1%|▏         | 12/827 [00:49<1:02:33,  4.61s/it][A
  2%|▏         | 13/827 [00:53<57:00,  4.20s/it]  [A
  2%|▏         | 14/827 [00:57<56:08,  4.14s/it][A
  2%|▏         | 15/827 [01:01<57:40,  4.26s/it][A
  2%|▏         | 16/827 [01:05<55:23,  4.10s/it][A
  2%|▏         | 17/827 [01:08<51:52,  3.84s/it][A
  2%|▏         | 18/827 [01:12<50:40,  3.76s/it][A
  2%|▏         | 19/827 [01:17<56:37,  4.20s/it][A
  2%|▏         | 20/827 [01:23<1:02:34,  4.65s/it][A
  3%|▎         | 21/827 [01:27<1:00:56,  4.54s/it][A
  3%|▎         | 22/827 [01:32<1:01:35,  4.59s/it][A
  3%|▎         | 23/827 [01:35<55:29,  4.14s/it]  [A
  3%|▎         | 24/827 [01:39<55:11,  4.12s/it][A
  3%|▎         | 25/827 [01:44<59:18,  4.44s/it][A
  3%|▎         | 26/827 [01:49<1:02:37,  4.69s/it][A
  3%|▎         | 27/827 [01:54<1:01:58,  4.65s/it][A
  3%|▎         | 28/827 [01:57<55:44,  4.19s/it]  [A
  4%|▎         | 29/827 [02:02<1:00:02,  4.51s/it][A
  4%|▎         | 30/827 [02:07<1:02:33,  4.71s/it][A
  4%|▎         | 31/827 [02:11<59:14,  4.46s/it]  [A
  4%|▍         | 32/827 [02:16<59:29,  4.49s/it][A
  4%|▍         | 33/827 [02:21<1:02:46,  4.74s/it][A
  4%|▍         | 34/827 [02:25<57:35,  4.36s/it]  [A
  4%|▍         | 35/827 [02:28<53:46,  4.07s/it][A
  4%|▍         | 36/827 [02:31<48:26,  3.67s/it][A
  4%|▍         | 37/827 [02:36<55:15,  4.20s/it][A
  5%|▍         | 38/827 [02:41<58:42,  4.46s/it][A
  5%|▍         | 39/827 [02:46<1:00:07,  4.58s/it][A
  5%|▍         | 40/827 [02:51<1:01:55,  4.72s/it][A
  5%|▍         | 41/827 [02:55<56:41,  4.33s/it]  [A
  5%|▌         | 42/827 [02:59<54:59,  4.20s/it][A
  5%|▌         | 43/827 [03:02<53:12,  4.07s/it][A
  5%|▌         | 44/827 [03:06<52:00,  3.99s/it][A
  5%|▌         | 45/827 [03:11<54:18,  4.17s/it][A
  6%|▌         | 46/827 [03:17<1:00:44,  4.67s/it][A
  6%|▌         | 47/827 [03:21<1:01:17,  4.72s/it][A
  6%|▌         | 48/827 [03:25<57:07,  4.40s/it]  [A
  6%|▌         | 49/827 [03:29<53:53,  4.16s/it][A
  6%|▌         | 50/827 [03:32<49:20,  3.81s/it][A
  6%|▌         | 51/827 [03:37<56:07,  4.34s/it][A
  6%|▋         | 52/827 [03:43<1:01:45,  4.78s/it][A
  6%|▋         | 53/827 [03:46<54:16,  4.21s/it]  [A
  7%|▋         | 54/827 [03:49<50:32,  3.92s/it][A
  7%|▋         | 55/827 [03:52<46:53,  3.64s/it][A
  7%|▋         | 56/827 [03:56<45:59,  3.58s/it][A
  7%|▋         | 57/827 [03:59<46:31,  3.62s/it][A
  7%|▋         | 58/827 [04:03<45:13,  3.53s/it][A
  7%|▋         | 59/827 [04:06<44:50,  3.50s/it][A
  7%|▋         | 60/827 [04:11<49:43,  3.89s/it][A
  7%|▋         | 61/827 [04:16<54:16,  4.25s/it][A
  7%|▋         | 62/827 [04:19<49:09,  3.86s/it][A
  8%|▊         | 63/827 [04:22<46:10,  3.63s/it][A
  8%|▊         | 64/827 [04:25<44:18,  3.48s/it][A
  8%|▊         | 65/827 [04:29<44:19,  3.49s/it][A
  8%|▊         | 66/827 [04:34<52:15,  4.12s/it][A
  8%|▊         | 67/827 [04:39<55:06,  4.35s/it][A
  8%|▊         | 68/827 [04:42<48:57,  3.87s/it][A
  8%|▊         | 69/827 [04:45<47:31,  3.76s/it][A
  8%|▊         | 70/827 [04:48<44:26,  3.52s/it][A
  9%|▊         | 71/827 [04:53<49:04,  3.90s/it][A
  9%|▊         | 72/827 [04:58<52:38,  4.18s/it][A
  9%|▉         | 73/827 [05:02<50:39,  4.03s/it][A
  9%|▉         | 74/827 [05:08<58:16,  4.64s/it][A
  9%|▉         | 75/827 [05:13<59:57,  4.78s/it][A
  9%|▉         | 76/827 [05:16<53:36,  4.28s/it][A
  9%|▉         | 77/827 [05:19<49:01,  3.92s/it][A
  9%|▉         | 78/827 [05:22<45:53,  3.68s/it][A
 10%|▉         | 79/827 [05:25<43:26,  3.48s/it][A
 10%|▉         | 80/827 [05:30<48:05,  3.86s/it][A
 10%|▉         | 81/827 [05:35<51:10,  4.12s/it][A
 10%|▉         | 82/827 [05:39<51:12,  4.12s/it][A
 10%|█         | 83/827 [05:43<52:04,  4.20s/it][A
 10%|█         | 84/827 [05:48<52:54,  4.27s/it][A
 10%|█         | 85/827 [05:51<51:33,  4.17s/it][A
 10%|█         | 86/827 [05:55<47:27,  3.84s/it][A
 11%|█         | 87/827 [06:02<59:11,  4.80s/it][A
 11%|█         | 88/827 [06:12<1:19:08,  6.43s/it][A
 11%|█         | 89/827 [06:20<1:23:54,  6.82s/it][A
 11%|█         | 90/827 [06:25<1:17:46,  6.33s/it][A
 11%|█         | 91/827 [06:28<1:08:02,  5.55s/it][A
 11%|█         | 92/827 [06:32<1:02:26,  5.10s/it][A
 11%|█         | 93/827 [06:35<54:24,  4.45s/it]  [A
 11%|█▏        | 94/827 [06:39<49:59,  4.09s/it][A
 11%|█▏        | 95/827 [06:42<47:18,  3.88s/it][A
 12%|█▏        | 96/827 [06:45<43:21,  3.56s/it][A
 12%|█▏        | 97/827 [06:47<39:17,  3.23s/it][A
 12%|█▏        | 98/827 [06:50<37:25,  3.08s/it][A
 12%|█▏        | 99/827 [06:53<36:03,  2.97s/it][A
 12%|█▏        | 100/827 [06:55<33:44,  2.79s/it][A
 12%|█▏        | 101/827 [06:59<37:06,  3.07s/it][A
 12%|█▏        | 102/827 [07:02<37:27,  3.10s/it][A
 12%|█▏        | 103/827 [07:05<36:34,  3.03s/it][A
 13%|█▎        | 104/827 [07:08<36:37,  3.04s/it][A
 13%|█▎        | 105/827 [07:12<38:23,  3.19s/it][A
 13%|█▎        | 106/827 [07:19<51:59,  4.33s/it][A
 13%|█▎        | 107/827 [07:25<59:18,  4.94s/it][A
 13%|█▎        | 108/827 [07:30<59:29,  4.96s/it][A
 13%|█▎        | 109/827 [07:34<55:04,  4.60s/it][A
 13%|█▎        | 110/827 [07:37<52:04,  4.36s/it][A
 13%|█▎        | 111/827 [07:43<56:27,  4.73s/it][A
 14%|█▎        | 112/827 [07:49<1:01:01,  5.12s/it][A
 14%|█▎        | 113/827 [07:53<57:28,  4.83s/it]  [A
 14%|█▍        | 114/827 [07:57<52:56,  4.45s/it][A
 14%|█▍        | 115/827 [08:00<49:30,  4.17s/it][A
 14%|█▍        | 116/827 [08:04<48:04,  4.06s/it][A
 14%|█▍        | 117/827 [08:09<50:32,  4.27s/it][A
 14%|█▍        | 118/827 [08:15<56:13,  4.76s/it][A
 14%|█▍        | 119/827 [08:20<58:51,  4.99s/it][A
 15%|█▍        | 120/827 [08:25<57:13,  4.86s/it][A
 15%|█▍        | 121/827 [08:28<51:16,  4.36s/it][A
 15%|█▍        | 122/827 [08:31<47:25,  4.04s/it][A
 15%|█▍        | 123/827 [08:34<43:18,  3.69s/it][A
 15%|█▍        | 124/827 [08:38<43:05,  3.68s/it][A
 15%|█▌        | 125/827 [08:42<45:04,  3.85s/it][A
 15%|█▌        | 126/827 [08:46<43:48,  3.75s/it][A
 15%|█▌        | 127/827 [08:49<43:07,  3.70s/it][A
 15%|█▌        | 128/827 [08:53<43:47,  3.76s/it][A
 16%|█▌        | 129/827 [08:56<41:19,  3.55s/it][A
 16%|█▌        | 130/827 [09:00<42:23,  3.65s/it][A
 16%|█▌        | 131/827 [09:04<44:27,  3.83s/it][A
 16%|█▌        | 132/827 [09:09<48:47,  4.21s/it][A
 16%|█▌        | 133/827 [09:14<50:25,  4.36s/it][A
 16%|█▌        | 134/827 [09:18<47:28,  4.11s/it][A
 16%|█▋        | 135/827 [09:23<51:42,  4.48s/it][A
 16%|█▋        | 136/827 [09:29<57:23,  4.98s/it][A
 17%|█▋        | 137/827 [09:33<52:15,  4.54s/it][A
 17%|█▋        | 138/827 [09:39<59:10,  5.15s/it][A
 17%|█▋        | 139/827 [09:46<1:05:31,  5.71s/it][A
 17%|█▋        | 140/827 [09:50<57:55,  5.06s/it]  [A
 17%|█▋        | 141/827 [09:54<54:20,  4.75s/it][A
 17%|█▋        | 142/827 [09:57<49:59,  4.38s/it][A
 17%|█▋        | 143/827 [10:01<47:17,  4.15s/it][A
 17%|█▋        | 144/827 [10:04<43:15,  3.80s/it][A
 18%|█▊        | 145/827 [10:08<44:02,  3.88s/it][A
 18%|█▊        | 146/827 [10:12<44:38,  3.93s/it][A
 18%|█▊        | 147/827 [10:16<43:13,  3.81s/it][A
 18%|█▊        | 148/827 [10:19<42:06,  3.72s/it][A
 18%|█▊        | 149/827 [10:23<41:05,  3.64s/it][A
 18%|█▊        | 150/827 [10:25<38:05,  3.38s/it][A
 18%|█▊        | 151/827 [10:28<35:10,  3.12s/it][A
 18%|█▊        | 152/827 [10:31<36:13,  3.22s/it][A
 19%|█▊        | 153/827 [10:35<36:45,  3.27s/it][A
 19%|█▊        | 154/827 [10:39<39:45,  3.55s/it][A
 19%|█▊        | 155/827 [10:44<46:01,  4.11s/it][A
 19%|█▉        | 156/827 [10:49<48:22,  4.33s/it][A
 19%|█▉        | 157/827 [10:53<45:35,  4.08s/it][A
 19%|█▉        | 158/827 [10:56<42:25,  3.81s/it][A
 19%|█▉        | 159/827 [11:01<46:15,  4.15s/it][A
 19%|█▉        | 160/827 [11:05<46:29,  4.18s/it][A
 19%|█▉        | 161/827 [11:08<41:43,  3.76s/it][A
 20%|█▉        | 162/827 [11:12<43:12,  3.90s/it][A
 20%|█▉        | 163/827 [11:16<43:33,  3.94s/it][A
 20%|█▉        | 164/827 [11:20<42:14,  3.82s/it][A
 20%|█▉        | 165/827 [11:23<40:42,  3.69s/it][A
 20%|██        | 166/827 [11:29<49:43,  4.51s/it][A
 20%|██        | 167/827 [11:39<1:06:02,  6.00s/it][A
 20%|██        | 168/827 [11:44<1:03:25,  5.77s/it][A
 20%|██        | 169/827 [11:48<55:34,  5.07s/it]  [A
 21%|██        | 170/827 [11:52<53:31,  4.89s/it][A
 21%|██        | 171/827 [11:56<50:18,  4.60s/it][A
 21%|██        | 172/827 [11:59<44:20,  4.06s/it][A
 21%|██        | 173/827 [12:02<40:30,  3.72s/it][A
 21%|██        | 174/827 [12:05<38:58,  3.58s/it][A
 21%|██        | 175/827 [12:09<39:44,  3.66s/it][A
 21%|██▏       | 176/827 [12:12<36:48,  3.39s/it][A
 21%|██▏       | 177/827 [12:14<34:36,  3.19s/it][A
 22%|██▏       | 178/827 [12:19<38:35,  3.57s/it][A
 22%|██▏       | 179/827 [12:24<45:14,  4.19s/it][A
 22%|██▏       | 180/827 [12:28<44:48,  4.16s/it][A
 22%|██▏       | 181/827 [12:32<44:25,  4.13s/it][A
 22%|██▏       | 182/827 [12:37<46:47,  4.35s/it][A
 22%|██▏       | 183/827 [12:41<43:17,  4.03s/it][A
 22%|██▏       | 184/827 [12:45<45:45,  4.27s/it][A
 22%|██▏       | 185/827 [12:51<48:20,  4.52s/it][A
 22%|██▏       | 186/827 [12:54<44:06,  4.13s/it][A
 23%|██▎       | 187/827 [12:57<40:40,  3.81s/it][A
 23%|██▎       | 188/827 [13:01<41:58,  3.94s/it][A
 23%|██▎       | 189/827 [13:06<45:22,  4.27s/it][A
 23%|██▎       | 190/827 [13:10<44:16,  4.17s/it][A
 23%|██▎       | 191/827 [13:14<44:16,  4.18s/it][A
 23%|██▎       | 192/827 [13:19<47:01,  4.44s/it][A
 23%|██▎       | 193/827 [13:23<45:25,  4.30s/it][A
 23%|██▎       | 194/827 [13:26<41:30,  3.93s/it][A
 24%|██▎       | 195/827 [13:30<39:00,  3.70s/it][A
 24%|██▎       | 196/827 [13:32<35:42,  3.40s/it][A
 24%|██▍       | 197/827 [13:35<34:01,  3.24s/it][A
 24%|██▍       | 198/827 [13:39<35:20,  3.37s/it][A
 24%|██▍       | 199/827 [13:42<35:53,  3.43s/it][A
 24%|██▍       | 200/827 [13:47<39:31,  3.78s/it][A
 24%|██▍       | 201/827 [13:52<43:54,  4.21s/it][A
 24%|██▍       | 202/827 [13:58<47:32,  4.56s/it][A
 25%|██▍       | 203/827 [14:03<50:07,  4.82s/it][A
 25%|██▍       | 204/827 [14:09<53:12,  5.12s/it][A
 25%|██▍       | 205/827 [14:12<48:19,  4.66s/it][A
 25%|██▍       | 206/827 [14:16<43:50,  4.24s/it][A
 25%|██▌       | 207/827 [14:19<41:28,  4.01s/it][A
 25%|██▌       | 208/827 [14:23<39:34,  3.84s/it][A
 25%|██▌       | 209/827 [14:26<37:42,  3.66s/it][A
 25%|██▌       | 210/827 [14:29<36:58,  3.60s/it][A
 26%|██▌       | 211/827 [14:34<41:26,  4.04s/it][A
 26%|██▌       | 212/827 [14:39<45:00,  4.39s/it][A
 26%|██▌       | 213/827 [14:45<48:47,  4.77s/it][A
 26%|██▌       | 214/827 [14:49<45:00,  4.41s/it][A
 26%|██▌       | 215/827 [14:52<41:10,  4.04s/it][A
 26%|██▌       | 216/827 [14:56<40:33,  3.98s/it][A
 26%|██▌       | 217/827 [15:00<40:50,  4.02s/it][A
 26%|██▋       | 218/827 [15:04<40:21,  3.98s/it][A
 26%|██▋       | 219/827 [15:07<37:29,  3.70s/it][A
 27%|██▋       | 220/827 [15:12<40:41,  4.02s/it][A
 27%|██▋       | 221/827 [15:17<45:04,  4.46s/it][A
 27%|██▋       | 222/827 [15:20<41:41,  4.13s/it][A
 27%|██▋       | 223/827 [15:24<39:02,  3.88s/it][A
 27%|██▋       | 224/827 [15:27<37:06,  3.69s/it][A
 27%|██▋       | 225/827 [15:30<35:34,  3.55s/it][A
 27%|██▋       | 226/827 [15:33<32:54,  3.29s/it][A
 27%|██▋       | 227/827 [15:38<37:59,  3.80s/it][A
 28%|██▊       | 228/827 [15:43<42:19,  4.24s/it][A
 28%|██▊       | 229/827 [15:47<40:44,  4.09s/it][A
 28%|██▊       | 230/827 [15:51<40:53,  4.11s/it][A
 28%|██▊       | 231/827 [15:54<38:57,  3.92s/it][A
 28%|██▊       | 232/827 [15:58<37:28,  3.78s/it][A
 28%|██▊       | 233/827 [16:03<41:24,  4.18s/it][A
 28%|██▊       | 234/827 [16:09<46:44,  4.73s/it][A
 28%|██▊       | 235/827 [16:14<47:25,  4.81s/it][A
 29%|██▊       | 236/827 [16:18<45:28,  4.62s/it][A
 29%|██▊       | 237/827 [16:25<50:30,  5.14s/it][A
 29%|██▉       | 238/827 [16:30<51:59,  5.30s/it][A
 29%|██▉       | 239/827 [16:36<52:45,  5.38s/it][A
 29%|██▉       | 240/827 [16:42<53:39,  5.48s/it][A
 29%|██▉       | 241/827 [16:48<55:01,  5.63s/it][A
 29%|██▉       | 242/827 [16:52<51:01,  5.23s/it][A
 29%|██▉       | 243/827 [16:57<50:17,  5.17s/it][A
 30%|██▉       | 244/827 [17:03<52:12,  5.37s/it][A
 30%|██▉       | 245/827 [17:06<47:33,  4.90s/it][A
 30%|██▉       | 246/827 [17:10<44:08,  4.56s/it][A
 30%|██▉       | 247/827 [17:15<44:51,  4.64s/it][A
 30%|██▉       | 248/827 [17:22<51:44,  5.36s/it][A
 30%|███       | 249/827 [17:27<49:34,  5.15s/it][A
 30%|███       | 250/827 [17:29<41:56,  4.36s/it][A
 30%|███       | 251/827 [17:34<41:27,  4.32s/it][A
 30%|███       | 252/827 [17:38<42:10,  4.40s/it][A
 31%|███       | 253/827 [17:41<37:57,  3.97s/it][A
 31%|███       | 254/827 [17:47<43:49,  4.59s/it][A
 31%|███       | 255/827 [17:54<49:32,  5.20s/it][A
 31%|███       | 256/827 [18:00<52:35,  5.53s/it][A
 31%|███       | 257/827 [18:04<49:23,  5.20s/it][A
 31%|███       | 258/827 [18:09<47:00,  4.96s/it][A
 31%|███▏      | 259/827 [18:15<50:59,  5.39s/it][A
 31%|███▏      | 260/827 [18:21<53:13,  5.63s/it][A
 32%|███▏      | 261/827 [18:25<48:12,  5.11s/it][A
 32%|███▏      | 262/827 [18:32<52:20,  5.56s/it][A
 32%|███▏      | 263/827 [18:38<54:39,  5.81s/it][A
 32%|███▏      | 264/827 [18:41<46:15,  4.93s/it][A
 32%|███▏      | 265/827 [18:44<40:27,  4.32s/it][A
 32%|███▏      | 266/827 [18:47<37:34,  4.02s/it][A
 32%|███▏      | 267/827 [18:51<35:37,  3.82s/it][A
 32%|███▏      | 268/827 [18:54<34:28,  3.70s/it][A
 33%|███▎      | 269/827 [18:59<37:25,  4.02s/it][A
 33%|███▎      | 270/827 [19:04<39:54,  4.30s/it][A
 33%|███▎      | 271/827 [19:07<37:28,  4.04s/it][A
 33%|███▎      | 272/827 [19:11<35:32,  3.84s/it][A
 33%|███▎      | 273/827 [19:17<42:36,  4.61s/it][A
 33%|███▎      | 274/827 [19:26<53:52,  5.85s/it][A
 33%|███▎      | 275/827 [19:31<52:00,  5.65s/it][A
 33%|███▎      | 276/827 [19:35<47:42,  5.20s/it][A
 33%|███▎      | 277/827 [19:39<43:52,  4.79s/it][A
 34%|███▎      | 278/827 [19:43<41:48,  4.57s/it][A
 34%|███▎      | 279/827 [19:47<38:42,  4.24s/it][A
 34%|███▍      | 280/827 [19:50<35:20,  3.88s/it][A
 34%|███▍      | 281/827 [19:54<36:08,  3.97s/it][A
 34%|███▍      | 282/827 [19:57<35:09,  3.87s/it][A
 34%|███▍      | 283/827 [20:01<34:31,  3.81s/it][A
 34%|███▍      | 284/827 [20:05<34:50,  3.85s/it][A
 34%|███▍      | 285/827 [20:10<37:10,  4.11s/it][A
 35%|███▍      | 286/827 [20:15<39:17,  4.36s/it][A
 35%|███▍      | 287/827 [20:17<34:42,  3.86s/it][A
 35%|███▍      | 288/827 [20:21<32:47,  3.65s/it][A
 35%|███▍      | 289/827 [20:24<31:55,  3.56s/it][A
 35%|███▌      | 290/827 [20:27<31:02,  3.47s/it][A
 35%|███▌      | 291/827 [20:30<29:12,  3.27s/it][A
 35%|███▌      | 292/827 [20:34<29:57,  3.36s/it][A
 35%|███▌      | 293/827 [20:37<29:00,  3.26s/it][A
 36%|███▌      | 294/827 [20:40<30:11,  3.40s/it][A
 36%|███▌      | 295/827 [20:44<30:40,  3.46s/it][A
 36%|███▌      | 296/827 [20:48<33:40,  3.80s/it][A
 36%|███▌      | 297/827 [20:53<34:20,  3.89s/it][A
 36%|███▌      | 298/827 [20:56<32:10,  3.65s/it][A
 36%|███▌      | 299/827 [20:59<31:47,  3.61s/it][A
 36%|███▋      | 300/827 [21:02<30:22,  3.46s/it][A
 36%|███▋      | 301/827 [21:05<29:37,  3.38s/it][A
 37%|███▋      | 302/827 [21:09<29:54,  3.42s/it][A
 37%|███▋      | 303/827 [21:11<27:18,  3.13s/it][A
 37%|███▋      | 304/827 [21:15<28:20,  3.25s/it][A
 37%|███▋      | 305/827 [21:20<32:21,  3.72s/it][A
 37%|███▋      | 306/827 [21:24<33:24,  3.85s/it][A
 37%|███▋      | 307/827 [21:27<32:16,  3.72s/it][A
 37%|███▋      | 308/827 [21:31<30:45,  3.55s/it][A
 37%|███▋      | 309/827 [21:34<30:57,  3.59s/it][A
 37%|███▋      | 310/827 [21:37<28:47,  3.34s/it][A
 38%|███▊      | 311/827 [21:40<27:32,  3.20s/it][A
 38%|███▊      | 312/827 [21:44<29:12,  3.40s/it][A
 38%|███▊      | 313/827 [21:48<31:15,  3.65s/it][A
 38%|███▊      | 314/827 [21:52<31:32,  3.69s/it][A
 38%|███▊      | 315/827 [21:56<33:59,  3.98s/it][A
 38%|███▊      | 316/827 [22:02<37:00,  4.35s/it][A
 38%|███▊      | 317/827 [22:07<39:13,  4.61s/it][A
 38%|███▊      | 318/827 [22:11<38:42,  4.56s/it][A
 39%|███▊      | 319/827 [22:15<37:19,  4.41s/it][A
 39%|███▊      | 320/827 [22:19<35:16,  4.17s/it][A
 39%|███▉      | 321/827 [22:24<36:56,  4.38s/it][A
 39%|███▉      | 322/827 [22:28<35:19,  4.20s/it][A
 39%|███▉      | 323/827 [22:32<35:51,  4.27s/it][A
 39%|███▉      | 324/827 [22:37<37:14,  4.44s/it][A
 39%|███▉      | 325/827 [22:40<34:26,  4.12s/it][A
 39%|███▉      | 326/827 [22:44<34:19,  4.11s/it][A
 40%|███▉      | 327/827 [22:49<36:04,  4.33s/it][A
 40%|███▉      | 328/827 [22:54<37:03,  4.46s/it][A
 40%|███▉      | 329/827 [23:00<42:14,  5.09s/it][A
 40%|███▉      | 330/827 [23:05<40:43,  4.92s/it][A
 40%|████      | 331/827 [23:10<40:23,  4.89s/it][A
 40%|████      | 332/827 [23:14<39:42,  4.81s/it][A
 40%|████      | 333/827 [23:20<41:00,  4.98s/it][A
 40%|████      | 334/827 [23:24<39:16,  4.78s/it][A
 41%|████      | 335/827 [23:28<36:05,  4.40s/it][A
 41%|████      | 336/827 [23:31<32:58,  4.03s/it][A
 41%|████      | 337/827 [23:34<31:02,  3.80s/it][A
 41%|████      | 338/827 [23:37<29:56,  3.67s/it][A
 41%|████      | 339/827 [23:41<28:46,  3.54s/it][A
 41%|████      | 340/827 [23:44<29:07,  3.59s/it][A
 41%|████      | 341/827 [23:48<30:11,  3.73s/it][A
 41%|████▏     | 342/827 [23:52<29:00,  3.59s/it][A
 41%|████▏     | 343/827 [23:55<28:23,  3.52s/it][A
 42%|████▏     | 344/827 [24:00<31:06,  3.86s/it][A
 42%|████▏     | 345/827 [24:05<34:11,  4.26s/it][A
 42%|████▏     | 346/827 [24:09<33:29,  4.18s/it][A
 42%|████▏     | 347/827 [24:13<32:51,  4.11s/it][A
 42%|████▏     | 348/827 [24:16<30:02,  3.76s/it][A
 42%|████▏     | 349/827 [24:21<32:18,  4.06s/it][A
 42%|████▏     | 350/827 [24:25<33:54,  4.27s/it][A
 42%|████▏     | 351/827 [24:29<32:14,  4.06s/it][A
 43%|████▎     | 352/827 [24:33<32:45,  4.14s/it][A
 43%|████▎     | 353/827 [24:37<31:25,  3.98s/it][A
 43%|████▎     | 354/827 [24:40<30:08,  3.82s/it][A
 43%|████▎     | 355/827 [24:45<33:19,  4.24s/it][A
 43%|████▎     | 356/827 [24:51<35:22,  4.51s/it][A
 43%|████▎     | 357/827 [24:54<33:13,  4.24s/it][A
 43%|████▎     | 358/827 [24:57<30:05,  3.85s/it][A
 43%|████▎     | 359/827 [25:01<29:39,  3.80s/it][A
 44%|████▎     | 360/827 [25:06<32:31,  4.18s/it][A
 44%|████▎     | 361/827 [25:10<31:57,  4.11s/it][A
 44%|████▍     | 362/827 [25:13<28:43,  3.71s/it][A
 44%|████▍     | 363/827 [25:17<30:44,  3.98s/it][A
 44%|████▍     | 364/827 [25:21<30:27,  3.95s/it][A
 44%|████▍     | 365/827 [25:25<29:18,  3.81s/it][A
 44%|████▍     | 366/827 [25:30<33:57,  4.42s/it][A
 44%|████▍     | 367/827 [25:35<34:09,  4.46s/it][A
 44%|████▍     | 368/827 [25:38<30:22,  3.97s/it][A
 45%|████▍     | 369/827 [25:42<29:51,  3.91s/it][A
 45%|████▍     | 370/827 [25:45<29:34,  3.88s/it][A
 45%|████▍     | 371/827 [25:50<31:13,  4.11s/it][A
 45%|████▍     | 372/827 [25:54<30:55,  4.08s/it][A
 45%|████▌     | 373/827 [25:59<32:58,  4.36s/it][A
 45%|████▌     | 374/827 [26:06<39:00,  5.17s/it][A
 45%|████▌     | 375/827 [26:15<48:05,  6.38s/it][A
 45%|████▌     | 376/827 [26:23<49:54,  6.64s/it][A
 46%|████▌     | 377/827 [26:27<45:34,  6.08s/it][A
 46%|████▌     | 378/827 [26:34<47:15,  6.31s/it][A
 46%|████▌     | 379/827 [26:44<54:40,  7.32s/it][A
 46%|████▌     | 380/827 [26:50<51:47,  6.95s/it][A
 46%|████▌     | 381/827 [26:54<44:47,  6.03s/it][A
 46%|████▌     | 382/827 [26:59<42:23,  5.71s/it][A
 46%|████▋     | 383/827 [27:03<38:56,  5.26s/it][A
 46%|████▋     | 384/827 [27:07<35:37,  4.83s/it][A
 47%|████▋     | 385/827 [27:10<32:07,  4.36s/it][A
 47%|████▋     | 386/827 [27:13<29:56,  4.07s/it][A
 47%|████▋     | 387/827 [27:17<29:32,  4.03s/it][A
 47%|████▋     | 388/827 [27:22<30:32,  4.17s/it][A
 47%|████▋     | 389/827 [27:26<29:26,  4.03s/it][A
 47%|████▋     | 390/827 [27:29<28:26,  3.90s/it][A
 47%|████▋     | 391/827 [27:33<27:21,  3.76s/it][A
 47%|████▋     | 392/827 [27:37<27:42,  3.82s/it][A
 48%|████▊     | 393/827 [27:41<29:28,  4.08s/it][A
 48%|████▊     | 394/827 [27:47<32:38,  4.52s/it][A
 48%|████▊     | 395/827 [27:52<32:55,  4.57s/it][A
 48%|████▊     | 396/827 [27:56<33:03,  4.60s/it][A
 48%|████▊     | 397/827 [27:59<29:54,  4.17s/it][A
 48%|████▊     | 398/827 [28:03<28:19,  3.96s/it][A
 48%|████▊     | 399/827 [28:06<27:02,  3.79s/it][A
 48%|████▊     | 400/827 [28:11<28:41,  4.03s/it][A
 48%|████▊     | 401/827 [28:16<30:43,  4.33s/it][A
 49%|████▊     | 402/827 [28:21<31:20,  4.43s/it][A
 49%|████▊     | 403/827 [28:25<32:06,  4.54s/it][A
 49%|████▉     | 404/827 [28:28<28:13,  4.00s/it][A
 49%|████▉     | 405/827 [28:32<26:55,  3.83s/it][A
 49%|████▉     | 406/827 [28:35<25:47,  3.68s/it][A
 49%|████▉     | 407/827 [28:38<23:57,  3.42s/it][A
 49%|████▉     | 408/827 [28:41<23:33,  3.37s/it][A
 49%|████▉     | 409/827 [28:44<23:11,  3.33s/it][A
 50%|████▉     | 410/827 [28:47<21:46,  3.13s/it][A
 50%|████▉     | 411/827 [28:50<22:01,  3.18s/it][A
 50%|████▉     | 412/827 [28:53<22:02,  3.19s/it][A
 50%|████▉     | 413/827 [28:58<25:27,  3.69s/it][A
 50%|█████     | 414/827 [29:03<28:22,  4.12s/it][A
 50%|█████     | 415/827 [29:06<25:51,  3.76s/it][A
 50%|█████     | 416/827 [29:11<28:05,  4.10s/it][A
 50%|█████     | 417/827 [29:16<30:08,  4.41s/it][A
 51%|█████     | 418/827 [29:22<32:14,  4.73s/it][A
 51%|█████     | 419/827 [29:26<31:44,  4.67s/it][A
 51%|█████     | 420/827 [29:30<30:45,  4.53s/it][A
 51%|█████     | 421/827 [29:34<29:02,  4.29s/it][A
 51%|█████     | 422/827 [29:37<26:43,  3.96s/it][A
 51%|█████     | 423/827 [29:41<25:03,  3.72s/it][A
 51%|█████▏    | 424/827 [29:44<25:18,  3.77s/it][A
 51%|█████▏    | 425/827 [29:49<27:49,  4.15s/it][A
 52%|█████▏    | 426/827 [29:55<29:35,  4.43s/it][A
 52%|█████▏    | 427/827 [30:00<31:04,  4.66s/it][A
 52%|█████▏    | 428/827 [30:05<31:17,  4.71s/it][A
 52%|█████▏    | 429/827 [30:08<29:21,  4.43s/it][A
 52%|█████▏    | 430/827 [30:12<28:33,  4.32s/it][A
 52%|█████▏    | 431/827 [30:17<28:20,  4.29s/it][A
 52%|█████▏    | 432/827 [30:23<31:36,  4.80s/it][A
 52%|█████▏    | 433/827 [30:29<34:43,  5.29s/it][A
 52%|█████▏    | 434/827 [30:34<34:35,  5.28s/it][A
 53%|█████▎    | 435/827 [30:38<30:25,  4.66s/it][A
 53%|█████▎    | 436/827 [30:42<29:57,  4.60s/it][A
 53%|█████▎    | 437/827 [30:47<30:05,  4.63s/it][A
 53%|█████▎    | 438/827 [30:50<28:00,  4.32s/it][A
 53%|█████▎    | 439/827 [30:54<26:48,  4.15s/it][A
 53%|█████▎    | 440/827 [30:58<26:24,  4.09s/it][A
 53%|█████▎    | 441/827 [31:01<24:09,  3.76s/it][A
 53%|█████▎    | 442/827 [31:04<23:06,  3.60s/it][A
 54%|█████▎    | 443/827 [31:07<22:08,  3.46s/it][A
 54%|█████▎    | 444/827 [31:12<23:44,  3.72s/it][A
 54%|█████▍    | 445/827 [31:17<27:26,  4.31s/it][A
 54%|█████▍    | 446/827 [31:21<25:53,  4.08s/it][A
 54%|█████▍    | 447/827 [31:26<27:05,  4.28s/it][A
 54%|█████▍    | 448/827 [31:31<29:19,  4.64s/it][A
 54%|█████▍    | 449/827 [31:39<35:26,  5.63s/it][A
 54%|█████▍    | 450/827 [31:45<35:12,  5.60s/it][A
 55%|█████▍    | 451/827 [31:49<31:59,  5.10s/it][A
 55%|█████▍    | 452/827 [31:54<32:58,  5.28s/it][A
 55%|█████▍    | 453/827 [32:00<33:23,  5.36s/it][A
 55%|█████▍    | 454/827 [32:03<29:59,  4.82s/it][A
 55%|█████▌    | 455/827 [32:08<29:23,  4.74s/it][A
 55%|█████▌    | 456/827 [32:14<32:01,  5.18s/it][A
 55%|█████▌    | 457/827 [32:19<31:05,  5.04s/it][A
 55%|█████▌    | 458/827 [32:23<29:11,  4.75s/it][A
 56%|█████▌    | 459/827 [32:27<28:44,  4.69s/it][A
 56%|█████▌    | 460/827 [32:34<31:24,  5.13s/it][A
 56%|█████▌    | 461/827 [32:41<36:02,  5.91s/it][A
 56%|█████▌    | 462/827 [32:46<33:14,  5.46s/it][A
 56%|█████▌    | 463/827 [32:49<29:02,  4.79s/it][A
 56%|█████▌    | 464/827 [32:54<29:42,  4.91s/it][A
 56%|█████▌    | 465/827 [32:58<28:27,  4.72s/it][A
 56%|█████▋    | 466/827 [33:02<26:34,  4.42s/it][A
 56%|█████▋    | 467/827 [33:06<26:01,  4.34s/it][A
 57%|█████▋    | 468/827 [33:11<26:32,  4.44s/it][A
 57%|█████▋    | 469/827 [33:16<27:21,  4.59s/it][A
 57%|█████▋    | 470/827 [33:19<25:10,  4.23s/it][A
 57%|█████▋    | 471/827 [33:24<26:03,  4.39s/it][A
 57%|█████▋    | 472/827 [33:31<30:19,  5.12s/it][A
 57%|█████▋    | 473/827 [33:35<28:55,  4.90s/it][A
 57%|█████▋    | 474/827 [33:39<27:21,  4.65s/it][A
 57%|█████▋    | 475/827 [33:42<24:23,  4.16s/it][A
 58%|█████▊    | 476/827 [33:45<21:29,  3.67s/it][A
 58%|█████▊    | 477/827 [33:47<19:31,  3.35s/it][A
 58%|█████▊    | 478/827 [33:52<21:05,  3.63s/it][A
 58%|█████▊    | 479/827 [33:57<23:29,  4.05s/it][A
 58%|█████▊    | 480/827 [34:02<24:46,  4.28s/it][A
 58%|█████▊    | 481/827 [34:06<25:43,  4.46s/it][A
 58%|█████▊    | 482/827 [34:09<23:00,  4.00s/it][A
 58%|█████▊    | 483/827 [34:12<20:07,  3.51s/it][A
 59%|█████▊    | 484/827 [34:15<19:35,  3.43s/it][A
 59%|█████▊    | 485/827 [34:18<18:39,  3.27s/it][A
 59%|█████▉    | 486/827 [34:21<18:56,  3.33s/it][A
 59%|█████▉    | 487/827 [34:25<19:47,  3.49s/it][A
 59%|█████▉    | 488/827 [34:31<22:45,  4.03s/it][A
 59%|█████▉    | 489/827 [34:36<24:20,  4.32s/it][A
 59%|█████▉    | 490/827 [34:40<24:43,  4.40s/it][A
 59%|█████▉    | 491/827 [34:44<23:10,  4.14s/it][A
 59%|█████▉    | 492/827 [34:47<22:24,  4.01s/it][A
 60%|█████▉    | 493/827 [34:51<21:50,  3.92s/it][A
 60%|█████▉    | 494/827 [34:55<21:34,  3.89s/it][A
 60%|█████▉    | 495/827 [34:59<21:42,  3.92s/it][A
 60%|█████▉    | 496/827 [35:03<21:51,  3.96s/it][A
 60%|██████    | 497/827 [35:06<19:48,  3.60s/it][A
 60%|██████    | 498/827 [35:09<19:24,  3.54s/it][A
 60%|██████    | 499/827 [35:14<21:21,  3.91s/it][A
 60%|██████    | 500/827 [35:17<20:16,  3.72s/it][A
 61%|██████    | 501/827 [35:20<18:52,  3.47s/it][A
 61%|██████    | 502/827 [35:25<21:08,  3.90s/it][A
 61%|██████    | 503/827 [35:30<22:28,  4.16s/it][A
 61%|██████    | 504/827 [35:33<21:04,  3.91s/it][A
 61%|██████    | 505/827 [35:37<21:01,  3.92s/it][A
 61%|██████    | 506/827 [35:41<20:53,  3.91s/it][A
 61%|██████▏   | 507/827 [35:47<23:48,  4.46s/it][A
 61%|██████▏   | 508/827 [35:53<26:35,  5.00s/it][A
 62%|██████▏   | 509/827 [35:56<23:22,  4.41s/it][A
 62%|██████▏   | 510/827 [36:00<22:29,  4.26s/it][A
 62%|██████▏   | 511/827 [36:03<20:38,  3.92s/it][A
 62%|██████▏   | 512/827 [36:06<19:05,  3.64s/it][A
 62%|██████▏   | 513/827 [36:10<19:26,  3.72s/it][A
 62%|██████▏   | 514/827 [36:13<19:14,  3.69s/it][A
 62%|██████▏   | 515/827 [36:17<18:57,  3.64s/it][A
 62%|██████▏   | 516/827 [36:21<19:30,  3.76s/it][A
 63%|██████▎   | 517/827 [36:24<18:54,  3.66s/it][A
 63%|██████▎   | 518/827 [36:27<17:54,  3.48s/it][A
 63%|██████▎   | 519/827 [36:31<17:11,  3.35s/it][A
 63%|██████▎   | 520/827 [36:34<17:17,  3.38s/it][A
 63%|██████▎   | 521/827 [36:38<18:43,  3.67s/it][A
 63%|██████▎   | 522/827 [36:44<21:22,  4.21s/it][A
 63%|██████▎   | 523/827 [36:47<19:29,  3.85s/it][A
 63%|██████▎   | 524/827 [36:51<20:03,  3.97s/it][A
 63%|██████▎   | 525/827 [36:57<23:26,  4.66s/it][A
 64%|██████▎   | 526/827 [37:03<25:00,  4.99s/it][A
 64%|██████▎   | 527/827 [37:07<24:01,  4.80s/it][A
 64%|██████▍   | 528/827 [37:12<23:59,  4.81s/it][A
 64%|██████▍   | 529/827 [37:18<24:33,  4.94s/it][A
 64%|██████▍   | 530/827 [37:22<23:56,  4.84s/it][A
 64%|██████▍   | 531/827 [37:25<21:17,  4.32s/it][A
 64%|██████▍   | 532/827 [37:29<20:11,  4.11s/it][A
 64%|██████▍   | 533/827 [37:34<22:17,  4.55s/it][A
 65%|██████▍   | 534/827 [37:40<23:36,  4.84s/it][A
 65%|██████▍   | 535/827 [37:45<24:04,  4.95s/it][A
 65%|██████▍   | 536/827 [37:49<21:59,  4.53s/it][A
 65%|██████▍   | 537/827 [37:51<19:02,  3.94s/it][A
 65%|██████▌   | 538/827 [37:54<17:52,  3.71s/it][A
 65%|██████▌   | 539/827 [37:58<17:00,  3.54s/it][A
 65%|██████▌   | 540/827 [38:01<17:08,  3.59s/it][A
 65%|██████▌   | 541/827 [38:05<17:49,  3.74s/it][A
 66%|██████▌   | 542/827 [38:09<18:10,  3.82s/it][A
 66%|██████▌   | 543/827 [38:14<19:18,  4.08s/it][A
 66%|██████▌   | 544/827 [38:19<20:36,  4.37s/it][A
 66%|██████▌   | 545/827 [38:23<20:16,  4.31s/it][A
 66%|██████▌   | 546/827 [38:28<20:25,  4.36s/it][A
 66%|██████▌   | 547/827 [38:32<20:23,  4.37s/it][A
 66%|██████▋   | 548/827 [38:35<18:07,  3.90s/it][A
 66%|██████▋   | 549/827 [38:38<16:56,  3.66s/it][A
 67%|██████▋   | 550/827 [38:42<16:37,  3.60s/it][A
 67%|██████▋   | 551/827 [38:46<17:22,  3.78s/it][A
 67%|██████▋   | 552/827 [38:50<17:22,  3.79s/it][A
 67%|██████▋   | 553/827 [38:53<17:04,  3.74s/it][A
 67%|██████▋   | 554/827 [38:56<15:41,  3.45s/it][A
 67%|██████▋   | 555/827 [38:59<15:25,  3.40s/it][A
 67%|██████▋   | 556/827 [39:03<15:14,  3.38s/it][A
 67%|██████▋   | 557/827 [39:05<14:26,  3.21s/it][A
 67%|██████▋   | 558/827 [39:10<15:51,  3.54s/it][A
 68%|██████▊   | 559/827 [39:14<16:23,  3.67s/it][A
 68%|██████▊   | 560/827 [39:17<15:53,  3.57s/it][A
 68%|██████▊   | 561/827 [39:21<16:01,  3.61s/it][A
 68%|██████▊   | 562/827 [39:25<16:14,  3.68s/it][A
 68%|██████▊   | 563/827 [39:28<16:06,  3.66s/it][A
 68%|██████▊   | 564/827 [39:32<15:47,  3.60s/it][A
 68%|██████▊   | 565/827 [39:37<17:28,  4.00s/it][A
 68%|██████▊   | 566/827 [39:41<18:11,  4.18s/it][A
 69%|██████▊   | 567/827 [39:44<17:00,  3.93s/it][A
 69%|██████▊   | 568/827 [39:48<16:57,  3.93s/it][A
 69%|██████▉   | 569/827 [39:52<16:17,  3.79s/it][A
 69%|██████▉   | 570/827 [39:56<16:42,  3.90s/it][A
 69%|██████▉   | 571/827 [40:01<18:32,  4.35s/it][A
 69%|██████▉   | 572/827 [40:07<19:57,  4.70s/it][A
 69%|██████▉   | 573/827 [40:13<21:00,  4.96s/it][A
 69%|██████▉   | 574/827 [40:17<20:15,  4.80s/it][A
 70%|██████▉   | 575/827 [40:23<21:07,  5.03s/it][A
 70%|██████▉   | 576/827 [40:27<19:52,  4.75s/it][A
 70%|██████▉   | 577/827 [40:30<18:20,  4.40s/it][A
 70%|██████▉   | 578/827 [40:34<17:45,  4.28s/it][A
 70%|███████   | 579/827 [40:38<16:37,  4.02s/it][A
 70%|███████   | 580/827 [40:43<18:11,  4.42s/it][A
 70%|███████   | 581/827 [40:50<21:29,  5.24s/it][A
 70%|███████   | 582/827 [40:55<20:33,  5.03s/it][A
 70%|███████   | 583/827 [40:58<18:32,  4.56s/it][A
 71%|███████   | 584/827 [41:01<16:51,  4.16s/it][A
 71%|███████   | 585/827 [41:04<15:29,  3.84s/it][A
 71%|███████   | 586/827 [41:09<15:54,  3.96s/it][A
 71%|███████   | 587/827 [41:17<20:48,  5.20s/it][A
 71%|███████   | 588/827 [41:24<23:09,  5.81s/it][A
 71%|███████   | 589/827 [41:29<22:02,  5.56s/it][A
 71%|███████▏  | 590/827 [41:36<23:13,  5.88s/it][A
 71%|███████▏  | 591/827 [41:43<24:26,  6.21s/it][A
 72%|███████▏  | 592/827 [41:46<21:03,  5.38s/it][A
 72%|███████▏  | 593/827 [41:50<19:08,  4.91s/it][A
 72%|███████▏  | 594/827 [41:53<17:01,  4.38s/it][A
 72%|███████▏  | 595/827 [41:57<16:14,  4.20s/it][A
 72%|███████▏  | 596/827 [42:04<20:01,  5.20s/it][A
 72%|███████▏  | 597/827 [42:11<21:17,  5.55s/it][A
 72%|███████▏  | 598/827 [42:14<18:29,  4.84s/it][A
 72%|███████▏  | 599/827 [42:17<16:58,  4.47s/it][A
 73%|███████▎  | 600/827 [42:22<17:02,  4.50s/it][A
 73%|███████▎  | 601/827 [42:27<16:54,  4.49s/it][A
 73%|███████▎  | 602/827 [42:31<16:16,  4.34s/it][A
 73%|███████▎  | 603/827 [42:34<15:09,  4.06s/it][A
 73%|███████▎  | 604/827 [42:38<14:49,  3.99s/it][A
 73%|███████▎  | 605/827 [42:41<14:18,  3.87s/it][A
 73%|███████▎  | 606/827 [42:45<14:15,  3.87s/it][A
 73%|███████▎  | 607/827 [42:49<14:33,  3.97s/it][A
 74%|███████▎  | 608/827 [42:53<14:02,  3.85s/it][A
 74%|███████▎  | 609/827 [42:59<16:23,  4.51s/it][A
 74%|███████▍  | 610/827 [43:06<18:29,  5.11s/it][A
 74%|███████▍  | 611/827 [43:10<17:11,  4.78s/it][A
 74%|███████▍  | 612/827 [43:14<16:42,  4.67s/it][A
 74%|███████▍  | 613/827 [43:19<16:51,  4.72s/it][A
 74%|███████▍  | 614/827 [43:23<16:13,  4.57s/it][A
 74%|███████▍  | 615/827 [43:27<15:24,  4.36s/it][A
 74%|███████▍  | 616/827 [43:30<14:11,  4.03s/it][A
 75%|███████▍  | 617/827 [43:36<15:39,  4.47s/it][A
 75%|███████▍  | 618/827 [43:43<18:05,  5.19s/it][A
 75%|███████▍  | 619/827 [43:46<15:46,  4.55s/it][A
 75%|███████▍  | 620/827 [43:50<15:03,  4.37s/it][A
 75%|███████▌  | 621/827 [43:53<13:37,  3.97s/it][A
 75%|███████▌  | 622/827 [43:55<12:30,  3.66s/it][A
 75%|███████▌  | 623/827 [43:59<12:25,  3.66s/it][A
 75%|███████▌  | 624/827 [44:04<13:38,  4.03s/it][A
 76%|███████▌  | 625/827 [44:08<13:00,  3.87s/it][A
 76%|███████▌  | 626/827 [44:11<12:50,  3.84s/it][A
 76%|███████▌  | 627/827 [44:15<12:32,  3.76s/it][A
 76%|███████▌  | 628/827 [44:18<11:45,  3.54s/it][A
 76%|███████▌  | 629/827 [44:21<11:31,  3.49s/it][A
 76%|███████▌  | 630/827 [44:25<11:11,  3.41s/it][A
 76%|███████▋  | 631/827 [44:30<13:17,  4.07s/it][A
 76%|███████▋  | 632/827 [44:35<14:19,  4.41s/it][A
 77%|███████▋  | 633/827 [44:41<15:32,  4.81s/it][A
 77%|███████▋  | 634/827 [44:48<17:04,  5.31s/it][A
 77%|███████▋  | 635/827 [44:51<14:49,  4.63s/it][A
 77%|███████▋  | 636/827 [44:55<14:33,  4.58s/it][A
 77%|███████▋  | 637/827 [45:00<15:08,  4.78s/it][A
 77%|███████▋  | 638/827 [45:05<15:02,  4.77s/it][A
 77%|███████▋  | 639/827 [45:08<13:32,  4.32s/it][A
 77%|███████▋  | 640/827 [45:12<12:24,  3.98s/it][A
 78%|███████▊  | 641/827 [45:16<12:40,  4.09s/it][A
 78%|███████▊  | 642/827 [45:20<12:20,  4.00s/it][A
 78%|███████▊  | 643/827 [45:25<13:09,  4.29s/it][A
 78%|███████▊  | 644/827 [45:29<13:22,  4.39s/it][A
 78%|███████▊  | 645/827 [45:33<12:27,  4.11s/it][A
 78%|███████▊  | 646/827 [45:36<11:24,  3.78s/it][A
 78%|███████▊  | 647/827 [45:39<11:09,  3.72s/it][A
 78%|███████▊  | 648/827 [45:44<11:35,  3.88s/it][A
 78%|███████▊  | 649/827 [45:47<11:24,  3.85s/it][A
 79%|███████▊  | 650/827 [45:51<10:51,  3.68s/it][A
 79%|███████▊  | 651/827 [45:54<10:29,  3.57s/it][A
 79%|███████▉  | 652/827 [45:57<10:15,  3.52s/it][A
 79%|███████▉  | 653/827 [46:01<10:33,  3.64s/it][A
 79%|███████▉  | 654/827 [46:07<12:13,  4.24s/it][A
 79%|███████▉  | 655/827 [46:12<13:20,  4.65s/it][A
 79%|███████▉  | 656/827 [46:15<11:46,  4.13s/it][A
 79%|███████▉  | 657/827 [46:18<10:49,  3.82s/it][A
 80%|███████▉  | 658/827 [46:23<11:46,  4.18s/it][A
 80%|███████▉  | 659/827 [46:32<15:09,  5.41s/it][A
 80%|███████▉  | 660/827 [46:37<15:06,  5.43s/it][A
 80%|███████▉  | 661/827 [46:43<15:17,  5.53s/it][A
 80%|████████  | 662/827 [46:47<14:03,  5.11s/it][A
 80%|████████  | 663/827 [46:51<13:19,  4.88s/it][A
 80%|████████  | 664/827 [46:57<13:27,  4.95s/it][A
 80%|████████  | 665/827 [47:01<12:43,  4.71s/it][A
 81%|████████  | 666/827 [47:05<12:12,  4.55s/it][A
 81%|████████  | 667/827 [47:08<11:15,  4.22s/it][A
 81%|████████  | 668/827 [47:12<10:58,  4.14s/it][A
 81%|████████  | 669/827 [47:15<09:56,  3.77s/it][A
 81%|████████  | 670/827 [47:18<09:25,  3.60s/it][A
 81%|████████  | 671/827 [47:23<09:42,  3.74s/it][A
 81%|████████▏ | 672/827 [47:26<09:29,  3.67s/it][A
 81%|████████▏ | 673/827 [47:29<09:02,  3.52s/it][A
 81%|████████▏ | 674/827 [47:33<08:53,  3.49s/it][A
 82%|████████▏ | 675/827 [47:37<09:14,  3.65s/it][A
 82%|████████▏ | 676/827 [47:41<10:04,  4.01s/it][A
 82%|████████▏ | 677/827 [47:47<10:55,  4.37s/it][A
 82%|████████▏ | 678/827 [47:52<11:20,  4.57s/it][A
 82%|████████▏ | 679/827 [47:56<11:11,  4.54s/it][A
 82%|████████▏ | 680/827 [47:59<10:06,  4.13s/it][A
 82%|████████▏ | 681/827 [48:05<11:04,  4.55s/it][A
 82%|████████▏ | 682/827 [48:10<11:21,  4.70s/it][A
 83%|████████▎ | 683/827 [48:13<09:59,  4.17s/it][A
 83%|████████▎ | 684/827 [48:16<09:09,  3.84s/it][A
 83%|████████▎ | 685/827 [48:19<08:20,  3.53s/it][A
 83%|████████▎ | 686/827 [48:25<10:16,  4.37s/it][A
 83%|████████▎ | 687/827 [48:32<12:14,  5.24s/it][A
 83%|████████▎ | 688/827 [48:38<12:15,  5.29s/it][A
 83%|████████▎ | 689/827 [48:43<11:53,  5.17s/it][A
 83%|████████▎ | 690/827 [48:46<10:27,  4.58s/it][A
 84%|████████▎ | 691/827 [48:49<09:35,  4.23s/it][A
 84%|████████▎ | 692/827 [48:53<09:25,  4.19s/it][A
 84%|████████▍ | 693/827 [48:57<08:50,  3.96s/it][A
 84%|████████▍ | 694/827 [49:00<08:21,  3.77s/it][A
 84%|████████▍ | 695/827 [49:05<08:47,  3.99s/it][A
 84%|████████▍ | 696/827 [49:10<09:18,  4.26s/it][A
 84%|████████▍ | 697/827 [49:13<08:53,  4.10s/it][A
 84%|████████▍ | 698/827 [49:16<07:51,  3.66s/it][A
 85%|████████▍ | 699/827 [49:19<07:34,  3.55s/it][A
 85%|████████▍ | 700/827 [49:26<09:38,  4.56s/it][A
 85%|████████▍ | 701/827 [49:33<10:48,  5.15s/it][A
 85%|████████▍ | 702/827 [49:37<10:26,  5.01s/it][A
 85%|████████▌ | 703/827 [49:41<09:40,  4.68s/it][A
 85%|████████▌ | 704/827 [49:44<08:40,  4.23s/it][A
 85%|████████▌ | 705/827 [49:47<07:40,  3.78s/it][A
 85%|████████▌ | 706/827 [49:50<07:08,  3.54s/it][A
 85%|████████▌ | 707/827 [49:54<07:29,  3.74s/it][A
 86%|████████▌ | 708/827 [49:58<07:15,  3.66s/it][A
 86%|████████▌ | 709/827 [50:02<07:22,  3.75s/it][A
 86%|████████▌ | 710/827 [50:06<07:19,  3.76s/it][A
 86%|████████▌ | 711/827 [50:09<07:17,  3.77s/it][A
 86%|████████▌ | 712/827 [50:12<06:52,  3.59s/it][A
 86%|████████▌ | 713/827 [50:16<06:47,  3.57s/it][A
 86%|████████▋ | 714/827 [50:20<07:05,  3.77s/it][A
 86%|████████▋ | 715/827 [50:25<07:20,  3.93s/it][A
 87%|████████▋ | 716/827 [50:28<07:16,  3.93s/it][A
 87%|████████▋ | 717/827 [50:34<07:53,  4.31s/it][A
 87%|████████▋ | 718/827 [50:39<08:18,  4.58s/it][A
 87%|████████▋ | 719/827 [50:42<07:41,  4.27s/it][A
 87%|████████▋ | 720/827 [50:46<07:21,  4.13s/it][A
 87%|████████▋ | 721/827 [50:49<06:48,  3.85s/it][A
 87%|████████▋ | 722/827 [50:53<06:44,  3.85s/it][A
 87%|████████▋ | 723/827 [50:57<06:33,  3.78s/it][A
 88%|████████▊ | 724/827 [51:00<05:53,  3.44s/it][A
 88%|████████▊ | 725/827 [51:05<06:47,  4.00s/it][A
 88%|████████▊ | 726/827 [51:10<07:27,  4.43s/it][A
 88%|████████▊ | 727/827 [51:14<07:16,  4.36s/it][A
 88%|████████▊ | 728/827 [51:18<06:36,  4.00s/it][A
 88%|████████▊ | 729/827 [51:21<05:58,  3.66s/it][A
 88%|████████▊ | 730/827 [51:25<06:08,  3.80s/it][A
 88%|████████▊ | 731/827 [51:30<06:47,  4.25s/it][A
 89%|████████▊ | 732/827 [51:34<06:49,  4.31s/it][A
 89%|████████▊ | 733/827 [51:39<06:47,  4.33s/it][A
 89%|████████▉ | 734/827 [51:41<05:58,  3.85s/it][A
 89%|████████▉ | 735/827 [51:46<06:02,  3.94s/it][A
 89%|████████▉ | 736/827 [51:51<06:49,  4.50s/it][A
 89%|████████▉ | 737/827 [51:56<06:59,  4.66s/it][A
 89%|████████▉ | 738/827 [52:00<06:18,  4.25s/it][A
 89%|████████▉ | 739/827 [52:05<06:39,  4.54s/it][A
 89%|████████▉ | 740/827 [52:11<07:10,  4.95s/it][A
 90%|████████▉ | 741/827 [52:15<06:41,  4.67s/it][A
 90%|████████▉ | 742/827 [52:19<06:28,  4.58s/it][A
 90%|████████▉ | 743/827 [52:25<06:54,  4.93s/it][A
 90%|████████▉ | 744/827 [52:29<06:26,  4.66s/it][A
 90%|█████████ | 745/827 [52:33<06:06,  4.46s/it][A
 90%|█████████ | 746/827 [52:39<06:37,  4.91s/it][A
 90%|█████████ | 747/827 [52:44<06:39,  4.99s/it][A
 90%|█████████ | 748/827 [52:48<06:11,  4.70s/it][A
 91%|█████████ | 749/827 [52:53<06:04,  4.68s/it][A
 91%|█████████ | 750/827 [52:57<05:53,  4.59s/it][A
 91%|█████████ | 751/827 [53:01<05:19,  4.21s/it][A
 91%|█████████ | 752/827 [53:04<04:50,  3.87s/it][A
 91%|█████████ | 753/827 [53:07<04:25,  3.59s/it][A
 91%|█████████ | 754/827 [53:10<04:11,  3.44s/it][A
 91%|█████████▏| 755/827 [53:14<04:35,  3.83s/it][A
 91%|█████████▏| 756/827 [53:19<04:58,  4.20s/it][A
 92%|█████████▏| 757/827 [53:24<04:56,  4.23s/it][A
 92%|█████████▏| 758/827 [53:28<04:54,  4.26s/it][A
 92%|█████████▏| 759/827 [53:31<04:25,  3.90s/it][A
 92%|█████████▏| 760/827 [53:37<05:05,  4.56s/it][A
 92%|█████████▏| 761/827 [53:44<05:47,  5.26s/it][A
 92%|█████████▏| 762/827 [53:51<06:05,  5.63s/it][A
 92%|█████████▏| 763/827 [53:57<06:09,  5.77s/it][A
 92%|█████████▏| 764/827 [54:00<05:22,  5.13s/it][A
 93%|█████████▎| 765/827 [54:03<04:37,  4.48s/it][A
 93%|█████████▎| 766/827 [54:06<04:06,  4.05s/it][A
 93%|█████████▎| 767/827 [54:10<03:55,  3.93s/it][A
 93%|█████████▎| 768/827 [54:14<03:54,  3.98s/it][A
 93%|█████████▎| 769/827 [54:19<03:59,  4.13s/it][A
 93%|█████████▎| 770/827 [54:22<03:43,  3.93s/it][A
 93%|█████████▎| 771/827 [54:26<03:39,  3.91s/it][A
 93%|█████████▎| 772/827 [54:30<03:34,  3.89s/it][A
 93%|█████████▎| 773/827 [54:34<03:37,  4.03s/it][A
 94%|█████████▎| 774/827 [54:37<03:23,  3.83s/it][A
 94%|█████████▎| 775/827 [54:42<03:23,  3.91s/it][A
 94%|█████████▍| 776/827 [54:45<03:14,  3.82s/it][A
 94%|█████████▍| 777/827 [54:48<03:02,  3.65s/it][A
 94%|█████████▍| 778/827 [54:52<02:59,  3.67s/it][A
 94%|█████████▍| 779/827 [54:55<02:48,  3.52s/it][A
 94%|█████████▍| 780/827 [54:59<02:40,  3.42s/it][A
 94%|█████████▍| 781/827 [55:02<02:33,  3.33s/it][A
 95%|█████████▍| 782/827 [55:05<02:27,  3.28s/it][A
 95%|█████████▍| 783/827 [55:08<02:26,  3.34s/it][A
 95%|█████████▍| 784/827 [55:12<02:25,  3.39s/it][A
 95%|█████████▍| 785/827 [55:16<02:27,  3.52s/it][A
 95%|█████████▌| 786/827 [55:20<02:37,  3.85s/it][A
 95%|█████████▌| 787/827 [55:26<02:52,  4.31s/it][A
 95%|█████████▌| 788/827 [55:30<02:45,  4.25s/it][A
 95%|█████████▌| 789/827 [55:35<02:47,  4.41s/it][A
 96%|█████████▌| 790/827 [55:38<02:34,  4.18s/it][A
 96%|█████████▌| 791/827 [55:41<02:18,  3.85s/it][A
 96%|█████████▌| 792/827 [55:45<02:09,  3.70s/it][A
 96%|█████████▌| 793/827 [55:50<02:20,  4.12s/it][A
 96%|█████████▌| 794/827 [55:55<02:27,  4.48s/it][A
 96%|█████████▌| 795/827 [56:01<02:41,  5.06s/it][A
 96%|█████████▋| 796/827 [56:07<02:37,  5.08s/it][A
 96%|█████████▋| 797/827 [56:10<02:15,  4.52s/it][A
 96%|█████████▋| 798/827 [56:13<01:56,  4.02s/it][A
 97%|█████████▋| 799/827 [56:15<01:40,  3.59s/it][A
 97%|█████████▋| 800/827 [56:22<02:06,  4.67s/it][A
 97%|█████████▋| 801/827 [56:30<02:20,  5.41s/it][A
 97%|█████████▋| 802/827 [56:34<02:09,  5.18s/it][A
 97%|█████████▋| 803/827 [56:40<02:09,  5.38s/it][A
 97%|█████████▋| 804/827 [56:45<02:01,  5.28s/it][A
 97%|█████████▋| 805/827 [56:48<01:42,  4.67s/it][A
 97%|█████████▋| 806/827 [56:51<01:27,  4.15s/it][A
 98%|█████████▊| 807/827 [56:55<01:18,  3.94s/it][A
 98%|█████████▊| 808/827 [56:58<01:13,  3.89s/it][A
 98%|█████████▊| 809/827 [57:02<01:06,  3.70s/it][A
 98%|█████████▊| 810/827 [57:05<01:02,  3.70s/it][A
 98%|█████████▊| 811/827 [57:09<00:58,  3.67s/it][A
 98%|█████████▊| 812/827 [57:13<00:58,  3.90s/it][A
 98%|█████████▊| 813/827 [57:19<01:00,  4.33s/it][A
 98%|█████████▊| 814/827 [57:24<01:01,  4.71s/it][A
 99%|█████████▊| 815/827 [57:29<00:54,  4.53s/it][A
 99%|█████████▊| 816/827 [57:32<00:47,  4.35s/it][A
 99%|█████████▉| 817/827 [57:36<00:40,  4.03s/it][A
 99%|█████████▉| 818/827 [57:41<00:40,  4.51s/it][A
 99%|█████████▉| 819/827 [57:46<00:37,  4.67s/it][A
 99%|█████████▉| 820/827 [57:51<00:32,  4.68s/it][A
 99%|█████████▉| 821/827 [57:55<00:26,  4.48s/it][A
 99%|█████████▉| 822/827 [57:59<00:21,  4.29s/it][A
100%|█████████▉| 823/827 [58:02<00:15,  3.83s/it][A
100%|█████████▉| 824/827 [58:05<00:10,  3.64s/it][A
100%|█████████▉| 825/827 [58:10<00:08,  4.17s/it][A
100%|█████████▉| 826/827 [58:16<00:04,  4.66s/it][A
100%|██████████| 827/827 [58:19<00:00,  3.99s/it][A                                                      
                                                 [A{'eval_loss': 0.6948117017745972, 'eval_runtime': 3505.0647, 'eval_samples_per_second': 0.943, 'eval_steps_per_second': 0.236, 'epoch': 1.29}
 65%|██████▍   | 600/930 [19:50:43<8:41:34, 94.83s/it]
100%|██████████| 827/827 [58:19<00:00,  3.99s/it][A
                                                 [A[INFO|trainer.py:4309] 2025-12-09 07:00:20,190 >> Saving model checkpoint to saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-600
/opt/conda/lib/python3.11/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in Qwen/Qwen2.5-VL-7B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2421] 2025-12-09 07:00:20,290 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-600/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-09 07:00:20,293 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-09 07:00:20,295 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-600/special_tokens_map.json
[2025-12-09 07:00:21,093] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step600 is about to be saved!
[2025-12-09 07:00:21,145] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-600/global_step600/mp_rank_00_model_states.pt
[2025-12-09 07:00:21,145] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-600/global_step600/mp_rank_00_model_states.pt...
[2025-12-09 07:00:21,207] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-600/global_step600/mp_rank_00_model_states.pt.
[2025-12-09 07:00:21,211] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-12-09 07:00:21,211] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-600/global_step600/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2025-12-09 07:00:21,211] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-600/global_step600/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2025-12-09 07:00:21,211] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-600/global_step600/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2025-12-09 07:00:21,291] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-600/global_step600/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2025-12-09 07:00:21,291] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-600/global_step600/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2025-12-09 07:00:21,291] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step600 is ready now!
[2025-12-09 07:00:21,293] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-12-09 07:00:21,297] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-600/global_step600/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2025-12-09 07:00:21,297] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-600/global_step600/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2025-12-09 07:00:21,297] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step600 is ready now!
[2025-12-09 07:00:21,297] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-12-09 07:00:21,297] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step600 is ready now!
[2025-12-09 07:00:21,313] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-600/global_step600/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2025-12-09 07:00:21,313] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-600/global_step600/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2025-12-09 07:00:21,313] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step600 is ready now!
[INFO|image_processing_base.py:253] 2025-12-09 07:00:21,326 >> Image processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-600/preprocessor_config.json
[INFO|tokenization_utils_base.py:2421] 2025-12-09 07:00:21,329 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-600/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-09 07:00:21,331 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-09 07:00:21,333 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-600/special_tokens_map.json
[INFO|video_processing_utils.py:600] 2025-12-09 07:00:21,473 >> Video processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-600/video_preprocessor_config.json
[INFO|processing_utils.py:814] 2025-12-09 07:00:21,475 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_traineval/checkpoint-600/chat_template.jinja
 65%|██████▍   | 601/930 [19:52:33<105:12:17, 1151.18s/it] 65%|██████▍   | 602/930 [19:54:14<76:10:04, 835.99s/it]   65%|██████▍   | 603/930 [19:55:38<55:26:06, 610.29s/it] 65%|██████▍   | 604/930 [19:57:08<41:09:07, 454.44s/it] 65%|██████▌   | 605/930 [19:58:58<31:41:12, 350.99s/it] 65%|██████▌   | 606/930 [20:00:46<25:00:59, 277.96s/it] 65%|██████▌   | 607/930 [20:02:23<20:05:21, 223.91s/it] 65%|██████▌   | 608/930 [20:04:01<16:38:23, 186.04s/it] 65%|██████▌   | 609/930 [20:05:42<14:18:51, 160.54s/it] 66%|██████▌   | 610/930 [20:07:16<12:29:18, 140.49s/it]                                                        {'loss': 0.7134, 'grad_norm': 0.1968998908996582, 'learning_rate': 3.210788609070504e-05, 'epoch': 1.31}
 66%|██████▌   | 610/930 [20:07:16<12:29:18, 140.49s/it] 66%|██████▌   | 611/930 [20:09:01<11:30:37, 129.90s/it] 66%|██████▌   | 612/930 [20:10:39<10:37:49, 120.34s/it] 66%|██████▌   | 613/930 [20:12:12<9:53:10, 112.27s/it]  66%|██████▌   | 614/930 [20:13:51<9:29:41, 108.17s/it] 66%|██████▌   | 615/930 [20:15:52<9:48:34, 112.11s/it] 66%|██████▌   | 616/930 [20:17:29<9:23:02, 107.59s/it] 66%|██████▋   | 617/930 [20:19:04<9:00:55, 103.69s/it] 66%|██████▋   | 618/930 [20:20:35<8:39:08, 99.84s/it]  67%|██████▋   | 619/930 [20:22:28<8:57:50, 103.76s/it] 67%|██████▋   | 620/930 [20:24:01<8:40:16, 100.70s/it]                                                       {'loss': 0.6865, 'grad_norm': 0.1634773164987564, 'learning_rate': 3.036847223658958e-05, 'epoch': 1.33}
 67%|██████▋   | 620/930 [20:24:01<8:40:16, 100.70s/it] 67%|██████▋   | 621/930 [20:25:43<8:40:14, 101.02s/it] 67%|██████▋   | 622/930 [20:27:17<8:27:15, 98.82s/it]  67%|██████▋   | 623/930 [20:28:43<8:06:26, 95.07s/it] 67%|██████▋   | 624/930 [20:30:23<8:12:05, 96.49s/it] 67%|██████▋   | 625/930 [20:31:51<7:57:56, 94.02s/it] 67%|██████▋   | 626/930 [20:33:27<7:58:48, 94.50s/it] 67%|██████▋   | 627/930 [20:35:08<8:06:43, 96.38s/it] 68%|██████▊   | 628/930 [20:36:49<8:12:42, 97.89s/it] 68%|██████▊   | 629/930 [20:38:27<8:11:17, 97.93s/it] 68%|██████▊   | 630/930 [20:39:44<7:38:20, 91.67s/it]                                                      {'loss': 0.703, 'grad_norm': 0.2066863477230072, 'learning_rate': 2.8656711999323176e-05, 'epoch': 1.36}
 68%|██████▊   | 630/930 [20:39:44<7:38:20, 91.67s/it] 68%|██████▊   | 631/930 [20:41:31<7:59:57, 96.31s/it] 68%|██████▊   | 632/930 [20:43:04<7:52:53, 95.21s/it] 68%|██████▊   | 633/930 [20:44:45<8:00:02, 96.98s/it] 68%|██████▊   | 634/930 [20:46:15<7:47:29, 94.76s/it] 68%|██████▊   | 635/930 [20:47:45<7:39:45, 93.51s/it] 68%|██████▊   | 636/930 [20:49:27<7:50:59, 96.12s/it] 68%|██████▊   | 637/930 [20:51:18<8:10:14, 100.39s/it] 69%|██████▊   | 638/930 [20:52:55<8:04:12, 99.49s/it]  69%|██████▊   | 639/930 [20:54:54<8:31:18, 105.42s/it] 69%|██████▉   | 640/930 [20:56:44<8:34:59, 106.55s/it]                                                       {'loss': 0.6793, 'grad_norm': 0.19228163361549377, 'learning_rate': 2.6975016620777073e-05, 'epoch': 1.38}
 69%|██████▉   | 640/930 [20:56:44<8:34:59, 106.55s/it] 69%|██████▉   | 641/930 [20:58:15<8:12:02, 102.15s/it] 69%|██████▉   | 642/930 [20:59:56<8:08:24, 101.75s/it] 69%|██████▉   | 643/930 [21:01:37<8:05:02, 101.40s/it] 69%|██████▉   | 644/930 [21:03:23<8:09:43, 102.74s/it] 69%|██████▉   | 645/930 [21:05:10<8:14:34, 104.12s/it] 69%|██████▉   | 646/930 [21:06:56<8:15:53, 104.77s/it] 70%|██████▉   | 647/930 [21:08:34<8:04:08, 102.64s/it] 70%|██████▉   | 648/930 [21:10:08<7:50:48, 100.17s/it] 70%|██████▉   | 649/930 [21:11:55<7:57:44, 102.01s/it] 70%|██████▉   | 650/930 [21:14:00<8:28:37, 108.99s/it]                                                       {'loss': 0.7095, 'grad_norm': 0.23095642030239105, 'learning_rate': 2.5325754992471884e-05, 'epoch': 1.4}
 70%|██████▉   | 650/930 [21:14:00<8:28:37, 108.99s/it] 70%|███████   | 651/930 [21:15:42<8:16:54, 106.86s/it] 70%|███████   | 652/930 [21:17:36<8:24:36, 108.91s/it] 70%|███████   | 653/930 [21:19:16<8:11:22, 106.44s/it] 70%|███████   | 654/930 [21:21:08<8:17:20, 108.12s/it] 70%|███████   | 655/930 [21:22:48<8:04:40, 105.75s/it] 71%|███████   | 656/930 [21:24:28<7:54:37, 103.93s/it] 71%|███████   | 657/930 [21:26:05<7:43:24, 101.85s/it] 71%|███████   | 658/930 [21:27:42<7:35:32, 100.49s/it] 71%|███████   | 659/930 [21:29:39<7:55:34, 105.29s/it] 71%|███████   | 660/930 [21:31:13<7:38:12, 101.82s/it]                                                       {'loss': 0.6956, 'grad_norm': 0.21985039114952087, 'learning_rate': 2.371125031867891e-05, 'epoch': 1.42}
 71%|███████   | 660/930 [21:31:13<7:38:12, 101.82s/it] 71%|███████   | 661/930 [21:32:46<7:24:35, 99.17s/it]  71%|███████   | 662/930 [21:34:40<7:43:20, 103.73s/it] 71%|███████▏  | 663/930 [21:36:32<7:52:00, 106.07s/it] 71%|███████▏  | 664/930 [21:38:12<7:42:26, 104.31s/it] 72%|███████▏  | 665/930 [21:39:49<7:31:14, 102.17s/it] 72%|███████▏  | 666/930 [21:41:49<7:53:23, 107.59s/it] 72%|███████▏  | 667/930 [21:43:23<7:33:48, 103.53s/it] 72%|███████▏  | 668/930 [21:45:09<7:34:30, 104.09s/it] 72%|███████▏  | 669/930 [21:46:43<7:20:19, 101.23s/it] 72%|███████▏  | 670/930 [21:48:28<7:22:43, 102.17s/it]                                                       {'loss': 0.7057, 'grad_norm': 0.19183480739593506, 'learning_rate': 2.2133776843878186e-05, 'epoch': 1.44}
 72%|███████▏  | 670/930 [21:48:28<7:22:43, 102.17s/it] 72%|███████▏  | 671/930 [21:50:04<7:13:06, 100.34s/it] 72%|███████▏  | 672/930 [21:51:28<6:50:26, 95.45s/it]  72%|███████▏  | 673/930 [21:53:10<6:57:57, 97.58s/it] 72%|███████▏  | 674/930 [21:54:39<6:45:12, 94.97s/it] 73%|███████▎  | 675/930 [21:56:22<6:53:08, 97.21s/it] 73%|███████▎  | 676/930 [21:58:02<6:55:23, 98.12s/it] 73%|███████▎  | 677/930 [21:59:29<6:39:52, 94.83s/it] 73%|███████▎  | 678/930 [22:01:12<6:48:02, 97.15s/it] 73%|███████▎  | 679/930 [22:02:37<6:32:20, 93.79s/it] 73%|███████▎  | 680/930 [22:04:20<6:41:26, 96.35s/it]                                                      {'loss': 0.6856, 'grad_norm': 0.24016854166984558, 'learning_rate': 2.059555664918268e-05, 'epoch': 1.46}
 73%|███████▎  | 680/930 [22:04:20<6:41:26, 96.35s/it] 73%|███████▎  | 681/930 [22:05:42<6:22:10, 92.09s/it] 73%|███████▎  | 682/930 [22:07:40<6:52:51, 99.88s/it] 73%|███████▎  | 683/930 [22:09:15<6:44:45, 98.32s/it] 74%|███████▎  | 684/930 [22:10:43<6:30:59, 95.36s/it] 74%|███████▎  | 685/930 [22:12:24<6:35:32, 96.87s/it] 74%|███████▍  | 686/930 [22:13:54<6:26:12, 94.97s/it] 74%|███████▍  | 687/930 [22:15:34<6:30:39, 96.46s/it] 74%|███████▍  | 688/930 [22:17:15<6:34:14, 97.75s/it] 74%|███████▍  | 689/930 [22:18:51<6:30:15, 97.16s/it] 74%|███████▍  | 690/930 [22:20:41<6:44:57, 101.24s/it]                                                       {'loss': 0.6618, 'grad_norm': 0.22033581137657166, 'learning_rate': 1.9098756522241634e-05, 'epoch': 1.48}
 74%|███████▍  | 690/930 [22:20:41<6:44:57, 101.24s/it] 74%|███████▍  | 691/930 [22:22:37<7:00:48, 105.64s/it] 74%|███████▍  | 692/930 [22:24:25<7:01:46, 106.33s/it] 75%|███████▍  | 693/930 [22:26:23<7:13:42, 109.80s/it] 75%|███████▍  | 694/930 [22:28:22<7:23:15, 112.69s/it] 75%|███████▍  | 695/930 [22:29:59<7:02:44, 107.93s/it] 75%|███████▍  | 696/930 [22:31:32<6:42:31, 103.21s/it] 75%|███████▍  | 697/930 [22:33:08<6:33:08, 101.24s/it] 75%|███████▌  | 698/930 [22:34:37<6:16:59, 97.50s/it]  75%|███████▌  | 699/930 [22:36:20<6:21:23, 99.06s/it] 75%|███████▌  | 700/930 [22:37:59<6:20:39, 99.30s/it]                                                      {'loss': 0.6958, 'grad_norm': 0.23123644292354584, 'learning_rate': 1.7645484905032128e-05, 'epoch': 1.51}
 75%|███████▌  | 700/930 [22:37:59<6:20:39, 99.30s/it] 75%|███████▌  | 701/930 [22:39:30<6:09:29, 96.81s/it] 75%|███████▌  | 702/930 [22:40:56<5:55:22, 93.52s/it] 76%|███████▌  | 703/930 [22:42:25<5:48:51, 92.21s/it] 76%|███████▌  | 704/930 [22:44:08<5:58:54, 95.28s/it] 76%|███████▌  | 705/930 [22:45:32<5:44:52, 91.97s/it] 76%|███████▌  | 706/930 [22:47:15<5:55:51, 95.32s/it] 76%|███████▌  | 707/930 [22:48:40<5:42:07, 92.05s/it] 76%|███████▌  | 708/930 [22:50:09<5:37:40, 91.27s/it] 76%|███████▌  | 709/930 [22:51:47<5:43:20, 93.21s/it] 76%|███████▋  | 710/930 [22:53:24<5:46:19, 94.45s/it]                                                      {'loss': 0.697, 'grad_norm': 0.2512299120426178, 'learning_rate': 1.6237788923838147e-05, 'epoch': 1.53}
 76%|███████▋  | 710/930 [22:53:24<5:46:19, 94.45s/it] 76%|███████▋  | 711/930 [22:55:04<5:50:03, 95.91s/it] 77%|███████▋  | 712/930 [22:56:51<6:00:56, 99.34s/it] 77%|███████▋  | 713/930 [22:58:56<6:27:37, 107.18s/it] 77%|███████▋  | 714/930 [23:00:33<6:14:42, 104.08s/it] 77%|███████▋  | 715/930 [23:02:13<6:08:48, 102.92s/it] 77%|███████▋  | 716/930 [23:04:06<6:17:36, 105.87s/it] 77%|███████▋  | 717/930 [23:06:01<6:25:19, 108.54s/it] 77%|███████▋  | 718/930 [23:07:34<6:07:23, 103.98s/it] 77%|███████▋  | 719/930 [23:09:08<5:54:59, 100.95s/it] 77%|███████▋  | 720/930 [23:11:05<6:10:11, 105.77s/it]                                                       {'loss': 0.6845, 'grad_norm': 0.20865070819854736, 'learning_rate': 1.4877651505601158e-05, 'epoch': 1.55}
 77%|███████▋  | 720/930 [23:11:05<6:10:11, 105.77s/it] 78%|███████▊  | 721/930 [23:12:39<5:55:43, 102.12s/it] 78%|███████▊  | 722/930 [23:14:27<6:00:46, 104.07s/it] 78%|███████▊  | 723/930 [23:16:02<5:49:39, 101.35s/it] 78%|███████▊  | 724/930 [23:17:32<5:35:20, 97.67s/it]  78%|███████▊  | 725/930 [23:19:07<5:31:30, 97.03s/it] 78%|███████▊  | 726/930 [23:20:31<5:16:08, 92.98s/it] 78%|███████▊  | 727/930 [23:22:08<5:18:41, 94.19s/it] 78%|███████▊  | 728/930 [23:23:41<5:16:33, 94.03s/it] 78%|███████▊  | 729/930 [23:25:11<5:10:45, 92.76s/it] 78%|███████▊  | 730/930 [23:27:07<5:32:50, 99.85s/it]                                                      {'loss': 0.6963, 'grad_norm': 0.2345481812953949, 'learning_rate': 1.3566988584703816e-05, 'epoch': 1.57}
 78%|███████▊  | 730/930 [23:27:07<5:32:50, 99.85s/it] 79%|███████▊  | 731/930 [23:28:51<5:34:23, 100.82s/it] 79%|███████▊  | 732/930 [23:30:37<5:37:52, 102.39s/it] 79%|███████▉  | 733/930 [23:32:35<5:51:43, 107.13s/it] 79%|███████▉  | 734/930 [23:34:08<5:36:32, 103.02s/it] 79%|███████▉  | 735/930 [23:35:31<5:14:51, 96.88s/it]  79%|███████▉  | 736/930 [23:37:00<5:05:57, 94.63s/it] 79%|███████▉  | 737/930 [23:38:39<5:08:28, 95.90s/it] 79%|███████▉  | 738/930 [23:40:12<5:04:26, 95.14s/it] 79%|███████▉  | 739/930 [23:42:00<5:14:42, 98.86s/it] 80%|███████▉  | 740/930 [23:43:57<5:30:29, 104.36s/it]                                                       {'loss': 0.7046, 'grad_norm': 0.2463536262512207, 'learning_rate': 1.2307646404121692e-05, 'epoch': 1.59}
 80%|███████▉  | 740/930 [23:43:57<5:30:29, 104.36s/it] 80%|███████▉  | 741/930 [23:45:21<5:09:41, 98.32s/it]  80%|███████▉  | 742/930 [23:47:04<5:12:28, 99.73s/it] 80%|███████▉  | 743/930 [23:49:02<5:28:01, 105.25s/it] 80%|████████  | 744/930 [23:50:55<5:33:20, 107.53s/it] 80%|████████  | 745/930 [23:52:21<5:11:20, 100.98s/it] 80%|████████  | 746/930 [23:54:09<5:16:01, 103.05s/it] 80%|████████  | 747/930 [23:55:51<5:13:13, 102.70s/it]slurmstepd: error: *** JOB 1565157 ON kn173 CANCELLED AT 2025-12-09T11:07:08 DUE TO TIME LIMIT ***
