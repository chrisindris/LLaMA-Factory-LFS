
==========
== CUDA ==
==========

CUDA Version 12.4.1

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

[INFO|2025-12-19 06:36:31] llamafactory.launcher:143 >> Initializing 4 distributed tasks at: 127.0.0.1:50645
W1219 06:36:32.470000 2667010 site-packages/torch/distributed/run.py:792] 
W1219 06:36:32.470000 2667010 site-packages/torch/distributed/run.py:792] *****************************************
W1219 06:36:32.470000 2667010 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1219 06:36:32.470000 2667010 site-packages/torch/distributed/run.py:792] *****************************************
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
[2025-12-19 06:36:39,212] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-19 06:36:39,564] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-19 06:36:39,650] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-12-19 06:36:39,697] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2025-12-19 06:36:43,938] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-19 06:36:43,938] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-19 06:36:43,938] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-19 06:36:43,938] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-12-19 06:36:43,939] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[INFO|2025-12-19 06:36:45] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.
[INFO|2025-12-19 06:36:45] llamafactory.hparams.parser:423 >> Process rank: 0, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|hub.py:421] 2025-12-19 06:36:45,649 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2025-12-19 06:36:45,663 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2025-12-19 06:36:45,673 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-12-19 06:36:45,673 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-12-19 06:36:45,673 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-12-19 06:36:45,673 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-19 06:36:45,673 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-19 06:36:45,674 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-12-19 06:36:45,674 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-12-19 06:36:45,880 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|hub.py:421] 2025-12-19 06:36:45,880 >> Offline mode: forcing local_files_only=True
[INFO|hub.py:421] 2025-12-19 06:36:45,881 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:316] 2025-12-19 06:36:45,884 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2025-12-19 06:36:45,886 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:316] 2025-12-19 06:36:45,892 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2025-12-19 06:36:45,893 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:428] 2025-12-19 06:36:45,908 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:1949] 2025-12-19 06:36:45,908 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2025-12-19 06:36:45,913 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-12-19 06:36:45,913 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-12-19 06:36:45,913 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-12-19 06:36:45,913 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-19 06:36:45,913 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-19 06:36:45,913 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-12-19 06:36:45,913 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-12-19 06:36:46,075 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:660] 2025-12-19 06:36:46,078 >> Offline mode: forcing local_files_only=True
[INFO|video_processing_utils.py:726] 2025-12-19 06:36:46,081 >> loading configuration file video_preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|video_processing_utils.py:770] 2025-12-19 06:36:46,086 >> Video processor Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:928] 2025-12-19 06:36:46,086 >> Offline mode: forcing local_files_only=True
[INFO|processing_utils.py:1116] 2025-12-19 06:36:46,093 >> loading configuration file processor_config.json from cache at None
[INFO|processing_utils.py:1199] 2025-12-19 06:36:46,377 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2_5_VLProcessor"
}

[INFO|2025-12-19 06:36:46] llamafactory.hparams.parser:423 >> Process rank: 2, world size: 4, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-12-19 06:36:46] llamafactory.hparams.parser:423 >> Process rank: 3, world size: 4, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-12-19 06:36:46] llamafactory.hparams.parser:423 >> Process rank: 1, world size: 4, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-12-19 06:36:46] llamafactory.data.loader:143 >> Loading dataset /scratch/indrisch/huggingface/hub/datasets--cvis-tmu--llamafactory-sqa3d-traces-multiimage-vqa_R.12_C.12_F.12_X.62/snapshots/7df1906772c39a22c69a2aad435670fc2c38fea7/data/...
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
[rank0]:[W1219 06:36:47.337768081 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W1219 06:36:47.502887619 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W1219 06:36:47.537012223 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W1219 06:36:47.629053279 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
Running tokenizer on dataset (num_proc=32):   0%|          | 0/33047 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=32):   3%|▎         | 1000/33047 [16:26<8:47:10,  1.01 examples/s]Running tokenizer on dataset (num_proc=32):   6%|▌         | 2000/33047 [16:33<3:32:11,  2.44 examples/s]Running tokenizer on dataset (num_proc=32):   9%|▉         | 3000/33047 [16:35<1:52:08,  4.47 examples/s]Running tokenizer on dataset (num_proc=32):   9%|▉         | 3000/33047 [16:46<1:52:08,  4.47 examples/s]Running tokenizer on dataset (num_proc=32):   9%|▉         | 3032/33047 [16:55<1:56:13,  4.30 examples/s]Running tokenizer on dataset (num_proc=32):  12%|█▏        | 4032/33047 [16:56<58:51,  8.22 examples/s]  Running tokenizer on dataset (num_proc=32):  15%|█▌        | 5032/33047 [16:59<34:24, 13.57 examples/s]Running tokenizer on dataset (num_proc=32):  15%|█▌        | 5097/33047 [17:16<34:19, 13.57 examples/s]Running tokenizer on dataset (num_proc=32):  18%|█▊        | 6097/33047 [17:17<23:18, 19.27 examples/s]Running tokenizer on dataset (num_proc=32):  19%|█▊        | 6130/33047 [17:28<25:29, 17.60 examples/s]Running tokenizer on dataset (num_proc=32):  22%|██▏       | 7130/33047 [17:29<14:21, 30.08 examples/s]Running tokenizer on dataset (num_proc=32):  25%|██▍       | 8162/33047 [17:40<10:16, 40.37 examples/s]Running tokenizer on dataset (num_proc=32):  28%|██▊       | 9162/33047 [17:47<07:21, 54.07 examples/s]Running tokenizer on dataset (num_proc=32):  31%|███       | 10162/33047 [17:48<04:56, 77.30 examples/s]Running tokenizer on dataset (num_proc=32):  34%|███▍      | 11226/33047 [17:59<04:20, 83.86 examples/s]Running tokenizer on dataset (num_proc=32):  37%|███▋      | 12226/33047 [18:03<03:18, 105.11 examples/s]Running tokenizer on dataset (num_proc=32):  40%|████      | 13226/33047 [18:08<02:38, 125.09 examples/s]Running tokenizer on dataset (num_proc=32):  43%|████▎     | 14259/33047 [18:15<02:24, 129.99 examples/s]Running tokenizer on dataset (num_proc=32):  46%|████▋     | 15292/33047 [18:20<02:01, 146.32 examples/s]Running tokenizer on dataset (num_proc=32):  46%|████▋     | 15357/33047 [18:32<03:23, 87.07 examples/s] Running tokenizer on dataset (num_proc=32):  49%|████▉     | 16357/33047 [18:33<02:07, 130.87 examples/s]Running tokenizer on dataset (num_proc=32):  50%|████▉     | 16423/33047 [18:35<02:21, 117.13 examples/s]Running tokenizer on dataset (num_proc=32):  53%|█████▎    | 17423/33047 [18:37<01:28, 176.30 examples/s]Running tokenizer on dataset (num_proc=32):  56%|█████▌    | 18423/33047 [18:37<00:54, 270.14 examples/s]Running tokenizer on dataset (num_proc=32):  59%|█████▉    | 19456/33047 [18:44<01:04, 212.18 examples/s]Running tokenizer on dataset (num_proc=32):  62%|██████▏   | 20489/33047 [18:52<01:10, 178.91 examples/s]Running tokenizer on dataset (num_proc=32):  65%|██████▌   | 21489/33047 [18:55<00:57, 202.38 examples/s]Running tokenizer on dataset (num_proc=32):  68%|██████▊   | 22489/33047 [18:59<00:49, 211.94 examples/s]Running tokenizer on dataset (num_proc=32):  68%|██████▊   | 22555/33047 [19:08<01:27, 120.49 examples/s]Running tokenizer on dataset (num_proc=32):  68%|██████▊   | 22588/33047 [19:08<01:26, 121.19 examples/s]Running tokenizer on dataset (num_proc=32):  71%|███████▏  | 23588/33047 [19:09<00:47, 199.41 examples/s]Running tokenizer on dataset (num_proc=32):  75%|███████▍  | 24621/33047 [19:13<00:37, 223.78 examples/s]Running tokenizer on dataset (num_proc=32):  78%|███████▊  | 25621/33047 [19:15<00:25, 289.50 examples/s]Running tokenizer on dataset (num_proc=32):  81%|████████  | 26621/33047 [19:17<00:19, 321.55 examples/s]Running tokenizer on dataset (num_proc=32):  81%|████████  | 26687/33047 [19:30<00:57, 110.68 examples/s]Running tokenizer on dataset (num_proc=32):  84%|████████▍ | 27687/33047 [19:30<00:29, 183.34 examples/s]Running tokenizer on dataset (num_proc=32):  87%|████████▋ | 28687/33047 [19:32<00:17, 253.94 examples/s]Running tokenizer on dataset (num_proc=32):  90%|████████▉ | 29687/33047 [19:33<00:10, 329.81 examples/s]Running tokenizer on dataset (num_proc=32):  93%|█████████▎| 30720/33047 [19:34<00:05, 444.21 examples/s]Running tokenizer on dataset (num_proc=32):  93%|█████████▎| 30785/33047 [19:46<00:05, 444.21 examples/s]Running tokenizer on dataset (num_proc=32):  96%|█████████▌| 31785/33047 [19:46<00:06, 185.65 examples/s]Running tokenizer on dataset (num_proc=32):  96%|█████████▋| 31818/33047 [19:52<00:09, 128.75 examples/s]Running tokenizer on dataset (num_proc=32):  99%|█████████▉| 32851/33047 [19:56<00:01, 153.50 examples/s]Running tokenizer on dataset (num_proc=32): 100%|█████████▉| 32948/33047 [20:06<00:01, 87.98 examples/s] Running tokenizer on dataset (num_proc=32): 100%|█████████▉| 32981/33047 [20:07<00:00, 86.59 examples/s]Running tokenizer on dataset (num_proc=32): 100%|██████████| 33047/33047 [20:24<00:00, 36.39 examples/s]Running tokenizer on dataset (num_proc=32): 100%|██████████| 33047/33047 [20:25<00:00, 26.97 examples/s]
training example:
input_ids:
[151644, 8948, 271, 262, 1446, 525, 264, 16585, 11129, 4142, 11528, 17847, 369, 27979, 647, 47522, 389, 264, 3175, 29519, 6109, 6839, 3941, 5248, 5335, 382, 262, 26149, 510, 262, 68252, 1283, 419, 1943, 11, 498, 686, 5258, 451, 6109, 5335, 438, 366, 1805, 29, 9492, 13, 18877, 1105, 438, 2155, 6194, 315, 279, 83490, 6109, 13, 28634, 311, 5335, 553, 220, 16, 5980, 1922, 304, 1973, 315, 11094, 25, 508, 16, 1125, 508, 17, 1125, 4593, 11, 508, 45, 29562, 262, 3596, 35304, 1718, 8575, 38439, 3298, 14017, 510, 262, 7288, 7854, 264, 12966, 220, 18, 35, 10502, 1614, 3941, 6194, 25, 3754, 6171, 3941, 5335, 11, 23583, 8674, 6249, 17040, 11, 323, 990, 5452, 14, 509, 8957, 14688, 21060, 369, 7990, 55916, 624, 262, 7288, 72077, 409, 849, 292, 3793, 26087, 983, 847, 1290, 97089, 17996, 4065, 14, 29898, 484, 32511, 56111, 7612, 1825, 35978, 28455, 448, 5091, 311, 279, 64171, 14, 8092, 13, 1416, 279, 58385, 374, 54761, 11, 1638, 311, 279, 1429, 38219, 8622, 1651, 323, 1977, 773, 624, 262, 7288, 5443, 1172, 9434, 5904, 26, 653, 537, 17023, 63133, 3565, 476, 17188, 389, 9250, 6540, 7797, 6770, 1633, 43898, 3793, 624, 262, 7288, 3197, 5248, 11178, 3000, 11, 8830, 553, 279, 1850, 2432, 311, 27979, 17133, 488, 4274, 1835, 3941, 6194, 13, 1416, 2058, 54761, 1283, 13295, 678, 5335, 11, 1584, 429, 9355, 382, 262, 30990, 320, 327, 32739, 1378, 10010, 11, 304, 419, 1973, 982, 262, 366, 26865, 397, 262, 14822, 14319, 29208, 32711, 304, 220, 18, 4142, 16, 15, 2805, 7354, 13, 356, 632, 5904, 448, 2168, 14937, 320, 68, 1302, 2572, 65750, 18, 5669, 3100, 7579, 18010, 1290, 315, 8718, 26, 508, 16, 5669, 1852, 1894, 11, 42396, 64212, 2823, 63594, 714, 4583, 320, 2640, 37294, 17, 20, 15, 11211, 7241, 5871, 568, 7036, 894, 17796, 8957, 14, 2969, 26745, 487, 323, 1246, 498, 19673, 432, 624, 262, 690, 26865, 397, 262, 366, 9217, 397, 262, 362, 3175, 63594, 4226, 1172, 11, 892, 1231, 387, 510, 262, 7288, 1416, 14016, 25, 3776, 37328, 4226, 11, 4504, 320, 68, 1302, 2572, 1036, 17, 32511, 476, 7414, 14, 2753, 320, 68, 1302, 2572, 1036, 9693, 854, 4292, 262, 7288, 1416, 537, 14016, 25, 362, 11652, 476, 1378, 624, 262, 1416, 4226, 537, 9434, 11, 421, 1052, 374, 902, 2797, 5904, 11, 476, 421, 279, 4226, 374, 35197, 54761, 25, 1036, 33260, 8253, 854, 624, 262, 690, 9217, 1339, 262, 62947, 609, 43797, 50, 510, 262, 7288, 3155, 4183, 13153, 279, 3405, 476, 1140, 5335, 26, 653, 4183, 2550, 4718, 476, 4960, 14158, 624, 262, 7288, 84468, 4285, 1894, 5036, 448, 518, 1429, 825, 22739, 26087, 4145, 3446, 838, 4322, 1574, 58689, 7256, 854, 4292, 262, 7288, 2823, 12966, 3941, 6194, 26, 421, 6194, 28295, 11, 62552, 279, 11299, 15432, 5904, 323, 5185, 432, 304, 366, 26865, 29816, 257, 151645, 198, 151644, 872, 198, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 30, 220, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151653, 151645, 198, 151644, 77091, 198, 198, 32462, 432, 11, 1077, 594, 1430, 311, 7071, 700, 1128, 594, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 62410, 13, 5512, 11, 1077, 594, 50087, 279, 14852, 6505, 13, 576, 21921, 374, 4363, 264, 4158, 11, 1378, 1737, 95112, 4982, 13, 2411, 220, 16, 16, 297, 62410, 320, 3481, 7950, 3082, 701, 582, 1184, 311, 1779, 429, 7702, 13, 21131, 518, 279, 5335, 11, 5310, 279, 825, 9027, 279, 21921, 3082, 11, 1052, 594, 264, 17846, 3403, 279, 19309, 11, 714, 3783, 11, 902, 2293, 11489, 11, 279, 3405, 374, 911, 279, 21921, 13, 13824, 11, 7196, 279, 330, 66, 75512, 1, 1588, 374, 279, 5819, 4982, 13, 13824, 11, 902, 11, 279, 19309, 702, 264, 21921, 3685, 30, 13824, 11, 902, 11, 279, 1887, 21921, 374, 279, 4158, 61607, 13, 13824, 11, 304, 279, 2168, 1380, 279, 6249, 4933, 279, 19309, 323, 21921, 11, 279, 1909, 315, 279, 21921, 2293, 11489, 11, 902, 11, 7196, 279, 17846, 30, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 2727, 330, 65, 77832, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 3263, 2055, 220, 16, 16, 297, 62410, 1035, 387, 279, 8416, 2115, 3082, 8674, 311, 279, 19309, 13, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 279, 19309, 3082, 13, 2160, 429, 949, 315, 279, 21921, 6505, 30, 13824, 11, 902, 11, 7196, 279, 21921, 374, 279, 4158, 61607, 1212, 279, 19309, 13, 13824, 11, 902, 11, 279, 1909, 315, 429, 21921, 2293, 11489, 11, 902, 11, 304, 1045, 5335, 11, 1052, 594, 264, 17846, 25911, 476, 21810, 13, 13824, 11, 902, 11, 1077, 594, 1779, 279, 1156, 2421, 5335, 13, 576, 17196, 45766, 374, 33866, 11, 1221, 279, 19309, 3082, 13, 13824, 11, 279, 4396, 4226, 25, 3330, 518, 279, 5335, 11, 279, 1633, 389, 1909, 315, 279, 14852, 21921, 320, 1782, 4158, 26482, 4982, 8, 518, 220, 16, 16, 297, 62410, 374, 264, 17846, 13, 13824, 11, 902, 11, 3783, 21677, 17846, 374, 389, 279, 7002, 3403, 279, 19309, 13, 13824, 11, 7196, 358, 5786, 878, 13, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 594, 3405, 25, 330, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 7521, 2055, 1156, 11, 10542, 279, 21921, 13, 576, 4158, 26482, 4982, 3143, 220, 16, 16, 297, 62410, 320, 3481, 7950, 568, 3555, 594, 389, 1909, 30, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 429, 3082, 13, 13824, 11, 9834, 0, 758, 279, 21844, 476, 21810, 11, 1052, 594, 264, 17846, 13, 2055, 279, 4226, 374, 264, 17846, 4192, 1249, 8253, 1128, 374, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 527, 20666, 11, 23643, 279, 27979, 6789, 323, 5424, 304, 279, 5335, 25, 18611, 16, 13, 95210, 279, 1036, 65, 77832, 21921, 854, 304, 3405, 25, 362, 4158, 11, 1378, 481, 26482, 5819, 4982, 374, 9434, 3143, 279, 1909, 481, 2115, 320, 16, 16, 297, 527, 20666, 8, 3082, 8674, 311, 1008, 37664, 1075, 279, 19309, 323, 17196, 13, 2303, 17, 13, 1374, 19991, 6171, 3403, 419, 21921, 25, 758, 279, 5335, 11, 264, 3070, 73225, 334, 374, 21810, 389, 279, 7002, 3403, 419, 21921, 3082, 13, 79226, 54216, 151645, 198]
inputs:
<|im_start|>system

    You are a careful vision–language assistant for spatial VQA on a single indoor scene shown across multiple images.

    INPUT:
    Immediately after this message, you will receive N scene images as <image> tags. Treat them as different views of the SAME scene. Refer to images by 1-based index in order of appearance: [1], [2], …, [N].

    REASONING PRINCIPLES:
    • Build a consistent 3D mental model across views: track objects across images, infer relative camera pose, and use scale/occlusion/shadows for depth cues.
    • Interpret deictic terms (“to my right/left/in front/behind”) EGOCENTRICALLY with respect to the narrator/agent. If the viewpoint is ambiguous, default to the most informative central view and say so.
    • Use only visible evidence; do not invent unseen details or rely on external knowledge beyond basic object/color terms.
    • When multiple candidates exist, resolve by the best match to spatial phrase + salience across views. If still ambiguous after checking all images, state that clearly.

    OUTPUT (exactly two blocks, in this order):
    <think>
    Step-by-step reasoning in 3–10 short steps. Cite evidence with image indices (e.g., “[3]: light wood desk right of monitor; [1]: same color, confirms”). Be concise but complete (aim ≤250 tokens unless necessary). Note any occlusion/ambiguity and how you resolved it.
    </think>
    <answer>
    A single concise answer only, which may be:
    • If sufficient: One-word answer, Count (e.g., “2”) or Yes/No (e.g., “yes”).
    • If not sufficient: A sentence or two.
    If answer not visible, if there is no clear evidence, or if the answer is genuinely ambiguous: “cannot determine”.
    </answer>

    STYLE & RULES:
    • Do NOT repeat the question or list images; do NOT output JSON or extra sections.
    • Prefer simple color names with at most one modifier (“light/dark/pale/beige”).
    • Be consistent across views; if views disagree, prioritize the clearest evidence and note it in <think>.
    <|im_end|>
<|im_start|>user
What is on top of the bathroom cabinet that is on my 11 o'clock? <|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|><|im_end|>
<|im_start|>assistant

Got it, let's try to figure out what's on top of the bathroom cabinet at 11 o'clock. First, let's visualize the bathroom setup. The cabinet is likely a white, two-drawer unit. At 11 o'clock (top-left area), we need to check that spot. Looking at the images, especially the one showing the cabinet area, there's a mirror above the sink, but wait, no—wait, the question is about the cabinet. Wait, maybe the "cabinet" here is the storage unit. Wait, no, the sink has a cabinet below? Wait, no, the main cabinet is the white drawers. Wait, in the image where the camera shows the sink and cabinet, the top of the cabinet—wait, no, maybe the mirror? Wait, no, let's check again. Wait, the user says "bathroom cabinet that is on my 11 o'clock". So 11 o'clock would be the upper left area relative to the sink. Looking at the images, there's a mirror mounted on the wall above the sink area. Is that part of the cabinet setup? Wait, no, maybe the cabinet is the white drawers under the sink. Wait, no, the top of that cabinet—wait, no, in some images, there's a mirror reflected or mounted. Wait, no, let's check the first few images. The shower curtain is colorful, then the sink area. Wait, the correct answer: looking at the images, the object on top of the bathroom cabinet (the white drawer unit) at 11 o'clock is a mirror. Wait, no, wait—the mirror is on the wall above the sink. Wait, maybe I misread. Wait, no, let's check again. Wait, the user's question: "What is on top of the bathroom cabinet that is on my 11 o'clock?" So first, identify the cabinet. The white drawer unit near 11 o'clock (top-left). What's on top? Looking at the images, there's a mirror mounted on the wall above that area. Wait, yes! In the reflection or mounted, there's a mirror. So the answer is a mirror.


To determine what is on top of the bathroom cabinet at 11 o’clock, analyze the spatial layout and elements in the images:  

1. Locate the “bathroom cabinet” in question: A white, two - drawer storage unit is visible near the top - left (11 o’clock) area relative to other fixtures like the sink and shower.  
2. Examine objects above this cabinet: In the images, a **mirror** is mounted on the wall above this cabinet area.  


Mirror<|im_end|>

label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 198, 32462, 432, 11, 1077, 594, 1430, 311, 7071, 700, 1128, 594, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 62410, 13, 5512, 11, 1077, 594, 50087, 279, 14852, 6505, 13, 576, 21921, 374, 4363, 264, 4158, 11, 1378, 1737, 95112, 4982, 13, 2411, 220, 16, 16, 297, 62410, 320, 3481, 7950, 3082, 701, 582, 1184, 311, 1779, 429, 7702, 13, 21131, 518, 279, 5335, 11, 5310, 279, 825, 9027, 279, 21921, 3082, 11, 1052, 594, 264, 17846, 3403, 279, 19309, 11, 714, 3783, 11, 902, 2293, 11489, 11, 279, 3405, 374, 911, 279, 21921, 13, 13824, 11, 7196, 279, 330, 66, 75512, 1, 1588, 374, 279, 5819, 4982, 13, 13824, 11, 902, 11, 279, 19309, 702, 264, 21921, 3685, 30, 13824, 11, 902, 11, 279, 1887, 21921, 374, 279, 4158, 61607, 13, 13824, 11, 304, 279, 2168, 1380, 279, 6249, 4933, 279, 19309, 323, 21921, 11, 279, 1909, 315, 279, 21921, 2293, 11489, 11, 902, 11, 7196, 279, 17846, 30, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 2727, 330, 65, 77832, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 3263, 2055, 220, 16, 16, 297, 62410, 1035, 387, 279, 8416, 2115, 3082, 8674, 311, 279, 19309, 13, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 279, 19309, 3082, 13, 2160, 429, 949, 315, 279, 21921, 6505, 30, 13824, 11, 902, 11, 7196, 279, 21921, 374, 279, 4158, 61607, 1212, 279, 19309, 13, 13824, 11, 902, 11, 279, 1909, 315, 429, 21921, 2293, 11489, 11, 902, 11, 304, 1045, 5335, 11, 1052, 594, 264, 17846, 25911, 476, 21810, 13, 13824, 11, 902, 11, 1077, 594, 1779, 279, 1156, 2421, 5335, 13, 576, 17196, 45766, 374, 33866, 11, 1221, 279, 19309, 3082, 13, 13824, 11, 279, 4396, 4226, 25, 3330, 518, 279, 5335, 11, 279, 1633, 389, 1909, 315, 279, 14852, 21921, 320, 1782, 4158, 26482, 4982, 8, 518, 220, 16, 16, 297, 62410, 374, 264, 17846, 13, 13824, 11, 902, 11, 3783, 21677, 17846, 374, 389, 279, 7002, 3403, 279, 19309, 13, 13824, 11, 7196, 358, 5786, 878, 13, 13824, 11, 902, 11, 1077, 594, 1779, 1549, 13, 13824, 11, 279, 1196, 594, 3405, 25, 330, 3838, 374, 389, 1909, 315, 279, 14852, 21921, 429, 374, 389, 847, 220, 16, 16, 297, 62410, 7521, 2055, 1156, 11, 10542, 279, 21921, 13, 576, 4158, 26482, 4982, 3143, 220, 16, 16, 297, 62410, 320, 3481, 7950, 568, 3555, 594, 389, 1909, 30, 21131, 518, 279, 5335, 11, 1052, 594, 264, 17846, 21810, 389, 279, 7002, 3403, 429, 3082, 13, 13824, 11, 9834, 0, 758, 279, 21844, 476, 21810, 11, 1052, 594, 264, 17846, 13, 2055, 279, 4226, 374, 264, 17846, 4192, 1249, 8253, 1128, 374, 389, 1909, 315, 279, 14852, 21921, 518, 220, 16, 16, 297, 527, 20666, 11, 23643, 279, 27979, 6789, 323, 5424, 304, 279, 5335, 25, 18611, 16, 13, 95210, 279, 1036, 65, 77832, 21921, 854, 304, 3405, 25, 362, 4158, 11, 1378, 481, 26482, 5819, 4982, 374, 9434, 3143, 279, 1909, 481, 2115, 320, 16, 16, 297, 527, 20666, 8, 3082, 8674, 311, 1008, 37664, 1075, 279, 19309, 323, 17196, 13, 2303, 17, 13, 1374, 19991, 6171, 3403, 419, 21921, 25, 758, 279, 5335, 11, 264, 3070, 73225, 334, 374, 21810, 389, 279, 7002, 3403, 419, 21921, 3082, 13, 79226, 54216, 151645, 198]
labels:

Got it, let's try to figure out what's on top of the bathroom cabinet at 11 o'clock. First, let's visualize the bathroom setup. The cabinet is likely a white, two-drawer unit. At 11 o'clock (top-left area), we need to check that spot. Looking at the images, especially the one showing the cabinet area, there's a mirror above the sink, but wait, no—wait, the question is about the cabinet. Wait, maybe the "cabinet" here is the storage unit. Wait, no, the sink has a cabinet below? Wait, no, the main cabinet is the white drawers. Wait, in the image where the camera shows the sink and cabinet, the top of the cabinet—wait, no, maybe the mirror? Wait, no, let's check again. Wait, the user says "bathroom cabinet that is on my 11 o'clock". So 11 o'clock would be the upper left area relative to the sink. Looking at the images, there's a mirror mounted on the wall above the sink area. Is that part of the cabinet setup? Wait, no, maybe the cabinet is the white drawers under the sink. Wait, no, the top of that cabinet—wait, no, in some images, there's a mirror reflected or mounted. Wait, no, let's check the first few images. The shower curtain is colorful, then the sink area. Wait, the correct answer: looking at the images, the object on top of the bathroom cabinet (the white drawer unit) at 11 o'clock is a mirror. Wait, no, wait—the mirror is on the wall above the sink. Wait, maybe I misread. Wait, no, let's check again. Wait, the user's question: "What is on top of the bathroom cabinet that is on my 11 o'clock?" So first, identify the cabinet. The white drawer unit near 11 o'clock (top-left). What's on top? Looking at the images, there's a mirror mounted on the wall above that area. Wait, yes! In the reflection or mounted, there's a mirror. So the answer is a mirror.


To determine what is on top of the bathroom cabinet at 11 o’clock, analyze the spatial layout and elements in the images:  

1. Locate the “bathroom cabinet” in question: A white, two - drawer storage unit is visible near the top - left (11 o’clock) area relative to other fixtures like the sink and shower.  
2. Examine objects above this cabinet: In the images, a **mirror** is mounted on the wall above this cabinet area.  


Mirror<|im_end|>

[INFO|hub.py:421] 2025-12-19 06:57:15,687 >> Offline mode: forcing local_files_only=True
[INFO|configuration_utils.py:765] 2025-12-19 06:57:15,693 >> loading configuration file config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
[INFO|configuration_utils.py:839] 2025-12-19 06:57:15,696 >> Model config Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 3584,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 18944,
  "max_position_embeddings": 128000,
  "max_window_layers": 28,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "_name_or_path": "Qwen/Qwen2.5-VL-7B-Instruct",
    "architectures": [
      "Qwen2_5_VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "dtype": "bfloat16",
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 3584,
    "initializer_range": 0.02,
    "intermediate_size": 18944,
    "layer_types": [
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention"
    ],
    "max_position_embeddings": 128000,
    "max_window_layers": 28,
    "model_type": "qwen2_5_vl_text",
    "num_attention_heads": 28,
    "num_hidden_layers": 28,
    "num_key_value_heads": 4,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "use_cache": true,
    "use_sliding_window": false,
    "vision_token_id": 151654,
    "vocab_size": 152064
  },
  "tie_word_embeddings": false,
  "transformers_version": "4.57.0",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 3584,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 152064
}

[INFO|2025-12-19 06:57:15] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
[INFO|2025-12-19 06:57:15] llamafactory.model.model_utils.liger_kernel:143 >> Liger kernel has been applied to the model.
[INFO|hub.py:421] 2025-12-19 06:57:15,902 >> Offline mode: forcing local_files_only=True
[WARNING|logging.py:328] 2025-12-19 06:57:15,904 >> `torch_dtype` is deprecated! Use `dtype` instead!
[INFO|hub.py:421] 2025-12-19 06:57:15,904 >> Offline mode: forcing local_files_only=True
[INFO|modeling_utils.py:4840] 2025-12-19 06:57:15,905 >> Offline mode: forcing local_files_only=True
[INFO|modeling_utils.py:1176] 2025-12-19 06:57:15,909 >> loading weights file model.safetensors from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/model.safetensors.index.json
[INFO|modeling_utils.py:2345] 2025-12-19 06:57:15,921 >> Instantiating Qwen2_5_VLForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:986] 2025-12-19 06:57:15,926 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

[INFO|modeling_utils.py:2345] 2025-12-19 06:57:15,930 >> Instantiating Qwen2_5_VisionTransformerPretrainedModel model under default dtype torch.bfloat16.
[INFO|modeling_utils.py:2345] 2025-12-19 06:57:15,956 >> Instantiating Qwen2_5_VLTextModel model under default dtype torch.bfloat16.
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:22<01:29, 22.40s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:22<01:29, 22.44s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:22<01:30, 22.72s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:22<01:30, 22.63s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:43<01:04, 21.53s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:43<01:04, 21.62s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:43<01:04, 21.51s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:43<01:04, 21.52s/it]Loading checkpoint shards:  60%|██████    | 3/5 [01:05<00:43, 21.90s/it]Loading checkpoint shards:  60%|██████    | 3/5 [01:05<00:43, 21.89s/it]Loading checkpoint shards:  60%|██████    | 3/5 [01:05<00:43, 21.85s/it]Loading checkpoint shards:  60%|██████    | 3/5 [01:05<00:43, 21.86s/it]Loading checkpoint shards:  80%|████████  | 4/5 [01:28<00:22, 22.24s/it]Loading checkpoint shards:  80%|████████  | 4/5 [01:28<00:22, 22.27s/it]Loading checkpoint shards:  80%|████████  | 4/5 [01:28<00:22, 22.23s/it]Loading checkpoint shards:  80%|████████  | 4/5 [01:28<00:22, 22.24s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:34<00:00, 16.37s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:34<00:00, 18.91s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [01:34<00:00, 16.37s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:34<00:00, 16.35s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:34<00:00, 18.89s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [01:34<00:00, 16.35s/it]Loading checkpoint shards: 100%|██████████| 5/5 [01:34<00:00, 18.86s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [01:34<00:00, 18.86s/it]
[INFO|configuration_utils.py:941] 2025-12-19 06:58:50,726 >> loading configuration file generation_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/generation_config.json
[INFO|configuration_utils.py:986] 2025-12-19 06:58:50,727 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 1e-06
}

[INFO|dynamic_module_utils.py:423] 2025-12-19 06:58:50,728 >> Could not locate the custom_generate/generate.py inside Qwen/Qwen2.5-VL-7B-Instruct.
[INFO|2025-12-19 06:58:50] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2025-12-19 06:58:50] llamafactory.model.model_utils.attention:143 >> Using FlashAttention-2 for faster training and inference.
[INFO|2025-12-19 06:58:50] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.
[INFO|2025-12-19 06:58:50] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA
[INFO|2025-12-19 06:58:50] llamafactory.model.model_utils.misc:143 >> Found linear modules: gate_proj,k_proj,q_proj,up_proj,v_proj,down_proj,o_proj
[INFO|2025-12-19 06:58:50] llamafactory.model.model_utils.visual:143 >> Set vision model not trainable: ['visual.patch_embed', 'visual.blocks'].
[INFO|2025-12-19 06:58:50] llamafactory.model.model_utils.visual:143 >> Set multi model projector not trainable: visual.merger.
[INFO|2025-12-19 06:58:51] llamafactory.model.loader:143 >> trainable params: 20,185,088 || all params: 8,312,351,744 || trainable%: 0.2428
name: base_model.model.model.visual.patch_embed.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.0.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.1.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.2.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.3.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.4.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.5.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.6.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.7.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.8.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.9.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.10.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.11.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.12.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.13.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.14.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.15.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.16.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.17.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.18.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.19.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.20.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.21.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.22.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.23.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.24.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.25.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.26.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.27.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.28.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.29.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.30.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.norm1.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.norm2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.qkv.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.qkv.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.attn.proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.gate_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.gate_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.up_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.up_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.down_proj.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.blocks.31.mlp.down_proj.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.ln_q.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.0.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.0.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.2.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.visual.merger.mlp.2.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.embed_tokens.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.0.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.0.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.1.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.1.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.2.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.2.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.3.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.3.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.4.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.4.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.5.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.5.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.6.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.6.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.7.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.7.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.8.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.8.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.9.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.9.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.10.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.10.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.11.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.11.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.12.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.12.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.13.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.13.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.14.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.14.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.15.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.15.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.16.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.16.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.17.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.17.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.18.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.18.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.19.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.19.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.20.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.20.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.21.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.21.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.22.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.22.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.23.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.23.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.24.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.24.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.25.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.25.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.26.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.26.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.q_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.k_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.base_layer.bias, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.v_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.self_attn.o_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.gate_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.up_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.up_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.up_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.down_proj.base_layer.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.mlp.down_proj.lora_A.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.mlp.down_proj.lora_B.default.weight, dtype: torch.float32, device: cuda:0, trainable: True
name: base_model.model.model.language_model.layers.27.input_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.layers.27.post_attention_layernorm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.model.language_model.norm.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
name: base_model.model.lm_head.weight, dtype: torch.bfloat16, device: cuda:0, trainable: False
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
[INFO|trainer.py:749] 2025-12-19 06:58:51,087 >> Using auto half precision backend
[WARNING|trainer.py:982] 2025-12-19 06:58:51,089 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Currently training with a batch size of: 2
Currently training with a batch size of: 2
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
[DEBUG|trainer.py:2373] 2025-12-19 06:58:51,422 >> Currently training with a batch size of: 2
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 8. Using DeepSpeed's value.
[INFO|deepspeed.py:383] 2025-12-19 06:58:51,438 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Currently training with a batch size of: 2
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Using /dev/shm/slurm.indrisch.160343/.cache/torch_extensions as PyTorch extensions root...
Creating extension directory /dev/shm/slurm.indrisch.160343/.cache/torch_extensions/cpu_adam...
Using /dev/shm/slurm.indrisch.160343/.cache/torch_extensions as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /dev/shm/slurm.indrisch.160343/.cache/torch_extensions/cpu_adam/build.ninja...
/opt/conda/lib/python3.11/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module cpu_adam...
Using envvar MAX_JOBS (16) as the number of workers...
Using /dev/shm/slurm.indrisch.160343/.cache/torch_extensions as PyTorch extensions root...
Using /dev/shm/slurm.indrisch.160343/.cache/torch_extensions as PyTorch extensions root...
[1/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.11/site-packages/torch/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o 
[2/3] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.11/site-packages/torch/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.11/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o 
[3/3] c++ cpu_adam.o cpu_adam_impl.o -shared -lcurand -L/usr/local/cuda/lib64 -L/opt/conda/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so
Loading extension module cpu_adam...
Time to load cpu_adam op: 23.31184458732605 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-12-19 06:59:15,885] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
Loading extension module cpu_adam...
Time to load cpu_adam op: 23.299158573150635 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-12-19 06:59:15,931] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.9, git-hash=unknown, git-branch=unknown
[2025-12-19 06:59:15,931] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
Loading extension module cpu_adam...
Time to load cpu_adam op: 23.10037899017334 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-12-19 06:59:15,948] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
Loading extension module cpu_adam...
Time to load cpu_adam op: 23.401362419128418 seconds
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2025-12-19 06:59:15,974] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 4
[2025-12-19 06:59:16,874] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-12-19 06:59:16,876] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-12-19 06:59:16,877] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-12-19 06:59:16,901] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2025-12-19 06:59:16,901] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2025-12-19 06:59:16,901] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2025-12-19 06:59:16,901] [INFO] [stage_1_and_2.py:150:__init__] Reduce bucket size 500000000
[2025-12-19 06:59:16,901] [INFO] [stage_1_and_2.py:151:__init__] Allgather bucket size 500000000
[2025-12-19 06:59:16,901] [INFO] [stage_1_and_2.py:152:__init__] CPU Offload: True
[2025-12-19 06:59:16,901] [INFO] [stage_1_and_2.py:153:__init__] Round robin gradient partitioning: True
***** Running training *****
  Num examples = 29,742
  Num Epochs = 1
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 8
  Total optimization steps = 465
  Number of trainable parameters = 20,185,088
***** Running training *****
  Num examples = 29,742
  Num Epochs = 1
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 8
  Total optimization steps = 465
  Number of trainable parameters = 20,185,088
***** Running training *****
  Num examples = 29,742
  Num Epochs = 1
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 8
  Total optimization steps = 465
  Number of trainable parameters = 20,185,088
[2025-12-19 06:59:17,239] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-12-19 06:59:17,240] [INFO] [utils.py:782:see_memory_usage] MA 15.48 GB         Max_MA 15.48 GB         CA 15.49 GB         Max_CA 15 GB 
[2025-12-19 06:59:17,240] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 29.98 GB, percent = 4.0%
[2025-12-19 06:59:17,460] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-12-19 06:59:17,460] [INFO] [utils.py:782:see_memory_usage] MA 15.48 GB         Max_MA 15.48 GB         CA 15.49 GB         Max_CA 15 GB 
[2025-12-19 06:59:17,460] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 30.0 GB, percent = 4.0%
[2025-12-19 06:59:17,460] [INFO] [stage_1_and_2.py:557:__init__] optimizer state initialized
[2025-12-19 06:59:17,656] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-12-19 06:59:17,656] [INFO] [utils.py:782:see_memory_usage] MA 15.48 GB         Max_MA 15.48 GB         CA 15.49 GB         Max_CA 15 GB 
[2025-12-19 06:59:17,656] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 30.0 GB, percent = 4.0%
[2025-12-19 06:59:17,658] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-12-19 06:59:17,659] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2025-12-19 06:59:17,659] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2025-12-19 06:59:17,659] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2025-12-19 06:59:17,663] [INFO] [config.py:1003:print] DeepSpeedEngine configuration:
[2025-12-19 06:59:17,663] [INFO] [config.py:1007:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-12-19 06:59:17,663] [INFO] [config.py:1007:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-12-19 06:59:17,663] [INFO] [config.py:1007:print]   amp_enabled .................. False
[2025-12-19 06:59:17,663] [INFO] [config.py:1007:print]   amp_params ................... False
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   bfloat16_enabled ............. True
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   bfloat16_immediate_grad_update  True
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   checkpoint_parallel_write_pipeline  False
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   checkpoint_tag_validation_enabled  True
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   checkpoint_tag_validation_fail  False
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1459eef036d0>
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   communication_data_type ...... None
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   curriculum_enabled_legacy .... False
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   curriculum_params_legacy ..... False
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   data_efficiency_enabled ...... False
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   dataloader_drop_last ......... False
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   disable_allgather ............ False
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   dump_state ................... False
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   dynamic_loss_scale_args ...... None
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   eigenvalue_enabled ........... False
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   eigenvalue_gas_boundary_resolution  1
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   eigenvalue_layer_num ......... 0
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   eigenvalue_max_iter .......... 100
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   eigenvalue_stability ......... 1e-06
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   eigenvalue_tol ............... 0.01
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   eigenvalue_verbose ........... False
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   elasticity_enabled ........... False
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   fp16_auto_cast ............... None
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   fp16_enabled ................. False
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   fp16_master_weights_and_gradients  False
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   global_rank .................. 0
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   grad_accum_dtype ............. None
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   gradient_accumulation_steps .. 8
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   gradient_clipping ............ 1.0
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   gradient_predivide_factor .... 1.0
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   graph_harvesting ............. False
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   initial_dynamic_scale ........ 1
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   load_universal_checkpoint .... False
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   loss_scale ................... 1.0
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   memory_breakdown ............. False
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   mics_hierarchial_params_gather  False
[2025-12-19 06:59:17,664] [INFO] [config.py:1007:print]   mics_shard_size .............. -1
[2025-12-19 06:59:17,665] [INFO] [config.py:1007:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-12-19 06:59:17,665] [INFO] [config.py:1007:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-12-19 06:59:17,665] [INFO] [config.py:1007:print]   optimizer_legacy_fusion ...... False
[2025-12-19 06:59:17,665] [INFO] [config.py:1007:print]   optimizer_name ............... None
[2025-12-19 06:59:17,665] [INFO] [config.py:1007:print]   optimizer_params ............. None
[2025-12-19 06:59:17,665] [INFO] [config.py:1007:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-12-19 06:59:17,665] [INFO] [config.py:1007:print]   pld_enabled .................. False
[2025-12-19 06:59:17,665] [INFO] [config.py:1007:print]   pld_params ................... False
[2025-12-19 06:59:17,665] [INFO] [config.py:1007:print]   prescale_gradients ........... False
[2025-12-19 06:59:17,665] [INFO] [config.py:1007:print]   scheduler_name ............... None
[2025-12-19 06:59:17,665] [INFO] [config.py:1007:print]   scheduler_params ............. None
[2025-12-19 06:59:17,665] [INFO] [config.py:1007:print]   seq_parallel_communication_data_type  torch.float32
[2025-12-19 06:59:17,665] [INFO] [config.py:1007:print]   sparse_attention ............. None
[2025-12-19 06:59:17,665] [INFO] [config.py:1007:print]   sparse_gradients_enabled ..... False
[2025-12-19 06:59:17,665] [INFO] [config.py:1007:print]   steps_per_print .............. inf
[2025-12-19 06:59:17,665] [INFO] [config.py:1007:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-12-19 06:59:17,665] [INFO] [config.py:1007:print]   timers_config ................ enabled=True synchronized=True
[2025-12-19 06:59:17,665] [INFO] [config.py:1007:print]   train_batch_size ............. 64
[2025-12-19 06:59:17,665] [INFO] [config.py:1007:print]   train_micro_batch_size_per_gpu  2
[2025-12-19 06:59:17,665] [INFO] [config.py:1007:print]   use_data_before_expert_parallel_  False
[2025-12-19 06:59:17,665] [INFO] [config.py:1007:print]   use_node_local_storage ....... False
[2025-12-19 06:59:17,665] [INFO] [config.py:1007:print]   wall_clock_breakdown ......... False
[2025-12-19 06:59:17,665] [INFO] [config.py:1007:print]   weight_quantization_config ... None
[2025-12-19 06:59:17,665] [INFO] [config.py:1007:print]   world_size ................... 4
[2025-12-19 06:59:17,665] [INFO] [config.py:1007:print]   zero_allow_untested_optimizer  True
[2025-12-19 06:59:17,665] [INFO] [config.py:1007:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=True zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-12-19 06:59:17,665] [INFO] [config.py:1007:print]   zero_enabled ................. True
[2025-12-19 06:59:17,665] [INFO] [config.py:1007:print]   zero_force_ds_cpu_optimizer .. True
[2025-12-19 06:59:17,665] [INFO] [config.py:1007:print]   zero_optimization_stage ...... 2
[2025-12-19 06:59:17,665] [INFO] [config.py:993:print_user_config]   json = {
    "train_batch_size": 64, 
    "train_micro_batch_size_per_gpu": 2, 
    "gradient_accumulation_steps": 8, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": false, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true, 
        "round_robin_gradients": true
    }, 
    "steps_per_print": inf
}
[INFO|trainer.py:2519] 2025-12-19 06:59:17,666 >> ***** Running training *****
[INFO|trainer.py:2520] 2025-12-19 06:59:17,666 >>   Num examples = 29,742
[INFO|trainer.py:2521] 2025-12-19 06:59:17,666 >>   Num Epochs = 1
[INFO|trainer.py:2522] 2025-12-19 06:59:17,666 >>   Instantaneous batch size per device = 2
[INFO|trainer.py:2525] 2025-12-19 06:59:17,666 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2526] 2025-12-19 06:59:17,666 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2527] 2025-12-19 06:59:17,666 >>   Total optimization steps = 465
[INFO|trainer.py:2528] 2025-12-19 06:59:17,670 >>   Number of trainable parameters = 20,185,088
[INFO|integration_utils.py:867] 2025-12-19 06:59:17,673 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
2025/12/19 06:59:18 ERROR main: failed to get logger path error="error creating log directory: mkdir /home/indrisch/.cache/wandb: permission denied"
2025/12/19 06:59:18 INFO server: will exit if parent process dies ppid=2667075
2025/12/19 06:59:18 INFO server: accepting connections addr=/tmp/wandb-2667075-2668313-2548011471/socket
2025/12/19 06:59:18 INFO connection: ManageConnectionData: new connection created id=1(@)
2025/12/19 06:59:18 INFO handleInformInit: received streamId=hzm5mnoi id=1(@)
2025/12/19 06:59:19 INFO handleInformInit: stream started streamId=hzm5mnoi id=1(@)
wandb: Tracking run with wandb version 0.23.1
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /scratch/indrisch/LLaMA-Factory/wandb/wandb/offline-run-20251219_065917-hzm5mnoi
  0%|          | 0/465 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /dev/shm/slurm.indrisch.160343/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /dev/shm/slurm.indrisch.160343/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /dev/shm/slurm.indrisch.160343/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:1171: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /dev/shm/slurm.indrisch.160343/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/cuda/jit_utils.cpp:1442.)
  split_sizes = (image_grid_thw.prod(-1) // self.visual.spatial_merge_size**2).tolist()
  0%|          | 1/465 [01:45<13:34:18, 105.30s/it]  0%|          | 2/465 [03:13<12:17:11, 95.53s/it]   1%|          | 3/465 [05:17<13:54:14, 108.34s/it]  1%|          | 4/465 [07:05<13:49:41, 107.99s/it]  1%|          | 5/465 [08:51<13:44:57, 107.60s/it]  1%|▏         | 6/465 [10:22<12:57:59, 101.70s/it]  2%|▏         | 7/465 [11:46<12:12:44, 95.99s/it]   2%|▏         | 8/465 [13:31<12:31:57, 98.72s/it]  2%|▏         | 9/465 [15:22<13:00:40, 102.72s/it]  2%|▏         | 10/465 [16:53<12:32:04, 99.17s/it]                                                   {'loss': 1.6618, 'grad_norm': 0.12507867813110352, 'learning_rate': 1.9148936170212766e-05, 'epoch': 0.02}
  2%|▏         | 10/465 [16:53<12:32:04, 99.17s/it]  2%|▏         | 11/465 [18:21<12:04:34, 95.76s/it]  3%|▎         | 12/465 [19:58<12:04:45, 95.99s/it]  3%|▎         | 13/465 [21:50<12:39:24, 100.81s/it]  3%|▎         | 14/465 [23:21<12:16:13, 97.95s/it]   3%|▎         | 15/465 [25:13<12:46:53, 102.25s/it]  3%|▎         | 16/465 [26:32<11:52:00, 95.14s/it]   4%|▎         | 17/465 [28:04<11:43:51, 94.27s/it]  4%|▍         | 18/465 [29:44<11:55:03, 95.98s/it]  4%|▍         | 19/465 [31:40<12:38:51, 102.09s/it]  4%|▍         | 20/465 [33:24<12:40:48, 102.58s/it]                                                    {'loss': 1.6014, 'grad_norm': 0.13246497511863708, 'learning_rate': 4.0425531914893614e-05, 'epoch': 0.04}
  4%|▍         | 20/465 [33:24<12:40:48, 102.58s/it]  5%|▍         | 21/465 [35:03<12:31:17, 101.53s/it]  5%|▍         | 22/465 [36:35<12:07:50, 98.58s/it]   5%|▍         | 23/465 [38:12<12:03:46, 98.25s/it]  5%|▌         | 24/465 [39:44<11:47:50, 96.31s/it]  5%|▌         | 25/465 [41:31<12:10:23, 99.60s/it]  6%|▌         | 26/465 [43:15<12:17:30, 100.80s/it]  6%|▌         | 27/465 [44:35<11:29:57, 94.51s/it]   6%|▌         | 28/465 [46:08<11:25:27, 94.11s/it]  6%|▌         | 29/465 [47:52<11:44:30, 96.95s/it]  6%|▋         | 30/465 [49:35<11:55:56, 98.75s/it]                                                   {'loss': 1.392, 'grad_norm': 0.10520986467599869, 'learning_rate': 6.170212765957447e-05, 'epoch': 0.06}
  6%|▋         | 30/465 [49:35<11:55:56, 98.75s/it]  7%|▋         | 31/465 [51:23<12:15:47, 101.72s/it]  7%|▋         | 32/465 [53:02<12:08:43, 100.98s/it]  7%|▋         | 33/465 [54:34<11:47:40, 98.29s/it]   7%|▋         | 34/465 [56:01<11:21:29, 94.87s/it]  8%|▊         | 35/465 [57:33<11:12:12, 93.80s/it]  8%|▊         | 36/465 [59:09<11:15:36, 94.49s/it]  8%|▊         | 37/465 [1:00:49<11:26:16, 96.21s/it]  8%|▊         | 38/465 [1:02:29<11:32:54, 97.36s/it]  8%|▊         | 39/465 [1:04:13<11:44:41, 99.25s/it]  9%|▊         | 40/465 [1:05:52<11:44:00, 99.39s/it]                                                     {'loss': 1.2197, 'grad_norm': 0.13294336199760437, 'learning_rate': 8.297872340425533e-05, 'epoch': 0.09}
  9%|▊         | 40/465 [1:05:52<11:44:00, 99.39s/it]  9%|▉         | 41/465 [1:07:23<11:22:45, 96.62s/it]  9%|▉         | 42/465 [1:08:52<11:05:57, 94.46s/it]  9%|▉         | 43/465 [1:10:28<11:07:44, 94.94s/it]  9%|▉         | 44/465 [1:12:18<11:38:16, 99.52s/it] 10%|▉         | 45/465 [1:14:03<11:47:49, 101.12s/it] 10%|▉         | 46/465 [1:15:41<11:38:23, 100.01s/it] 10%|█         | 47/465 [1:17:28<11:52:26, 102.27s/it] 10%|█         | 48/465 [1:19:14<11:58:12, 103.34s/it] 11%|█         | 49/465 [1:20:40<11:20:12, 98.11s/it]  11%|█         | 50/465 [1:22:29<11:41:51, 101.47s/it]                                                      {'loss': 1.1108, 'grad_norm': 0.1397312581539154, 'learning_rate': 9.999435142363484e-05, 'epoch': 0.11}
 11%|█         | 50/465 [1:22:29<11:41:51, 101.47s/it] 11%|█         | 51/465 [1:24:29<12:18:13, 106.99s/it] 11%|█         | 52/465 [1:26:14<12:12:05, 106.36s/it] 11%|█▏        | 53/465 [1:27:48<11:45:13, 102.70s/it] 12%|█▏        | 54/465 [1:29:24<11:28:41, 100.54s/it] 12%|█▏        | 55/465 [1:30:45<10:48:46, 94.94s/it]  12%|█▏        | 56/465 [1:32:27<11:00:01, 96.82s/it] 12%|█▏        | 57/465 [1:34:23<11:38:21, 102.70s/it] 12%|█▏        | 58/465 [1:36:09<11:42:15, 103.53s/it] 13%|█▎        | 59/465 [1:37:56<11:48:41, 104.73s/it] 13%|█▎        | 60/465 [1:39:29<11:22:13, 101.07s/it]                                                      {'loss': 1.0256, 'grad_norm': 0.1229352131485939, 'learning_rate': 9.979678522550382e-05, 'epoch': 0.13}
 13%|█▎        | 60/465 [1:39:29<11:22:13, 101.07s/it] 13%|█▎        | 61/465 [1:41:03<11:06:47, 99.03s/it]  13%|█▎        | 62/465 [1:42:39<10:59:14, 98.15s/it] 14%|█▎        | 63/465 [1:44:17<10:57:11, 98.09s/it] 14%|█▍        | 64/465 [1:45:45<10:36:17, 95.20s/it] 14%|█▍        | 65/465 [1:47:31<10:55:25, 98.31s/it] 14%|█▍        | 66/465 [1:49:12<11:00:03, 99.26s/it] 14%|█▍        | 67/465 [1:50:46<10:47:49, 97.66s/it] 15%|█▍        | 68/465 [1:52:27<10:52:43, 98.65s/it] 15%|█▍        | 69/465 [1:54:06<10:51:07, 98.65s/it] 15%|█▌        | 70/465 [1:55:30<10:20:50, 94.31s/it]                                                     {'loss': 0.9784, 'grad_norm': 0.12410382181406021, 'learning_rate': 9.931806517013612e-05, 'epoch': 0.15}
 15%|█▌        | 70/465 [1:55:30<10:20:50, 94.31s/it] 15%|█▌        | 71/465 [1:57:19<10:47:35, 98.62s/it] 15%|█▌        | 72/465 [1:58:43<10:17:51, 94.33s/it] 16%|█▌        | 73/465 [2:00:19<10:19:02, 94.75s/it] 16%|█▌        | 74/465 [2:01:53<10:16:29, 94.60s/it] 16%|█▌        | 75/465 [2:03:18<9:56:04, 91.70s/it]  16%|█▋        | 76/465 [2:05:00<10:13:37, 94.65s/it] 17%|█▋        | 77/465 [2:06:40<10:23:11, 96.37s/it] 17%|█▋        | 78/465 [2:08:16<10:20:28, 96.20s/it] 17%|█▋        | 79/465 [2:09:54<10:22:58, 96.84s/it] 17%|█▋        | 80/465 [2:11:44<10:46:56, 100.82s/it]                                                      {'loss': 0.9607, 'grad_norm': 0.12962515652179718, 'learning_rate': 9.856089412257606e-05, 'epoch': 0.17}
 17%|█▋        | 80/465 [2:11:44<10:46:56, 100.82s/it] 17%|█▋        | 81/465 [2:13:22<10:38:32, 99.77s/it]  18%|█▊        | 82/465 [2:15:11<10:54:51, 102.59s/it] 18%|█▊        | 83/465 [2:16:56<10:57:34, 103.28s/it] 18%|█▊        | 84/465 [2:18:31<10:39:56, 100.78s/it] 18%|█▊        | 85/465 [2:19:43<9:45:00, 92.37s/it]   18%|█▊        | 86/465 [2:21:36<10:22:18, 98.52s/it] 19%|█▊        | 87/465 [2:23:20<10:30:00, 100.00s/it] 19%|█▉        | 88/465 [2:25:01<10:31:53, 100.57s/it] 19%|█▉        | 89/465 [2:26:29<10:06:07, 96.72s/it]  19%|█▉        | 90/465 [2:27:53<9:39:42, 92.75s/it]                                                     {'loss': 0.923, 'grad_norm': 0.12148254364728928, 'learning_rate': 9.752954708892377e-05, 'epoch': 0.19}
 19%|█▉        | 90/465 [2:27:53<9:39:42, 92.75s/it] 20%|█▉        | 91/465 [2:29:27<9:40:18, 93.10s/it] 20%|█▉        | 92/465 [2:31:04<9:46:12, 94.30s/it] 20%|██        | 93/465 [2:33:02<10:29:12, 101.49s/it] 20%|██        | 94/465 [2:34:28<9:59:18, 96.92s/it]   20%|██        | 95/465 [2:36:16<10:18:14, 100.26s/it] 21%|██        | 96/465 [2:38:10<10:42:11, 104.42s/it] 21%|██        | 97/465 [2:39:40<10:14:02, 100.12s/it] 21%|██        | 98/465 [2:41:22<10:14:52, 100.52s/it] 21%|██▏       | 99/465 [2:43:22<10:49:08, 106.42s/it] 22%|██▏       | 100/465 [2:44:54<10:20:14, 101.96s/it]                                                       {'loss': 0.9061, 'grad_norm': 0.12718234956264496, 'learning_rate': 9.622984707954732e-05, 'epoch': 0.22}
 22%|██▏       | 100/465 [2:44:54<10:20:14, 101.96s/it] 22%|██▏       | 101/465 [2:46:54<10:52:42, 107.59s/it] 22%|██▏       | 102/465 [2:48:37<10:41:28, 106.03s/it] 22%|██▏       | 103/465 [2:50:27<10:47:45, 107.36s/it] 22%|██▏       | 104/465 [2:52:00<10:19:59, 103.05s/it] 23%|██▎       | 105/465 [2:53:34<10:02:16, 100.38s/it] 23%|██▎       | 106/465 [2:55:22<10:13:42, 102.57s/it] 23%|██▎       | 107/465 [2:56:52<9:50:00, 98.88s/it]   23%|██▎       | 108/465 [2:58:22<9:32:40, 96.25s/it] 23%|██▎       | 109/465 [2:59:53<9:20:25, 94.45s/it] 24%|██▎       | 110/465 [3:01:25<9:14:22, 93.70s/it]                                                     {'loss': 0.9154, 'grad_norm': 0.12963126599788666, 'learning_rate': 9.466913223222467e-05, 'epoch': 0.24}
 24%|██▎       | 110/465 [3:01:25<9:14:22, 93.70s/it] 24%|██▍       | 111/465 [3:03:12<9:37:14, 97.84s/it] 24%|██▍       | 112/465 [3:04:46<9:28:49, 96.68s/it] 24%|██▍       | 113/465 [3:06:39<9:56:14, 101.63s/it] 25%|██▍       | 114/465 [3:08:23<9:57:50, 102.20s/it] 25%|██▍       | 115/465 [3:10:08<10:00:59, 103.03s/it] 25%|██▍       | 116/465 [3:11:47<9:52:59, 101.95s/it]  25%|██▌       | 117/465 [3:13:37<10:04:27, 104.22s/it] 25%|██▌       | 118/465 [3:15:18<9:57:09, 103.26s/it]  26%|██▌       | 119/465 [3:17:01<9:55:47, 103.32s/it] 26%|██▌       | 120/465 [3:18:28<9:24:44, 98.22s/it]                                                      {'loss': 0.8938, 'grad_norm': 0.12720556557178497, 'learning_rate': 9.285621438083998e-05, 'epoch': 0.26}
 26%|██▌       | 120/465 [3:18:28<9:24:44, 98.22s/it] 26%|██▌       | 121/465 [3:20:20<9:47:12, 102.42s/it] 26%|██▌       | 122/465 [3:22:36<10:42:37, 112.41s/it] 26%|██▋       | 123/465 [3:23:57<9:47:12, 103.02s/it]  27%|██▋       | 124/465 [3:25:44<9:52:50, 104.31s/it] 27%|██▋       | 125/465 [3:27:13<9:24:20, 99.59s/it]  27%|██▋       | 126/465 [3:28:42<9:06:02, 96.65s/it] 27%|██▋       | 127/465 [3:30:21<9:07:47, 97.24s/it] 28%|██▊       | 128/465 [3:31:57<9:04:53, 97.01s/it] 28%|██▊       | 129/465 [3:33:59<9:45:06, 104.48s/it] 28%|██▊       | 130/465 [3:35:49<9:52:44, 106.16s/it]                                                      {'loss': 0.8607, 'grad_norm': 0.12727807462215424, 'learning_rate': 9.080132930355567e-05, 'epoch': 0.28}
 28%|██▊       | 130/465 [3:35:49<9:52:44, 106.16s/it] 28%|██▊       | 131/465 [3:37:21<9:26:50, 101.83s/it] 28%|██▊       | 132/465 [3:39:07<9:31:58, 103.06s/it] 29%|██▊       | 133/465 [3:40:35<9:05:17, 98.55s/it]  29%|██▉       | 134/465 [3:42:07<8:52:37, 96.55s/it] 29%|██▉       | 135/465 [3:43:35<8:36:58, 93.99s/it] 29%|██▉       | 136/465 [3:45:23<8:57:49, 98.08s/it] 29%|██▉       | 137/465 [3:46:50<8:39:18, 95.00s/it] 30%|██▉       | 138/465 [3:48:41<9:02:29, 99.54s/it] 30%|██▉       | 139/465 [3:50:16<8:53:33, 98.20s/it] 30%|███       | 140/465 [3:52:00<9:01:42, 100.01s/it]                                                      {'loss': 0.8538, 'grad_norm': 0.13051559031009674, 'learning_rate': 8.851607893136065e-05, 'epoch': 0.3}
 30%|███       | 140/465 [3:52:00<9:01:42, 100.01s/it] 30%|███       | 141/465 [3:53:32<8:47:47, 97.74s/it]  31%|███       | 142/465 [3:55:14<8:52:16, 98.87s/it] 31%|███       | 143/465 [3:56:43<8:35:06, 95.98s/it] 31%|███       | 144/465 [3:58:20<8:34:39, 96.20s/it] 31%|███       | 145/465 [4:00:01<8:41:43, 97.82s/it] 31%|███▏      | 146/465 [4:01:47<8:52:54, 100.23s/it] 32%|███▏      | 147/465 [4:03:36<9:05:34, 102.94s/it] 32%|███▏      | 148/465 [4:05:02<8:36:32, 97.77s/it]  32%|███▏      | 149/465 [4:06:53<8:55:29, 101.67s/it] 32%|███▏      | 150/465 [4:08:31<8:47:41, 100.51s/it]                                                      {'loss': 0.8639, 'grad_norm': 0.1302109807729721, 'learning_rate': 8.601336584328659e-05, 'epoch': 0.32}
 32%|███▏      | 150/465 [4:08:31<8:47:41, 100.51s/it] 32%|███▏      | 151/465 [4:10:07<8:39:05, 99.19s/it]  33%|███▎      | 152/465 [4:11:38<8:25:04, 96.82s/it] 33%|███▎      | 153/465 [4:13:15<8:24:13, 96.97s/it] 33%|███▎      | 154/465 [4:14:55<8:27:00, 97.81s/it] 33%|███▎      | 155/465 [4:16:27<8:15:25, 95.89s/it] 34%|███▎      | 156/465 [4:18:10<8:25:11, 98.09s/it] 34%|███▍      | 157/465 [4:19:30<7:56:23, 92.80s/it] 34%|███▍      | 158/465 [4:21:13<8:10:25, 95.85s/it] 34%|███▍      | 159/465 [4:22:56<8:19:41, 97.98s/it] 34%|███▍      | 160/465 [4:24:30<8:11:42, 96.73s/it]                                                     {'loss': 0.8603, 'grad_norm': 0.15345777571201324, 'learning_rate': 8.330732041813367e-05, 'epoch': 0.34}
 34%|███▍      | 160/465 [4:24:30<8:11:42, 96.73s/it] 35%|███▍      | 161/465 [4:25:47<7:39:54, 90.77s/it] 35%|███▍      | 162/465 [4:27:14<7:32:13, 89.55s/it] 35%|███▌      | 163/465 [4:28:46<7:34:57, 90.39s/it] 35%|███▌      | 164/465 [4:30:28<7:51:05, 93.91s/it] 35%|███▌      | 165/465 [4:32:15<8:08:36, 97.72s/it] 36%|███▌      | 166/465 [4:34:16<8:41:35, 104.67s/it] 36%|███▌      | 167/465 [4:36:00<8:39:25, 104.58s/it] 36%|███▌      | 168/465 [4:37:24<8:06:51, 98.36s/it]  36%|███▋      | 169/465 [4:38:50<7:47:34, 94.78s/it] 37%|███▋      | 170/465 [4:40:18<7:35:45, 92.70s/it]                                                     {'loss': 0.836, 'grad_norm': 0.14287011325359344, 'learning_rate': 8.041322105400922e-05, 'epoch': 0.37}
 37%|███▋      | 170/465 [4:40:18<7:35:45, 92.70s/it] 37%|███▋      | 171/465 [4:42:05<7:54:41, 96.88s/it] 37%|███▋      | 172/465 [4:43:49<8:04:18, 99.18s/it] 37%|███▋      | 173/465 [4:45:22<7:52:50, 97.16s/it] 37%|███▋      | 174/465 [4:47:02<7:55:43, 98.09s/it] 38%|███▊      | 175/465 [4:48:33<7:43:53, 95.98s/it] 38%|███▊      | 176/465 [4:50:14<7:49:56, 97.57s/it] 38%|███▊      | 177/465 [4:51:43<7:36:07, 95.03s/it] 38%|███▊      | 178/465 [4:53:09<7:20:56, 92.18s/it] 38%|███▊      | 179/465 [4:54:36<7:12:30, 90.74s/it] 39%|███▊      | 180/465 [4:56:14<7:20:51, 92.81s/it]                                                     {'loss': 0.8524, 'grad_norm': 0.15229861438274384, 'learning_rate': 7.734740790612136e-05, 'epoch': 0.39}
 39%|███▊      | 180/465 [4:56:14<7:20:51, 92.81s/it] 39%|███▉      | 181/465 [4:57:42<7:12:46, 91.43s/it] 39%|███▉      | 182/465 [4:59:18<7:17:27, 92.75s/it] 39%|███▉      | 183/465 [5:00:41<7:01:33, 89.69s/it] 40%|███▉      | 184/465 [5:02:13<7:03:51, 90.50s/it] 40%|███▉      | 185/465 [5:03:45<7:04:05, 90.88s/it] 40%|████      | 186/465 [5:05:18<7:05:43, 91.56s/it] 40%|████      | 187/465 [5:07:03<7:22:43, 95.55s/it] 40%|████      | 188/465 [5:08:32<7:12:14, 93.63s/it] 41%|████      | 189/465 [5:10:26<7:38:37, 99.70s/it] 41%|████      | 190/465 [5:12:03<7:33:40, 98.98s/it]                                                     {'loss': 0.8454, 'grad_norm': 0.15970838069915771, 'learning_rate': 7.412719062986632e-05, 'epoch': 0.41}
 41%|████      | 190/465 [5:12:03<7:33:40, 98.98s/it] 41%|████      | 191/465 [5:13:34<7:21:32, 96.69s/it] 41%|████▏     | 192/465 [5:14:56<7:00:00, 92.31s/it] 42%|████▏     | 193/465 [5:16:40<7:13:08, 95.55s/it] 42%|████▏     | 194/465 [5:18:09<7:02:57, 93.64s/it] 42%|████▏     | 195/465 [5:19:44<7:03:48, 94.18s/it] 42%|████▏     | 196/465 [5:21:25<7:10:36, 96.05s/it] 42%|████▏     | 197/465 [5:23:14<7:26:37, 99.99s/it] 43%|████▎     | 198/465 [5:25:00<7:33:39, 101.95s/it] 43%|████▎     | 199/465 [5:27:01<7:57:22, 107.68s/it] 43%|████▎     | 200/465 [5:28:40<7:43:49, 105.02s/it]                                                      {'loss': 0.825, 'grad_norm': 0.15387536585330963, 'learning_rate': 7.077075065009433e-05, 'epoch': 0.43}
 43%|████▎     | 200/465 [5:28:40<7:43:49, 105.02s/it]
***** Running Evaluation *****

***** Running Evaluation *****
  Num examples = 3305
  Num examples = 3305
[INFO|trainer.py:4643] 2025-12-19 12:28:00,252 >> 
***** Running Evaluation *****
  Batch size = 1
  Batch size = 1
[INFO|trainer.py:4645] 2025-12-19 12:28:00,252 >>   Num examples = 3305

***** Running Evaluation *****
[INFO|trainer.py:4648] 2025-12-19 12:28:00,252 >>   Batch size = 1
  Num examples = 3305
  Batch size = 1

  0%|          | 0/827 [00:00<?, ?it/s][A
  0%|          | 2/827 [00:04<28:29,  2.07s/it][A
  0%|          | 3/827 [00:06<31:36,  2.30s/it][A
  0%|          | 4/827 [00:09<35:11,  2.57s/it][A
  1%|          | 5/827 [00:13<40:50,  2.98s/it][A
  1%|          | 6/827 [00:17<44:08,  3.23s/it][A
  1%|          | 7/827 [00:21<47:42,  3.49s/it][A
  1%|          | 8/827 [00:27<58:39,  4.30s/it][A
  1%|          | 9/827 [00:34<1:08:57,  5.06s/it][A
  1%|          | 10/827 [00:41<1:17:20,  5.68s/it][A
  1%|▏         | 11/827 [00:44<1:07:31,  4.96s/it][A
  1%|▏         | 12/827 [00:47<59:07,  4.35s/it]  [A
  2%|▏         | 13/827 [00:50<54:26,  4.01s/it][A
  2%|▏         | 14/827 [00:54<52:43,  3.89s/it][A
  2%|▏         | 15/827 [00:58<53:55,  3.98s/it][A
  2%|▏         | 16/827 [01:01<51:46,  3.83s/it][A
  2%|▏         | 17/827 [01:05<48:49,  3.62s/it][A
  2%|▏         | 18/827 [01:08<47:52,  3.55s/it][A
  2%|▏         | 19/827 [01:13<52:28,  3.90s/it][A
  2%|▏         | 20/827 [01:18<58:11,  4.33s/it][A
  3%|▎         | 21/827 [01:22<57:42,  4.30s/it][A
  3%|▎         | 22/827 [01:27<59:09,  4.41s/it][A
  3%|▎         | 23/827 [01:30<53:09,  3.97s/it][A
  3%|▎         | 24/827 [01:34<52:28,  3.92s/it][A
  3%|▎         | 25/827 [01:39<57:04,  4.27s/it][A
  3%|▎         | 26/827 [01:44<1:01:07,  4.58s/it][A
  3%|▎         | 27/827 [01:49<1:00:33,  4.54s/it][A
  3%|▎         | 28/827 [01:51<53:33,  4.02s/it]  [A
  4%|▎         | 29/827 [01:56<55:52,  4.20s/it][A
  4%|▎         | 30/827 [02:01<58:42,  4.42s/it][A
  4%|▎         | 31/827 [02:05<56:25,  4.25s/it][A
  4%|▍         | 32/827 [02:09<56:36,  4.27s/it][A
  4%|▍         | 33/827 [02:14<1:00:11,  4.55s/it][A
  4%|▍         | 34/827 [02:18<54:59,  4.16s/it]  [A
  4%|▍         | 35/827 [02:21<51:41,  3.92s/it][A
  4%|▍         | 36/827 [02:24<46:41,  3.54s/it][A
  4%|▍         | 37/827 [02:29<53:49,  4.09s/it][A
  5%|▍         | 38/827 [02:34<57:39,  4.38s/it][A
  5%|▍         | 39/827 [02:39<58:50,  4.48s/it][A
  5%|▍         | 40/827 [02:44<1:00:36,  4.62s/it][A
  5%|▍         | 41/827 [02:47<54:29,  4.16s/it]  [A
  5%|▌         | 42/827 [02:50<52:38,  4.02s/it][A
  5%|▌         | 43/827 [02:54<51:10,  3.92s/it][A
  5%|▌         | 44/827 [02:58<49:13,  3.77s/it][A
  5%|▌         | 45/827 [03:02<51:58,  3.99s/it][A
  6%|▌         | 46/827 [03:07<56:45,  4.36s/it][A
  6%|▌         | 47/827 [03:12<57:47,  4.45s/it][A
  6%|▌         | 48/827 [03:16<54:37,  4.21s/it][A
  6%|▌         | 49/827 [03:19<52:06,  4.02s/it][A
  6%|▌         | 50/827 [03:22<47:52,  3.70s/it][A
  6%|▌         | 51/827 [03:27<52:31,  4.06s/it][A
  6%|▋         | 52/827 [03:33<58:58,  4.57s/it][A
  6%|▋         | 53/827 [03:36<52:08,  4.04s/it][A
  7%|▋         | 54/827 [03:39<48:49,  3.79s/it][A
  7%|▋         | 55/827 [03:42<45:34,  3.54s/it][A
  7%|▋         | 56/827 [03:45<44:01,  3.43s/it][A
  7%|▋         | 57/827 [03:48<44:46,  3.49s/it][A
  7%|▋         | 58/827 [03:52<43:53,  3.42s/it][A
  7%|▋         | 59/827 [03:55<43:50,  3.43s/it][A
  7%|▋         | 60/827 [04:00<48:39,  3.81s/it][A
  7%|▋         | 61/827 [04:05<53:21,  4.18s/it][A
  7%|▋         | 62/827 [04:08<48:23,  3.80s/it][A
  8%|▊         | 63/827 [04:11<45:02,  3.54s/it][A
  8%|▊         | 64/827 [04:14<43:09,  3.39s/it][A
  8%|▊         | 65/827 [04:17<42:16,  3.33s/it][A
  8%|▊         | 66/827 [04:22<48:45,  3.84s/it][A
  8%|▊         | 67/827 [04:27<52:15,  4.13s/it][A
  8%|▊         | 68/827 [04:29<45:48,  3.62s/it][A
  8%|▊         | 69/827 [04:32<44:13,  3.50s/it][A
  8%|▊         | 70/827 [04:35<41:28,  3.29s/it][A
  9%|▊         | 71/827 [04:39<44:28,  3.53s/it][A
  9%|▊         | 72/827 [04:44<48:27,  3.85s/it][A
  9%|▉         | 73/827 [04:48<47:34,  3.79s/it][A
  9%|▉         | 74/827 [04:54<55:38,  4.43s/it][A
  9%|▉         | 75/827 [04:59<57:50,  4.62s/it][A
  9%|▉         | 76/827 [05:02<51:35,  4.12s/it][A
  9%|▉         | 77/827 [05:05<47:19,  3.79s/it][A
  9%|▉         | 78/827 [05:08<44:32,  3.57s/it][A
 10%|▉         | 79/827 [05:11<42:01,  3.37s/it][A
 10%|▉         | 80/827 [05:15<46:51,  3.76s/it][A
 10%|▉         | 81/827 [05:20<50:11,  4.04s/it][A
 10%|▉         | 82/827 [05:24<49:59,  4.03s/it][A
 10%|█         | 83/827 [05:28<50:05,  4.04s/it][A
 10%|█         | 84/827 [05:32<50:47,  4.10s/it][A
 10%|█         | 85/827 [05:36<49:37,  4.01s/it][A
 10%|█         | 86/827 [05:39<45:55,  3.72s/it][A
 11%|█         | 87/827 [05:45<54:28,  4.42s/it][A
 11%|█         | 88/827 [05:55<1:13:53,  6.00s/it][A
 11%|█         | 89/827 [06:02<1:18:33,  6.39s/it][A
 11%|█         | 90/827 [06:07<1:13:40,  6.00s/it][A
 11%|█         | 91/827 [06:11<1:04:12,  5.23s/it][A
 11%|█         | 92/827 [06:15<59:44,  4.88s/it]  [A
 11%|█         | 93/827 [06:18<52:25,  4.29s/it][A
 11%|█▏        | 94/827 [06:21<48:18,  3.95s/it][A
 11%|█▏        | 95/827 [06:24<45:03,  3.69s/it][A
 12%|█▏        | 96/827 [06:27<41:37,  3.42s/it][A
 12%|█▏        | 97/827 [06:29<37:50,  3.11s/it][A
 12%|█▏        | 98/827 [06:32<35:54,  2.96s/it][A
 12%|█▏        | 99/827 [06:34<34:38,  2.86s/it][A
 12%|█▏        | 100/827 [06:37<32:33,  2.69s/it][A
 12%|█▏        | 101/827 [06:40<35:30,  2.93s/it][A
 12%|█▏        | 102/827 [06:43<35:58,  2.98s/it][A
 12%|█▏        | 103/827 [06:46<34:59,  2.90s/it][A
 13%|█▎        | 104/827 [06:49<34:54,  2.90s/it][A
 13%|█▎        | 105/827 [06:52<36:46,  3.06s/it][A
 13%|█▎        | 106/827 [06:59<50:31,  4.21s/it][A
 13%|█▎        | 107/827 [07:05<58:24,  4.87s/it][A
 13%|█▎        | 108/827 [07:10<57:02,  4.76s/it][A
 13%|█▎        | 109/827 [07:14<53:03,  4.43s/it][A
 13%|█▎        | 110/827 [07:17<50:37,  4.24s/it][A
 13%|█▎        | 111/827 [07:23<54:47,  4.59s/it][A
 14%|█▎        | 112/827 [07:29<1:00:18,  5.06s/it][A
 14%|█▎        | 113/827 [07:33<57:02,  4.79s/it]  [A
 14%|█▍        | 114/827 [07:37<52:30,  4.42s/it][A
 14%|█▍        | 115/827 [07:40<48:21,  4.08s/it][A
 14%|█▍        | 116/827 [07:44<46:47,  3.95s/it][A
 14%|█▍        | 117/827 [07:48<48:20,  4.09s/it][A
 14%|█▍        | 118/827 [07:53<52:22,  4.43s/it][A
 14%|█▍        | 119/827 [07:58<54:55,  4.65s/it][A
 15%|█▍        | 120/827 [08:03<54:09,  4.60s/it][A
 15%|█▍        | 121/827 [08:06<48:51,  4.15s/it][A
 15%|█▍        | 122/827 [08:09<45:35,  3.88s/it][A
 15%|█▍        | 123/827 [08:12<41:29,  3.54s/it][A
 15%|█▍        | 124/827 [08:16<41:42,  3.56s/it][A
 15%|█▌        | 125/827 [08:20<43:36,  3.73s/it][A
 15%|█▌        | 126/827 [08:23<42:33,  3.64s/it][A
 15%|█▌        | 127/827 [08:26<40:37,  3.48s/it][A
 15%|█▌        | 128/827 [08:30<41:14,  3.54s/it][A
 16%|█▌        | 129/827 [08:33<38:47,  3.33s/it][A
 16%|█▌        | 130/827 [08:37<40:02,  3.45s/it][A
 16%|█▌        | 131/827 [08:41<42:06,  3.63s/it][A
 16%|█▌        | 132/827 [08:45<46:09,  3.98s/it][A
 16%|█▌        | 133/827 [08:50<47:46,  4.13s/it][A
 16%|█▌        | 134/827 [08:53<45:22,  3.93s/it][A
 16%|█▋        | 135/827 [08:58<48:07,  4.17s/it][A
 16%|█▋        | 136/827 [09:04<54:22,  4.72s/it][A
 17%|█▋        | 137/827 [09:07<49:26,  4.30s/it][A
 17%|█▋        | 138/827 [09:14<56:29,  4.92s/it][A
 17%|█▋        | 139/827 [09:21<1:03:05,  5.50s/it][A
 17%|█▋        | 140/827 [09:24<55:57,  4.89s/it]  [A
 17%|█▋        | 141/827 [09:28<52:57,  4.63s/it][A
 17%|█▋        | 142/827 [09:31<48:26,  4.24s/it][A
 17%|█▋        | 143/827 [09:35<45:50,  4.02s/it][A
 17%|█▋        | 144/827 [09:38<42:12,  3.71s/it][A
 18%|█▊        | 145/827 [09:42<43:10,  3.80s/it][A
 18%|█▊        | 146/827 [09:46<43:58,  3.87s/it][A
 18%|█▊        | 147/827 [09:49<42:35,  3.76s/it][A
 18%|█▊        | 148/827 [09:53<40:39,  3.59s/it][A
 18%|█▊        | 149/827 [09:56<39:11,  3.47s/it][A
 18%|█▊        | 150/827 [09:59<36:36,  3.24s/it][A
 18%|█▊        | 151/827 [10:01<33:47,  3.00s/it][A
 18%|█▊        | 152/827 [10:04<35:05,  3.12s/it][A
 19%|█▊        | 153/827 [10:08<35:56,  3.20s/it][A
 19%|█▊        | 154/827 [10:12<38:14,  3.41s/it][A
 19%|█▊        | 155/827 [10:17<44:27,  3.97s/it][A
 19%|█▉        | 156/827 [10:22<46:55,  4.20s/it][A
 19%|█▉        | 157/827 [10:25<44:09,  3.95s/it][A
 19%|█▉        | 158/827 [10:28<41:17,  3.70s/it][A
 19%|█▉        | 159/827 [10:33<43:25,  3.90s/it][A
 19%|█▉        | 160/827 [10:37<44:14,  3.98s/it][A
 19%|█▉        | 161/827 [10:39<39:58,  3.60s/it][A
 20%|█▉        | 162/827 [10:44<41:47,  3.77s/it][A
 20%|█▉        | 163/827 [10:48<42:27,  3.84s/it][A
 20%|█▉        | 164/827 [10:51<41:00,  3.71s/it][A
 20%|█▉        | 165/827 [10:54<38:44,  3.51s/it][A
 20%|██        | 166/827 [11:00<45:42,  4.15s/it][A
 20%|██        | 167/827 [11:08<1:00:49,  5.53s/it][A
 20%|██        | 168/827 [11:14<59:36,  5.43s/it]  [A
 20%|██        | 169/827 [11:17<52:49,  4.82s/it][A
 21%|██        | 170/827 [11:21<50:06,  4.58s/it][A
 21%|██        | 171/827 [11:25<47:20,  4.33s/it][A
 21%|██        | 172/827 [11:27<41:59,  3.85s/it][A
 21%|██        | 173/827 [11:30<38:23,  3.52s/it][A
 21%|██        | 174/827 [11:33<36:25,  3.35s/it][A
 21%|██        | 175/827 [11:37<37:33,  3.46s/it][A
 21%|██▏       | 176/827 [11:40<35:06,  3.24s/it][A
 21%|██▏       | 177/827 [11:42<33:19,  3.08s/it][A
 22%|██▏       | 178/827 [11:46<36:16,  3.35s/it][A
 22%|██▏       | 179/827 [11:52<42:50,  3.97s/it][A
 22%|██▏       | 180/827 [11:56<42:40,  3.96s/it][A
 22%|██▏       | 181/827 [12:00<42:47,  3.97s/it][A
 22%|██▏       | 182/827 [12:05<45:36,  4.24s/it][A
 22%|██▏       | 183/827 [12:08<41:54,  3.90s/it][A
 22%|██▏       | 184/827 [12:12<42:59,  4.01s/it][A
 22%|██▏       | 185/827 [12:17<45:24,  4.24s/it][A
 22%|██▏       | 186/827 [12:20<41:28,  3.88s/it][A
 23%|██▎       | 187/827 [12:23<38:45,  3.63s/it][A
 23%|██▎       | 188/827 [12:27<39:55,  3.75s/it][A
 23%|██▎       | 189/827 [12:32<43:30,  4.09s/it][A
 23%|██▎       | 190/827 [12:36<42:48,  4.03s/it][A
 23%|██▎       | 191/827 [12:40<42:26,  4.00s/it][A
 23%|██▎       | 192/827 [12:44<43:58,  4.15s/it][A
 23%|██▎       | 193/827 [12:48<43:03,  4.07s/it][A
 23%|██▎       | 194/827 [12:51<39:45,  3.77s/it][A
 24%|██▎       | 195/827 [12:54<37:44,  3.58s/it][A
 24%|██▎       | 196/827 [12:57<34:21,  3.27s/it][A
 24%|██▍       | 197/827 [12:59<32:47,  3.12s/it][A
 24%|██▍       | 198/827 [13:03<33:07,  3.16s/it][A
 24%|██▍       | 199/827 [13:06<33:57,  3.24s/it][A
 24%|██▍       | 200/827 [13:11<38:03,  3.64s/it][A
 24%|██▍       | 201/827 [13:16<42:49,  4.10s/it][A
 24%|██▍       | 202/827 [13:21<46:15,  4.44s/it][A
 25%|██▍       | 203/827 [13:27<49:05,  4.72s/it][A
 25%|██▍       | 204/827 [13:32<52:24,  5.05s/it][A
 25%|██▍       | 205/827 [13:36<47:53,  4.62s/it][A
 25%|██▍       | 206/827 [13:39<43:26,  4.20s/it][A
 25%|██▌       | 207/827 [13:42<40:31,  3.92s/it][A
 25%|██▌       | 208/827 [13:46<37:59,  3.68s/it][A
 25%|██▌       | 209/827 [13:49<36:16,  3.52s/it][A
 25%|██▌       | 210/827 [13:52<35:26,  3.45s/it][A
 26%|██▌       | 211/827 [13:57<39:11,  3.82s/it][A
 26%|██▌       | 212/827 [14:01<41:58,  4.10s/it][A
 26%|██▌       | 213/827 [14:07<45:49,  4.48s/it][A
 26%|██▌       | 214/827 [14:10<42:27,  4.16s/it][A
 26%|██▌       | 215/827 [14:13<39:00,  3.82s/it][A
 26%|██▌       | 216/827 [14:17<37:31,  3.68s/it][A
 26%|██▌       | 217/827 [14:20<38:04,  3.75s/it][A
 26%|██▋       | 218/827 [14:24<38:04,  3.75s/it][A
 26%|██▋       | 219/827 [14:27<35:45,  3.53s/it][A
 27%|██▋       | 220/827 [14:32<38:10,  3.77s/it][A
 27%|██▋       | 221/827 [14:37<42:47,  4.24s/it][A
 27%|██▋       | 222/827 [14:40<39:28,  3.92s/it][A
 27%|██▋       | 223/827 [14:43<37:14,  3.70s/it][A
 27%|██▋       | 224/827 [14:46<35:18,  3.51s/it][A
 27%|██▋       | 225/827 [14:49<34:07,  3.40s/it][A
 27%|██▋       | 226/827 [14:52<31:25,  3.14s/it][A
 27%|██▋       | 227/827 [14:57<36:50,  3.68s/it][A
 28%|██▊       | 228/827 [15:02<41:23,  4.15s/it][A
 28%|██▊       | 229/827 [15:06<39:30,  3.96s/it][A
 28%|██▊       | 230/827 [15:09<38:50,  3.90s/it][A
 28%|██▊       | 231/827 [15:13<37:05,  3.73s/it][A
 28%|██▊       | 232/827 [15:16<35:37,  3.59s/it][A
 28%|██▊       | 233/827 [15:21<40:00,  4.04s/it][A
 28%|██▊       | 234/827 [15:27<44:04,  4.46s/it][A
 28%|██▊       | 235/827 [15:32<45:23,  4.60s/it][A
 29%|██▊       | 236/827 [15:36<43:51,  4.45s/it][A
 29%|██▊       | 237/827 [15:42<48:34,  4.94s/it][A
 29%|██▉       | 238/827 [15:47<50:14,  5.12s/it][A
 29%|██▉       | 239/827 [15:52<49:01,  5.00s/it][A
 29%|██▉       | 240/827 [15:58<50:36,  5.17s/it][A
 29%|██▉       | 241/827 [16:04<52:54,  5.42s/it][A
 29%|██▉       | 242/827 [16:08<48:46,  5.00s/it][A
 29%|██▉       | 243/827 [16:12<47:52,  4.92s/it][A
 30%|██▉       | 244/827 [16:18<50:08,  5.16s/it][A
 30%|██▉       | 245/827 [16:22<46:01,  4.75s/it][A
 30%|██▉       | 246/827 [16:26<42:57,  4.44s/it][A
 30%|██▉       | 247/827 [16:30<43:30,  4.50s/it][A
 30%|██▉       | 248/827 [16:37<50:09,  5.20s/it][A
 30%|███       | 249/827 [16:42<48:08,  5.00s/it][A
 30%|███       | 250/827 [16:44<40:33,  4.22s/it][A
 30%|███       | 251/827 [16:48<39:11,  4.08s/it][A
 30%|███       | 252/827 [16:52<40:27,  4.22s/it][A
 31%|███       | 253/827 [16:55<36:33,  3.82s/it][A
 31%|███       | 254/827 [17:01<42:15,  4.42s/it][A
 31%|███       | 255/827 [17:08<48:16,  5.06s/it][A
 31%|███       | 256/827 [17:14<51:34,  5.42s/it][A
 31%|███       | 257/827 [17:18<48:51,  5.14s/it][A
 31%|███       | 258/827 [17:22<46:03,  4.86s/it][A
 31%|███▏      | 259/827 [17:29<49:51,  5.27s/it][A
 31%|███▏      | 260/827 [17:35<52:13,  5.53s/it][A
 32%|███▏      | 261/827 [17:39<47:16,  5.01s/it][A
 32%|███▏      | 262/827 [17:44<49:09,  5.22s/it][A
 32%|███▏      | 263/827 [17:51<51:54,  5.52s/it][A
 32%|███▏      | 264/827 [17:53<43:56,  4.68s/it][A
 32%|███▏      | 265/827 [17:56<37:55,  4.05s/it][A
 32%|███▏      | 266/827 [17:59<35:10,  3.76s/it][A
 32%|███▏      | 267/827 [18:02<33:36,  3.60s/it][A
 32%|███▏      | 268/827 [18:06<33:07,  3.56s/it][A
 33%|███▎      | 269/827 [18:10<36:00,  3.87s/it][A
 33%|███▎      | 270/827 [18:15<38:31,  4.15s/it][A
 33%|███▎      | 271/827 [18:18<36:25,  3.93s/it][A
 33%|███▎      | 272/827 [18:22<34:53,  3.77s/it][A
 33%|███▎      | 273/827 [18:27<40:00,  4.33s/it][A
 33%|███▎      | 274/827 [18:36<50:54,  5.52s/it][A
 33%|███▎      | 275/827 [18:41<49:42,  5.40s/it][A
 33%|███▎      | 276/827 [18:45<45:59,  5.01s/it][A
 33%|███▎      | 277/827 [18:49<42:32,  4.64s/it][A
 34%|███▎      | 278/827 [18:53<40:35,  4.44s/it][A
 34%|███▎      | 279/827 [18:56<37:23,  4.09s/it][A
 34%|███▍      | 280/827 [18:59<34:07,  3.74s/it][A
 34%|███▍      | 281/827 [19:03<35:02,  3.85s/it][A
 34%|███▍      | 282/827 [19:07<34:18,  3.78s/it][A
 34%|███▍      | 283/827 [19:10<33:27,  3.69s/it][A
 34%|███▍      | 284/827 [19:14<33:51,  3.74s/it][A
 34%|███▍      | 285/827 [19:18<35:20,  3.91s/it][A
 35%|███▍      | 286/827 [19:23<37:50,  4.20s/it][A
 35%|███▍      | 287/827 [19:26<33:20,  3.70s/it][A
 35%|███▍      | 288/827 [19:29<31:27,  3.50s/it][A
 35%|███▍      | 289/827 [19:32<30:34,  3.41s/it][A
 35%|███▌      | 290/827 [19:35<30:03,  3.36s/it][A
 35%|███▌      | 291/827 [19:38<28:23,  3.18s/it][A
 35%|███▌      | 292/827 [19:41<29:04,  3.26s/it][A
 35%|███▌      | 293/827 [19:44<28:19,  3.18s/it][A
 36%|███▌      | 294/827 [19:48<29:43,  3.35s/it][A
 36%|███▌      | 295/827 [19:52<30:03,  3.39s/it][A
 36%|███▌      | 296/827 [19:56<32:13,  3.64s/it][A
 36%|███▌      | 297/827 [20:00<32:52,  3.72s/it][A
 36%|███▌      | 298/827 [20:03<31:06,  3.53s/it][A
 36%|███▌      | 299/827 [20:06<30:03,  3.42s/it][A
 36%|███▋      | 300/827 [20:09<28:44,  3.27s/it][A
 36%|███▋      | 301/827 [20:12<27:45,  3.17s/it][A
 37%|███▋      | 302/827 [20:15<28:14,  3.23s/it][A
 37%|███▋      | 303/827 [20:18<25:50,  2.96s/it][A
 37%|███▋      | 304/827 [20:21<26:47,  3.07s/it][A
 37%|███▋      | 305/827 [20:25<29:50,  3.43s/it][A
 37%|███▋      | 306/827 [20:29<31:25,  3.62s/it][A
 37%|███▋      | 307/827 [20:32<30:24,  3.51s/it][A
 37%|███▋      | 308/827 [20:35<29:00,  3.35s/it][A
 37%|███▋      | 309/827 [20:39<29:03,  3.37s/it][A
 37%|███▋      | 310/827 [20:42<27:08,  3.15s/it][A
 38%|███▊      | 311/827 [20:44<25:38,  2.98s/it][A
 38%|███▊      | 312/827 [20:48<26:56,  3.14s/it][A
 38%|███▊      | 313/827 [20:52<29:37,  3.46s/it][A
 38%|███▊      | 314/827 [20:55<29:47,  3.48s/it][A
 38%|███▊      | 315/827 [21:00<32:23,  3.80s/it][A
 38%|███▊      | 316/827 [21:04<34:11,  4.02s/it][A
 38%|███▊      | 317/827 [21:09<36:27,  4.29s/it][A
 38%|███▊      | 318/827 [21:14<36:29,  4.30s/it][A
 39%|███▊      | 319/827 [21:18<35:50,  4.23s/it][A
 39%|███▊      | 320/827 [21:21<33:44,  3.99s/it][A
 39%|███▉      | 321/827 [21:26<35:20,  4.19s/it][A
 39%|███▉      | 322/827 [21:29<33:53,  4.03s/it][A
 39%|███▉      | 323/827 [21:34<34:41,  4.13s/it][A
 39%|███▉      | 324/827 [21:39<36:18,  4.33s/it][A
 39%|███▉      | 325/827 [21:42<33:07,  3.96s/it][A
 39%|███▉      | 326/827 [21:46<32:52,  3.94s/it][A
 40%|███▉      | 327/827 [21:50<34:39,  4.16s/it][A
 40%|███▉      | 328/827 [21:55<35:33,  4.28s/it][A
 40%|███▉      | 329/827 [22:01<41:06,  4.95s/it][A
 40%|███▉      | 330/827 [22:06<39:59,  4.83s/it][A
 40%|████      | 331/827 [22:11<39:33,  4.78s/it][A
 40%|████      | 332/827 [22:15<38:31,  4.67s/it][A
 40%|████      | 333/827 [22:20<39:46,  4.83s/it][A
 40%|████      | 334/827 [22:24<38:19,  4.67s/it][A
 41%|████      | 335/827 [22:28<35:28,  4.33s/it][A
 41%|████      | 336/827 [22:31<32:02,  3.92s/it][A
 41%|████      | 337/827 [22:34<30:11,  3.70s/it][A
 41%|████      | 338/827 [22:37<28:39,  3.52s/it][A
 41%|████      | 339/827 [22:40<27:23,  3.37s/it][A
 41%|████      | 340/827 [22:44<27:35,  3.40s/it][A
 41%|████      | 341/827 [22:48<28:43,  3.55s/it][A
 41%|████▏     | 342/827 [22:51<27:57,  3.46s/it][A
 41%|████▏     | 343/827 [22:54<27:10,  3.37s/it][A
 42%|████▏     | 344/827 [22:59<29:53,  3.71s/it][A
 42%|████▏     | 345/827 [23:04<32:51,  4.09s/it][A
 42%|████▏     | 346/827 [23:07<32:11,  4.02s/it][A
 42%|████▏     | 347/827 [23:11<31:34,  3.95s/it][A
 42%|████▏     | 348/827 [23:14<28:58,  3.63s/it][A
 42%|████▏     | 349/827 [23:18<30:42,  3.85s/it][A
 42%|████▏     | 350/827 [23:23<32:30,  4.09s/it][A
 42%|████▏     | 351/827 [23:26<30:36,  3.86s/it][A
 43%|████▎     | 352/827 [23:31<31:10,  3.94s/it][A
 43%|████▎     | 353/827 [23:34<29:50,  3.78s/it][A
 43%|████▎     | 354/827 [23:37<28:30,  3.62s/it][A
 43%|████▎     | 355/827 [23:42<30:40,  3.90s/it][A
 43%|████▎     | 356/827 [23:47<32:58,  4.20s/it][A
 43%|████▎     | 357/827 [23:50<31:16,  3.99s/it][A
 43%|████▎     | 358/827 [23:53<28:43,  3.67s/it][A
 43%|████▎     | 359/827 [23:56<28:06,  3.60s/it][A
 44%|████▎     | 360/827 [24:02<31:22,  4.03s/it][A
 44%|████▎     | 361/827 [24:05<31:08,  4.01s/it][A
 44%|████▍     | 362/827 [24:08<27:27,  3.54s/it][A
 44%|████▍     | 363/827 [24:12<28:45,  3.72s/it][A
 44%|████▍     | 364/827 [24:16<28:37,  3.71s/it][A
 44%|████▍     | 365/827 [24:19<27:19,  3.55s/it][A
 44%|████▍     | 366/827 [24:24<31:42,  4.13s/it][A
 44%|████▍     | 367/827 [24:29<32:21,  4.22s/it][A
 44%|████▍     | 368/827 [24:32<28:58,  3.79s/it][A
 45%|████▍     | 369/827 [24:35<28:49,  3.78s/it][A
 45%|████▍     | 370/827 [24:39<27:51,  3.66s/it][A
 45%|████▍     | 371/827 [24:43<29:13,  3.85s/it][A
 45%|████▍     | 372/827 [24:47<28:46,  3.79s/it][A
 45%|████▌     | 373/827 [24:52<31:10,  4.12s/it][A
 45%|████▌     | 374/827 [24:58<36:56,  4.89s/it][A
 45%|████▌     | 375/827 [25:07<44:39,  5.93s/it][A
 45%|████▌     | 376/827 [25:14<47:15,  6.29s/it][A
 46%|████▌     | 377/827 [25:18<43:27,  5.79s/it][A
 46%|████▌     | 378/827 [25:24<43:40,  5.84s/it][A
 46%|████▌     | 379/827 [25:33<50:35,  6.78s/it][A
 46%|████▌     | 380/827 [25:39<48:18,  6.49s/it][A
 46%|████▌     | 381/827 [25:43<42:06,  5.66s/it][A
 46%|████▌     | 382/827 [25:48<40:02,  5.40s/it][A
 46%|████▋     | 383/827 [25:52<37:06,  5.01s/it][A
 46%|████▋     | 384/827 [25:55<33:59,  4.60s/it][A
 47%|████▋     | 385/827 [25:59<30:47,  4.18s/it][A
 47%|████▋     | 386/827 [26:02<28:37,  3.89s/it][A
 47%|████▋     | 387/827 [26:06<28:12,  3.85s/it][A
 47%|████▋     | 388/827 [26:10<28:54,  3.95s/it][A
 47%|████▋     | 389/827 [26:13<28:10,  3.86s/it][A
 47%|████▋     | 390/827 [26:17<27:16,  3.74s/it][A
 47%|████▋     | 391/827 [26:20<26:35,  3.66s/it][A
 47%|████▋     | 392/827 [26:24<26:03,  3.59s/it][A
 48%|████▊     | 393/827 [26:28<27:35,  3.82s/it][A
 48%|████▊     | 394/827 [26:33<30:51,  4.28s/it][A
 48%|████▊     | 395/827 [26:38<31:24,  4.36s/it][A
 48%|████▊     | 396/827 [26:43<31:55,  4.44s/it][A
 48%|████▊     | 397/827 [26:46<28:53,  4.03s/it][A
 48%|████▊     | 398/827 [26:49<27:26,  3.84s/it][A
 48%|████▊     | 399/827 [26:52<26:11,  3.67s/it][A
 48%|████▊     | 400/827 [26:57<28:01,  3.94s/it][A
 48%|████▊     | 401/827 [27:02<29:54,  4.21s/it][A
 49%|████▊     | 402/827 [27:06<29:26,  4.16s/it][A
 49%|████▊     | 403/827 [27:10<30:25,  4.31s/it][A
 49%|████▉     | 404/827 [27:13<26:42,  3.79s/it][A
 49%|████▉     | 405/827 [27:16<25:43,  3.66s/it][A
 49%|████▉     | 406/827 [27:20<24:49,  3.54s/it][A
 49%|████▉     | 407/827 [27:22<23:00,  3.29s/it][A
 49%|████▉     | 408/827 [27:25<22:07,  3.17s/it][A
 49%|████▉     | 409/827 [27:28<21:53,  3.14s/it][A
 50%|████▉     | 410/827 [27:31<20:36,  2.97s/it][A
 50%|████▉     | 411/827 [27:34<20:34,  2.97s/it][A
 50%|████▉     | 412/827 [27:37<20:42,  2.99s/it][A
 50%|████▉     | 413/827 [27:42<24:05,  3.49s/it][A
 50%|█████     | 414/827 [27:47<27:11,  3.95s/it][A
 50%|█████     | 415/827 [27:49<24:44,  3.60s/it][A
 50%|█████     | 416/827 [27:54<25:54,  3.78s/it][A
 50%|█████     | 417/827 [27:59<28:21,  4.15s/it][A
 51%|█████     | 418/827 [28:04<31:02,  4.55s/it][A
 51%|█████     | 419/827 [28:08<29:37,  4.36s/it][A
 51%|█████     | 420/827 [28:12<29:05,  4.29s/it][A
 51%|█████     | 421/827 [28:16<27:46,  4.10s/it][A
 51%|█████     | 422/827 [28:19<25:38,  3.80s/it][A
 51%|█████     | 423/827 [28:22<24:04,  3.58s/it][A
 51%|█████▏    | 424/827 [28:26<24:36,  3.66s/it][A
 51%|█████▏    | 425/827 [28:31<26:52,  4.01s/it][A
 52%|█████▏    | 426/827 [28:36<28:40,  4.29s/it][A
 52%|█████▏    | 427/827 [28:40<29:33,  4.43s/it][A
 52%|█████▏    | 428/827 [28:45<30:04,  4.52s/it][A
 52%|█████▏    | 429/827 [28:49<28:10,  4.25s/it][A
 52%|█████▏    | 430/827 [28:53<27:25,  4.15s/it][A
 52%|█████▏    | 431/827 [28:57<27:02,  4.10s/it][A
 52%|█████▏    | 432/827 [29:03<30:39,  4.66s/it][A
 52%|█████▏    | 433/827 [29:09<34:02,  5.18s/it][A
 52%|█████▏    | 434/827 [29:14<33:49,  5.17s/it][A
 53%|█████▎    | 435/827 [29:17<29:49,  4.57s/it][A
 53%|█████▎    | 436/827 [29:22<29:29,  4.52s/it][A
 53%|█████▎    | 437/827 [29:26<29:43,  4.57s/it][A
 53%|█████▎    | 438/827 [29:30<27:15,  4.21s/it][A
 53%|█████▎    | 439/827 [29:33<25:58,  4.02s/it][A
 53%|█████▎    | 440/827 [29:37<25:41,  3.98s/it][A
 53%|█████▎    | 441/827 [29:40<23:26,  3.64s/it][A
 53%|█████▎    | 442/827 [29:43<21:51,  3.41s/it][A
 54%|█████▎    | 443/827 [29:46<21:06,  3.30s/it][A
 54%|█████▎    | 444/827 [29:50<21:51,  3.42s/it][A
 54%|█████▍    | 445/827 [29:55<25:31,  4.01s/it][A
 54%|█████▍    | 446/827 [29:58<24:09,  3.80s/it][A
 54%|█████▍    | 447/827 [30:03<25:29,  4.02s/it][A
 54%|█████▍    | 448/827 [30:08<28:05,  4.45s/it][A
 54%|█████▍    | 449/827 [30:16<34:34,  5.49s/it][A
 54%|█████▍    | 450/827 [30:22<34:33,  5.50s/it][A
 55%|█████▍    | 451/827 [30:26<31:26,  5.02s/it][A
 55%|█████▍    | 452/827 [30:31<31:24,  5.02s/it][A
 55%|█████▍    | 453/827 [30:36<32:06,  5.15s/it][A
 55%|█████▍    | 454/827 [30:39<28:39,  4.61s/it][A
 55%|█████▌    | 455/827 [30:44<28:20,  4.57s/it][A
 55%|█████▌    | 456/827 [30:50<31:15,  5.06s/it][A
 55%|█████▌    | 457/827 [30:55<30:26,  4.94s/it][A
 55%|█████▌    | 458/827 [30:59<28:12,  4.59s/it][A
 56%|█████▌    | 459/827 [31:03<27:37,  4.50s/it][A
 56%|█████▌    | 460/827 [31:08<29:28,  4.82s/it][A
 56%|█████▌    | 461/827 [31:16<33:50,  5.55s/it][A
 56%|█████▌    | 462/827 [31:20<31:40,  5.21s/it][A
 56%|█████▌    | 463/827 [31:23<27:36,  4.55s/it][A
 56%|█████▌    | 464/827 [31:28<28:12,  4.66s/it][A
 56%|█████▌    | 465/827 [31:32<27:18,  4.53s/it][A
 56%|█████▋    | 466/827 [31:36<25:37,  4.26s/it][A
 56%|█████▋    | 467/827 [31:40<25:17,  4.21s/it][A
 57%|█████▋    | 468/827 [31:45<25:49,  4.32s/it][A
 57%|█████▋    | 469/827 [31:49<26:37,  4.46s/it][A
 57%|█████▋    | 470/827 [31:53<24:28,  4.11s/it][A
 57%|█████▋    | 471/827 [31:57<24:26,  4.12s/it][A
 57%|█████▋    | 472/827 [32:03<28:18,  4.78s/it][A
 57%|█████▋    | 473/827 [32:07<27:14,  4.62s/it][A
 57%|█████▋    | 474/827 [32:11<26:08,  4.44s/it][A
 57%|█████▋    | 475/827 [32:14<23:29,  4.00s/it][A
 58%|█████▊    | 476/827 [32:17<20:36,  3.52s/it][A
 58%|█████▊    | 477/827 [32:19<18:44,  3.21s/it][A
 58%|█████▊    | 478/827 [32:23<20:13,  3.48s/it][A
 58%|█████▊    | 479/827 [32:28<22:41,  3.91s/it][A
 58%|█████▊    | 480/827 [32:33<23:49,  4.12s/it][A
 58%|█████▊    | 481/827 [32:38<24:49,  4.31s/it][A
 58%|█████▊    | 482/827 [32:40<22:10,  3.86s/it][A
 58%|█████▊    | 483/827 [32:43<19:32,  3.41s/it][A
 59%|█████▊    | 484/827 [32:46<18:56,  3.31s/it][A
 59%|█████▊    | 485/827 [32:49<18:10,  3.19s/it][A
 59%|█████▉    | 486/827 [32:52<18:41,  3.29s/it][A
 59%|█████▉    | 487/827 [32:56<18:56,  3.34s/it][A
 59%|█████▉    | 488/827 [33:01<21:17,  3.77s/it][A
 59%|█████▉    | 489/827 [33:05<22:43,  4.03s/it][A
 59%|█████▉    | 490/827 [33:10<23:25,  4.17s/it][A
 59%|█████▉    | 491/827 [33:13<21:39,  3.87s/it][A
 59%|█████▉    | 492/827 [33:16<20:56,  3.75s/it][A
 60%|█████▉    | 493/827 [33:20<20:13,  3.63s/it][A
 60%|█████▉    | 494/827 [33:23<19:54,  3.59s/it][A
 60%|█████▉    | 495/827 [33:27<19:53,  3.59s/it][A
 60%|█████▉    | 496/827 [33:31<20:07,  3.65s/it][A
 60%|██████    | 497/827 [33:33<18:33,  3.37s/it][A
 60%|██████    | 498/827 [33:36<18:12,  3.32s/it][A
 60%|██████    | 499/827 [33:41<19:51,  3.63s/it][A
 60%|██████    | 500/827 [33:44<19:00,  3.49s/it][A
 61%|██████    | 501/827 [33:47<17:58,  3.31s/it][A
 61%|██████    | 502/827 [33:52<20:12,  3.73s/it][A
 61%|██████    | 503/827 [33:56<21:37,  4.00s/it][A
 61%|██████    | 504/827 [33:59<20:16,  3.77s/it][A
 61%|██████    | 505/827 [34:03<20:12,  3.77s/it][A
 61%|██████    | 506/827 [34:07<20:06,  3.76s/it][A
 61%|██████▏   | 507/827 [34:12<21:58,  4.12s/it][A
 61%|██████▏   | 508/827 [34:18<25:10,  4.74s/it][A
 62%|██████▏   | 509/827 [34:21<21:49,  4.12s/it][A
 62%|██████▏   | 510/827 [34:24<20:57,  3.97s/it][A
 62%|██████▏   | 511/827 [34:27<19:21,  3.68s/it][A
 62%|██████▏   | 512/827 [34:30<17:59,  3.43s/it][A
 62%|██████▏   | 513/827 [34:34<18:37,  3.56s/it][A
 62%|██████▏   | 514/827 [34:38<18:33,  3.56s/it][A
 62%|██████▏   | 515/827 [34:41<18:10,  3.50s/it][A
 62%|██████▏   | 516/827 [34:45<18:45,  3.62s/it][A
 63%|██████▎   | 517/827 [34:48<18:15,  3.53s/it][A
 63%|██████▎   | 518/827 [34:51<17:10,  3.34s/it][A
 63%|██████▎   | 519/827 [34:54<16:29,  3.21s/it][A
 63%|██████▎   | 520/827 [34:57<16:27,  3.22s/it][A
 63%|██████▎   | 521/827 [35:01<17:50,  3.50s/it][A
 63%|██████▎   | 522/827 [35:07<20:33,  4.05s/it][A
 63%|██████▎   | 523/827 [35:10<18:54,  3.73s/it][A
 63%|██████▎   | 524/827 [35:14<19:33,  3.87s/it][A
 63%|██████▎   | 525/827 [35:20<22:42,  4.51s/it][A
 64%|██████▎   | 526/827 [35:26<24:22,  4.86s/it][A
 64%|██████▎   | 527/827 [35:30<23:31,  4.71s/it][A
 64%|██████▍   | 528/827 [35:35<23:16,  4.67s/it][A
 64%|██████▍   | 529/827 [35:39<23:31,  4.74s/it][A
 64%|██████▍   | 530/827 [35:44<23:05,  4.66s/it][A
 64%|██████▍   | 531/827 [35:47<20:39,  4.19s/it][A
 64%|██████▍   | 532/827 [35:50<19:29,  3.96s/it][A
 64%|██████▍   | 533/827 [35:56<21:43,  4.43s/it][A
 65%|██████▍   | 534/827 [36:01<23:11,  4.75s/it][A
 65%|██████▍   | 535/827 [36:06<23:27,  4.82s/it][A
 65%|██████▍   | 536/827 [36:10<21:24,  4.41s/it][A
 65%|██████▍   | 537/827 [36:12<18:29,  3.83s/it][A
 65%|██████▌   | 538/827 [36:15<17:19,  3.60s/it][A
 65%|██████▌   | 539/827 [36:18<16:28,  3.43s/it][A
 65%|██████▌   | 540/827 [36:22<16:36,  3.47s/it][A
 65%|██████▌   | 541/827 [36:26<17:23,  3.65s/it][A
 66%|██████▌   | 542/827 [36:30<17:32,  3.69s/it][A
 66%|██████▌   | 543/827 [36:34<18:31,  3.91s/it][A
 66%|██████▌   | 544/827 [36:39<19:39,  4.17s/it][A
 66%|██████▌   | 545/827 [36:43<19:28,  4.14s/it][A
 66%|██████▌   | 546/827 [36:47<19:36,  4.19s/it][A
 66%|██████▌   | 547/827 [36:52<19:37,  4.20s/it][A
 66%|██████▋   | 548/827 [36:55<17:36,  3.79s/it][A
 66%|██████▋   | 549/827 [36:57<16:16,  3.51s/it][A
 67%|██████▋   | 550/827 [37:01<16:05,  3.48s/it][A
 67%|██████▋   | 551/827 [37:05<16:46,  3.65s/it][A
 67%|██████▋   | 552/827 [37:08<16:33,  3.61s/it][A
 67%|██████▋   | 553/827 [37:12<16:23,  3.59s/it][A
 67%|██████▋   | 554/827 [37:14<14:58,  3.29s/it][A
 67%|██████▋   | 555/827 [37:18<14:51,  3.28s/it][A
 67%|██████▋   | 556/827 [37:21<14:48,  3.28s/it][A
 67%|██████▋   | 557/827 [37:24<14:02,  3.12s/it][A
 67%|██████▋   | 558/827 [37:28<15:29,  3.46s/it][A
 68%|██████▊   | 559/827 [37:32<16:05,  3.60s/it][A
 68%|██████▊   | 560/827 [37:35<15:26,  3.47s/it][A
 68%|██████▊   | 561/827 [37:38<15:06,  3.41s/it][A
 68%|██████▊   | 562/827 [37:42<15:29,  3.51s/it][A
 68%|██████▊   | 563/827 [37:46<15:22,  3.49s/it][A
 68%|██████▊   | 564/827 [37:49<15:11,  3.47s/it][A
 68%|██████▊   | 565/827 [37:54<16:30,  3.78s/it][A
 68%|██████▊   | 566/827 [37:58<17:25,  4.01s/it][A
 69%|██████▊   | 567/827 [38:01<16:11,  3.74s/it][A
 69%|██████▊   | 568/827 [38:05<15:52,  3.68s/it][A
 69%|██████▉   | 569/827 [38:08<15:30,  3.61s/it][A
 69%|██████▉   | 570/827 [38:12<15:33,  3.63s/it][A
 69%|██████▉   | 571/827 [38:17<17:18,  4.06s/it][A
 69%|██████▉   | 572/827 [38:22<18:28,  4.35s/it][A
 69%|██████▉   | 573/827 [38:27<19:39,  4.64s/it][A
 69%|██████▉   | 574/827 [38:32<19:13,  4.56s/it][A
 70%|██████▉   | 575/827 [38:37<20:23,  4.86s/it][A
 70%|██████▉   | 576/827 [38:41<19:23,  4.63s/it][A
 70%|██████▉   | 577/827 [38:44<17:27,  4.19s/it][A
 70%|██████▉   | 578/827 [38:48<16:48,  4.05s/it][A
 70%|███████   | 579/827 [38:51<15:47,  3.82s/it][A
 70%|███████   | 580/827 [38:56<16:45,  4.07s/it][A
 70%|███████   | 581/827 [39:03<19:57,  4.87s/it][A
 70%|███████   | 582/827 [39:07<19:14,  4.71s/it][A
 70%|███████   | 583/827 [39:10<17:21,  4.27s/it][A
 71%|███████   | 584/827 [39:14<15:58,  3.94s/it][A
 71%|███████   | 585/827 [39:16<14:39,  3.64s/it][A
 71%|███████   | 586/827 [39:21<15:15,  3.80s/it][A
 71%|███████   | 587/827 [39:28<19:06,  4.78s/it][A
 71%|███████   | 588/827 [39:35<21:48,  5.47s/it][A
 71%|███████   | 589/827 [39:40<21:02,  5.30s/it][A
 71%|███████▏  | 590/827 [39:46<22:12,  5.62s/it][A
 71%|███████▏  | 591/827 [39:53<23:36,  6.00s/it][A
 72%|███████▏  | 592/827 [39:56<20:17,  5.18s/it][A
 72%|███████▏  | 593/827 [40:00<18:11,  4.67s/it][A
 72%|███████▏  | 594/827 [40:03<16:18,  4.20s/it][A
 72%|███████▏  | 595/827 [40:06<15:11,  3.93s/it][A
 72%|███████▏  | 596/827 [40:13<18:13,  4.73s/it][A
 72%|███████▏  | 597/827 [40:19<19:41,  5.14s/it][A
 72%|███████▏  | 598/827 [40:22<17:09,  4.50s/it][A
 72%|███████▏  | 599/827 [40:25<15:45,  4.15s/it][A
 73%|███████▎  | 600/827 [40:29<15:42,  4.15s/it][A
 73%|███████▎  | 601/827 [40:33<15:22,  4.08s/it][A
 73%|███████▎  | 602/827 [40:37<14:57,  3.99s/it][A
 73%|███████▎  | 603/827 [40:40<14:04,  3.77s/it][A
 73%|███████▎  | 604/827 [40:44<13:47,  3.71s/it][A
 73%|███████▎  | 605/827 [40:47<13:21,  3.61s/it][A
 73%|███████▎  | 606/827 [40:51<13:05,  3.56s/it][A
 73%|███████▎  | 607/827 [40:55<13:27,  3.67s/it][A
 74%|███████▎  | 608/827 [40:58<12:54,  3.54s/it][A
 74%|███████▎  | 609/827 [41:04<15:29,  4.26s/it][A
 74%|███████▍  | 610/827 [41:10<17:48,  4.92s/it][A
 74%|███████▍  | 611/827 [41:14<16:20,  4.54s/it][A
 74%|███████▍  | 612/827 [41:18<15:45,  4.40s/it][A
 74%|███████▍  | 613/827 [41:22<15:47,  4.43s/it][A
 74%|███████▍  | 614/827 [41:26<15:18,  4.31s/it][A
 74%|███████▍  | 615/827 [41:30<14:24,  4.08s/it][A
 74%|███████▍  | 616/827 [41:33<13:27,  3.83s/it][A
 75%|███████▍  | 617/827 [41:38<14:19,  4.09s/it][A
 75%|███████▍  | 618/827 [41:44<16:45,  4.81s/it][A
 75%|███████▍  | 619/827 [41:47<14:45,  4.26s/it][A
 75%|███████▍  | 620/827 [41:51<14:19,  4.15s/it][A
 75%|███████▌  | 621/827 [41:54<12:46,  3.72s/it][A
 75%|███████▌  | 622/827 [41:57<11:50,  3.47s/it][A
 75%|███████▌  | 623/827 [42:00<11:42,  3.44s/it][A
 75%|███████▌  | 624/827 [42:05<12:56,  3.83s/it][A
 76%|███████▌  | 625/827 [42:08<12:27,  3.70s/it][A
 76%|███████▌  | 626/827 [42:12<12:10,  3.63s/it][A
 76%|███████▌  | 627/827 [42:15<11:55,  3.58s/it][A
 76%|███████▌  | 628/827 [42:18<11:06,  3.35s/it][A
 76%|███████▌  | 629/827 [42:21<10:48,  3.28s/it][A
 76%|███████▌  | 630/827 [42:24<10:36,  3.23s/it][A
 76%|███████▋  | 631/827 [42:30<12:50,  3.93s/it][A
 76%|███████▋  | 632/827 [42:35<14:01,  4.31s/it][A
 77%|███████▋  | 633/827 [42:41<15:02,  4.65s/it][A
 77%|███████▋  | 634/827 [42:47<16:37,  5.17s/it][A
 77%|███████▋  | 635/827 [42:50<14:27,  4.52s/it][A
 77%|███████▋  | 636/827 [42:54<13:45,  4.32s/it][A
 77%|███████▋  | 637/827 [42:59<14:04,  4.45s/it][A
 77%|███████▋  | 638/827 [43:03<14:06,  4.48s/it][A
 77%|███████▋  | 639/827 [43:06<12:43,  4.06s/it][A
 77%|███████▋  | 640/827 [43:09<11:43,  3.76s/it][A
 78%|███████▊  | 641/827 [43:13<12:02,  3.89s/it][A
 78%|███████▊  | 642/827 [43:17<11:53,  3.86s/it][A
 78%|███████▊  | 643/827 [43:22<12:44,  4.16s/it][A
 78%|███████▊  | 644/827 [43:27<13:04,  4.29s/it][A
 78%|███████▊  | 645/827 [43:30<12:11,  4.02s/it][A
 78%|███████▊  | 646/827 [43:33<11:10,  3.71s/it][A
 78%|███████▊  | 647/827 [43:36<10:50,  3.61s/it][A
 78%|███████▊  | 648/827 [43:40<10:57,  3.67s/it][A
 78%|███████▊  | 649/827 [43:44<10:48,  3.65s/it][A
 79%|███████▊  | 650/827 [43:47<10:13,  3.47s/it][A
 79%|███████▊  | 651/827 [43:50<09:58,  3.40s/it][A
 79%|███████▉  | 652/827 [43:53<09:45,  3.35s/it][A
 79%|███████▉  | 653/827 [43:57<09:57,  3.44s/it][A
 79%|███████▉  | 654/827 [44:02<11:25,  3.96s/it][A
 79%|███████▉  | 655/827 [44:08<12:39,  4.41s/it][A
 79%|███████▉  | 656/827 [44:11<11:14,  3.95s/it][A
 79%|███████▉  | 657/827 [44:13<10:19,  3.65s/it][A
 80%|███████▉  | 658/827 [44:18<11:24,  4.05s/it][A
 80%|███████▉  | 659/827 [44:27<14:51,  5.31s/it][A
 80%|███████▉  | 660/827 [44:32<14:43,  5.29s/it][A
 80%|███████▉  | 661/827 [44:38<14:52,  5.37s/it][A
 80%|████████  | 662/827 [44:41<13:37,  4.95s/it][A
 80%|████████  | 663/827 [44:45<12:44,  4.66s/it][A
 80%|████████  | 664/827 [44:50<12:55,  4.76s/it][A
 80%|████████  | 665/827 [44:54<12:11,  4.52s/it][A
 81%|████████  | 666/827 [44:58<11:46,  4.39s/it][A
 81%|████████  | 667/827 [45:02<10:55,  4.09s/it][A
 81%|████████  | 668/827 [45:06<10:39,  4.02s/it][A
 81%|████████  | 669/827 [45:09<09:38,  3.66s/it][A
 81%|████████  | 670/827 [45:12<09:06,  3.48s/it][A
 81%|████████  | 671/827 [45:16<09:23,  3.61s/it][A
 81%|████████▏ | 672/827 [45:19<09:14,  3.58s/it][A
 81%|████████▏ | 673/827 [45:22<08:47,  3.42s/it][A
 81%|████████▏ | 674/827 [45:25<08:41,  3.41s/it][A
 82%|████████▏ | 675/827 [45:29<08:46,  3.47s/it][A
 82%|████████▏ | 676/827 [45:34<09:28,  3.77s/it][A
 82%|████████▏ | 677/827 [45:39<10:26,  4.18s/it][A
 82%|████████▏ | 678/827 [45:43<10:41,  4.31s/it][A
 82%|████████▏ | 679/827 [45:48<10:43,  4.35s/it][A
 82%|████████▏ | 680/827 [45:51<09:46,  3.99s/it][A
 82%|████████▏ | 681/827 [45:56<10:16,  4.22s/it][A
 82%|████████▏ | 682/827 [46:01<10:46,  4.46s/it][A
 83%|████████▎ | 683/827 [46:03<09:29,  3.95s/it][A
 83%|████████▎ | 684/827 [46:06<08:46,  3.68s/it][A
 83%|████████▎ | 685/827 [46:09<08:03,  3.41s/it][A
 83%|████████▎ | 686/827 [46:15<09:25,  4.01s/it][A
 83%|████████▎ | 687/827 [46:22<11:22,  4.87s/it][A
 83%|████████▎ | 688/827 [46:27<11:30,  4.97s/it][A
 83%|████████▎ | 689/827 [46:32<11:18,  4.92s/it][A
 83%|████████▎ | 690/827 [46:35<10:00,  4.38s/it][A
 84%|████████▎ | 691/827 [46:38<09:06,  4.02s/it][A
 84%|████████▎ | 692/827 [46:42<09:01,  4.01s/it][A
 84%|████████▍ | 693/827 [46:45<08:28,  3.79s/it][A
 84%|████████▍ | 694/827 [46:48<08:03,  3.64s/it][A
 84%|████████▍ | 695/827 [46:53<08:26,  3.84s/it][A
 84%|████████▍ | 696/827 [46:57<08:58,  4.11s/it][A
 84%|████████▍ | 697/827 [47:01<08:27,  3.91s/it][A
 84%|████████▍ | 698/827 [47:03<07:31,  3.50s/it][A
 85%|████████▍ | 699/827 [47:07<07:16,  3.41s/it][A
 85%|████████▍ | 700/827 [47:13<09:23,  4.44s/it][A
 85%|████████▍ | 701/827 [47:20<10:38,  5.06s/it][A
 85%|████████▍ | 702/827 [47:25<10:19,  4.95s/it][A
 85%|████████▌ | 703/827 [47:29<09:33,  4.63s/it][A
 85%|████████▌ | 704/827 [47:32<08:33,  4.17s/it][A
 85%|████████▌ | 705/827 [47:34<07:34,  3.72s/it][A
 85%|████████▌ | 706/827 [47:37<06:53,  3.41s/it][A
 85%|████████▌ | 707/827 [47:41<07:02,  3.52s/it][A
 86%|████████▌ | 708/827 [47:44<06:51,  3.46s/it][A
 86%|████████▌ | 709/827 [47:48<07:04,  3.60s/it][A
 86%|████████▌ | 710/827 [47:52<07:00,  3.59s/it][A
 86%|████████▌ | 711/827 [47:55<07:00,  3.62s/it][A
 86%|████████▌ | 712/827 [47:58<06:39,  3.47s/it][A
 86%|████████▌ | 713/827 [48:02<06:36,  3.48s/it][A
 86%|████████▋ | 714/827 [48:06<06:43,  3.58s/it][A
 86%|████████▋ | 715/827 [48:10<07:05,  3.80s/it][A
 87%|████████▋ | 716/827 [48:14<06:54,  3.74s/it][A
 87%|████████▋ | 717/827 [48:19<07:35,  4.14s/it][A
 87%|████████▋ | 718/827 [48:24<07:57,  4.38s/it][A
 87%|████████▋ | 719/827 [48:27<07:14,  4.02s/it][A
 87%|████████▋ | 720/827 [48:30<06:52,  3.86s/it][A
 87%|████████▋ | 721/827 [48:34<06:27,  3.65s/it][A
 87%|████████▋ | 722/827 [48:37<06:23,  3.66s/it][A
 87%|████████▋ | 723/827 [48:41<06:13,  3.59s/it][A
 88%|████████▊ | 724/827 [48:43<05:37,  3.28s/it][A
 88%|████████▊ | 725/827 [48:48<06:18,  3.71s/it][A
 88%|████████▊ | 726/827 [48:53<07:05,  4.21s/it][A
 88%|████████▊ | 727/827 [48:57<06:54,  4.14s/it][A
 88%|████████▊ | 728/827 [49:00<06:19,  3.83s/it][A
 88%|████████▊ | 729/827 [49:03<05:45,  3.52s/it][A
 88%|████████▊ | 730/827 [49:07<05:52,  3.64s/it][A
 88%|████████▊ | 731/827 [49:12<06:34,  4.11s/it][A
 89%|████████▊ | 732/827 [49:17<06:39,  4.21s/it][A
 89%|████████▊ | 733/827 [49:21<06:37,  4.22s/it][A
 89%|████████▉ | 734/827 [49:24<05:46,  3.72s/it][A
 89%|████████▉ | 735/827 [49:27<05:47,  3.78s/it][A
 89%|████████▉ | 736/827 [49:33<06:33,  4.33s/it][A
 89%|████████▉ | 737/827 [49:38<06:43,  4.49s/it][A
 89%|████████▉ | 738/827 [49:41<06:00,  4.05s/it][A
 89%|████████▉ | 739/827 [49:46<06:25,  4.38s/it][A
 89%|████████▉ | 740/827 [49:52<06:59,  4.82s/it][A
 90%|████████▉ | 741/827 [49:56<06:27,  4.50s/it][A
 90%|████████▉ | 742/827 [50:00<06:17,  4.44s/it][A
 90%|████████▉ | 743/827 [50:06<06:41,  4.78s/it][A
 90%|████████▉ | 744/827 [50:10<06:17,  4.54s/it][A
 90%|█████████ | 745/827 [50:14<05:59,  4.38s/it][A
 90%|█████████ | 746/827 [50:19<06:15,  4.64s/it][A
 90%|█████████ | 747/827 [50:24<06:23,  4.79s/it][A
 90%|█████████ | 748/827 [50:28<05:49,  4.43s/it][A
 91%|█████████ | 749/827 [50:32<05:48,  4.47s/it][A
 91%|█████████ | 750/827 [50:36<05:41,  4.44s/it][A
 91%|█████████ | 751/827 [50:40<05:08,  4.06s/it][A
 91%|█████████ | 752/827 [50:43<04:39,  3.73s/it][A
 91%|█████████ | 753/827 [50:45<04:16,  3.46s/it][A
 91%|█████████ | 754/827 [50:48<04:01,  3.31s/it][A
 91%|█████████▏| 755/827 [50:52<04:15,  3.55s/it][A
 91%|█████████▏| 756/827 [50:57<04:41,  3.97s/it][A
 92%|█████████▏| 757/827 [51:02<04:41,  4.02s/it][A
 92%|█████████▏| 758/827 [51:06<04:41,  4.08s/it][A
 92%|█████████▏| 759/827 [51:09<04:14,  3.74s/it][A
 92%|█████████▏| 760/827 [51:15<04:54,  4.39s/it][A
 92%|█████████▏| 761/827 [51:21<05:36,  5.10s/it][A
 92%|█████████▏| 762/827 [51:27<05:43,  5.29s/it][A
 92%|█████████▏| 763/827 [51:33<05:52,  5.51s/it][A
 92%|█████████▏| 764/827 [51:36<05:05,  4.85s/it][A
 93%|█████████▎| 765/827 [51:39<04:25,  4.28s/it][A
 93%|█████████▎| 766/827 [51:42<03:53,  3.83s/it][A
 93%|█████████▎| 767/827 [51:46<03:43,  3.72s/it][A
 93%|█████████▎| 768/827 [51:49<03:38,  3.70s/it][A
 93%|█████████▎| 769/827 [51:54<03:46,  3.91s/it][A
 93%|█████████▎| 770/827 [51:57<03:34,  3.76s/it][A
 93%|█████████▎| 771/827 [52:01<03:30,  3.76s/it][A
 93%|█████████▎| 772/827 [52:05<03:25,  3.73s/it][A
 93%|█████████▎| 773/827 [52:09<03:25,  3.80s/it][A
 94%|█████████▎| 774/827 [52:12<03:11,  3.62s/it][A
 94%|█████████▎| 775/827 [52:16<03:12,  3.71s/it][A
 94%|█████████▍| 776/827 [52:19<03:03,  3.60s/it][A
 94%|█████████▍| 777/827 [52:22<02:54,  3.49s/it][A
 94%|█████████▍| 778/827 [52:26<02:53,  3.54s/it][A
 94%|█████████▍| 779/827 [52:29<02:44,  3.43s/it][A
 94%|█████████▍| 780/827 [52:32<02:37,  3.35s/it][A
 94%|█████████▍| 781/827 [52:35<02:28,  3.23s/it][A
 95%|█████████▍| 782/827 [52:38<02:20,  3.13s/it][A
 95%|█████████▍| 783/827 [52:41<02:21,  3.21s/it][A
 95%|█████████▍| 784/827 [52:45<02:19,  3.25s/it][A
 95%|█████████▍| 785/827 [52:49<02:22,  3.39s/it][A
 95%|█████████▌| 786/827 [52:53<02:26,  3.58s/it][A
 95%|█████████▌| 787/827 [52:58<02:43,  4.09s/it][A
 95%|█████████▌| 788/827 [53:02<02:39,  4.09s/it][A
 95%|█████████▌| 789/827 [53:07<02:42,  4.28s/it][A
 96%|█████████▌| 790/827 [53:10<02:31,  4.08s/it][A
 96%|█████████▌| 791/827 [53:13<02:14,  3.75s/it][A
 96%|█████████▌| 792/827 [53:17<02:06,  3.61s/it][A
 96%|█████████▌| 793/827 [53:21<02:11,  3.86s/it][A
 96%|█████████▌| 794/827 [53:26<02:21,  4.28s/it][A
 96%|█████████▌| 795/827 [53:32<02:31,  4.72s/it][A
 96%|█████████▋| 796/827 [53:37<02:27,  4.77s/it][A
 96%|█████████▋| 797/827 [53:40<02:09,  4.30s/it][A
 96%|█████████▋| 798/827 [53:43<01:52,  3.86s/it][A
 97%|█████████▋| 799/827 [53:45<01:36,  3.43s/it][A
 97%|█████████▋| 800/827 [53:52<01:55,  4.28s/it][A
 97%|█████████▋| 801/827 [53:59<02:12,  5.11s/it][A
 97%|█████████▋| 802/827 [54:03<02:04,  4.99s/it][A
 97%|█████████▋| 803/827 [54:09<02:05,  5.23s/it][A
 97%|█████████▋| 804/827 [54:14<01:58,  5.17s/it][A
 97%|█████████▋| 805/827 [54:17<01:39,  4.51s/it][A
 97%|█████████▋| 806/827 [54:20<01:24,  4.01s/it][A
 98%|█████████▊| 807/827 [54:23<01:15,  3.79s/it][A
 98%|█████████▊| 808/827 [54:27<01:10,  3.73s/it][A
 98%|█████████▊| 809/827 [54:30<01:03,  3.55s/it][A
 98%|█████████▊| 810/827 [54:33<00:59,  3.51s/it][A
 98%|█████████▊| 811/827 [54:37<00:55,  3.49s/it][A
 98%|█████████▊| 812/827 [54:41<00:56,  3.75s/it][A
 98%|█████████▊| 813/827 [54:47<00:59,  4.23s/it][A
 98%|█████████▊| 814/827 [54:52<00:59,  4.58s/it][A
 99%|█████████▊| 815/827 [54:56<00:51,  4.32s/it][A
 99%|█████████▊| 816/827 [55:00<00:46,  4.19s/it][A
 99%|█████████▉| 817/827 [55:03<00:38,  3.84s/it][A
 99%|█████████▉| 818/827 [55:08<00:38,  4.23s/it][A
 99%|█████████▉| 819/827 [55:13<00:35,  4.43s/it][A
 99%|█████████▉| 820/827 [55:17<00:31,  4.51s/it][A
 99%|█████████▉| 821/827 [55:21<00:25,  4.31s/it][A
 99%|█████████▉| 822/827 [55:25<00:20,  4.13s/it][A
100%|█████████▉| 823/827 [55:28<00:14,  3.69s/it][A
100%|█████████▉| 824/827 [55:31<00:10,  3.48s/it][A
100%|█████████▉| 825/827 [55:36<00:08,  4.01s/it][A
100%|█████████▉| 826/827 [55:42<00:04,  4.54s/it][A
100%|██████████| 827/827 [55:44<00:00,  3.90s/it][A                                                      
                                                 [A{'eval_loss': 0.8179689645767212, 'eval_runtime': 3349.8428, 'eval_samples_per_second': 0.987, 'eval_steps_per_second': 0.247, 'epoch': 0.43}
 43%|████▎     | 200/465 [6:24:30<7:43:49, 105.02s/it]
100%|██████████| 827/827 [55:44<00:00,  3.90s/it][A
                                                 [A[INFO|trainer.py:4309] 2025-12-19 13:23:57,467 >> Saving model checkpoint to saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-200
/opt/conda/lib/python3.11/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in Qwen/Qwen2.5-VL-7B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2421] 2025-12-19 13:23:57,557 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-200/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-19 13:23:57,563 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-19 13:23:57,565 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-200/special_tokens_map.json
[2025-12-19 13:23:59,279] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!
[2025-12-19 13:23:59,300] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-200/global_step200/mp_rank_00_model_states.pt
[2025-12-19 13:23:59,300] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-200/global_step200/mp_rank_00_model_states.pt...
[2025-12-19 13:23:59,361] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-200/global_step200/mp_rank_00_model_states.pt.
[2025-12-19 13:23:59,364] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-12-19 13:23:59,365] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2025-12-19 13:23:59,365] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2025-12-19 13:23:59,365] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2025-12-19 13:23:59,442] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-12-19 13:23:59,452] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-12-19 13:23:59,452] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2025-12-19 13:23:59,452] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2025-12-19 13:23:59,452] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2025-12-19 13:23:59,452] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2025-12-19 13:23:59,458] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2025-12-19 13:23:59,458] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2025-12-19 13:23:59,458] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[2025-12-19 13:23:59,464] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2025-12-19 13:23:59,464] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-200/global_step200/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2025-12-19 13:23:59,465] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[INFO|image_processing_base.py:253] 2025-12-19 13:23:59,479 >> Image processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-200/preprocessor_config.json
[INFO|tokenization_utils_base.py:2421] 2025-12-19 13:23:59,487 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-200/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-19 13:23:59,493 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-19 13:23:59,495 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-200/special_tokens_map.json
[INFO|video_processing_utils.py:600] 2025-12-19 13:23:59,610 >> Video processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-200/video_preprocessor_config.json
[INFO|processing_utils.py:814] 2025-12-19 13:23:59,613 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-200/chat_template.jinja
 43%|████▎     | 201/465 [6:26:02<81:06:14, 1105.96s/it] 43%|████▎     | 202/465 [6:27:34<58:34:35, 801.81s/it]  44%|████▎     | 203/465 [6:29:00<42:44:05, 587.20s/it] 44%|████▍     | 204/465 [6:30:45<32:05:16, 442.59s/it] 44%|████▍     | 205/465 [6:32:34<24:43:49, 342.42s/it] 44%|████▍     | 206/465 [6:34:13<19:22:15, 269.25s/it] 45%|████▍     | 207/465 [6:36:01<15:50:03, 220.94s/it] 45%|████▍     | 208/465 [6:37:40<13:10:16, 184.50s/it] 45%|████▍     | 209/465 [6:39:14<11:11:09, 157.30s/it] 45%|████▌     | 210/465 [6:40:55<9:56:12, 140.28s/it]                                                       {'loss': 0.8242, 'grad_norm': 0.1761849820613861, 'learning_rate': 6.729703850834381e-05, 'epoch': 0.45}
 45%|████▌     | 210/465 [6:40:55<9:56:12, 140.28s/it] 45%|████▌     | 211/465 [6:42:35<9:02:37, 128.18s/it] 46%|████▌     | 212/465 [6:44:15<8:25:27, 119.87s/it] 46%|████▌     | 213/465 [6:45:39<7:38:26, 109.15s/it] 46%|████▌     | 214/465 [6:47:01<7:01:47, 100.83s/it] 46%|████▌     | 215/465 [6:48:33<6:50:05, 98.42s/it]  46%|████▋     | 216/465 [6:50:22<7:01:27, 101.56s/it] 47%|████▋     | 217/465 [6:52:15<7:13:43, 104.93s/it] 47%|████▋     | 218/465 [6:53:47<6:56:07, 101.08s/it] 47%|████▋     | 219/465 [6:55:12<6:34:41, 96.27s/it]  47%|████▋     | 220/465 [6:56:50<6:34:42, 96.66s/it]                                                     {'loss': 0.838, 'grad_norm': 0.14978350698947906, 'learning_rate': 6.372566686762426e-05, 'epoch': 0.47}
 47%|████▋     | 220/465 [6:56:50<6:34:42, 96.66s/it] 48%|████▊     | 221/465 [6:58:38<6:46:29, 99.96s/it] 48%|████▊     | 222/465 [7:00:44<7:16:53, 107.88s/it] 48%|████▊     | 223/465 [7:02:12<6:50:50, 101.86s/it] 48%|████▊     | 224/465 [7:03:53<6:48:45, 101.76s/it] 48%|████▊     | 225/465 [7:05:25<6:34:37, 98.65s/it]  49%|████▊     | 226/465 [7:06:57<6:25:43, 96.84s/it] 49%|████▉     | 227/465 [7:08:49<6:41:36, 101.25s/it] 49%|████▉     | 228/465 [7:10:29<6:38:51, 100.98s/it] 49%|████▉     | 229/465 [7:11:56<6:20:40, 96.78s/it]  49%|████▉     | 230/465 [7:13:14<5:56:42, 91.07s/it]                                                     {'loss': 0.812, 'grad_norm': 0.15683606266975403, 'learning_rate': 6.0076799778845105e-05, 'epoch': 0.49}
 49%|████▉     | 230/465 [7:13:14<5:56:42, 91.07s/it] 50%|████▉     | 231/465 [7:14:29<5:36:14, 86.22s/it] 50%|████▉     | 232/465 [7:16:03<5:44:12, 88.64s/it] 50%|█████     | 233/465 [7:17:51<6:05:33, 94.54s/it] 50%|█████     | 234/465 [7:19:34<6:13:02, 96.89s/it] 51%|█████     | 235/465 [7:21:14<6:15:17, 97.90s/it] 51%|█████     | 236/465 [7:22:58<6:20:41, 99.75s/it] 51%|█████     | 237/465 [7:24:41<6:22:51, 100.75s/it] 51%|█████     | 238/465 [7:26:14<6:12:40, 98.50s/it]  51%|█████▏    | 239/465 [7:27:47<6:04:36, 96.80s/it] 52%|█████▏    | 240/465 [7:29:42<6:23:27, 102.25s/it]                                                      {'loss': 0.8199, 'grad_norm': 0.180522620677948, 'learning_rate': 5.637103883409525e-05, 'epoch': 0.52}
 52%|█████▏    | 240/465 [7:29:42<6:23:27, 102.25s/it] 52%|█████▏    | 241/465 [7:31:21<6:18:17, 101.33s/it] 52%|█████▏    | 242/465 [7:32:52<6:04:47, 98.15s/it]  52%|█████▏    | 243/465 [7:34:47<6:22:18, 103.33s/it] 52%|█████▏    | 244/465 [7:36:29<6:18:09, 102.67s/it] 53%|█████▎    | 245/465 [7:38:11<6:15:58, 102.54s/it] 53%|█████▎    | 246/465 [7:39:53<6:13:47, 102.41s/it] 53%|█████▎    | 247/465 [7:41:20<5:54:50, 97.66s/it]  53%|█████▎    | 248/465 [7:42:54<5:50:03, 96.79s/it] 54%|█████▎    | 249/465 [7:44:46<6:04:34, 101.27s/it] 54%|█████▍    | 250/465 [7:46:27<6:02:24, 101.14s/it]                                                      {'loss': 0.8156, 'grad_norm': 0.13293904066085815, 'learning_rate': 5.2629306849554386e-05, 'epoch': 0.54}
 54%|█████▍    | 250/465 [7:46:27<6:02:24, 101.14s/it] 54%|█████▍    | 251/465 [7:48:07<5:59:08, 100.69s/it] 54%|█████▍    | 252/465 [7:49:32<5:41:43, 96.26s/it]  54%|█████▍    | 253/465 [7:50:56<5:27:08, 92.59s/it] 55%|█████▍    | 254/465 [7:52:30<5:26:40, 92.89s/it] 55%|█████▍    | 255/465 [7:53:55<5:16:53, 90.54s/it] 55%|█████▌    | 256/465 [7:55:16<5:05:13, 87.62s/it] 55%|█████▌    | 257/465 [7:56:55<5:16:09, 91.20s/it] 55%|█████▌    | 258/465 [7:58:49<5:38:07, 98.01s/it] 56%|█████▌    | 259/465 [8:00:24<5:32:48, 96.93s/it] 56%|█████▌    | 260/465 [8:02:09<5:39:52, 99.48s/it]                                                     {'loss': 0.8198, 'grad_norm': 0.1540675312280655, 'learning_rate': 4.88727297347654e-05, 'epoch': 0.56}
 56%|█████▌    | 260/465 [8:02:09<5:39:52, 99.48s/it] 56%|█████▌    | 261/465 [8:03:57<5:46:20, 101.87s/it] 56%|█████▋    | 262/465 [8:05:32<5:37:41, 99.81s/it]  57%|█████▋    | 263/465 [8:07:11<5:35:52, 99.76s/it] 57%|█████▋    | 264/465 [8:08:26<5:08:46, 92.17s/it] 57%|█████▋    | 265/465 [8:09:57<5:06:37, 91.99s/it] 57%|█████▋    | 266/465 [8:11:40<5:16:04, 95.30s/it] 57%|█████▋    | 267/465 [8:13:18<5:17:17, 96.15s/it] 58%|█████▊    | 268/465 [8:14:57<5:18:14, 96.92s/it] 58%|█████▊    | 269/465 [8:16:29<5:11:08, 95.25s/it] 58%|█████▊    | 270/465 [8:17:53<4:58:36, 91.88s/it]                                                     {'loss': 0.8052, 'grad_norm': 0.181541308760643, 'learning_rate': 4.5122517215236595e-05, 'epoch': 0.58}
 58%|█████▊    | 270/465 [8:17:53<4:58:36, 91.88s/it] 58%|█████▊    | 271/465 [8:19:40<5:12:38, 96.69s/it] 58%|█████▊    | 272/465 [8:21:36<5:28:43, 102.19s/it] 59%|█████▊    | 273/465 [8:23:05<5:14:25, 98.26s/it]  59%|█████▉    | 274/465 [8:24:49<5:18:30, 100.06s/it] 59%|█████▉    | 275/465 [8:26:06<4:55:00, 93.16s/it]  59%|█████▉    | 276/465 [8:27:24<4:39:39, 88.78s/it] 60%|█████▉    | 277/465 [8:28:51<4:36:18, 88.18s/it] 60%|█████▉    | 278/465 [8:30:40<4:54:24, 94.46s/it] 60%|██████    | 279/465 [8:32:10<4:47:55, 92.88s/it] 60%|██████    | 280/465 [8:33:32<4:37:02, 89.85s/it]                                                     {'loss': 0.8131, 'grad_norm': 0.17978143692016602, 'learning_rate': 4.139984308181709e-05, 'epoch': 0.6}
 60%|██████    | 280/465 [8:33:32<4:37:02, 89.85s/it] 60%|██████    | 281/465 [8:35:15<4:47:33, 93.77s/it] 61%|██████    | 282/465 [8:37:05<5:00:23, 98.49s/it] 61%|██████    | 283/465 [8:38:43<4:58:32, 98.42s/it] 61%|██████    | 284/465 [8:40:34<5:08:38, 102.31s/it] 61%|██████▏   | 285/465 [8:42:23<5:12:51, 104.29s/it] 62%|██████▏   | 286/465 [8:43:45<4:51:11, 97.61s/it]  62%|██████▏   | 287/465 [8:45:22<4:48:59, 97.42s/it] 62%|██████▏   | 288/465 [8:47:11<4:57:01, 100.69s/it] 62%|██████▏   | 289/465 [8:48:40<4:45:39, 97.38s/it]  62%|██████▏   | 290/465 [8:50:20<4:45:51, 98.01s/it]                                                     {'loss': 0.7945, 'grad_norm': 0.16355709731578827, 'learning_rate': 3.772572564296005e-05, 'epoch': 0.62}
 62%|██████▏   | 290/465 [8:50:20<4:45:51, 98.01s/it] 63%|██████▎   | 291/465 [8:51:36<4:24:56, 91.36s/it] 63%|██████▎   | 292/465 [8:53:21<4:35:15, 95.47s/it] 63%|██████▎   | 293/465 [8:55:04<4:40:09, 97.73s/it] 63%|██████▎   | 294/465 [8:56:46<4:42:25, 99.10s/it] 63%|██████▎   | 295/465 [8:58:20<4:36:20, 97.53s/it] 64%|██████▎   | 296/465 [9:00:03<4:39:28, 99.22s/it] 64%|██████▍   | 297/465 [9:01:56<4:49:42, 103.47s/it] 64%|██████▍   | 298/465 [9:03:24<4:35:08, 98.85s/it]  64%|██████▍   | 299/465 [9:05:03<4:33:14, 98.76s/it] 65%|██████▍   | 300/465 [9:06:44<4:33:30, 99.46s/it]                                                     {'loss': 0.7986, 'grad_norm': 0.16765442490577698, 'learning_rate': 3.4120909054843376e-05, 'epoch': 0.65}
 65%|██████▍   | 300/465 [9:06:44<4:33:30, 99.46s/it] 65%|██████▍   | 301/465 [9:08:10<4:20:55, 95.46s/it] 65%|██████▍   | 302/465 [9:09:44<4:18:07, 95.02s/it] 65%|██████▌   | 303/465 [9:11:33<4:27:30, 99.08s/it] 65%|██████▌   | 304/465 [9:13:05<4:20:00, 96.89s/it] 66%|██████▌   | 305/465 [9:15:03<4:35:37, 103.36s/it] 66%|██████▌   | 306/465 [9:16:46<4:33:22, 103.16s/it] 66%|██████▌   | 307/465 [9:18:07<4:14:21, 96.59s/it]  66%|██████▌   | 308/465 [9:19:42<4:11:22, 96.06s/it] 66%|██████▋   | 309/465 [9:21:22<4:12:47, 97.23s/it] 67%|██████▋   | 310/465 [9:23:18<4:25:40, 102.84s/it]                                                      {'loss': 0.8085, 'grad_norm': 0.1554606854915619, 'learning_rate': 3.0605746199360754e-05, 'epoch': 0.67}
 67%|██████▋   | 310/465 [9:23:18<4:25:40, 102.84s/it] 67%|██████▋   | 311/465 [9:25:12<4:32:26, 106.15s/it] 67%|██████▋   | 312/465 [9:26:56<4:29:37, 105.74s/it] 67%|██████▋   | 313/465 [9:28:37<4:23:50, 104.15s/it] 68%|██████▊   | 314/465 [9:30:24<4:24:27, 105.08s/it] 68%|██████▊   | 315/465 [9:31:59<4:15:11, 102.07s/it] 68%|██████▊   | 316/465 [9:33:40<4:12:18, 101.60s/it] 68%|██████▊   | 317/465 [9:35:06<3:59:38, 97.15s/it]  68%|██████▊   | 318/465 [9:36:42<3:56:36, 96.58s/it] 69%|██████▊   | 319/465 [9:38:27<4:01:25, 99.21s/it] 69%|██████▉   | 320/465 [9:40:03<3:57:30, 98.28s/it]                                                     {'loss': 0.7895, 'grad_norm': 0.2013440579175949, 'learning_rate': 2.7200083771256818e-05, 'epoch': 0.69}
 69%|██████▉   | 320/465 [9:40:03<3:57:30, 98.28s/it] 69%|██████▉   | 321/465 [9:41:32<3:49:14, 95.52s/it] 69%|██████▉   | 322/465 [9:43:14<3:52:00, 97.35s/it] 69%|██████▉   | 323/465 [9:44:44<3:45:20, 95.21s/it] 70%|██████▉   | 324/465 [9:46:13<3:39:34, 93.44s/it] 70%|██████▉   | 325/465 [9:47:52<3:41:45, 95.04s/it] 70%|███████   | 326/465 [9:49:31<3:42:34, 96.08s/it] 70%|███████   | 327/465 [9:51:07<3:41:17, 96.21s/it] 71%|███████   | 328/465 [9:52:45<3:40:51, 96.73s/it] 71%|███████   | 329/465 [9:54:22<3:39:16, 96.74s/it] 71%|███████   | 330/465 [9:55:56<3:35:56, 95.97s/it]                                                     {'loss': 0.7799, 'grad_norm': 0.17666515707969666, 'learning_rate': 2.3923150223207176e-05, 'epoch': 0.71}
 71%|███████   | 330/465 [9:55:56<3:35:56, 95.97s/it] 71%|███████   | 331/465 [9:57:34<3:35:41, 96.58s/it] 71%|███████▏  | 332/465 [9:59:17<3:38:40, 98.65s/it] 72%|███████▏  | 333/465 [10:00:52<3:34:22, 97.45s/it] 72%|███████▏  | 334/465 [10:02:23<3:28:24, 95.46s/it] 72%|███████▏  | 335/465 [10:03:45<3:18:01, 91.40s/it] 72%|███████▏  | 336/465 [10:05:10<3:12:29, 89.53s/it] 72%|███████▏  | 337/465 [10:07:00<3:23:58, 95.61s/it] 73%|███████▎  | 338/465 [10:08:33<3:21:08, 95.03s/it] 73%|███████▎  | 339/465 [10:10:06<3:17:49, 94.20s/it] 73%|███████▎  | 340/465 [10:11:40<3:16:05, 94.13s/it]                                                      {'loss': 0.7883, 'grad_norm': 0.1647438257932663, 'learning_rate': 2.0793447201508286e-05, 'epoch': 0.73}
 73%|███████▎  | 340/465 [10:11:40<3:16:05, 94.13s/it] 73%|███████▎  | 341/465 [10:13:19<3:17:45, 95.69s/it] 74%|███████▎  | 342/465 [10:14:58<3:18:26, 96.80s/it] 74%|███████▍  | 343/465 [10:16:30<3:13:54, 95.37s/it] 74%|███████▍  | 344/465 [10:18:14<3:17:16, 97.83s/it] 74%|███████▍  | 345/465 [10:19:51<3:15:18, 97.66s/it] 74%|███████▍  | 346/465 [10:21:28<3:13:02, 97.33s/it] 75%|███████▍  | 347/465 [10:23:00<3:08:35, 95.90s/it] 75%|███████▍  | 348/465 [10:24:44<3:11:30, 98.21s/it] 75%|███████▌  | 349/465 [10:26:11<3:03:15, 94.78s/it] 75%|███████▌  | 350/465 [10:27:47<3:02:27, 95.20s/it]                                                      {'loss': 0.7804, 'grad_norm': 0.17117029428482056, 'learning_rate': 1.7828645085333644e-05, 'epoch': 0.75}
 75%|███████▌  | 350/465 [10:27:47<3:02:27, 95.20s/it] 75%|███████▌  | 351/465 [10:29:35<3:08:21, 99.14s/it] 76%|███████▌  | 352/465 [10:31:01<2:59:04, 95.09s/it] 76%|███████▌  | 353/465 [10:32:42<3:00:40, 96.79s/it] 76%|███████▌  | 354/465 [10:34:13<2:55:47, 95.03s/it] 76%|███████▋  | 355/465 [10:35:51<2:56:19, 96.18s/it] 77%|███████▋  | 356/465 [10:37:39<3:01:10, 99.73s/it] 77%|███████▋  | 357/465 [10:39:25<3:02:39, 101.47s/it] 77%|███████▋  | 358/465 [10:40:44<2:48:52, 94.69s/it]  77%|███████▋  | 359/465 [10:42:08<2:41:55, 91.65s/it] 77%|███████▋  | 360/465 [10:43:45<2:42:49, 93.05s/it]                                                      {'loss': 0.795, 'grad_norm': 0.21422386169433594, 'learning_rate': 1.5045483219344386e-05, 'epoch': 0.77}
 77%|███████▋  | 360/465 [10:43:45<2:42:49, 93.05s/it] 78%|███████▊  | 361/465 [10:45:25<2:44:59, 95.19s/it] 78%|███████▊  | 362/465 [10:46:55<2:40:31, 93.51s/it] 78%|███████▊  | 363/465 [10:48:12<2:30:41, 88.64s/it] 78%|███████▊  | 364/465 [10:49:34<2:26:08, 86.81s/it] 78%|███████▊  | 365/465 [10:51:09<2:28:33, 89.14s/it] 79%|███████▊  | 366/465 [10:52:51<2:33:23, 92.97s/it] 79%|███████▉  | 367/465 [10:54:32<2:35:39, 95.30s/it] 79%|███████▉  | 368/465 [10:56:02<2:31:49, 93.91s/it] 79%|███████▉  | 369/465 [10:57:26<2:25:34, 90.98s/it] 80%|███████▉  | 370/465 [10:59:05<2:27:27, 93.13s/it]                                                      {'loss': 0.7967, 'grad_norm': 0.17961560189723969, 'learning_rate': 1.2459675402943289e-05, 'epoch': 0.8}
 80%|███████▉  | 370/465 [10:59:05<2:27:27, 93.13s/it] 80%|███████▉  | 371/465 [11:00:41<2:27:27, 94.12s/it] 80%|████████  | 372/465 [11:02:09<2:23:11, 92.38s/it] 80%|████████  | 373/465 [11:03:40<2:20:52, 91.88s/it] 80%|████████  | 374/465 [11:05:26<2:25:56, 96.22s/it] 81%|████████  | 375/465 [11:07:08<2:26:36, 97.74s/it] 81%|████████  | 376/465 [11:08:47<2:25:50, 98.32s/it] 81%|████████  | 377/465 [11:10:24<2:23:37, 97.93s/it] 81%|████████▏ | 378/465 [11:12:00<2:20:56, 97.21s/it] 82%|████████▏ | 379/465 [11:13:33<2:17:35, 96.00s/it] 82%|████████▏ | 380/465 [11:15:00<2:12:06, 93.25s/it]                                                      {'loss': 0.8027, 'grad_norm': 0.1706993281841278, 'learning_rate': 1.00858211697822e-05, 'epoch': 0.82}
 82%|████████▏ | 380/465 [11:15:00<2:12:06, 93.25s/it] 82%|████████▏ | 381/465 [11:16:33<2:10:32, 93.25s/it] 82%|████████▏ | 382/465 [11:18:24<2:16:09, 98.43s/it] 82%|████████▏ | 383/465 [11:20:04<2:15:22, 99.06s/it] 83%|████████▎ | 384/465 [11:21:38<2:11:43, 97.58s/it] 83%|████████▎ | 385/465 [11:23:21<2:12:02, 99.03s/it] 83%|████████▎ | 386/465 [11:24:54<2:08:06, 97.30s/it] 83%|████████▎ | 387/465 [11:26:19<2:01:38, 93.57s/it] 83%|████████▎ | 388/465 [11:28:08<2:06:09, 98.31s/it] 84%|████████▎ | 389/465 [11:29:44<2:03:32, 97.54s/it] 84%|████████▍ | 390/465 [11:31:21<2:01:38, 97.31s/it]                                                      {'loss': 0.7852, 'grad_norm': 0.16693224012851715, 'learning_rate': 7.937323358440935e-06, 'epoch': 0.84}
 84%|████████▍ | 390/465 [11:31:21<2:01:38, 97.31s/it] 84%|████████▍ | 391/465 [11:33:16<2:06:48, 102.82s/it] 84%|████████▍ | 392/465 [11:34:48<2:01:09, 99.58s/it]  85%|████████▍ | 393/465 [11:36:32<2:00:49, 100.68s/it] 85%|████████▍ | 394/465 [11:38:10<1:58:11, 99.88s/it]  85%|████████▍ | 395/465 [11:39:51<1:57:04, 100.35s/it] 85%|████████▌ | 396/465 [11:41:28<1:54:08, 99.25s/it]  85%|████████▌ | 397/465 [11:43:15<1:55:21, 101.79s/it] 86%|████████▌ | 398/465 [11:44:48<1:50:30, 98.96s/it]  86%|████████▌ | 399/465 [11:46:40<1:53:18, 103.01s/it] 86%|████████▌ | 400/465 [11:48:11<1:47:31, 99.25s/it]                                                       {'loss': 0.7713, 'grad_norm': 0.17937906086444855, 'learning_rate': 6.026312439675552e-06, 'epoch': 0.86}
 86%|████████▌ | 400/465 [11:48:11<1:47:31, 99.25s/it]
***** Running Evaluation *****

***** Running Evaluation *****
  Num examples = 3305
  Batch size = 1
  Num examples = 3305
  Batch size = 1
[INFO|trainer.py:4643] 2025-12-19 18:47:30,869 >> 
***** Running Evaluation *****

***** Running Evaluation *****
[INFO|trainer.py:4645] 2025-12-19 18:47:30,869 >>   Num examples = 3305
  Num examples = 3305
  Batch size = 1
[INFO|trainer.py:4648] 2025-12-19 18:47:30,869 >>   Batch size = 1

  0%|          | 0/827 [00:00<?, ?it/s][A
  0%|          | 2/827 [00:04<28:10,  2.05s/it][A
  0%|          | 3/827 [00:06<31:21,  2.28s/it][A
  0%|          | 4/827 [00:09<35:09,  2.56s/it][A
  1%|          | 5/827 [00:13<40:49,  2.98s/it][A
  1%|          | 6/827 [00:17<43:13,  3.16s/it][A
  1%|          | 7/827 [00:20<46:34,  3.41s/it][A
  1%|          | 8/827 [00:26<55:47,  4.09s/it][A
  1%|          | 9/827 [00:33<1:06:44,  4.90s/it][A
  1%|          | 10/827 [00:40<1:15:06,  5.52s/it][A
  1%|▏         | 11/827 [00:43<1:05:54,  4.85s/it][A
  1%|▏         | 12/827 [00:46<58:04,  4.27s/it]  [A
  2%|▏         | 13/827 [00:49<52:51,  3.90s/it][A
  2%|▏         | 14/827 [00:53<51:37,  3.81s/it][A
  2%|▏         | 15/827 [00:57<53:06,  3.92s/it][A
  2%|▏         | 16/827 [01:00<51:08,  3.78s/it][A
  2%|▏         | 17/827 [01:03<48:02,  3.56s/it][A
  2%|▏         | 18/827 [01:07<46:58,  3.48s/it][A
  2%|▏         | 19/827 [01:11<51:25,  3.82s/it][A
  2%|▏         | 20/827 [01:16<57:25,  4.27s/it][A
  3%|▎         | 21/827 [01:20<55:08,  4.10s/it][A
  3%|▎         | 22/827 [01:25<56:57,  4.25s/it][A
  3%|▎         | 23/827 [01:28<51:13,  3.82s/it][A
  3%|▎         | 24/827 [01:31<51:05,  3.82s/it][A
  3%|▎         | 25/827 [01:36<53:45,  4.02s/it][A
  3%|▎         | 26/827 [01:41<58:26,  4.38s/it][A
  3%|▎         | 27/827 [01:46<58:38,  4.40s/it][A
  3%|▎         | 28/827 [01:48<52:14,  3.92s/it][A
  4%|▎         | 29/827 [01:53<55:00,  4.14s/it][A
  4%|▎         | 30/827 [01:58<58:04,  4.37s/it][A
  4%|▎         | 31/827 [02:02<55:54,  4.21s/it][A
  4%|▍         | 32/827 [02:06<54:50,  4.14s/it][A
  4%|▍         | 33/827 [02:11<59:05,  4.47s/it][A
  4%|▍         | 34/827 [02:14<53:48,  4.07s/it][A
  4%|▍         | 35/827 [02:17<50:49,  3.85s/it][A
  4%|▍         | 36/827 [02:20<46:05,  3.50s/it][A
  4%|▍         | 37/827 [02:25<51:05,  3.88s/it][A
  5%|▍         | 38/827 [02:30<55:17,  4.20s/it][A
  5%|▍         | 39/827 [02:34<56:52,  4.33s/it][A
  5%|▍         | 40/827 [02:39<59:10,  4.51s/it][A
  5%|▍         | 41/827 [02:43<53:31,  4.09s/it][A
  5%|▌         | 42/827 [02:46<51:54,  3.97s/it][A
  5%|▌         | 43/827 [02:50<50:27,  3.86s/it][A
  5%|▌         | 44/827 [02:53<48:35,  3.72s/it][A
  5%|▌         | 45/827 [02:57<50:20,  3.86s/it][A
  6%|▌         | 46/827 [03:03<55:28,  4.26s/it][A
  6%|▌         | 47/827 [03:07<56:55,  4.38s/it][A
  6%|▌         | 48/827 [03:11<53:05,  4.09s/it][A
  6%|▌         | 49/827 [03:14<50:14,  3.87s/it][A
  6%|▌         | 50/827 [03:17<46:08,  3.56s/it][A
  6%|▌         | 51/827 [03:22<51:11,  3.96s/it][A
  6%|▋         | 52/827 [03:27<57:49,  4.48s/it][A
  6%|▋         | 53/827 [03:30<51:00,  3.95s/it][A
  7%|▋         | 54/827 [03:33<47:02,  3.65s/it][A
  7%|▋         | 55/827 [03:36<44:21,  3.45s/it][A
  7%|▋         | 56/827 [03:39<43:11,  3.36s/it][A
  7%|▋         | 57/827 [03:43<44:13,  3.45s/it][A
  7%|▋         | 58/827 [03:46<42:19,  3.30s/it][A
  7%|▋         | 59/827 [03:49<42:18,  3.31s/it][A
  7%|▋         | 60/827 [03:53<45:42,  3.58s/it][A
  7%|▋         | 61/827 [03:58<50:44,  3.97s/it][A
  7%|▋         | 62/827 [04:01<46:13,  3.63s/it][A
  8%|▊         | 63/827 [04:04<43:08,  3.39s/it][A
  8%|▊         | 64/827 [04:07<41:31,  3.27s/it][A
  8%|▊         | 65/827 [04:10<41:06,  3.24s/it][A
  8%|▊         | 66/827 [04:15<47:43,  3.76s/it][A
  8%|▊         | 67/827 [04:20<51:24,  4.06s/it][A
  8%|▊         | 68/827 [04:22<45:14,  3.58s/it][A
  8%|▊         | 69/827 [04:25<43:50,  3.47s/it][A
  8%|▊         | 70/827 [04:28<41:09,  3.26s/it][A
  9%|▊         | 71/827 [04:32<44:13,  3.51s/it][A
  9%|▊         | 72/827 [04:37<48:14,  3.83s/it][A
  9%|▉         | 73/827 [04:40<46:13,  3.68s/it][A
  9%|▉         | 74/827 [04:46<52:50,  4.21s/it][A
  9%|▉         | 75/827 [04:51<55:23,  4.42s/it][A
  9%|▉         | 76/827 [04:54<49:48,  3.98s/it][A
  9%|▉         | 77/827 [04:57<45:54,  3.67s/it][A
  9%|▉         | 78/827 [05:00<43:20,  3.47s/it][A
 10%|▉         | 79/827 [05:02<41:17,  3.31s/it][A
 10%|▉         | 80/827 [05:07<44:34,  3.58s/it][A
 10%|▉         | 81/827 [05:11<48:08,  3.87s/it][A
 10%|▉         | 82/827 [05:15<48:33,  3.91s/it][A
 10%|█         | 83/827 [05:19<49:07,  3.96s/it][A
 10%|█         | 84/827 [05:23<49:16,  3.98s/it][A
 10%|█         | 85/827 [05:27<48:33,  3.93s/it][A
 10%|█         | 86/827 [05:30<44:43,  3.62s/it][A
 11%|█         | 87/827 [05:36<53:36,  4.35s/it][A
 11%|█         | 88/827 [05:46<1:13:00,  5.93s/it][A
 11%|█         | 89/827 [05:53<1:17:52,  6.33s/it][A
 11%|█         | 90/827 [05:58<1:13:04,  5.95s/it][A
 11%|█         | 91/827 [06:01<1:03:50,  5.20s/it][A
 11%|█         | 92/827 [06:05<58:35,  4.78s/it]  [A
 11%|█         | 93/827 [06:08<51:15,  4.19s/it][A
 11%|█▏        | 94/827 [06:11<47:09,  3.86s/it][A
 11%|█▏        | 95/827 [06:14<44:04,  3.61s/it][A
 12%|█▏        | 96/827 [06:17<40:52,  3.35s/it][A
 12%|█▏        | 97/827 [06:19<37:14,  3.06s/it][A
 12%|█▏        | 98/827 [06:22<35:07,  2.89s/it][A
 12%|█▏        | 99/827 [06:24<34:02,  2.81s/it][A
 12%|█▏        | 100/827 [06:27<31:48,  2.63s/it][A
 12%|█▏        | 101/827 [06:30<34:59,  2.89s/it][A
 12%|█▏        | 102/827 [06:33<35:38,  2.95s/it][A
 12%|█▏        | 103/827 [06:36<34:50,  2.89s/it][A
 13%|█▎        | 104/827 [06:39<34:56,  2.90s/it][A
 13%|█▎        | 105/827 [06:42<35:52,  2.98s/it][A
 13%|█▎        | 106/827 [06:48<47:15,  3.93s/it][A
 13%|█▎        | 107/827 [06:54<55:30,  4.63s/it][A
 13%|█▎        | 108/827 [06:59<55:03,  4.60s/it][A
 13%|█▎        | 109/827 [07:03<51:28,  4.30s/it][A
 13%|█▎        | 110/827 [07:06<48:34,  4.06s/it][A
 13%|█▎        | 111/827 [07:11<51:23,  4.31s/it][A
 14%|█▎        | 112/827 [07:17<56:16,  4.72s/it][A
 14%|█▎        | 113/827 [07:21<53:12,  4.47s/it][A
 14%|█▍        | 114/827 [07:24<49:03,  4.13s/it][A
 14%|█▍        | 115/827 [07:27<45:57,  3.87s/it][A
 14%|█▍        | 116/827 [07:31<45:12,  3.81s/it][A
 14%|█▍        | 117/827 [07:35<47:12,  3.99s/it][A
 14%|█▍        | 118/827 [07:41<51:34,  4.37s/it][A
 14%|█▍        | 119/827 [07:46<54:20,  4.60s/it][A
 15%|█▍        | 120/827 [07:50<53:37,  4.55s/it][A
 15%|█▍        | 121/827 [07:53<48:19,  4.11s/it][A
 15%|█▍        | 122/827 [07:56<44:52,  3.82s/it][A
 15%|█▍        | 123/827 [07:59<40:31,  3.45s/it][A
 15%|█▍        | 124/827 [08:02<40:02,  3.42s/it][A
 15%|█▌        | 125/827 [08:06<41:28,  3.55s/it][A
 15%|█▌        | 126/827 [08:09<40:43,  3.49s/it][A
 15%|█▌        | 127/827 [08:13<39:26,  3.38s/it][A
 15%|█▌        | 128/827 [08:16<40:31,  3.48s/it][A
 16%|█▌        | 129/827 [08:19<37:47,  3.25s/it][A
 16%|█▌        | 130/827 [08:23<38:56,  3.35s/it][A
 16%|█▌        | 131/827 [08:26<40:40,  3.51s/it][A
 16%|█▌        | 132/827 [08:31<45:14,  3.91s/it][A
 16%|█▌        | 133/827 [08:36<47:05,  4.07s/it][A
 16%|█▌        | 134/827 [08:39<44:48,  3.88s/it][A
 16%|█▋        | 135/827 [08:44<47:50,  4.15s/it][A
 16%|█▋        | 136/827 [08:50<54:03,  4.69s/it][A
 17%|█▋        | 137/827 [08:53<48:35,  4.23s/it][A
 17%|█▋        | 138/827 [08:59<54:10,  4.72s/it][A
 17%|█▋        | 139/827 [09:06<1:00:55,  5.31s/it][A
 17%|█▋        | 140/827 [09:09<54:12,  4.73s/it]  [A
 17%|█▋        | 141/827 [09:13<51:05,  4.47s/it][A
 17%|█▋        | 142/827 [09:16<46:43,  4.09s/it][A
 17%|█▋        | 143/827 [09:20<44:21,  3.89s/it][A
 17%|█▋        | 144/827 [09:22<40:34,  3.57s/it][A
 18%|█▊        | 145/827 [09:26<40:54,  3.60s/it][A
 18%|█▊        | 146/827 [09:30<41:49,  3.68s/it][A
 18%|█▊        | 147/827 [09:33<40:19,  3.56s/it][A
 18%|█▊        | 148/827 [09:36<39:10,  3.46s/it][A
 18%|█▊        | 149/827 [09:40<38:08,  3.38s/it][A
 18%|█▊        | 150/827 [09:42<35:34,  3.15s/it][A
 18%|█▊        | 151/827 [09:45<33:05,  2.94s/it][A
 18%|█▊        | 152/827 [09:48<33:39,  2.99s/it][A
 19%|█▊        | 153/827 [09:51<33:55,  3.02s/it][A
 19%|█▊        | 154/827 [09:55<36:52,  3.29s/it][A
 19%|█▊        | 155/827 [10:00<42:15,  3.77s/it][A
 19%|█▉        | 156/827 [10:04<45:14,  4.05s/it][A
 19%|█▉        | 157/827 [10:08<42:25,  3.80s/it][A
 19%|█▉        | 158/827 [10:11<39:43,  3.56s/it][A
 19%|█▉        | 159/827 [10:15<42:14,  3.79s/it][A
 19%|█▉        | 160/827 [10:19<43:23,  3.90s/it][A
 19%|█▉        | 161/827 [10:22<39:00,  3.51s/it][A
 20%|█▉        | 162/827 [10:25<39:38,  3.58s/it][A
 20%|█▉        | 163/827 [10:29<40:37,  3.67s/it][A
 20%|█▉        | 164/827 [10:33<39:46,  3.60s/it][A
 20%|█▉        | 165/827 [10:36<37:50,  3.43s/it][A
 20%|██        | 166/827 [10:41<45:08,  4.10s/it][A
 20%|██        | 167/827 [10:50<1:00:38,  5.51s/it][A
 20%|██        | 168/827 [10:55<59:31,  5.42s/it]  [A
 20%|██        | 169/827 [10:59<51:59,  4.74s/it][A
 21%|██        | 170/827 [11:03<49:25,  4.51s/it][A
 21%|██        | 171/827 [11:06<46:56,  4.29s/it][A
 21%|██        | 172/827 [11:09<41:41,  3.82s/it][A
 21%|██        | 173/827 [11:12<37:49,  3.47s/it][A
 21%|██        | 174/827 [11:15<36:11,  3.33s/it][A
 21%|██        | 175/827 [11:18<37:30,  3.45s/it][A
 21%|██▏       | 176/827 [11:21<34:54,  3.22s/it][A
 21%|██▏       | 177/827 [11:24<32:59,  3.05s/it][A
 22%|██▏       | 178/827 [11:28<36:05,  3.34s/it][A
 22%|██▏       | 179/827 [11:33<42:47,  3.96s/it][A
 22%|██▏       | 180/827 [11:37<42:14,  3.92s/it][A
 22%|██▏       | 181/827 [11:41<41:09,  3.82s/it][A
 22%|██▏       | 182/827 [11:45<43:35,  4.06s/it][A
 22%|██▏       | 183/827 [11:48<40:07,  3.74s/it][A
 22%|██▏       | 184/827 [11:52<41:38,  3.89s/it][A
 22%|██▏       | 185/827 [11:57<44:26,  4.15s/it][A
 22%|██▏       | 186/827 [12:00<40:45,  3.81s/it][A
 23%|██▎       | 187/827 [12:03<37:25,  3.51s/it][A
 23%|██▎       | 188/827 [12:07<38:09,  3.58s/it][A
 23%|██▎       | 189/827 [12:12<41:51,  3.94s/it][A
 23%|██▎       | 190/827 [12:15<40:29,  3.81s/it][A
 23%|██▎       | 191/827 [12:19<40:38,  3.83s/it][A
 23%|██▎       | 192/827 [12:23<42:37,  4.03s/it][A
 23%|██▎       | 193/827 [12:27<42:07,  3.99s/it][A
 23%|██▎       | 194/827 [12:30<38:46,  3.67s/it][A
 24%|██▎       | 195/827 [12:33<36:40,  3.48s/it][A
 24%|██▎       | 196/827 [12:36<33:34,  3.19s/it][A
 24%|██▍       | 197/827 [12:39<32:11,  3.07s/it][A
 24%|██▍       | 198/827 [12:42<32:43,  3.12s/it][A
 24%|██▍       | 199/827 [12:45<33:43,  3.22s/it][A
 24%|██▍       | 200/827 [12:49<36:21,  3.48s/it][A
 24%|██▍       | 201/827 [12:54<40:56,  3.92s/it][A
 24%|██▍       | 202/827 [12:59<44:09,  4.24s/it][A
 25%|██▍       | 203/827 [13:04<45:43,  4.40s/it][A
 25%|██▍       | 204/827 [13:10<49:37,  4.78s/it][A
 25%|██▍       | 205/827 [13:13<44:59,  4.34s/it][A
 25%|██▍       | 206/827 [13:16<41:28,  4.01s/it][A
 25%|██▌       | 207/827 [13:20<39:08,  3.79s/it][A
 25%|██▌       | 208/827 [13:23<37:08,  3.60s/it][A
 25%|██▌       | 209/827 [13:26<35:47,  3.47s/it][A
 25%|██▌       | 210/827 [13:29<34:47,  3.38s/it][A
 26%|██▌       | 211/827 [13:34<38:47,  3.78s/it][A
 26%|██▌       | 212/827 [13:39<41:48,  4.08s/it][A
 26%|██▌       | 213/827 [13:44<45:45,  4.47s/it][A
 26%|██▌       | 214/827 [13:47<42:16,  4.14s/it][A
 26%|██▌       | 215/827 [13:50<38:40,  3.79s/it][A
 26%|██▌       | 216/827 [13:54<37:23,  3.67s/it][A
 26%|██▌       | 217/827 [13:58<37:59,  3.74s/it][A
 26%|██▋       | 218/827 [14:01<38:03,  3.75s/it][A
 26%|██▋       | 219/827 [14:04<35:22,  3.49s/it][A
 27%|██▋       | 220/827 [14:09<38:05,  3.77s/it][A
 27%|██▋       | 221/827 [14:14<42:42,  4.23s/it][A
 27%|██▋       | 222/827 [14:17<39:23,  3.91s/it][A
 27%|██▋       | 223/827 [14:20<37:08,  3.69s/it][A
 27%|██▋       | 224/827 [14:23<34:49,  3.46s/it][A
 27%|██▋       | 225/827 [14:26<33:44,  3.36s/it][A
 27%|██▋       | 226/827 [14:29<31:08,  3.11s/it][A
 27%|██▋       | 227/827 [14:33<34:57,  3.50s/it][A
 28%|██▊       | 228/827 [14:38<39:36,  3.97s/it][A
 28%|██▊       | 229/827 [14:42<37:13,  3.73s/it][A
 28%|██▊       | 230/827 [14:45<37:15,  3.74s/it][A
 28%|██▊       | 231/827 [14:49<36:01,  3.63s/it][A
 28%|██▊       | 232/827 [14:52<34:34,  3.49s/it][A
 28%|██▊       | 233/827 [14:56<37:47,  3.82s/it][A
 28%|██▊       | 234/827 [15:02<42:09,  4.27s/it][A
 28%|██▊       | 235/827 [15:07<44:03,  4.46s/it][A
 29%|██▊       | 236/827 [15:10<42:06,  4.28s/it][A
 29%|██▊       | 237/827 [15:16<45:57,  4.67s/it][A
 29%|██▉       | 238/827 [15:22<48:25,  4.93s/it][A
 29%|██▉       | 239/827 [15:26<47:46,  4.87s/it][A
 29%|██▉       | 240/827 [15:32<49:46,  5.09s/it][A
 29%|██▉       | 241/827 [15:37<51:02,  5.23s/it][A
 29%|██▉       | 242/827 [15:41<47:23,  4.86s/it][A
 29%|██▉       | 243/827 [15:46<45:39,  4.69s/it][A
 30%|██▉       | 244/827 [15:51<48:23,  4.98s/it][A
 30%|██▉       | 245/827 [15:55<43:40,  4.50s/it][A
 30%|██▉       | 246/827 [15:58<41:00,  4.23s/it][A
 30%|██▉       | 247/827 [16:03<40:59,  4.24s/it][A
 30%|██▉       | 248/827 [16:09<47:21,  4.91s/it][A
 30%|███       | 249/827 [16:14<46:10,  4.79s/it][A
 30%|███       | 250/827 [16:16<39:05,  4.06s/it][A
 30%|███       | 251/827 [16:20<38:05,  3.97s/it][A
 30%|███       | 252/827 [16:24<39:41,  4.14s/it][A
 31%|███       | 253/827 [16:27<35:42,  3.73s/it][A
 31%|███       | 254/827 [16:32<39:55,  4.18s/it][A
 31%|███       | 255/827 [16:39<46:23,  4.87s/it][A
 31%|███       | 256/827 [16:45<49:02,  5.15s/it][A
 31%|███       | 257/827 [16:49<46:31,  4.90s/it][A
 31%|███       | 258/827 [16:53<43:31,  4.59s/it][A
 31%|███▏      | 259/827 [16:59<47:08,  4.98s/it][A
 31%|███▏      | 260/827 [17:04<49:21,  5.22s/it][A
 32%|███▏      | 261/827 [17:08<45:15,  4.80s/it][A
 32%|███▏      | 262/827 [17:14<47:33,  5.05s/it][A
 32%|███▏      | 263/827 [17:20<50:45,  5.40s/it][A
 32%|███▏      | 264/827 [17:23<43:00,  4.58s/it][A
 32%|███▏      | 265/827 [17:25<37:17,  3.98s/it][A
 32%|███▏      | 266/827 [17:28<34:42,  3.71s/it][A
 32%|███▏      | 267/827 [17:32<32:54,  3.53s/it][A
 32%|███▏      | 268/827 [17:35<31:45,  3.41s/it][A
 33%|███▎      | 269/827 [17:39<33:47,  3.63s/it][A
 33%|███▎      | 270/827 [17:44<36:56,  3.98s/it][A
 33%|███▎      | 271/827 [17:47<34:24,  3.71s/it][A
 33%|███▎      | 272/827 [17:50<33:11,  3.59s/it][A
 33%|███▎      | 273/827 [17:56<38:46,  4.20s/it][A
 33%|███▎      | 274/827 [18:04<50:07,  5.44s/it][A
 33%|███▎      | 275/827 [18:09<49:09,  5.34s/it][A
 33%|███▎      | 276/827 [18:13<44:40,  4.86s/it][A
 33%|███▎      | 277/827 [18:17<41:17,  4.50s/it][A
 34%|███▎      | 278/827 [18:20<39:38,  4.33s/it][A
 34%|███▎      | 279/827 [18:24<36:25,  3.99s/it][A
 34%|███▍      | 280/827 [18:27<33:20,  3.66s/it][A
 34%|███▍      | 281/827 [18:30<33:39,  3.70s/it][A
 34%|███▍      | 282/827 [18:34<33:04,  3.64s/it][A
 34%|███▍      | 283/827 [18:37<31:59,  3.53s/it][A
 34%|███▍      | 284/827 [18:41<32:39,  3.61s/it][A
 34%|███▍      | 285/827 [18:45<34:19,  3.80s/it][A
 35%|███▍      | 286/827 [18:50<37:05,  4.11s/it][A
 35%|███▍      | 287/827 [18:52<32:33,  3.62s/it][A
 35%|███▍      | 288/827 [18:55<30:50,  3.43s/it][A
 35%|███▍      | 289/827 [18:58<29:41,  3.31s/it][A
 35%|███▌      | 290/827 [19:02<28:57,  3.24s/it][A
 35%|███▌      | 291/827 [19:04<27:26,  3.07s/it][A
 35%|███▌      | 292/827 [19:08<28:04,  3.15s/it][A
 35%|███▌      | 293/827 [19:10<27:13,  3.06s/it][A
 36%|███▌      | 294/827 [19:14<28:15,  3.18s/it][A
 36%|███▌      | 295/827 [19:17<29:06,  3.28s/it][A
 36%|███▌      | 296/827 [19:22<31:35,  3.57s/it][A
 36%|███▌      | 297/827 [19:25<32:04,  3.63s/it][A
 36%|███▌      | 298/827 [19:28<30:02,  3.41s/it][A
 36%|███▌      | 299/827 [19:31<29:19,  3.33s/it][A
 36%|███▋      | 300/827 [19:34<28:14,  3.22s/it][A
 36%|███▋      | 301/827 [19:37<27:30,  3.14s/it][A
 37%|███▋      | 302/827 [19:41<28:11,  3.22s/it][A
 37%|███▋      | 303/827 [19:43<25:42,  2.94s/it][A
 37%|███▋      | 304/827 [19:46<26:29,  3.04s/it][A
 37%|███▋      | 305/827 [19:51<29:35,  3.40s/it][A
 37%|███▋      | 306/827 [19:55<31:12,  3.59s/it][A
 37%|███▋      | 307/827 [19:58<30:04,  3.47s/it][A
 37%|███▋      | 308/827 [20:01<28:23,  3.28s/it][A
 37%|███▋      | 309/827 [20:04<28:33,  3.31s/it][A
 37%|███▋      | 310/827 [20:07<26:45,  3.10s/it][A
 38%|███▊      | 311/827 [20:09<25:21,  2.95s/it][A
 38%|███▊      | 312/827 [20:13<26:35,  3.10s/it][A
 38%|███▊      | 313/827 [20:17<29:23,  3.43s/it][A
 38%|███▊      | 314/827 [20:20<29:20,  3.43s/it][A
 38%|███▊      | 315/827 [20:25<31:51,  3.73s/it][A
 38%|███▊      | 316/827 [20:29<33:48,  3.97s/it][A
 38%|███▊      | 317/827 [20:34<36:11,  4.26s/it][A
 38%|███▊      | 318/827 [20:39<36:19,  4.28s/it][A
 39%|███▊      | 319/827 [20:42<35:10,  4.15s/it][A
 39%|███▊      | 320/827 [20:46<32:53,  3.89s/it][A
 39%|███▉      | 321/827 [20:50<34:21,  4.07s/it][A
 39%|███▉      | 322/827 [20:54<33:12,  3.94s/it][A
 39%|███▉      | 323/827 [20:58<32:54,  3.92s/it][A
 39%|███▉      | 324/827 [21:02<34:48,  4.15s/it][A
 39%|███▉      | 325/827 [21:05<32:02,  3.83s/it][A
 39%|███▉      | 326/827 [21:09<31:27,  3.77s/it][A
 40%|███▉      | 327/827 [21:14<33:31,  4.02s/it][A
 40%|███▉      | 328/827 [21:18<33:55,  4.08s/it][A
 40%|███▉      | 329/827 [21:24<38:52,  4.68s/it][A
 40%|███▉      | 330/827 [21:28<38:01,  4.59s/it][A
 40%|████      | 331/827 [21:33<38:10,  4.62s/it][A
 40%|████      | 332/827 [21:37<36:41,  4.45s/it][A
 40%|████      | 333/827 [21:42<38:16,  4.65s/it][A
 40%|████      | 334/827 [21:46<36:21,  4.43s/it][A
 41%|████      | 335/827 [21:50<33:51,  4.13s/it][A
 41%|████      | 336/827 [21:52<30:27,  3.72s/it][A
 41%|████      | 337/827 [21:56<29:05,  3.56s/it][A
 41%|████      | 338/827 [21:59<27:52,  3.42s/it][A
 41%|████      | 339/827 [22:02<26:49,  3.30s/it][A
 41%|████      | 340/827 [22:05<27:04,  3.34s/it][A
 41%|████      | 341/827 [22:09<28:28,  3.51s/it][A
 41%|████▏     | 342/827 [22:12<27:29,  3.40s/it][A
 41%|████▏     | 343/827 [22:15<26:47,  3.32s/it][A
 42%|████▏     | 344/827 [22:20<29:12,  3.63s/it][A
 42%|████▏     | 345/827 [22:25<32:28,  4.04s/it][A
 42%|████▏     | 346/827 [22:28<31:55,  3.98s/it][A
 42%|████▏     | 347/827 [22:32<30:55,  3.87s/it][A
 42%|████▏     | 348/827 [22:35<28:33,  3.58s/it][A
 42%|████▏     | 349/827 [22:39<30:16,  3.80s/it][A
 42%|████▏     | 350/827 [22:44<32:13,  4.05s/it][A
 42%|████▏     | 351/827 [22:47<30:27,  3.84s/it][A
 43%|████▎     | 352/827 [22:51<30:50,  3.89s/it][A
 43%|████▎     | 353/827 [22:55<29:23,  3.72s/it][A
 43%|████▎     | 354/827 [22:58<28:06,  3.57s/it][A
 43%|████▎     | 355/827 [23:02<30:19,  3.85s/it][A
 43%|████▎     | 356/827 [23:07<32:21,  4.12s/it][A
 43%|████▎     | 357/827 [23:11<30:49,  3.94s/it][A
 43%|████▎     | 358/827 [23:13<28:06,  3.60s/it][A
 43%|████▎     | 359/827 [23:17<27:44,  3.56s/it][A
 44%|████▎     | 360/827 [23:21<30:06,  3.87s/it][A
 44%|████▎     | 361/827 [23:25<30:07,  3.88s/it][A
 44%|████▍     | 362/827 [23:28<26:48,  3.46s/it][A
 44%|████▍     | 363/827 [23:32<28:18,  3.66s/it][A
 44%|████▍     | 364/827 [23:36<28:17,  3.67s/it][A
 44%|████▍     | 365/827 [23:39<27:09,  3.53s/it][A
 44%|████▍     | 366/827 [23:44<31:32,  4.11s/it][A
 44%|████▍     | 367/827 [23:49<32:18,  4.21s/it][A
 44%|████▍     | 368/827 [23:51<28:40,  3.75s/it][A
 45%|████▍     | 369/827 [23:55<28:02,  3.67s/it][A
 45%|████▍     | 370/827 [23:58<27:16,  3.58s/it][A
 45%|████▍     | 371/827 [24:03<28:54,  3.80s/it][A
 45%|████▍     | 372/827 [24:06<28:35,  3.77s/it][A
 45%|████▌     | 373/827 [24:11<30:04,  3.97s/it][A
 45%|████▌     | 374/827 [24:17<36:01,  4.77s/it][A
 45%|████▌     | 375/827 [24:26<43:50,  5.82s/it][A
 45%|████▌     | 376/827 [24:33<46:41,  6.21s/it][A
 46%|████▌     | 377/827 [24:37<42:50,  5.71s/it][A
 46%|████▌     | 378/827 [24:43<43:09,  5.77s/it][A
 46%|████▌     | 379/827 [24:52<50:09,  6.72s/it][A
 46%|████▌     | 380/827 [24:58<48:01,  6.45s/it][A
 46%|████▌     | 381/827 [25:02<41:38,  5.60s/it][A
 46%|████▌     | 382/827 [25:06<38:52,  5.24s/it][A
 46%|████▋     | 383/827 [25:10<36:17,  4.91s/it][A
 46%|████▋     | 384/827 [25:14<33:22,  4.52s/it][A
 47%|████▋     | 385/827 [25:17<30:15,  4.11s/it][A
 47%|████▋     | 386/827 [25:20<28:10,  3.83s/it][A
 47%|████▋     | 387/827 [25:24<27:55,  3.81s/it][A
 47%|████▋     | 388/827 [25:28<28:46,  3.93s/it][A
 47%|████▋     | 389/827 [25:32<28:10,  3.86s/it][A
 47%|████▋     | 390/827 [25:35<27:08,  3.73s/it][A
 47%|████▋     | 391/827 [25:38<26:03,  3.59s/it][A
 47%|████▋     | 392/827 [25:42<25:39,  3.54s/it][A
 48%|████▊     | 393/827 [25:46<26:59,  3.73s/it][A
 48%|████▊     | 394/827 [25:51<29:51,  4.14s/it][A
 48%|████▊     | 395/827 [25:56<30:31,  4.24s/it][A
 48%|████▊     | 396/827 [26:00<31:06,  4.33s/it][A
 48%|████▊     | 397/827 [26:03<27:51,  3.89s/it][A
 48%|████▊     | 398/827 [26:06<26:22,  3.69s/it][A
 48%|████▊     | 399/827 [26:09<25:27,  3.57s/it][A
 48%|████▊     | 400/827 [26:14<26:40,  3.75s/it][A
 48%|████▊     | 401/827 [26:18<28:45,  4.05s/it][A
 49%|████▊     | 402/827 [26:22<28:39,  4.05s/it][A
 49%|████▊     | 403/827 [26:27<29:53,  4.23s/it][A
 49%|████▉     | 404/827 [26:30<26:15,  3.72s/it][A
 49%|████▉     | 405/827 [26:33<24:55,  3.54s/it][A
 49%|████▉     | 406/827 [26:36<24:11,  3.45s/it][A
 49%|████▉     | 407/827 [26:39<22:33,  3.22s/it][A
 49%|████▉     | 408/827 [26:42<21:49,  3.13s/it][A
 49%|████▉     | 409/827 [26:45<21:43,  3.12s/it][A
 50%|████▉     | 410/827 [26:47<20:26,  2.94s/it][A
 50%|████▉     | 411/827 [26:50<20:29,  2.96s/it][A
 50%|████▉     | 412/827 [26:53<20:33,  2.97s/it][A
 50%|████▉     | 413/827 [26:57<23:11,  3.36s/it][A
 50%|█████     | 414/827 [27:02<26:24,  3.84s/it][A
 50%|█████     | 415/827 [27:05<24:15,  3.53s/it][A
 50%|█████     | 416/827 [27:09<25:34,  3.73s/it][A
 50%|█████     | 417/827 [27:14<27:40,  4.05s/it][A
 51%|█████     | 418/827 [27:20<30:06,  4.42s/it][A
 51%|█████     | 419/827 [27:23<28:59,  4.26s/it][A
 51%|█████     | 420/827 [27:28<28:39,  4.22s/it][A
 51%|█████     | 421/827 [27:31<27:29,  4.06s/it][A
 51%|█████     | 422/827 [27:34<25:18,  3.75s/it][A
 51%|█████     | 423/827 [27:37<23:41,  3.52s/it][A
 51%|█████▏    | 424/827 [27:41<23:41,  3.53s/it][A
 51%|█████▏    | 425/827 [27:45<25:21,  3.79s/it][A
 52%|█████▏    | 426/827 [27:50<27:32,  4.12s/it][A
 52%|█████▏    | 427/827 [27:55<28:44,  4.31s/it][A
 52%|█████▏    | 428/827 [28:00<29:32,  4.44s/it][A
 52%|█████▏    | 429/827 [28:03<27:50,  4.20s/it][A
 52%|█████▏    | 430/827 [28:07<26:40,  4.03s/it][A
 52%|█████▏    | 431/827 [28:11<26:32,  4.02s/it][A
 52%|█████▏    | 432/827 [28:16<28:45,  4.37s/it][A
 52%|█████▏    | 433/827 [28:22<32:27,  4.94s/it][A
 52%|█████▏    | 434/827 [28:27<32:40,  4.99s/it][A
 53%|█████▎    | 435/827 [28:30<28:36,  4.38s/it][A
 53%|█████▎    | 436/827 [28:34<27:56,  4.29s/it][A
 53%|█████▎    | 437/827 [28:39<28:07,  4.33s/it][A
 53%|█████▎    | 438/827 [28:42<26:05,  4.03s/it][A
 53%|█████▎    | 439/827 [28:46<24:51,  3.84s/it][A
 53%|█████▎    | 440/827 [28:49<24:52,  3.86s/it][A
 53%|█████▎    | 441/827 [28:52<22:53,  3.56s/it][A
 53%|█████▎    | 442/827 [28:55<21:25,  3.34s/it][A
 54%|█████▎    | 443/827 [28:58<20:48,  3.25s/it][A
 54%|█████▎    | 444/827 [29:02<21:33,  3.38s/it][A
 54%|█████▍    | 445/827 [29:07<25:17,  3.97s/it][A
 54%|█████▍    | 446/827 [29:10<23:42,  3.73s/it][A
 54%|█████▍    | 447/827 [29:15<24:54,  3.93s/it][A
 54%|█████▍    | 448/827 [29:20<26:28,  4.19s/it][A
 54%|█████▍    | 449/827 [29:27<32:25,  5.15s/it][A
 54%|█████▍    | 450/827 [29:32<32:33,  5.18s/it][A
 55%|█████▍    | 451/827 [29:36<29:31,  4.71s/it][A
 55%|█████▍    | 452/827 [29:41<30:06,  4.82s/it][A
 55%|█████▍    | 453/827 [29:46<31:09,  5.00s/it][A
 55%|█████▍    | 454/827 [29:50<28:02,  4.51s/it][A
 55%|█████▌    | 455/827 [29:54<26:57,  4.35s/it][A
 55%|█████▌    | 456/827 [30:00<29:37,  4.79s/it][A
 55%|█████▌    | 457/827 [30:04<28:44,  4.66s/it][A
 55%|█████▌    | 458/827 [30:08<27:01,  4.40s/it][A
 56%|█████▌    | 459/827 [30:12<26:49,  4.37s/it][A
 56%|█████▌    | 460/827 [30:18<28:57,  4.74s/it][A
 56%|█████▌    | 461/827 [30:25<33:31,  5.50s/it][A
 56%|█████▌    | 462/827 [30:29<31:25,  5.17s/it][A
 56%|█████▌    | 463/827 [30:32<27:20,  4.51s/it][A
 56%|█████▌    | 464/827 [30:37<27:27,  4.54s/it][A
 56%|█████▌    | 465/827 [30:41<26:47,  4.44s/it][A
 56%|█████▋    | 466/827 [30:44<24:50,  4.13s/it][A
 56%|█████▋    | 467/827 [30:48<24:21,  4.06s/it][A
 57%|█████▋    | 468/827 [30:52<24:27,  4.09s/it][A
 57%|█████▋    | 469/827 [30:57<25:38,  4.30s/it][A
 57%|█████▋    | 470/827 [31:00<23:29,  3.95s/it][A
 57%|█████▋    | 471/827 [31:04<23:39,  3.99s/it][A
 57%|█████▋    | 472/827 [31:11<27:45,  4.69s/it][A
 57%|█████▋    | 473/827 [31:15<26:50,  4.55s/it][A
 57%|█████▋    | 474/827 [31:19<25:37,  4.35s/it][A
 57%|█████▋    | 475/827 [31:22<23:04,  3.93s/it][A
 58%|█████▊    | 476/827 [31:24<20:14,  3.46s/it][A
 58%|█████▊    | 477/827 [31:27<18:18,  3.14s/it][A
 58%|█████▊    | 478/827 [31:30<19:20,  3.32s/it][A
 58%|█████▊    | 479/827 [31:35<21:56,  3.78s/it][A
 58%|█████▊    | 480/827 [31:39<22:40,  3.92s/it][A
 58%|█████▊    | 481/827 [31:44<23:58,  4.16s/it][A
 58%|█████▊    | 482/827 [31:47<21:35,  3.75s/it][A
 58%|█████▊    | 483/827 [31:49<18:50,  3.29s/it][A
 59%|█████▊    | 484/827 [31:52<18:28,  3.23s/it][A
 59%|█████▊    | 485/827 [31:55<17:40,  3.10s/it][A
 59%|█████▉    | 486/827 [31:58<17:56,  3.16s/it][A
 59%|█████▉    | 487/827 [32:02<18:21,  3.24s/it][A
 59%|█████▉    | 488/827 [32:07<20:52,  3.69s/it][A
 59%|█████▉    | 489/827 [32:11<22:24,  3.98s/it][A
 59%|█████▉    | 490/827 [32:16<23:12,  4.13s/it][A
 59%|█████▉    | 491/827 [32:19<21:32,  3.85s/it][A
 59%|█████▉    | 492/827 [32:22<20:48,  3.73s/it][A
 60%|█████▉    | 493/827 [32:26<20:08,  3.62s/it][A
 60%|█████▉    | 494/827 [32:29<19:52,  3.58s/it][A
 60%|█████▉    | 495/827 [32:33<19:52,  3.59s/it][A
 60%|█████▉    | 496/827 [32:37<20:05,  3.64s/it][A
 60%|██████    | 497/827 [32:39<18:24,  3.35s/it][A
 60%|██████    | 498/827 [32:42<17:55,  3.27s/it][A
 60%|██████    | 499/827 [32:47<19:39,  3.60s/it][A
 60%|██████    | 500/827 [32:50<18:53,  3.47s/it][A
 61%|██████    | 501/827 [32:53<17:39,  3.25s/it][A
 61%|██████    | 502/827 [32:57<19:25,  3.59s/it][A
 61%|██████    | 503/827 [33:02<21:04,  3.90s/it][A
 61%|██████    | 504/827 [33:05<19:51,  3.69s/it][A
 61%|██████    | 505/827 [33:08<19:45,  3.68s/it][A
 61%|██████    | 506/827 [33:12<19:34,  3.66s/it][A
 61%|██████▏   | 507/827 [33:17<21:33,  4.04s/it][A
 61%|██████▏   | 508/827 [33:23<24:54,  4.68s/it][A
 62%|██████▏   | 509/827 [33:26<21:37,  4.08s/it][A
 62%|██████▏   | 510/827 [33:29<20:48,  3.94s/it][A
 62%|██████▏   | 511/827 [33:32<19:15,  3.66s/it][A
 62%|██████▏   | 512/827 [33:35<17:51,  3.40s/it][A
 62%|██████▏   | 513/827 [33:39<17:57,  3.43s/it][A
 62%|██████▏   | 514/827 [33:42<18:04,  3.47s/it][A
 62%|██████▏   | 515/827 [33:45<17:36,  3.39s/it][A
 62%|██████▏   | 516/827 [33:49<18:12,  3.51s/it][A
 63%|██████▎   | 517/827 [33:53<17:51,  3.46s/it][A
 63%|██████▎   | 518/827 [33:55<16:43,  3.25s/it][A
 63%|██████▎   | 519/827 [33:58<16:11,  3.15s/it][A
 63%|██████▎   | 520/827 [34:02<16:12,  3.17s/it][A
 63%|██████▎   | 521/827 [34:06<17:43,  3.47s/it][A
 63%|██████▎   | 522/827 [34:11<20:16,  3.99s/it][A
 63%|██████▎   | 523/827 [34:14<18:13,  3.60s/it][A
 63%|██████▎   | 524/827 [34:17<18:36,  3.69s/it][A
 63%|██████▎   | 525/827 [34:23<21:22,  4.25s/it][A
 64%|██████▎   | 526/827 [34:29<23:21,  4.66s/it][A
 64%|██████▎   | 527/827 [34:33<22:16,  4.45s/it][A
 64%|██████▍   | 528/827 [34:37<22:23,  4.49s/it][A
 64%|██████▍   | 529/827 [34:42<22:51,  4.60s/it][A
 64%|██████▍   | 530/827 [34:47<22:35,  4.56s/it][A
 64%|██████▍   | 531/827 [34:49<19:59,  4.05s/it][A
 64%|██████▍   | 532/827 [34:53<19:03,  3.88s/it][A
 64%|██████▍   | 533/827 [34:58<20:38,  4.21s/it][A
 65%|██████▍   | 534/827 [35:03<22:01,  4.51s/it][A
 65%|██████▍   | 535/827 [35:08<22:40,  4.66s/it][A
 65%|██████▍   | 536/827 [35:12<20:53,  4.31s/it][A
 65%|██████▍   | 537/827 [35:14<18:06,  3.75s/it][A
 65%|██████▌   | 538/827 [35:17<17:01,  3.54s/it][A
 65%|██████▌   | 539/827 [35:20<16:13,  3.38s/it][A
 65%|██████▌   | 540/827 [35:23<16:11,  3.38s/it][A
 65%|██████▌   | 541/827 [35:27<16:43,  3.51s/it][A
 66%|██████▌   | 542/827 [35:31<17:02,  3.59s/it][A
 66%|██████▌   | 543/827 [35:35<18:13,  3.85s/it][A
 66%|██████▌   | 544/827 [35:40<19:24,  4.11s/it][A
 66%|██████▌   | 545/827 [35:44<19:17,  4.10s/it][A
 66%|██████▌   | 546/827 [35:48<19:19,  4.13s/it][A
 66%|██████▌   | 547/827 [35:53<19:24,  4.16s/it][A
 66%|██████▋   | 548/827 [35:55<17:15,  3.71s/it][A
 66%|██████▋   | 549/827 [35:58<16:00,  3.46s/it][A
 67%|██████▋   | 550/827 [36:02<15:54,  3.45s/it][A
 67%|██████▋   | 551/827 [36:06<16:31,  3.59s/it][A
 67%|██████▋   | 552/827 [36:09<16:22,  3.57s/it][A
 67%|██████▋   | 553/827 [36:13<16:13,  3.55s/it][A
 67%|██████▋   | 554/827 [36:15<14:46,  3.25s/it][A
 67%|██████▋   | 555/827 [36:18<14:21,  3.17s/it][A
 67%|██████▋   | 556/827 [36:21<14:17,  3.17s/it][A
 67%|██████▋   | 557/827 [36:24<13:29,  3.00s/it][A
 67%|██████▋   | 558/827 [36:28<14:35,  3.25s/it][A
 68%|██████▊   | 559/827 [36:32<15:20,  3.43s/it][A
 68%|██████▊   | 560/827 [36:35<14:54,  3.35s/it][A
 68%|██████▊   | 561/827 [36:38<14:45,  3.33s/it][A
 68%|██████▊   | 562/827 [36:42<15:13,  3.45s/it][A
 68%|██████▊   | 563/827 [36:45<15:08,  3.44s/it][A
 68%|██████▊   | 564/827 [36:49<15:00,  3.42s/it][A
 68%|██████▊   | 565/827 [36:53<16:21,  3.75s/it][A
 68%|██████▊   | 566/827 [36:58<17:19,  3.98s/it][A
 69%|██████▊   | 567/827 [37:01<16:02,  3.70s/it][A
 69%|██████▊   | 568/827 [37:04<15:44,  3.65s/it][A
 69%|██████▉   | 569/827 [37:07<15:01,  3.50s/it][A
 69%|██████▉   | 570/827 [37:11<15:11,  3.55s/it][A
 69%|██████▉   | 571/827 [37:16<17:02,  3.99s/it][A
 69%|██████▉   | 572/827 [37:21<18:15,  4.30s/it][A
 69%|██████▉   | 573/827 [37:26<19:27,  4.60s/it][A
 69%|██████▉   | 574/827 [37:30<18:32,  4.40s/it][A
 70%|██████▉   | 575/827 [37:35<19:29,  4.64s/it][A
 70%|██████▉   | 576/827 [37:39<18:25,  4.41s/it][A
 70%|██████▉   | 577/827 [37:43<16:50,  4.04s/it][A
 70%|██████▉   | 578/827 [37:46<16:24,  3.95s/it][A
 70%|███████   | 579/827 [37:49<15:23,  3.72s/it][A
 70%|███████   | 580/827 [37:54<16:29,  4.01s/it][A
 70%|███████   | 581/827 [38:01<19:44,  4.82s/it][A
 70%|███████   | 582/827 [38:05<19:00,  4.66s/it][A
 70%|███████   | 583/827 [38:08<17:04,  4.20s/it][A
 71%|███████   | 584/827 [38:11<15:38,  3.86s/it][A
 71%|███████   | 585/827 [38:14<14:24,  3.57s/it][A
 71%|███████   | 586/827 [38:18<14:38,  3.64s/it][A
 71%|███████   | 587/827 [38:25<18:40,  4.67s/it][A
 71%|███████   | 588/827 [38:32<21:29,  5.40s/it][A
 71%|███████   | 589/827 [38:37<20:27,  5.16s/it][A
 71%|███████▏  | 590/827 [38:43<21:10,  5.36s/it][A
 71%|███████▏  | 591/827 [38:49<22:40,  5.76s/it][A
 72%|███████▏  | 592/827 [38:53<19:39,  5.02s/it][A
 72%|███████▏  | 593/827 [38:56<17:44,  4.55s/it][A
 72%|███████▏  | 594/827 [38:59<15:52,  4.09s/it][A
 72%|███████▏  | 595/827 [39:02<14:49,  3.84s/it][A
 72%|███████▏  | 596/827 [39:09<18:00,  4.68s/it][A
 72%|███████▏  | 597/827 [39:15<19:29,  5.09s/it][A
 72%|███████▏  | 598/827 [39:18<17:00,  4.46s/it][A
 72%|███████▏  | 599/827 [39:21<15:38,  4.12s/it][A
 73%|███████▎  | 600/827 [39:25<15:37,  4.13s/it][A
 73%|███████▎  | 601/827 [39:29<15:17,  4.06s/it][A
 73%|███████▎  | 602/827 [39:33<14:54,  3.98s/it][A
 73%|███████▎  | 603/827 [39:36<13:46,  3.69s/it][A
 73%|███████▎  | 604/827 [39:40<13:33,  3.65s/it][A
 73%|███████▎  | 605/827 [39:43<13:04,  3.54s/it][A
 73%|███████▎  | 606/827 [39:46<12:55,  3.51s/it][A
 73%|███████▎  | 607/827 [39:50<13:20,  3.64s/it][A
 74%|███████▎  | 608/827 [39:54<12:50,  3.52s/it][A
 74%|███████▎  | 609/827 [39:59<14:33,  4.01s/it][A
 74%|███████▍  | 610/827 [40:05<17:01,  4.71s/it][A
 74%|███████▍  | 611/827 [40:09<15:46,  4.38s/it][A
 74%|███████▍  | 612/827 [40:13<15:23,  4.29s/it][A
 74%|███████▍  | 613/827 [40:17<15:29,  4.34s/it][A
 74%|███████▍  | 614/827 [40:21<15:05,  4.25s/it][A
 74%|███████▍  | 615/827 [40:25<14:15,  4.04s/it][A
 74%|███████▍  | 616/827 [40:28<13:18,  3.78s/it][A
 75%|███████▍  | 617/827 [40:33<14:11,  4.05s/it][A
 75%|███████▍  | 618/827 [40:39<16:39,  4.78s/it][A
 75%|███████▍  | 619/827 [40:42<14:33,  4.20s/it][A
 75%|███████▍  | 620/827 [40:46<13:54,  4.03s/it][A
 75%|███████▌  | 621/827 [40:48<12:26,  3.63s/it][A
 75%|███████▌  | 622/827 [40:51<11:36,  3.40s/it][A
 75%|███████▌  | 623/827 [40:54<11:21,  3.34s/it][A
 75%|███████▌  | 624/827 [40:59<12:33,  3.71s/it][A
 76%|███████▌  | 625/827 [41:02<12:10,  3.62s/it][A
 76%|███████▌  | 626/827 [41:06<11:56,  3.56s/it][A
 76%|███████▌  | 627/827 [41:09<11:45,  3.53s/it][A
 76%|███████▌  | 628/827 [41:12<10:59,  3.32s/it][A
 76%|███████▌  | 629/827 [41:15<10:43,  3.25s/it][A
 76%|███████▌  | 630/827 [41:18<10:16,  3.13s/it][A
 76%|███████▋  | 631/827 [41:23<12:07,  3.71s/it][A
 76%|███████▋  | 632/827 [41:28<13:17,  4.09s/it][A
 77%|███████▋  | 633/827 [41:33<13:55,  4.30s/it][A
 77%|███████▋  | 634/827 [41:39<15:49,  4.92s/it][A
 77%|███████▋  | 635/827 [41:42<13:48,  4.32s/it][A
 77%|███████▋  | 636/827 [41:46<13:19,  4.19s/it][A
 77%|███████▋  | 637/827 [41:51<13:45,  4.35s/it][A
 77%|███████▋  | 638/827 [41:55<13:54,  4.42s/it][A
 77%|███████▋  | 639/827 [41:58<12:36,  4.02s/it][A
 77%|███████▋  | 640/827 [42:01<11:30,  3.69s/it][A
 78%|███████▊  | 641/827 [42:05<11:44,  3.79s/it][A
 78%|███████▊  | 642/827 [42:09<11:33,  3.75s/it][A
 78%|███████▊  | 643/827 [42:13<12:04,  3.94s/it][A
 78%|███████▊  | 644/827 [42:18<12:31,  4.11s/it][A
 78%|███████▊  | 645/827 [42:21<11:45,  3.88s/it][A
 78%|███████▊  | 646/827 [42:24<10:50,  3.59s/it][A
 78%|███████▊  | 647/827 [42:27<10:31,  3.51s/it][A
 78%|███████▊  | 648/827 [42:31<10:43,  3.60s/it][A
 78%|███████▊  | 649/827 [42:35<10:41,  3.60s/it][A
 79%|███████▊  | 650/827 [42:38<10:06,  3.43s/it][A
 79%|███████▊  | 651/827 [42:41<09:51,  3.36s/it][A
 79%|███████▉  | 652/827 [42:44<09:33,  3.28s/it][A
 79%|███████▉  | 653/827 [42:48<09:46,  3.37s/it][A
 79%|███████▉  | 654/827 [42:53<11:17,  3.92s/it][A
 79%|███████▉  | 655/827 [42:58<12:32,  4.38s/it][A
 79%|███████▉  | 656/827 [43:01<11:00,  3.86s/it][A
 79%|███████▉  | 657/827 [43:04<10:09,  3.58s/it][A
 80%|███████▉  | 658/827 [43:08<10:44,  3.81s/it][A
 80%|███████▉  | 659/827 [43:16<13:52,  4.96s/it][A
 80%|███████▉  | 660/827 [43:21<13:52,  4.99s/it][A
 80%|███████▉  | 661/827 [43:26<14:06,  5.10s/it][A
 80%|████████  | 662/827 [43:30<13:01,  4.74s/it][A
 80%|████████  | 663/827 [43:34<12:16,  4.49s/it][A
 80%|████████  | 664/827 [43:39<12:35,  4.64s/it][A
 80%|████████  | 665/827 [43:43<11:44,  4.35s/it][A
 81%|████████  | 666/827 [43:47<11:27,  4.27s/it][A
 81%|████████  | 667/827 [43:50<10:31,  3.95s/it][A
 81%|████████  | 668/827 [43:54<10:20,  3.90s/it][A
 81%|████████  | 669/827 [43:57<09:23,  3.56s/it][A
 81%|████████  | 670/827 [44:00<08:51,  3.39s/it][A
 81%|████████  | 671/827 [44:03<08:54,  3.42s/it][A
 81%|████████▏ | 672/827 [44:07<08:51,  3.43s/it][A
 81%|████████▏ | 673/827 [44:10<08:31,  3.32s/it][A
 81%|████████▏ | 674/827 [44:13<08:25,  3.30s/it][A
 82%|████████▏ | 675/827 [44:17<08:36,  3.40s/it][A
 82%|████████▏ | 676/827 [44:21<09:21,  3.72s/it][A
 82%|████████▏ | 677/827 [44:26<10:19,  4.13s/it][A
 82%|████████▏ | 678/827 [44:31<10:37,  4.28s/it][A
 82%|████████▏ | 679/827 [44:35<10:39,  4.32s/it][A
 82%|████████▏ | 680/827 [44:38<09:32,  3.89s/it][A
 82%|████████▏ | 681/827 [44:43<10:07,  4.16s/it][A
 82%|████████▏ | 682/827 [44:48<10:39,  4.41s/it][A
 83%|████████▎ | 683/827 [44:51<09:25,  3.93s/it][A
 83%|████████▎ | 684/827 [44:54<08:39,  3.63s/it][A
 83%|████████▎ | 685/827 [44:56<07:54,  3.34s/it][A
 83%|████████▎ | 686/827 [45:02<09:19,  3.97s/it][A
 83%|████████▎ | 687/827 [45:09<11:19,  4.85s/it][A
 83%|████████▎ | 688/827 [45:13<11:10,  4.83s/it][A
 83%|████████▎ | 689/827 [45:18<11:04,  4.81s/it][A
 83%|████████▎ | 690/827 [45:21<09:51,  4.32s/it][A
 84%|████████▎ | 691/827 [45:25<08:58,  3.96s/it][A
 84%|████████▎ | 692/827 [45:28<08:54,  3.96s/it][A
 84%|████████▍ | 693/827 [45:32<08:24,  3.76s/it][A
 84%|████████▍ | 694/827 [45:35<07:56,  3.58s/it][A
 84%|████████▍ | 695/827 [45:39<08:06,  3.68s/it][A
 84%|████████▍ | 696/827 [45:44<08:41,  3.98s/it][A
 84%|████████▍ | 697/827 [45:47<08:15,  3.81s/it][A
 84%|████████▍ | 698/827 [45:49<07:22,  3.43s/it][A
 85%|████████▍ | 699/827 [45:52<06:57,  3.26s/it][A
 85%|████████▍ | 700/827 [45:59<08:46,  4.14s/it][A
 85%|████████▍ | 701/827 [46:05<10:00,  4.76s/it][A
 85%|████████▍ | 702/827 [46:09<09:38,  4.63s/it][A
 85%|████████▌ | 703/827 [46:13<09:01,  4.36s/it][A
 85%|████████▌ | 704/827 [46:16<08:10,  3.99s/it][A
 85%|████████▌ | 705/827 [46:19<07:15,  3.57s/it][A
 85%|████████▌ | 706/827 [46:21<06:40,  3.31s/it][A
 85%|████████▌ | 707/827 [46:25<06:53,  3.45s/it][A
 86%|████████▌ | 708/827 [46:28<06:46,  3.41s/it][A
 86%|████████▌ | 709/827 [46:32<06:55,  3.52s/it][A
 86%|████████▌ | 710/827 [46:36<06:49,  3.50s/it][A
 86%|████████▌ | 711/827 [46:39<06:49,  3.53s/it][A
 86%|████████▌ | 712/827 [46:42<06:24,  3.35s/it][A
 86%|████████▌ | 713/827 [46:45<06:19,  3.33s/it][A
 86%|████████▋ | 714/827 [46:49<06:32,  3.47s/it][A
 86%|████████▋ | 715/827 [46:53<06:56,  3.71s/it][A
 87%|████████▋ | 716/827 [46:57<06:46,  3.67s/it][A
 87%|████████▋ | 717/827 [47:02<07:27,  4.07s/it][A
 87%|████████▋ | 718/827 [47:07<07:52,  4.33s/it][A
 87%|████████▋ | 719/827 [47:10<07:11,  3.99s/it][A
 87%|████████▋ | 720/827 [47:14<06:50,  3.84s/it][A
 87%|████████▋ | 721/827 [47:17<06:21,  3.60s/it][A
 87%|████████▋ | 722/827 [47:20<06:18,  3.60s/it][A
 87%|████████▋ | 723/827 [47:24<06:07,  3.54s/it][A
 88%|████████▊ | 724/827 [47:26<05:33,  3.24s/it][A
 88%|████████▊ | 725/827 [47:31<06:14,  3.68s/it][A
 88%|████████▊ | 726/827 [47:36<07:03,  4.20s/it][A
 88%|████████▊ | 727/827 [47:40<06:53,  4.13s/it][A
 88%|████████▊ | 728/827 [47:43<06:18,  3.82s/it][A
 88%|████████▊ | 729/827 [47:46<05:36,  3.43s/it][A
 88%|████████▊ | 730/827 [47:50<05:40,  3.51s/it][A
 88%|████████▊ | 731/827 [47:54<06:11,  3.87s/it][A
 89%|████████▊ | 732/827 [47:59<06:22,  4.02s/it][A
 89%|████████▊ | 733/827 [48:03<06:25,  4.10s/it][A
 89%|████████▉ | 734/827 [48:05<05:35,  3.61s/it][A
 89%|████████▉ | 735/827 [48:09<05:35,  3.65s/it][A
 89%|████████▉ | 736/827 [48:14<06:16,  4.14s/it][A
 89%|████████▉ | 737/827 [48:19<06:30,  4.34s/it][A
 89%|████████▉ | 738/827 [48:22<05:51,  3.95s/it][A
 89%|████████▉ | 739/827 [48:27<06:05,  4.15s/it][A
 89%|████████▉ | 740/827 [48:33<06:38,  4.58s/it][A
 90%|████████▉ | 741/827 [48:36<06:12,  4.34s/it][A
 90%|████████▉ | 742/827 [48:40<05:53,  4.16s/it][A
 90%|████████▉ | 743/827 [48:45<06:19,  4.52s/it][A
 90%|████████▉ | 744/827 [48:49<05:55,  4.28s/it][A
 90%|█████████ | 745/827 [48:53<05:42,  4.17s/it][A
 90%|█████████ | 746/827 [48:58<06:03,  4.49s/it][A
 90%|█████████ | 747/827 [49:03<06:14,  4.68s/it][A
 90%|█████████ | 748/827 [49:07<05:44,  4.36s/it][A
 91%|█████████ | 749/827 [49:11<05:32,  4.26s/it][A
 91%|█████████ | 750/827 [49:15<05:29,  4.28s/it][A
 91%|█████████ | 751/827 [49:18<04:57,  3.91s/it][A
 91%|█████████ | 752/827 [49:21<04:30,  3.61s/it][A
 91%|█████████ | 753/827 [49:24<04:08,  3.35s/it][A
 91%|█████████ | 754/827 [49:27<03:53,  3.20s/it][A
 91%|█████████▏| 755/827 [49:31<04:10,  3.47s/it][A
 91%|█████████▏| 756/827 [49:36<04:38,  3.92s/it][A
 92%|█████████▏| 757/827 [49:40<04:31,  3.89s/it][A
 92%|█████████▏| 758/827 [49:44<04:34,  3.98s/it][A
 92%|█████████▏| 759/827 [49:47<04:09,  3.67s/it][A
 92%|█████████▏| 760/827 [49:52<04:37,  4.15s/it][A
 92%|█████████▏| 761/827 [49:59<05:23,  4.90s/it][A
 92%|█████████▏| 762/827 [50:05<05:34,  5.15s/it][A
 92%|█████████▏| 763/827 [50:11<05:46,  5.42s/it][A
 92%|█████████▏| 764/827 [50:14<05:00,  4.77s/it][A
 93%|█████████▎| 765/827 [50:17<04:18,  4.17s/it][A
 93%|█████████▎| 766/827 [50:19<03:49,  3.76s/it][A
 93%|█████████▎| 767/827 [50:23<03:40,  3.67s/it][A
 93%|█████████▎| 768/827 [50:27<03:36,  3.67s/it][A
 93%|█████████▎| 769/827 [50:31<03:45,  3.89s/it][A
 93%|█████████▎| 770/827 [50:34<03:32,  3.73s/it][A
 93%|█████████▎| 771/827 [50:38<03:28,  3.72s/it][A
 93%|█████████▎| 772/827 [50:42<03:21,  3.66s/it][A
 93%|█████████▎| 773/827 [50:46<03:22,  3.76s/it][A
 94%|█████████▎| 774/827 [50:49<03:10,  3.59s/it][A
 94%|█████████▎| 775/827 [50:53<03:09,  3.65s/it][A
 94%|█████████▍| 776/827 [50:56<03:02,  3.57s/it][A
 94%|█████████▍| 777/827 [50:59<02:53,  3.46s/it][A
 94%|█████████▍| 778/827 [51:03<02:50,  3.48s/it][A
 94%|█████████▍| 779/827 [51:06<02:39,  3.31s/it][A
 94%|█████████▍| 780/827 [51:09<02:31,  3.23s/it][A
 94%|█████████▍| 781/827 [51:12<02:24,  3.15s/it][A
 95%|█████████▍| 782/827 [51:14<02:17,  3.05s/it][A
 95%|█████████▍| 783/827 [51:18<02:18,  3.14s/it][A
 95%|█████████▍| 784/827 [51:21<02:15,  3.15s/it][A
 95%|█████████▍| 785/827 [51:25<02:18,  3.29s/it][A
 95%|█████████▌| 786/827 [51:29<02:24,  3.51s/it][A
 95%|█████████▌| 787/827 [51:34<02:42,  4.06s/it][A
 95%|█████████▌| 788/827 [51:38<02:34,  3.97s/it][A
 95%|█████████▌| 789/827 [51:42<02:35,  4.08s/it][A
 96%|█████████▌| 790/827 [51:46<02:25,  3.93s/it][A
 96%|█████████▌| 791/827 [51:49<02:10,  3.64s/it][A
 96%|█████████▌| 792/827 [51:52<02:00,  3.44s/it][A
 96%|█████████▌| 793/827 [51:56<02:07,  3.74s/it][A
 96%|█████████▌| 794/827 [52:01<02:18,  4.19s/it][A
 96%|█████████▌| 795/827 [52:07<02:29,  4.66s/it][A
 96%|█████████▋| 796/827 [52:12<02:26,  4.73s/it][A
 96%|█████████▋| 797/827 [52:15<02:05,  4.18s/it][A
 96%|█████████▋| 798/827 [52:18<01:49,  3.76s/it][A
 97%|█████████▋| 799/827 [52:20<01:34,  3.37s/it][A
 97%|█████████▋| 800/827 [52:26<01:54,  4.24s/it][A
 97%|█████████▋| 801/827 [52:33<02:12,  5.09s/it][A
 97%|█████████▋| 802/827 [52:38<02:02,  4.89s/it][A
 97%|█████████▋| 803/827 [52:43<01:59,  4.99s/it][A
 97%|█████████▋| 804/827 [52:48<01:54,  4.97s/it][A
 97%|█████████▋| 805/827 [52:51<01:36,  4.39s/it][A
 97%|█████████▋| 806/827 [52:54<01:21,  3.88s/it][A
 98%|█████████▊| 807/827 [52:57<01:13,  3.69s/it][A
 98%|█████████▊| 808/827 [53:01<01:09,  3.67s/it][A
 98%|█████████▊| 809/827 [53:04<01:03,  3.52s/it][A
 98%|█████████▊| 810/827 [53:07<00:59,  3.49s/it][A
 98%|█████████▊| 811/827 [53:11<00:55,  3.49s/it][A
 98%|█████████▊| 812/827 [53:14<00:54,  3.60s/it][A
 98%|█████████▊| 813/827 [53:20<00:56,  4.05s/it][A
 98%|█████████▊| 814/827 [53:25<00:57,  4.45s/it][A
 99%|█████████▊| 815/827 [53:29<00:50,  4.24s/it][A
 99%|█████████▊| 816/827 [53:33<00:45,  4.13s/it][A
 99%|█████████▉| 817/827 [53:36<00:38,  3.81s/it][A
 99%|█████████▉| 818/827 [53:41<00:38,  4.23s/it][A
 99%|█████████▉| 819/827 [53:46<00:35,  4.44s/it][A
 99%|█████████▉| 820/827 [53:50<00:31,  4.48s/it][A
 99%|█████████▉| 821/827 [53:54<00:25,  4.24s/it][A
 99%|█████████▉| 822/827 [53:58<00:20,  4.05s/it][A
100%|█████████▉| 823/827 [54:00<00:14,  3.64s/it][A
100%|█████████▉| 824/827 [54:03<00:10,  3.41s/it][A
100%|█████████▉| 825/827 [54:08<00:07,  3.77s/it][A
100%|█████████▉| 826/827 [54:13<00:04,  4.33s/it][A
100%|██████████| 827/827 [54:16<00:00,  3.75s/it][A                                                      
                                                 [A{'eval_loss': 0.7679453492164612, 'eval_runtime': 3261.456, 'eval_samples_per_second': 1.013, 'eval_steps_per_second': 0.254, 'epoch': 0.86}
 86%|████████▌ | 400/465 [12:42:32<1:47:31, 99.25s/it]
100%|██████████| 827/827 [54:16<00:00,  3.75s/it][A
                                                 [A[INFO|trainer.py:4309] 2025-12-19 19:41:58,753 >> Saving model checkpoint to saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-400
/opt/conda/lib/python3.11/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in Qwen/Qwen2.5-VL-7B-Instruct - will assume that the vocabulary was not modified.
  warnings.warn(
[INFO|tokenization_utils_base.py:2421] 2025-12-19 19:41:58,836 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-400/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-19 19:41:58,840 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-19 19:41:58,843 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-400/special_tokens_map.json
[2025-12-19 19:41:59,924] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step400 is about to be saved!
[2025-12-19 19:41:59,946] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-400/global_step400/mp_rank_00_model_states.pt
[2025-12-19 19:41:59,946] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-400/global_step400/mp_rank_00_model_states.pt...
[2025-12-19 19:42:00,014] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-400/global_step400/mp_rank_00_model_states.pt.
[2025-12-19 19:42:00,017] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-12-19 19:42:00,017] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2025-12-19 19:42:00,017] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2025-12-19 19:42:00,018] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2025-12-19 19:42:00,092] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2025-12-19 19:42:00,092] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2025-12-19 19:42:00,092] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2025-12-19 19:42:00,109] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2025-12-19 19:42:00,109] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2025-12-19 19:42:00,109] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2025-12-19 19:42:00,114] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2025-12-19 19:42:00,115] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2025-12-19 19:42:00,115] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[2025-12-19 19:42:00,185] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-12-19 19:42:00,189] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-12-19 19:42:00,189] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[INFO|image_processing_base.py:253] 2025-12-19 19:42:00,215 >> Image processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-400/preprocessor_config.json
[INFO|tokenization_utils_base.py:2421] 2025-12-19 19:42:00,218 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-400/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-19 19:42:00,220 >> tokenizer config file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-19 19:42:00,223 >> Special tokens file saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-400/special_tokens_map.json
[INFO|video_processing_utils.py:600] 2025-12-19 19:42:00,336 >> Video processor saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-400/video_preprocessor_config.json
[INFO|processing_utils.py:814] 2025-12-19 19:42:00,338 >> chat template saved in saves/qwen2_5vl-7b/lora/sft/SQA3Devery24_R12C12F12X62_traineval/checkpoint-400/chat_template.jinja
 86%|████████▌ | 401/465 [12:44:15<19:10:43, 1078.80s/it] 86%|████████▋ | 402/465 [12:45:59<13:45:35, 786.27s/it]  87%|████████▋ | 403/465 [12:47:32<9:57:32, 578.27s/it] W1219 19:47:10.986000 2667010 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2667075 closing signal SIGTERM
W1219 19:47:10.989000 2667010 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2667076 closing signal SIGTERM
W1219 19:47:10.990000 2667010 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2667078 closing signal SIGTERM
2025/12/19 19:47:11 INFO server: parent process exited, terminating service process
E1219 19:47:12.076000 2667010 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: -7) local_rank: 2 (pid: 2667077) of binary: /opt/conda/bin/python3.11
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/opt/conda/lib/python3.11/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/opt/conda/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=======================================================
/app/src/llamafactory/launcher.py FAILED
-------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
-------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-12-19_19:47:10
  host      : trig0061.scinet.local
  rank      : 2 (local_rank: 2)
  exitcode  : -7 (pid: 2667077)
  error_file: <N/A>
  traceback : Signal 7 (SIGBUS) received by PID 2667077
=======================================================
Traceback (most recent call last):
  File "/opt/conda/bin/llamafactory-cli", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/app/src/llamafactory/cli.py", line 24, in main
    launcher.launch()
  File "/app/src/llamafactory/launcher.py", line 110, in launch
    process = subprocess.run(
              ^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/subprocess.py", line 571, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['torchrun', '--nnodes', '1', '--node_rank', '0', '--nproc_per_node', '4', '--master_addr', '127.0.0.1', '--master_port', '50645', '/app/src/llamafactory/launcher.py', '/scratch/indrisch/LLaMA-Factory/examples/train_lora/qwen2_5vl_lora_sft_SQA3Devery24_R12C12F12X62_traineval.yaml']' returned non-zero exit status 1.
INFO:    Terminating fuse-overlayfs after timeout
INFO:    Timeouts can be caused by a running background process

scontrol show job 160343
JobId=160343 JobName=slurm_qwen2_5vl_lora_sft_SQA3Devery24_R12C12F12X62_traineval.sh
   UserId=indrisch(3122343) GroupId=indrisch(3122343) MCS_label=N/A
   Priority=136603 Nice=0 Account=def-wangcs QOS=normal
   JobState=COMPLETING Reason=NonZeroExitCode Dependency=(null)
   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=1:0
   RunTime=13:11:05 TimeLimit=21:00:00 TimeMin=N/A
   SubmitTime=2025-12-19T02:39:41 EligibleTime=2025-12-19T02:39:41
   AccrueTime=2025-12-19T02:39:41
   StartTime=2025-12-19T06:36:10 EndTime=2025-12-19T19:47:15 Deadline=N/A
   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2025-12-19T06:36:10 Scheduler=Backfill
   Partition=compute_full_node AllocNode:Sid=trig-login01:3659788
   ReqNodeList=(null) ExcNodeList=(null)
   NodeList=trig0061
   BatchHost=trig0061
   NumNodes=1 NumCPUs=96 NumTasks=1 CPUs/Task=96 ReqB:S:C:T=0:0:*:*
   ReqTRES=cpu=96,mem=770000M,node=1,billing=4,gres/gpu=4
   AllocTRES=cpu=96,mem=770000M,node=1,billing=4,gres/gpu=4
   Socks/Node=* NtasksPerN:B:S:C=1:0:*:* CoreSpec=*
   MinCPUsNode=96 MinMemoryNode=770000M MinTmpDiskNode=0
   Features=(null) DelayBoot=00:00:00
   OverSubscribe=NO Contiguous=0 Licenses=(null) Network=(null)
   Command=/scratch/indrisch/LLaMA-Factory/preliminaries/qwen2_5vl_lora_sft_SQA3D/slurm_qwen2_5vl_lora_sft_SQA3Devery24_R12C12F12X62_traineval.sh
   WorkDir=/scratch/indrisch/LLaMA-Factory/preliminaries/qwen2_5vl_lora_sft_SQA3D
   Comment=/opt/slurm/bin/sbatch --export=NONE --get-user-env=L slurm_qwen2_5vl_lora_sft_SQA3Devery24_R12C12F12X62_traineval.sh 
   StdErr=/scratch/indrisch/LLaMA-Factory/preliminaries/qwen2_5vl_lora_sft_SQA3D/%N-qwen2_5vl_lora_sft_SQA3Devery24_R12C12F12X62_traineval-160343.out
   StdIn=/dev/null
   StdOut=/scratch/indrisch/LLaMA-Factory/preliminaries/qwen2_5vl_lora_sft_SQA3D/%N-qwen2_5vl_lora_sft_SQA3Devery24_R12C12F12X62_traineval-160343.out
   TresPerNode=gres/gpu:h100:4
   TresPerTask=cpu=96
   

sacct -j 160343
JobID           JobName    Account    Elapsed  MaxVMSize     MaxRSS  SystemCPU    UserCPU ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- ---------- ---------- -------- 
160343       slurm_qwe+ def-wangcs   13:11:05                         00:00:00   00:00:00      0:0 
160343.batch      batch def-wangcs   13:11:05                         00:00:00   00:00:00      0:0 
160343.exte+     extern def-wangcs   13:11:05                         00:00:00   00:00:00      0:0 

