WARNING: Skipping /dev/shm bind mount: already mounted

==========
== CUDA ==
==========

CUDA Version 12.4.1

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

[INFO|2026-01-01 04:44:37] llamafactory.launcher:143 >> Initializing 4 distributed tasks at: 127.0.0.1:52561
W0101 04:44:38.765000 165 site-packages/torch/distributed/run.py:792] 
W0101 04:44:38.765000 165 site-packages/torch/distributed/run.py:792] *****************************************
W0101 04:44:38.765000 165 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0101 04:44:38.765000 165 site-packages/torch/distributed/run.py:792] *****************************************
[DEBUG template.py] Starting videor1 template registration at line 2117
[DEBUG template.py] Current templates before videor1 registration: ['alpaca', 'aquila', 'atom', 'baichuan', 'baichuan2', 'bailing', 'bailing_v2', 'belle', 'bluelm', 'breeze']...
[DEBUG template.py] Attempting to get videor1 plugin...
[DEBUG template.py] Successfully got videor1 plugin: <class 'llamafactory.data.mm_plugin.Videor1Plugin'>
[DEBUG template.py] About to call register_template for videor1...
[DEBUG template.py] Successfully registered videor1 template
[DEBUG template.py] Templates after videor1 registration: ['alpaca', 'aquila', 'atom', 'baichuan', 'baichuan2', 'bailing', 'bailing_v2', 'belle', 'bluelm', 'breeze', 'chatglm2', 'chatglm3', 'chatml', 'chatml_de', 'codegeex2']...
[DEBUG template.py] videor1 in TEMPLATES: True
[DEBUG template.py] Module template.py loaded completely. Total templates registered: 120
[DEBUG template.py] videor1 template registered: True
[DEBUG template.py] videor1 template details: Template(format_user=StringFormatter(slots=['<|im_start|>user\n{{content}}<|im_end|>\n<|im_start|>assistant\n'], tool_format=None), format_assistant=StringFormatter(slots=['{{content}}<|im_end|>\n'], tool_format=None), format_system=StringFormatter(slots=['<|im_start|>system\n{{content}}<|im_end|>\n'], tool_format=None), format_function=FunctionFormatter(slots=['{{content}}<|im_end|>\n'], tool_format='qwen'), format_observation=StringFormatter(slots=['<|im_start|>user\n<tool_response>\n{{content}}\n</tool_response><|im_end|>\n<|im_start|>assistant\n'], tool_format=None), format_tools=ToolFormatter(slots=[], tool_format='qwen'), format_prefix=EmptyFormatter(slots=[], tool_format=None), default_system='You are a helpful assistant.', stop_words=['<|im_end|>'], thought_words=('<think>\n', '\n</think>\n\n'), efficient_eos=False, replace_eos=True, replace_jinja_template=False, enable_thinking=True, mm_plugin=Videor1Plugin(image_token='<|image_pad|>', video_token='<|video_pad|>', audio_token=None, expand_mm_tokens=True, vision_bos_token='<|vision_start|>', vision_eos_token='<|vision_end|>'))
⚙️  Running in WANDB offline mode
[DEBUG template.py] Starting videor1 template registration at line 2117
[DEBUG template.py] Current templates before videor1 registration: ['alpaca', 'aquila', 'atom', 'baichuan', 'baichuan2', 'bailing', 'bailing_v2', 'belle', 'bluelm', 'breeze']...
[DEBUG template.py] Attempting to get videor1 plugin...
[DEBUG template.py] Successfully got videor1 plugin: <class 'llamafactory.data.mm_plugin.Videor1Plugin'>
[DEBUG template.py] About to call register_template for videor1...
[DEBUG template.py] Successfully registered videor1 template
[DEBUG template.py] Templates after videor1 registration: ['alpaca', 'aquila', 'atom', 'baichuan', 'baichuan2', 'bailing', 'bailing_v2', 'belle', 'bluelm', 'breeze', 'chatglm2', 'chatglm3', 'chatml', 'chatml_de', 'codegeex2']...
[DEBUG template.py] videor1 in TEMPLATES: True
[DEBUG template.py] Module template.py loaded completely. Total templates registered: 120
[DEBUG template.py] videor1 template registered: True
[DEBUG template.py] videor1 template details: Template(format_user=StringFormatter(slots=['<|im_start|>user\n{{content}}<|im_end|>\n<|im_start|>assistant\n'], tool_format=None), format_assistant=StringFormatter(slots=['{{content}}<|im_end|>\n'], tool_format=None), format_system=StringFormatter(slots=['<|im_start|>system\n{{content}}<|im_end|>\n'], tool_format=None), format_function=FunctionFormatter(slots=['{{content}}<|im_end|>\n'], tool_format='qwen'), format_observation=StringFormatter(slots=['<|im_start|>user\n<tool_response>\n{{content}}\n</tool_response><|im_end|>\n<|im_start|>assistant\n'], tool_format=None), format_tools=ToolFormatter(slots=[], tool_format='qwen'), format_prefix=EmptyFormatter(slots=[], tool_format=None), default_system='You are a helpful assistant.', stop_words=['<|im_end|>'], thought_words=('<think>\n', '\n</think>\n\n'), efficient_eos=False, replace_eos=True, replace_jinja_template=False, enable_thinking=True, mm_plugin=Videor1Plugin(image_token='<|image_pad|>', video_token='<|video_pad|>', audio_token=None, expand_mm_tokens=True, vision_bos_token='<|vision_start|>', vision_eos_token='<|vision_end|>'))
[DEBUG template.py] Starting videor1 template registration at line 2117
[DEBUG template.py] Current templates before videor1 registration: ['alpaca', 'aquila', 'atom', 'baichuan', 'baichuan2', 'bailing', 'bailing_v2', 'belle', 'bluelm', 'breeze']...
[DEBUG template.py] Attempting to get videor1 plugin...
[DEBUG template.py] Successfully got videor1 plugin: <class 'llamafactory.data.mm_plugin.Videor1Plugin'>
[DEBUG template.py] About to call register_template for videor1...
[DEBUG template.py] Successfully registered videor1 template
[DEBUG template.py] Templates after videor1 registration: ['alpaca', 'aquila', 'atom', 'baichuan', 'baichuan2', 'bailing', 'bailing_v2', 'belle', 'bluelm', 'breeze', 'chatglm2', 'chatglm3', 'chatml', 'chatml_de', 'codegeex2']...
[DEBUG template.py] videor1 in TEMPLATES: True
[DEBUG template.py] Module template.py loaded completely. Total templates registered: 120
[DEBUG template.py] videor1 template registered: True
[DEBUG template.py] videor1 template details: Template(format_user=StringFormatter(slots=['<|im_start|>user\n{{content}}<|im_end|>\n<|im_start|>assistant\n'], tool_format=None), format_assistant=StringFormatter(slots=['{{content}}<|im_end|>\n'], tool_format=None), format_system=StringFormatter(slots=['<|im_start|>system\n{{content}}<|im_end|>\n'], tool_format=None), format_function=FunctionFormatter(slots=['{{content}}<|im_end|>\n'], tool_format='qwen'), format_observation=StringFormatter(slots=['<|im_start|>user\n<tool_response>\n{{content}}\n</tool_response><|im_end|>\n<|im_start|>assistant\n'], tool_format=None), format_tools=ToolFormatter(slots=[], tool_format='qwen'), format_prefix=EmptyFormatter(slots=[], tool_format=None), default_system='You are a helpful assistant.', stop_words=['<|im_end|>'], thought_words=('<think>\n', '\n</think>\n\n'), efficient_eos=False, replace_eos=True, replace_jinja_template=False, enable_thinking=True, mm_plugin=Videor1Plugin(image_token='<|image_pad|>', video_token='<|video_pad|>', audio_token=None, expand_mm_tokens=True, vision_bos_token='<|vision_start|>', vision_eos_token='<|vision_end|>'))
[DEBUG template.py] Starting videor1 template registration at line 2117
[DEBUG template.py] Current templates before videor1 registration: ['alpaca', 'aquila', 'atom', 'baichuan', 'baichuan2', 'bailing', 'bailing_v2', 'belle', 'bluelm', 'breeze']...
[DEBUG template.py] Attempting to get videor1 plugin...
[DEBUG template.py] Successfully got videor1 plugin: <class 'llamafactory.data.mm_plugin.Videor1Plugin'>
[DEBUG template.py] About to call register_template for videor1...
[DEBUG template.py] Successfully registered videor1 template
[DEBUG template.py] Templates after videor1 registration: ['alpaca', 'aquila', 'atom', 'baichuan', 'baichuan2', 'bailing', 'bailing_v2', 'belle', 'bluelm', 'breeze', 'chatglm2', 'chatglm3', 'chatml', 'chatml_de', 'codegeex2']...
[DEBUG template.py] videor1 in TEMPLATES: True
[DEBUG template.py] Module template.py loaded completely. Total templates registered: 120
[DEBUG template.py] videor1 template registered: True
[DEBUG template.py] videor1 template details: Template(format_user=StringFormatter(slots=['<|im_start|>user\n{{content}}<|im_end|>\n<|im_start|>assistant\n'], tool_format=None), format_assistant=StringFormatter(slots=['{{content}}<|im_end|>\n'], tool_format=None), format_system=StringFormatter(slots=['<|im_start|>system\n{{content}}<|im_end|>\n'], tool_format=None), format_function=FunctionFormatter(slots=['{{content}}<|im_end|>\n'], tool_format='qwen'), format_observation=StringFormatter(slots=['<|im_start|>user\n<tool_response>\n{{content}}\n</tool_response><|im_end|>\n<|im_start|>assistant\n'], tool_format=None), format_tools=ToolFormatter(slots=[], tool_format='qwen'), format_prefix=EmptyFormatter(slots=[], tool_format=None), default_system='You are a helpful assistant.', stop_words=['<|im_end|>'], thought_words=('<think>\n', '\n</think>\n\n'), efficient_eos=False, replace_eos=True, replace_jinja_template=False, enable_thinking=True, mm_plugin=Videor1Plugin(image_token='<|image_pad|>', video_token='<|video_pad|>', audio_token=None, expand_mm_tokens=True, vision_bos_token='<|vision_start|>', vision_eos_token='<|vision_end|>'))
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
⚙️  Running in WANDB offline mode
[2026-01-01 04:44:46,574] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-01 04:44:47,245] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-01 04:44:47,248] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2026-01-01 04:44:47,324] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/opt/conda/lib/python3.11/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
[2026-01-01 04:44:50,907] [INFO] [comm.py:669:init_distributed] cdb=None
[2026-01-01 04:44:50,907] [INFO] [comm.py:669:init_distributed] cdb=None
[2026-01-01 04:44:50,911] [INFO] [comm.py:669:init_distributed] cdb=None
[2026-01-01 04:44:50,951] [INFO] [comm.py:669:init_distributed] cdb=None
[2026-01-01 04:44:50,951] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[INFO|2026-01-01 04:44:51] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.
[INFO|2026-01-01 04:44:51] llamafactory.hparams.parser:455 >> Process rank: 0, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|hub.py:421] 2026-01-01 04:44:51,478 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:1949] 2026-01-01 04:44:51,489 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2026-01-01 04:44:51,494 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2026-01-01 04:44:51,494 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2026-01-01 04:44:51,494 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2026-01-01 04:44:51,494 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-01 04:44:51,494 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-01 04:44:51,494 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2026-01-01 04:44:51,494 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2026-01-01 04:44:51,729 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|hub.py:421] 2026-01-01 04:44:51,730 >> Offline mode: forcing local_files_only=True
[INFO|hub.py:421] 2026-01-01 04:44:51,732 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:316] 2026-01-01 04:44:51,734 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2026-01-01 04:44:51,735 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:316] 2026-01-01 04:44:51,740 >> Offline mode: forcing local_files_only=True
[INFO|image_processing_base.py:383] 2026-01-01 04:44:51,741 >> loading configuration file preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|image_processing_base.py:428] 2026-01-01 04:44:51,754 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:1949] 2026-01-01 04:44:51,754 >> Offline mode: forcing local_files_only=True
[INFO|tokenization_utils_base.py:2095] 2026-01-01 04:44:51,756 >> loading file vocab.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
[INFO|tokenization_utils_base.py:2095] 2026-01-01 04:44:51,756 >> loading file merges.txt from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
[INFO|tokenization_utils_base.py:2095] 2026-01-01 04:44:51,756 >> loading file tokenizer.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2026-01-01 04:44:51,756 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-01 04:44:51,756 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2026-01-01 04:44:51,756 >> loading file tokenizer_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2026-01-01 04:44:51,756 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2026-01-01 04:44:51,951 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:660] 2026-01-01 04:44:51,953 >> Offline mode: forcing local_files_only=True
[INFO|video_processing_utils.py:726] 2026-01-01 04:44:51,954 >> loading configuration file video_preprocessor_config.json from cache at /scratch/indrisch/huggingface/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
[INFO|video_processing_utils.py:770] 2026-01-01 04:44:51,958 >> Video processor Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

[INFO|processing_utils.py:928] 2026-01-01 04:44:51,959 >> Offline mode: forcing local_files_only=True
[INFO|processing_utils.py:1116] 2026-01-01 04:44:51,962 >> loading configuration file processor_config.json from cache at None
[INFO|processing_utils.py:1199] 2026-01-01 04:44:52,304 >> Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_pixels": 3136,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 12845056,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 3136,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2_5_VLProcessor"
}

[DEBUG get_template_and_fix_tokenizer] Requested template: qwen2_vl
[DEBUG get_template_and_fix_tokenizer] Available templates: ['alpaca', 'aquila', 'atom', 'baichuan', 'baichuan2', 'bailing', 'bailing_v2', 'belle', 'bluelm', 'breeze', 'chatglm2', 'chatglm3', 'chatml', 'chatml_de', 'codegeex2', 'codegeex4', 'cohere', 'cpm', 'cpm3', 'cpm4']...
[DEBUG get_template_and_fix_tokenizer] Total templates registered: 120
[DEBUG get_template_and_fix_tokenizer] videor1 in TEMPLATES: True
[INFO|2026-01-01 04:44:52] llamafactory.data.loader:143 >> Loading dataset /scratch/indrisch/huggingface/hub/datasets--cvis-tmu--llamafactory-sqa3d-traces-multiimage-vqa_R0.0_C1.0_F0.0_X0.0/snapshots/cd6673598d0ac0c9d3e27e56ca6e6cc39b5b4c8a/data/...
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
Setting num_proc from 16 to 2 for the train split as it only contains 2 shards.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 3304 examples [00:00, 18415.73 examples/s]Generating train split: 12252 examples [00:00, 25443.42 examples/s]Generating train split: 21200 examples [00:00, 38475.73 examples/s]Generating train split: 33047 examples [00:00, 42446.42 examples/s]
[INFO|2026-01-01 04:44:53] llamafactory.hparams.parser:455 >> Process rank: 3, world size: 4, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
[INFO|2026-01-01 04:44:53] llamafactory.hparams.parser:455 >> Process rank: 1, world size: 4, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2026-01-01 04:44:53] llamafactory.hparams.parser:455 >> Process rank: 2, world size: 4, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
Converting format of dataset (num_proc=16):   0%|          | 0/33047 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   0%|          | 1/33047 [00:00<5:11:40,  1.77 examples/s][DEBUG get_template_and_fix_tokenizer] Requested template: qwen2_vl
[DEBUG get_template_and_fix_tokenizer] Available templates: ['alpaca', 'aquila', 'atom', 'baichuan', 'baichuan2', 'bailing', 'bailing_v2', 'belle', 'bluelm', 'breeze', 'chatglm2', 'chatglm3', 'chatml', 'chatml_de', 'codegeex2', 'codegeex4', 'cohere', 'cpm', 'cpm3', 'cpm4']...
[DEBUG get_template_and_fix_tokenizer] Total templates registered: 120
[DEBUG get_template_and_fix_tokenizer] videor1 in TEMPLATES: True
[DEBUG get_template_and_fix_tokenizer] Requested template: qwen2_vl
[DEBUG get_template_and_fix_tokenizer] Available templates: ['alpaca', 'aquila', 'atom', 'baichuan', 'baichuan2', 'bailing', 'bailing_v2', 'belle', 'bluelm', 'breeze', 'chatglm2', 'chatglm3', 'chatml', 'chatml_de', 'codegeex2', 'codegeex4', 'cohere', 'cpm', 'cpm3', 'cpm4']...
[DEBUG get_template_and_fix_tokenizer] Total templates registered: 120
[DEBUG get_template_and_fix_tokenizer] videor1 in TEMPLATES: True
[rank3]:[W101 04:44:54.468805210 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W101 04:44:54.469681723 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[DEBUG get_template_and_fix_tokenizer] Requested template: qwen2_vl
[DEBUG get_template_and_fix_tokenizer] Available templates: ['alpaca', 'aquila', 'atom', 'baichuan', 'baichuan2', 'bailing', 'bailing_v2', 'belle', 'bluelm', 'breeze', 'chatglm2', 'chatglm3', 'chatml', 'chatml_de', 'codegeex2', 'codegeex4', 'cohere', 'cpm', 'cpm3', 'cpm4']...
[DEBUG get_template_and_fix_tokenizer] Total templates registered: 120
[DEBUG get_template_and_fix_tokenizer] videor1 in TEMPLATES: True
[rank1]:[W101 04:44:54.510342383 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Converting format of dataset (num_proc=16):   0%|          | 2/33047 [00:01<4:31:15,  2.03 examples/s]Converting format of dataset (num_proc=16):   0%|          | 4/33047 [00:01<2:06:53,  4.34 examples/s]Converting format of dataset (num_proc=16):   0%|          | 8/33047 [00:01<1:05:02,  8.47 examples/s]Converting format of dataset (num_proc=16):   0%|          | 10/33047 [00:01<1:11:28,  7.70 examples/s]Converting format of dataset (num_proc=16):   0%|          | 11/33047 [00:01<1:22:48,  6.65 examples/s]Converting format of dataset (num_proc=16):   0%|          | 12/33047 [00:02<1:23:54,  6.56 examples/s]Converting format of dataset (num_proc=16):   0%|          | 15/33047 [00:02<57:59,  9.49 examples/s]  Converting format of dataset (num_proc=16):   0%|          | 18/33047 [00:02<42:27, 12.97 examples/s]Converting format of dataset (num_proc=16):   0%|          | 20/33047 [00:02<57:14,  9.62 examples/s]Converting format of dataset (num_proc=16):   0%|          | 24/33047 [00:02<40:04, 13.73 examples/s]Converting format of dataset (num_proc=16):   0%|          | 26/33047 [00:02<37:50, 14.54 examples/s]Converting format of dataset (num_proc=16):   0%|          | 28/33047 [00:03<49:08, 11.20 examples/s]Converting format of dataset (num_proc=16):   0%|          | 30/33047 [00:03<1:22:28,  6.67 examples/s]Converting format of dataset (num_proc=16):   0%|          | 32/33047 [00:04<1:15:58,  7.24 examples/s]Converting format of dataset (num_proc=16):   0%|          | 34/33047 [00:04<1:05:16,  8.43 examples/s]Converting format of dataset (num_proc=16):   0%|          | 38/33047 [00:04<49:43, 11.06 examples/s]  Converting format of dataset (num_proc=16):   0%|          | 41/33047 [00:04<48:44, 11.29 examples/s]Converting format of dataset (num_proc=16):   0%|          | 44/33047 [00:04<40:59, 13.42 examples/s]Converting format of dataset (num_proc=16):   0%|          | 46/33047 [00:05<45:08, 12.18 examples/s]Converting format of dataset (num_proc=16):   0%|          | 48/33047 [00:05<1:19:49,  6.89 examples/s]Converting format of dataset (num_proc=16):   0%|          | 53/33047 [00:06<1:01:07,  9.00 examples/s]Converting format of dataset (num_proc=16):   0%|          | 55/33047 [00:06<54:14, 10.14 examples/s]  Converting format of dataset (num_proc=16):   0%|          | 57/33047 [00:06<49:32, 11.10 examples/s]Converting format of dataset (num_proc=16):   0%|          | 60/33047 [00:06<42:19, 12.99 examples/s]Converting format of dataset (num_proc=16):   0%|          | 62/33047 [00:06<39:45, 13.82 examples/s]Converting format of dataset (num_proc=16):   0%|          | 64/33047 [00:06<40:34, 13.55 examples/s]Converting format of dataset (num_proc=16):   0%|          | 66/33047 [00:07<49:33, 11.09 examples/s]Converting format of dataset (num_proc=16):   0%|          | 71/33047 [00:07<41:52, 13.12 examples/s]Converting format of dataset (num_proc=16):   0%|          | 73/33047 [00:07<43:40, 12.58 examples/s]Converting format of dataset (num_proc=16):   0%|          | 75/33047 [00:07<46:22, 11.85 examples/s]Converting format of dataset (num_proc=16):   0%|          | 77/33047 [00:08<58:13,  9.44 examples/s]Converting format of dataset (num_proc=16):   0%|          | 80/33047 [00:08<45:10, 12.16 examples/s]Converting format of dataset (num_proc=16):   0%|          | 83/33047 [00:08<44:25, 12.37 examples/s]Converting format of dataset (num_proc=16):   0%|          | 86/33047 [00:08<43:47, 12.54 examples/s]Converting format of dataset (num_proc=16):   0%|          | 88/33047 [00:08<46:17, 11.86 examples/s]Converting format of dataset (num_proc=16):   0%|          | 91/33047 [00:08<41:01, 13.39 examples/s]Converting format of dataset (num_proc=16):   0%|          | 94/33047 [00:09<38:02, 14.44 examples/s]Converting format of dataset (num_proc=16):   0%|          | 96/33047 [00:09<48:02, 11.43 examples/s]Converting format of dataset (num_proc=16):   0%|          | 99/33047 [00:09<41:41, 13.17 examples/s]Converting format of dataset (num_proc=16):   0%|          | 101/33047 [00:09<40:28, 13.56 examples/s]Converting format of dataset (num_proc=16):   0%|          | 103/33047 [00:10<1:08:46,  7.98 examples/s]Converting format of dataset (num_proc=16):   0%|          | 105/33047 [00:10<1:01:29,  8.93 examples/s]Converting format of dataset (num_proc=16):   0%|          | 112/33047 [00:10<38:02, 14.43 examples/s]  Converting format of dataset (num_proc=16):   0%|          | 114/33047 [00:10<40:55, 13.41 examples/s]Converting format of dataset (num_proc=16):   0%|          | 116/33047 [00:11<42:04, 13.04 examples/s]Converting format of dataset (num_proc=16):   0%|          | 118/33047 [00:11<41:03, 13.36 examples/s]Converting format of dataset (num_proc=16):   0%|          | 121/33047 [00:11<34:06, 16.09 examples/s]Converting format of dataset (num_proc=16):   0%|          | 123/33047 [00:11<37:28, 14.64 examples/s]Converting format of dataset (num_proc=16):   0%|          | 125/33047 [00:11<46:22, 11.83 examples/s]Converting format of dataset (num_proc=16):   0%|          | 128/33047 [00:11<39:59, 13.72 examples/s]Converting format of dataset (num_proc=16):   0%|          | 130/33047 [00:12<58:52,  9.32 examples/s]Converting format of dataset (num_proc=16):   0%|          | 132/33047 [00:12<53:00, 10.35 examples/s]Converting format of dataset (num_proc=16):   0%|          | 135/33047 [00:12<45:56, 11.94 examples/s]Converting format of dataset (num_proc=16):   0%|          | 137/33047 [00:12<51:52, 10.57 examples/s]Converting format of dataset (num_proc=16):   0%|          | 141/33047 [00:13<37:11, 14.75 examples/s]Converting format of dataset (num_proc=16):   0%|          | 143/33047 [00:13<39:14, 13.98 examples/s]Converting format of dataset (num_proc=16):   0%|          | 146/33047 [00:13<44:14, 12.39 examples/s]Converting format of dataset (num_proc=16):   0%|          | 150/33047 [00:13<33:10, 16.53 examples/s]Converting format of dataset (num_proc=16):   0%|          | 153/33047 [00:13<33:55, 16.16 examples/s]Converting format of dataset (num_proc=16):   0%|          | 159/33047 [00:14<27:02, 20.27 examples/s]Converting format of dataset (num_proc=16):   0%|          | 162/33047 [00:14<30:20, 18.07 examples/s]Converting format of dataset (num_proc=16):   0%|          | 164/33047 [00:14<37:35, 14.58 examples/s]Converting format of dataset (num_proc=16):   1%|          | 167/33047 [00:14<37:22, 14.66 examples/s]Converting format of dataset (num_proc=16):   1%|          | 170/33047 [00:14<35:23, 15.48 examples/s]Converting format of dataset (num_proc=16):   1%|          | 172/33047 [00:14<34:21, 15.94 examples/s]Converting format of dataset (num_proc=16):   1%|          | 174/33047 [00:15<33:51, 16.18 examples/s]Converting format of dataset (num_proc=16):   1%|          | 179/33047 [00:15<23:25, 23.39 examples/s]Converting format of dataset (num_proc=16):   1%|          | 183/33047 [00:15<37:28, 14.62 examples/s]Converting format of dataset (num_proc=16):   1%|          | 186/33047 [00:15<43:34, 12.57 examples/s]Converting format of dataset (num_proc=16):   1%|          | 188/33047 [00:16<41:00, 13.35 examples/s]Converting format of dataset (num_proc=16):   1%|          | 191/33047 [00:16<35:21, 15.49 examples/s]Converting format of dataset (num_proc=16):   1%|          | 194/33047 [00:16<32:39, 16.77 examples/s]Converting format of dataset (num_proc=16):   1%|          | 197/33047 [00:16<37:43, 14.51 examples/s]Converting format of dataset (num_proc=16):   1%|          | 201/33047 [00:16<30:37, 17.88 examples/s]Converting format of dataset (num_proc=16):   1%|          | 205/33047 [00:16<30:46, 17.79 examples/s]Converting format of dataset (num_proc=16):   1%|          | 208/33047 [00:17<41:59, 13.03 examples/s]Converting format of dataset (num_proc=16):   1%|          | 215/33047 [00:17<29:17, 18.68 examples/s]Converting format of dataset (num_proc=16):   1%|          | 218/33047 [00:18<50:40, 10.80 examples/s]Converting format of dataset (num_proc=16):   1%|          | 220/33047 [00:18<46:51, 11.67 examples/s]Converting format of dataset (num_proc=16):   1%|          | 223/33047 [00:18<43:47, 12.49 examples/s]Converting format of dataset (num_proc=16):   1%|          | 225/33047 [00:18<50:14, 10.89 examples/s]Converting format of dataset (num_proc=16):   1%|          | 228/33047 [00:19<1:06:13,  8.26 examples/s]Converting format of dataset (num_proc=16):   1%|          | 232/33047 [00:19<48:21, 11.31 examples/s]  Converting format of dataset (num_proc=16):   1%|          | 234/33047 [00:19<44:34, 12.27 examples/s]Converting format of dataset (num_proc=16):   1%|          | 236/33047 [00:19<50:28, 10.83 examples/s]Converting format of dataset (num_proc=16):   1%|          | 241/33047 [00:20<36:59, 14.78 examples/s]Converting format of dataset (num_proc=16):   1%|          | 243/33047 [00:20<47:05, 11.61 examples/s]Converting format of dataset (num_proc=16):   1%|          | 245/33047 [00:20<55:56,  9.77 examples/s]Converting format of dataset (num_proc=16):   1%|          | 247/33047 [00:20<50:14, 10.88 examples/s]Converting format of dataset (num_proc=16):   1%|          | 258/33047 [00:20<21:00, 26.01 examples/s]Converting format of dataset (num_proc=16):   1%|          | 263/33047 [00:21<26:37, 20.52 examples/s]Converting format of dataset (num_proc=16):   1%|          | 267/33047 [00:21<29:06, 18.77 examples/s]Converting format of dataset (num_proc=16):   1%|          | 272/33047 [00:21<30:42, 17.79 examples/s]Converting format of dataset (num_proc=16):   1%|          | 275/33047 [00:22<28:27, 19.19 examples/s]Converting format of dataset (num_proc=16):   1%|          | 278/33047 [00:22<27:57, 19.53 examples/s]Converting format of dataset (num_proc=16):   1%|          | 282/33047 [00:22<24:12, 22.55 examples/s]Converting format of dataset (num_proc=16):   1%|          | 288/33047 [00:22<20:50, 26.20 examples/s]Converting format of dataset (num_proc=16):   1%|          | 291/33047 [00:22<22:11, 24.60 examples/s]Converting format of dataset (num_proc=16):   1%|          | 297/33047 [00:23<28:53, 18.90 examples/s]Converting format of dataset (num_proc=16):   1%|          | 301/33047 [00:23<33:50, 16.13 examples/s]Converting format of dataset (num_proc=16):   1%|          | 304/33047 [00:23<32:10, 16.96 examples/s]Converting format of dataset (num_proc=16):   1%|          | 307/33047 [00:23<31:32, 17.30 examples/s]Converting format of dataset (num_proc=16):   1%|          | 311/33047 [00:23<33:17, 16.39 examples/s]Converting format of dataset (num_proc=16):   1%|          | 315/33047 [00:24<29:21, 18.59 examples/s]Converting format of dataset (num_proc=16):   1%|          | 318/33047 [00:24<28:25, 19.19 examples/s]Converting format of dataset (num_proc=16):   1%|          | 321/33047 [00:24<42:57, 12.70 examples/s]Converting format of dataset (num_proc=16):   1%|          | 324/33047 [00:24<37:06, 14.70 examples/s]Converting format of dataset (num_proc=16):   1%|          | 328/33047 [00:25<36:57, 14.76 examples/s]Converting format of dataset (num_proc=16):   1%|          | 335/33047 [00:25<34:34, 15.77 examples/s]Converting format of dataset (num_proc=16):   1%|          | 337/33047 [00:25<42:47, 12.74 examples/s]Converting format of dataset (num_proc=16):   1%|          | 339/33047 [00:26<46:43, 11.67 examples/s]Converting format of dataset (num_proc=16):   1%|          | 341/33047 [00:26<43:07, 12.64 examples/s]Converting format of dataset (num_proc=16):   1%|          | 356/33047 [00:26<19:52, 27.42 examples/s]Converting format of dataset (num_proc=16):   1%|          | 363/33047 [00:26<18:52, 28.85 examples/s]Converting format of dataset (num_proc=16):   1%|          | 367/33047 [00:26<18:28, 29.47 examples/s]Converting format of dataset (num_proc=16):   1%|          | 371/33047 [00:27<22:52, 23.80 examples/s]Converting format of dataset (num_proc=16):   1%|          | 374/33047 [00:27<33:36, 16.20 examples/s]Converting format of dataset (num_proc=16):   1%|          | 378/33047 [00:27<32:39, 16.67 examples/s]Converting format of dataset (num_proc=16):   1%|          | 382/33047 [00:27<28:35, 19.04 examples/s]Converting format of dataset (num_proc=16):   1%|          | 385/33047 [00:27<26:16, 20.72 examples/s]Converting format of dataset (num_proc=16):   1%|          | 390/33047 [00:28<20:53, 26.05 examples/s]Converting format of dataset (num_proc=16):   1%|          | 397/33047 [00:28<16:57, 32.08 examples/s]Converting format of dataset (num_proc=16):   1%|          | 407/33047 [00:28<15:07, 35.96 examples/s]Converting format of dataset (num_proc=16):   1%|          | 411/33047 [00:28<17:12, 31.59 examples/s]Converting format of dataset (num_proc=16):   1%|▏         | 415/33047 [00:28<19:17, 28.18 examples/s]Converting format of dataset (num_proc=16):   1%|▏         | 419/33047 [00:28<20:28, 26.57 examples/s]Converting format of dataset (num_proc=16):   1%|▏         | 422/33047 [00:29<24:24, 22.28 examples/s]Converting format of dataset (num_proc=16):   1%|▏         | 426/33047 [00:29<23:48, 22.84 examples/s]Converting format of dataset (num_proc=16):   1%|▏         | 430/33047 [00:29<22:21, 24.32 examples/s]Converting format of dataset (num_proc=16):   1%|▏         | 433/33047 [00:29<23:42, 22.93 examples/s]Converting format of dataset (num_proc=16):   1%|▏         | 438/33047 [00:29<24:28, 22.21 examples/s]Converting format of dataset (num_proc=16):   1%|▏         | 441/33047 [00:29<23:16, 23.35 examples/s]Converting format of dataset (num_proc=16):   1%|▏         | 447/33047 [00:30<34:05, 15.94 examples/s]Converting format of dataset (num_proc=16):   1%|▏         | 453/33047 [00:30<26:42, 20.33 examples/s]Converting format of dataset (num_proc=16):   1%|▏         | 456/33047 [00:30<27:20, 19.87 examples/s]Converting format of dataset (num_proc=16):   1%|▏         | 463/33047 [00:31<20:38, 26.31 examples/s]Converting format of dataset (num_proc=16):   1%|▏         | 467/33047 [00:31<26:39, 20.37 examples/s]Converting format of dataset (num_proc=16):   1%|▏         | 471/33047 [00:31<33:45, 16.09 examples/s]Converting format of dataset (num_proc=16):   1%|▏         | 475/33047 [00:31<28:22, 19.13 examples/s]Converting format of dataset (num_proc=16):   1%|▏         | 484/33047 [00:31<18:44, 28.95 examples/s]Converting format of dataset (num_proc=16):   1%|▏         | 489/33047 [00:32<17:23, 31.21 examples/s]Converting format of dataset (num_proc=16):   1%|▏         | 493/33047 [00:32<25:08, 21.58 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 500/33047 [00:32<25:21, 21.40 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 503/33047 [00:32<25:55, 20.93 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 521/33047 [00:33<12:28, 43.48 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 530/33047 [00:33<17:54, 30.25 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 536/33047 [00:33<20:27, 26.49 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 542/33047 [00:34<19:15, 28.14 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 546/33047 [00:34<25:40, 21.09 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 550/33047 [00:34<24:50, 21.80 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 560/33047 [00:34<17:20, 31.22 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 565/33047 [00:34<18:04, 29.94 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 570/33047 [00:35<28:54, 18.72 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 577/33047 [00:35<27:09, 19.92 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 581/33047 [00:35<24:59, 21.66 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 586/33047 [00:36<23:27, 23.06 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 591/33047 [00:36<21:54, 24.69 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 595/33047 [00:36<35:41, 15.15 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 603/33047 [00:37<25:05, 21.55 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 608/33047 [00:37<26:50, 20.14 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 611/33047 [00:37<26:24, 20.48 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 615/33047 [00:37<26:23, 20.49 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 622/33047 [00:37<19:24, 27.85 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 627/33047 [00:38<24:16, 22.25 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 630/33047 [00:38<32:36, 16.57 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 637/33047 [00:38<23:11, 23.28 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 643/33047 [00:38<18:41, 28.88 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 653/33047 [00:38<14:10, 38.10 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 659/33047 [00:39<14:33, 37.09 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 669/33047 [00:39<17:16, 31.23 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 674/33047 [00:39<17:33, 30.73 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 678/33047 [00:39<16:59, 31.75 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 687/33047 [00:39<17:20, 31.09 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 706/33047 [00:40<10:24, 51.77 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 721/33047 [00:40<08:35, 62.66 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 731/33047 [00:40<13:26, 40.07 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 740/33047 [00:41<16:05, 33.45 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 745/33047 [00:41<16:43, 32.18 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 765/33047 [00:41<11:29, 46.85 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 786/33047 [00:41<08:06, 66.34 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 795/33047 [00:42<15:44, 34.16 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 803/33047 [00:42<14:39, 36.67 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 811/33047 [00:43<19:41, 27.29 examples/s]Converting format of dataset (num_proc=16):   2%|▏         | 816/33047 [00:43<18:35, 28.89 examples/s]Converting format of dataset (num_proc=16):   3%|▎         | 842/33047 [00:43<10:25, 51.52 examples/s]Converting format of dataset (num_proc=16):   3%|▎         | 854/33047 [00:43<09:34, 56.08 examples/s]Converting format of dataset (num_proc=16):   3%|▎         | 864/33047 [00:43<10:47, 49.72 examples/s]Converting format of dataset (num_proc=16):   3%|▎         | 871/33047 [00:44<10:17, 52.07 examples/s]Converting format of dataset (num_proc=16):   3%|▎         | 884/33047 [00:44<09:00, 59.55 examples/s]Converting format of dataset (num_proc=16):   3%|▎         | 896/33047 [00:44<08:34, 62.47 examples/s]Converting format of dataset (num_proc=16):   3%|▎         | 905/33047 [00:44<13:33, 39.50 examples/s]Converting format of dataset (num_proc=16):   3%|▎         | 920/33047 [00:45<11:43, 45.65 examples/s]Converting format of dataset (num_proc=16):   3%|▎         | 933/33047 [00:45<11:53, 45.04 examples/s]Converting format of dataset (num_proc=16):   3%|▎         | 951/33047 [00:45<10:18, 51.89 examples/s]Converting format of dataset (num_proc=16):   3%|▎         | 963/33047 [00:45<09:32, 56.03 examples/s]Converting format of dataset (num_proc=16):   3%|▎         | 972/33047 [00:46<09:33, 55.97 examples/s]Converting format of dataset (num_proc=16):   3%|▎         | 986/33047 [00:46<07:42, 69.36 examples/s]Converting format of dataset (num_proc=16):   3%|▎         | 996/33047 [00:46<15:01, 35.53 examples/s]Converting format of dataset (num_proc=16):   3%|▎         | 1005/33047 [00:47<14:10, 37.66 examples/s]Converting format of dataset (num_proc=16):   3%|▎         | 1011/33047 [00:47<13:14, 40.31 examples/s]Converting format of dataset (num_proc=16):   3%|▎         | 1018/33047 [00:47<13:22, 39.89 examples/s]Converting format of dataset (num_proc=16):   3%|▎         | 1024/33047 [00:47<17:17, 30.86 examples/s]Converting format of dataset (num_proc=16):   3%|▎         | 1052/33047 [00:47<08:50, 60.37 examples/s]Converting format of dataset (num_proc=16):   3%|▎         | 1071/33047 [00:47<07:16, 73.30 examples/s]Converting format of dataset (num_proc=16):   3%|▎         | 1086/33047 [00:48<08:47, 60.63 examples/s]Converting format of dataset (num_proc=16):   3%|▎         | 1094/33047 [00:48<10:32, 50.51 examples/s]Converting format of dataset (num_proc=16):   3%|▎         | 1125/33047 [00:48<06:23, 83.23 examples/s]Converting format of dataset (num_proc=16):   3%|▎         | 1137/33047 [00:49<07:56, 66.93 examples/s]Converting format of dataset (num_proc=16):   3%|▎         | 1148/33047 [00:49<08:56, 59.50 examples/s]Converting format of dataset (num_proc=16):   4%|▎         | 1160/33047 [00:49<08:25, 63.02 examples/s]Converting format of dataset (num_proc=16):   4%|▎         | 1174/33047 [00:49<07:52, 67.51 examples/s]Converting format of dataset (num_proc=16):   4%|▎         | 1205/33047 [00:49<05:21, 99.03 examples/s]Converting format of dataset (num_proc=16):   4%|▎         | 1219/33047 [00:50<06:21, 83.47 examples/s]Converting format of dataset (num_proc=16):   4%|▎         | 1232/33047 [00:50<07:24, 71.55 examples/s]Converting format of dataset (num_proc=16):   4%|▍         | 1261/33047 [00:50<05:49, 91.03 examples/s]Converting format of dataset (num_proc=16):   4%|▍         | 1274/33047 [00:50<07:13, 73.37 examples/s]Converting format of dataset (num_proc=16):   4%|▍         | 1295/33047 [00:50<05:55, 89.23 examples/s]Converting format of dataset (num_proc=16):   4%|▍         | 1306/33047 [00:51<06:43, 78.70 examples/s]Converting format of dataset (num_proc=16):   4%|▍         | 1317/33047 [00:51<07:46, 68.06 examples/s]Converting format of dataset (num_proc=16):   4%|▍         | 1327/33047 [00:51<10:23, 50.90 examples/s]Converting format of dataset (num_proc=16):   4%|▍         | 1368/33047 [00:51<05:23, 98.04 examples/s]Converting format of dataset (num_proc=16):   4%|▍         | 1383/33047 [00:51<05:10, 102.04 examples/s]Converting format of dataset (num_proc=16):   4%|▍         | 1399/33047 [00:52<05:03, 104.34 examples/s]Converting format of dataset (num_proc=16):   4%|▍         | 1417/33047 [00:52<04:43, 111.46 examples/s]Converting format of dataset (num_proc=16):   4%|▍         | 1432/33047 [00:52<07:35, 69.37 examples/s] Converting format of dataset (num_proc=16):   4%|▍         | 1444/33047 [00:52<08:02, 65.56 examples/s]Converting format of dataset (num_proc=16):   5%|▍         | 1516/33047 [00:53<04:09, 126.26 examples/s]Converting format of dataset (num_proc=16):   5%|▍         | 1547/33047 [00:53<04:23, 119.76 examples/s]Converting format of dataset (num_proc=16):   5%|▍         | 1562/33047 [00:53<05:15, 99.67 examples/s] Converting format of dataset (num_proc=16):   5%|▍         | 1577/33047 [00:53<05:20, 98.16 examples/s]Converting format of dataset (num_proc=16):   5%|▍         | 1600/33047 [00:54<05:14, 99.91 examples/s]Converting format of dataset (num_proc=16):   5%|▍         | 1612/33047 [00:54<05:43, 91.40 examples/s]Converting format of dataset (num_proc=16):   5%|▍         | 1645/33047 [00:54<04:36, 113.38 examples/s]Converting format of dataset (num_proc=16):   5%|▌         | 1672/33047 [00:54<04:08, 126.47 examples/s]Converting format of dataset (num_proc=16):   5%|▌         | 1697/33047 [00:54<03:57, 131.88 examples/s]Converting format of dataset (num_proc=16):   5%|▌         | 1735/33047 [00:55<03:05, 168.90 examples/s]Converting format of dataset (num_proc=16):   5%|▌         | 1767/33047 [00:55<02:41, 193.65 examples/s]Converting format of dataset (num_proc=16):   5%|▌         | 1793/33047 [00:55<02:45, 188.32 examples/s]Converting format of dataset (num_proc=16):   6%|▌         | 1838/33047 [00:55<03:27, 150.45 examples/s]Converting format of dataset (num_proc=16):   6%|▌         | 1892/33047 [00:55<03:13, 160.86 examples/s]Converting format of dataset (num_proc=16):   6%|▌         | 1951/33047 [00:56<02:46, 186.25 examples/s]Converting format of dataset (num_proc=16):   6%|▌         | 1996/33047 [00:56<02:30, 206.16 examples/s]Converting format of dataset (num_proc=16):   6%|▌         | 2030/33047 [00:56<02:40, 193.52 examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 2093/33047 [00:56<02:29, 206.74 examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 2117/33047 [00:57<03:32, 145.83 examples/s]Converting format of dataset (num_proc=16):   7%|▋         | 2233/33047 [00:57<01:55, 267.52 examples/s]Converting format of dataset (num_proc=16):   7%|▋         | 2295/33047 [00:57<01:46, 288.07 examples/s]Converting format of dataset (num_proc=16):   7%|▋         | 2379/33047 [00:57<01:22, 372.08 examples/s]Converting format of dataset (num_proc=16):   7%|▋         | 2437/33047 [00:58<02:03, 248.44 examples/s]Converting format of dataset (num_proc=16):   8%|▊         | 2482/33047 [00:58<02:00, 254.26 examples/s]Converting format of dataset (num_proc=16):   8%|▊         | 2598/33047 [00:58<01:18, 389.71 examples/s]Converting format of dataset (num_proc=16):   8%|▊         | 2808/33047 [00:58<00:43, 693.96 examples/s]Converting format of dataset (num_proc=16):   9%|▉         | 2929/33047 [00:58<00:37, 797.61 examples/s]Converting format of dataset (num_proc=16):   9%|▉         | 3039/33047 [00:58<00:40, 740.95 examples/s]Converting format of dataset (num_proc=16):  10%|▉         | 3183/33047 [00:58<00:35, 838.96 examples/s]Converting format of dataset (num_proc=16):  10%|█         | 3307/33047 [00:59<00:46, 638.62 examples/s]Converting format of dataset (num_proc=16):  10%|█         | 3390/33047 [00:59<00:55, 533.50 examples/s]Converting format of dataset (num_proc=16):  11%|█▏        | 3726/33047 [00:59<00:28, 1013.04 examples/s]Converting format of dataset (num_proc=16):  12%|█▏        | 3981/33047 [00:59<00:23, 1259.70 examples/s]Converting format of dataset (num_proc=16):  13%|█▎        | 4174/33047 [00:59<00:22, 1263.69 examples/s]Converting format of dataset (num_proc=16):  14%|█▍        | 4765/33047 [00:59<00:12, 2249.79 examples/s]Converting format of dataset (num_proc=16):  16%|█▌        | 5273/33047 [01:00<00:09, 2905.13 examples/s]Converting format of dataset (num_proc=16):  17%|█▋        | 5648/33047 [01:00<00:08, 3087.77 examples/s]Converting format of dataset (num_proc=16):  18%|█▊        | 6025/33047 [01:00<00:11, 2439.30 examples/s]Converting format of dataset (num_proc=16):  19%|█▉        | 6352/33047 [01:00<00:11, 2353.64 examples/s]Converting format of dataset (num_proc=16):  20%|██        | 6659/33047 [01:01<00:19, 1350.52 examples/s]Converting format of dataset (num_proc=16):  23%|██▎       | 7698/33047 [01:01<00:09, 2650.18 examples/s]Converting format of dataset (num_proc=16):  26%|██▌       | 8657/33047 [01:01<00:06, 3827.70 examples/s]Converting format of dataset (num_proc=16):  29%|██▉       | 9607/33047 [01:01<00:04, 4926.56 examples/s]Converting format of dataset (num_proc=16):  32%|███▏      | 10484/33047 [01:01<00:03, 5747.57 examples/s]Converting format of dataset (num_proc=16):  35%|███▍      | 11413/33047 [01:01<00:03, 6492.92 examples/s]Converting format of dataset (num_proc=16):  37%|███▋      | 12357/33047 [01:01<00:02, 7210.31 examples/s]Converting format of dataset (num_proc=16):  40%|████      | 13314/33047 [01:01<00:02, 7777.02 examples/s]Converting format of dataset (num_proc=16):  43%|████▎     | 14240/33047 [01:01<00:02, 8164.05 examples/s]Converting format of dataset (num_proc=16):  46%|████▌     | 15197/33047 [01:01<00:02, 8495.53 examples/s]Converting format of dataset (num_proc=16):  49%|████▉     | 16138/33047 [01:02<00:01, 8748.36 examples/s]Converting format of dataset (num_proc=16):  52%|█████▏    | 17053/33047 [01:02<00:01, 8839.50 examples/s]Converting format of dataset (num_proc=16):  55%|█████▍    | 18055/33047 [01:02<00:01, 9152.78 examples/s]Converting format of dataset (num_proc=16):  57%|█████▋    | 18988/33047 [01:02<00:01, 9187.79 examples/s]Converting format of dataset (num_proc=16):  60%|██████    | 19940/33047 [01:02<00:01, 9105.79 examples/s]Converting format of dataset (num_proc=16):  63%|██████▎   | 20872/33047 [01:02<00:01, 9160.06 examples/s]Converting format of dataset (num_proc=16):  66%|██████▌   | 21819/33047 [01:02<00:01, 9220.58 examples/s]Converting format of dataset (num_proc=16):  69%|██████▉   | 22763/33047 [01:02<00:01, 9226.35 examples/s]Converting format of dataset (num_proc=16):  72%|███████▏  | 23707/33047 [01:02<00:01, 9261.39 examples/s]Converting format of dataset (num_proc=16):  75%|███████▍  | 24660/33047 [01:02<00:00, 9139.33 examples/s]Converting format of dataset (num_proc=16):  78%|███████▊  | 25622/33047 [01:03<00:00, 9233.18 examples/s]Converting format of dataset (num_proc=16):  80%|████████  | 26580/33047 [01:03<00:00, 9143.28 examples/s]Converting format of dataset (num_proc=16):  83%|████████▎ | 27513/33047 [01:03<00:00, 9158.11 examples/s]Converting format of dataset (num_proc=16):  86%|████████▌ | 28453/33047 [01:03<00:00, 9220.36 examples/s]Converting format of dataset (num_proc=16):  89%|████████▉ | 29399/33047 [01:03<00:00, 8979.39 examples/s]Converting format of dataset (num_proc=16):  92%|█████████▏| 30321/33047 [01:03<00:00, 8979.62 examples/s]Converting format of dataset (num_proc=16):  95%|█████████▍| 31248/33047 [01:03<00:00, 8615.24 examples/s]Converting format of dataset (num_proc=16):  97%|█████████▋| 32148/33047 [01:03<00:00, 8275.39 examples/s]Converting format of dataset (num_proc=16): 100%|█████████▉| 33020/33047 [01:04<00:00, 6121.86 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 33047/33047 [01:04<00:00, 515.09 examples/s] 
[rank0]:[W101 04:45:57.942802635 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
rg32601:230:230 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
rg32601:230:230 [0] NCCL INFO Bootstrap : Using eno8303:10.83.106.101<0>
rg32601:230:230 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
rg32601:230:230 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
rg32601:230:230 [0] NCCL INFO NET/Plugin: Using internal network plugin.
rg32601:230:230 [0] NCCL INFO cudaDriverVersion 12080
NCCL version 2.21.5+cuda12.4
rg32601:230:230 [0] NCCL INFO Comm config Blocking set to 1
rg32601:231:231 [1] NCCL INFO cudaDriverVersion 12080
rg32601:231:231 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
rg32601:231:231 [1] NCCL INFO Bootstrap : Using eno8303:10.83.106.101<0>
rg32601:232:232 [2] NCCL INFO cudaDriverVersion 12080
rg32601:232:232 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
rg32601:232:232 [2] NCCL INFO Bootstrap : Using eno8303:10.83.106.101<0>
rg32601:233:233 [3] NCCL INFO cudaDriverVersion 12080
rg32601:233:233 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
rg32601:233:233 [3] NCCL INFO Bootstrap : Using eno8303:10.83.106.101<0>
rg32601:231:231 [1] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
rg32601:231:231 [1] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
rg32601:231:231 [1] NCCL INFO NET/Plugin: Using internal network plugin.
rg32601:231:231 [1] NCCL INFO Comm config Blocking set to 1
rg32601:232:232 [2] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
rg32601:232:232 [2] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
rg32601:232:232 [2] NCCL INFO NET/Plugin: Using internal network plugin.
rg32601:233:233 [3] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
rg32601:233:233 [3] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
rg32601:233:233 [3] NCCL INFO NET/Plugin: Using internal network plugin.
rg32601:232:232 [2] NCCL INFO Comm config Blocking set to 1
rg32601:233:233 [3] NCCL INFO Comm config Blocking set to 1
rg32601:232:795 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
rg32601:233:796 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
rg32601:231:794 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
rg32601:230:793 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
rg32601:232:795 [2] NCCL INFO Failed to open libibverbs.so[.1]
rg32601:231:794 [1] NCCL INFO Failed to open libibverbs.so[.1]
rg32601:233:796 [3] NCCL INFO Failed to open libibverbs.so[.1]
rg32601:232:795 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
rg32601:231:794 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
rg32601:233:796 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
rg32601:233:796 [3] NCCL INFO NET/Socket : Using [0]eno8303:10.83.106.101<0> [1]ib0:10.85.106.101<0>
rg32601:231:794 [1] NCCL INFO NET/Socket : Using [0]eno8303:10.83.106.101<0> [1]ib0:10.85.106.101<0>
rg32601:232:795 [2] NCCL INFO NET/Socket : Using [0]eno8303:10.83.106.101<0> [1]ib0:10.85.106.101<0>
rg32601:230:793 [0] NCCL INFO Failed to open libibverbs.so[.1]
rg32601:233:796 [3] NCCL INFO Using non-device net plugin version 0
rg32601:231:794 [1] NCCL INFO Using non-device net plugin version 0
rg32601:232:795 [2] NCCL INFO Using non-device net plugin version 0
rg32601:233:796 [3] NCCL INFO Using network Socket
rg32601:230:793 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
rg32601:231:794 [1] NCCL INFO Using network Socket
rg32601:232:795 [2] NCCL INFO Using network Socket
rg32601:230:793 [0] NCCL INFO NET/Socket : Using [0]eno8303:10.83.106.101<0> [1]ib0:10.85.106.101<0>
rg32601:230:793 [0] NCCL INFO Using non-device net plugin version 0
rg32601:230:793 [0] NCCL INFO Using network Socket
rg32601:230:793 [0] NCCL INFO ncclCommInitRank comm 0x558232cccd80 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 4e000 commId 0x5b6041024b89da51 - Init START
rg32601:231:794 [1] NCCL INFO ncclCommInitRank comm 0x5612ebbdf500 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 5f000 commId 0x5b6041024b89da51 - Init START
rg32601:232:795 [2] NCCL INFO ncclCommInitRank comm 0x55b28c1453b0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId cb000 commId 0x5b6041024b89da51 - Init START
rg32601:233:796 [3] NCCL INFO ncclCommInitRank comm 0x556992648a70 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId db000 commId 0x5b6041024b89da51 - Init START
rg32601:230:793 [0] NCCL INFO Setting affinity for GPU 0 to ffff0000
rg32601:230:793 [0] NCCL INFO NVLS multicast support is not available on dev 0
rg32601:233:796 [3] NCCL INFO Setting affinity for GPU 3 to ffff,00000000
rg32601:233:796 [3] NCCL INFO NVLS multicast support is not available on dev 3
rg32601:232:795 [2] NCCL INFO Setting affinity for GPU 2 to ffff0000,00000000
rg32601:232:795 [2] NCCL INFO NVLS multicast support is not available on dev 2
rg32601:231:794 [1] NCCL INFO Setting affinity for GPU 1 to ffff
rg32601:231:794 [1] NCCL INFO NVLS multicast support is not available on dev 1
rg32601:232:795 [2] NCCL INFO comm 0x55b28c1453b0 rank 2 nRanks 4 nNodes 1 localRanks 4 localRank 2 MNNVL 0
rg32601:230:793 [0] NCCL INFO comm 0x558232cccd80 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0
rg32601:232:795 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 1/-1/-1->2->0 [5] 1/-1/-1->2->0 [6] 1/-1/-1->2->0 [7] 1/-1/-1->2->0 [8] -1/-1/-1->2->3 [9] -1/-1/-1->2->3 [10] -1/-1/-1->2->3 [11] -1/-1/-1->2->3 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 1/-1/-1->2->0 [17] 1/-1/-1->2->0 [18] 1/-1/-1->2->0 [19] 1/-1/-1->2->0 [20] -1/-1/-1->2->3 [21] -1/-1/-1->2->3 [22] -1/-1/-1->2->3 [23] -1/-1/-1->2->3
rg32601:233:796 [3] NCCL INFO comm 0x556992648a70 rank 3 nRanks 4 nNodes 1 localRanks 4 localRank 3 MNNVL 0
rg32601:232:795 [2] NCCL INFO P2P Chunksize set to 524288
rg32601:230:793 [0] NCCL INFO Channel 00/24 :    0   1   2   3
rg32601:230:793 [0] NCCL INFO Channel 01/24 :    0   1   3   2
rg32601:230:793 [0] NCCL INFO Channel 02/24 :    0   2   3   1
rg32601:233:796 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] -1/-1/-1->3->1 [5] -1/-1/-1->3->1 [6] -1/-1/-1->3->1 [7] -1/-1/-1->3->1 [8] 2/-1/-1->3->0 [9] 2/-1/-1->3->0 [10] 2/-1/-1->3->0 [11] 2/-1/-1->3->0 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] -1/-1/-1->3->2 [15] -1/-1/-1->3->2 [16] -1/-1/-1->3->1 [17] -1/-1/-1->3->1 [18] -1/-1/-1->3->1 [19] -1/-1/-1->3->1 [20] 2/-1/-1->3->0 [21] 2/-1/-1->3->0 [22] 2/-1/-1->3->0 [23] 2/-1/-1->3->0
rg32601:230:793 [0] NCCL INFO Channel 03/24 :    0   2   1   3
rg32601:233:796 [3] NCCL INFO P2P Chunksize set to 524288
rg32601:230:793 [0] NCCL INFO Channel 04/24 :    0   3   1   2
rg32601:230:793 [0] NCCL INFO Channel 05/24 :    0   3   2   1
rg32601:230:793 [0] NCCL INFO Channel 06/24 :    0   1   2   3
rg32601:230:793 [0] NCCL INFO Channel 07/24 :    0   1   3   2
rg32601:230:793 [0] NCCL INFO Channel 08/24 :    0   2   3   1
rg32601:230:793 [0] NCCL INFO Channel 09/24 :    0   2   1   3
rg32601:230:793 [0] NCCL INFO Channel 10/24 :    0   3   1   2
rg32601:230:793 [0] NCCL INFO Channel 11/24 :    0   3   2   1
rg32601:230:793 [0] NCCL INFO Channel 12/24 :    0   1   2   3
rg32601:230:793 [0] NCCL INFO Channel 13/24 :    0   1   3   2
rg32601:230:793 [0] NCCL INFO Channel 14/24 :    0   2   3   1
rg32601:230:793 [0] NCCL INFO Channel 15/24 :    0   2   1   3
rg32601:230:793 [0] NCCL INFO Channel 16/24 :    0   3   1   2
rg32601:230:793 [0] NCCL INFO Channel 17/24 :    0   3   2   1
rg32601:230:793 [0] NCCL INFO Channel 18/24 :    0   1   2   3
rg32601:230:793 [0] NCCL INFO Channel 19/24 :    0   1   3   2
rg32601:230:793 [0] NCCL INFO Channel 20/24 :    0   2   3   1
rg32601:230:793 [0] NCCL INFO Channel 21/24 :    0   2   1   3
rg32601:230:793 [0] NCCL INFO Channel 22/24 :    0   3   1   2
rg32601:230:793 [0] NCCL INFO Channel 23/24 :    0   3   2   1
rg32601:230:793 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 2/-1/-1->0->-1 [5] 2/-1/-1->0->-1 [6] 2/-1/-1->0->-1 [7] 2/-1/-1->0->-1 [8] 3/-1/-1->0->1 [9] 3/-1/-1->0->1 [10] 3/-1/-1->0->1 [11] 3/-1/-1->0->1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 2/-1/-1->0->-1 [17] 2/-1/-1->0->-1 [18] 2/-1/-1->0->-1 [19] 2/-1/-1->0->-1 [20] 3/-1/-1->0->1 [21] 3/-1/-1->0->1 [22] 3/-1/-1->0->1 [23] 3/-1/-1->0->1
rg32601:230:793 [0] NCCL INFO P2P Chunksize set to 524288
rg32601:231:794 [1] NCCL INFO comm 0x5612ebbdf500 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0
rg32601:231:794 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 3/-1/-1->1->2 [5] 3/-1/-1->1->2 [6] 3/-1/-1->1->2 [7] 3/-1/-1->1->2 [8] 0/-1/-1->1->-1 [9] 0/-1/-1->1->-1 [10] 0/-1/-1->1->-1 [11] 0/-1/-1->1->-1 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 3/-1/-1->1->2 [17] 3/-1/-1->1->2 [18] 3/-1/-1->1->2 [19] 3/-1/-1->1->2 [20] 0/-1/-1->1->-1 [21] 0/-1/-1->1->-1 [22] 0/-1/-1->1->-1 [23] 0/-1/-1->1->-1
rg32601:231:794 [1] NCCL INFO P2P Chunksize set to 524288
rg32601:232:795 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 03/0 : 3[3] -> 0[0] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 06/0 : 3[3] -> 0[0] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 09/0 : 3[3] -> 0[0] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 12/0 : 3[3] -> 0[0] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 15/0 : 3[3] -> 0[0] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 18/0 : 2[2] -> 3[3] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 18/0 : 3[3] -> 0[0] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 20/0 : 2[2] -> 3[3] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 21/0 : 3[3] -> 0[0] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 16/0 : 1[1] -> 2[2] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 18/0 : 1[1] -> 2[2] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 22/0 : 1[1] -> 2[2] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 02/0 : 3[3] -> 1[1] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 04/0 : 3[3] -> 1[1] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 08/0 : 3[3] -> 1[1] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 10/0 : 3[3] -> 1[1] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 02/0 : 0[0] -> 2[2] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 01/0 : 2[2] -> 0[0] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 14/0 : 3[3] -> 1[1] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 03/0 : 0[0] -> 2[2] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 01/0 : 1[1] -> 3[3] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 04/0 : 2[2] -> 0[0] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 16/0 : 3[3] -> 1[1] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 08/0 : 0[0] -> 2[2] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 03/0 : 1[1] -> 3[3] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 07/0 : 2[2] -> 0[0] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 20/0 : 3[3] -> 1[1] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 09/0 : 0[0] -> 2[2] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 07/0 : 1[1] -> 3[3] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 10/0 : 2[2] -> 0[0] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 22/0 : 3[3] -> 1[1] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 14/0 : 0[0] -> 2[2] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 09/0 : 1[1] -> 3[3] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 13/0 : 2[2] -> 0[0] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 15/0 : 0[0] -> 2[2] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 13/0 : 1[1] -> 3[3] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 16/0 : 2[2] -> 0[0] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 20/0 : 0[0] -> 2[2] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 15/0 : 1[1] -> 3[3] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 19/0 : 2[2] -> 0[0] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 21/0 : 0[0] -> 2[2] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 19/0 : 1[1] -> 3[3] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 22/0 : 2[2] -> 0[0] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 21/0 : 1[1] -> 3[3] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 04/0 : 0[0] -> 3[3] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 05/0 : 0[0] -> 3[3] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 10/0 : 0[0] -> 3[3] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 11/0 : 0[0] -> 3[3] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 16/0 : 0[0] -> 3[3] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 17/0 : 2[2] -> 1[1] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 17/0 : 0[0] -> 3[3] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 22/0 : 0[0] -> 3[3] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 17/0 : 3[3] -> 2[2] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 21/0 : 2[2] -> 1[1] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 19/0 : 3[3] -> 2[2] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 23/0 : 0[0] -> 3[3] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 23/0 : 2[2] -> 1[1] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 23/0 : 3[3] -> 2[2] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Connected all rings
rg32601:230:793 [0] NCCL INFO Connected all rings
rg32601:232:795 [2] NCCL INFO Connected all rings
rg32601:233:796 [3] NCCL INFO Connected all rings
rg32601:230:793 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 08/0 : 3[3] -> 0[0] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 10/0 : 3[3] -> 0[0] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 11/0 : 3[3] -> 0[0] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 17/0 : 1[1] -> 2[2] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 19/0 : 1[1] -> 2[2] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 20/0 : 3[3] -> 0[0] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 21/0 : 2[2] -> 3[3] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 22/0 : 3[3] -> 0[0] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 22/0 : 2[2] -> 3[3] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 23/0 : 3[3] -> 0[0] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 23/0 : 2[2] -> 3[3] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 04/0 : 0[0] -> 2[2] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 05/0 : 0[0] -> 2[2] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 04/0 : 1[1] -> 3[3] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 06/0 : 0[0] -> 2[2] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 05/0 : 3[3] -> 1[1] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 06/0 : 3[3] -> 1[1] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 05/0 : 1[1] -> 3[3] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 07/0 : 0[0] -> 2[2] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 07/0 : 3[3] -> 1[1] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 05/0 : 2[2] -> 0[0] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 06/0 : 1[1] -> 3[3] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 16/0 : 1[1] -> 3[3] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 17/0 : 1[1] -> 3[3] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 18/0 : 1[1] -> 3[3] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 06/0 : 2[2] -> 0[0] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 17/0 : 2[2] -> 0[0] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 18/0 : 2[2] -> 0[0] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 17/0 : 3[3] -> 1[1] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 18/0 : 3[3] -> 1[1] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 19/0 : 3[3] -> 1[1] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 16/0 : 0[0] -> 2[2] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 17/0 : 0[0] -> 2[2] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 18/0 : 0[0] -> 2[2] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 19/0 : 0[0] -> 2[2] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 20/0 : 3[3] -> 2[2] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 21/0 : 3[3] -> 2[2] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/CUMEM
rg32601:233:796 [3] NCCL INFO Channel 22/0 : 3[3] -> 2[2] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 08/0 : 0[0] -> 3[3] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 09/0 : 0[0] -> 3[3] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 20/0 : 0[0] -> 3[3] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM
rg32601:230:793 [0] NCCL INFO Channel 21/0 : 0[0] -> 3[3] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 16/0 : 2[2] -> 1[1] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 18/0 : 2[2] -> 1[1] via P2P/CUMEM
rg32601:232:795 [2] NCCL INFO Channel 19/0 : 2[2] -> 1[1] via P2P/CUMEM
rg32601:231:794 [1] NCCL INFO Connected all trees
rg32601:233:796 [3] NCCL INFO Connected all trees
rg32601:230:793 [0] NCCL INFO Connected all trees
rg32601:232:795 [2] NCCL INFO Connected all trees
rg32601:231:794 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
rg32601:233:796 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
rg32601:232:795 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
rg32601:231:794 [1] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 16 p2p channels per peer
rg32601:233:796 [3] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 16 p2p channels per peer
rg32601:232:795 [2] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 16 p2p channels per peer
rg32601:230:793 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
rg32601:230:793 [0] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 16 p2p channels per peer
rg32601:231:794 [1] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
rg32601:232:795 [2] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
rg32601:231:794 [1] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
rg32601:230:793 [0] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
rg32601:232:795 [2] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
rg32601:231:794 [1] NCCL INFO ncclCommInitRank comm 0x5612ebbdf500 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 5f000 commId 0x5b6041024b89da51 - Init COMPLETE
rg32601:230:793 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
rg32601:232:795 [2] NCCL INFO ncclCommInitRank comm 0x55b28c1453b0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId cb000 commId 0x5b6041024b89da51 - Init COMPLETE
rg32601:230:793 [0] NCCL INFO ncclCommInitRank comm 0x558232cccd80 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 4e000 commId 0x5b6041024b89da51 - Init COMPLETE
rg32601:233:796 [3] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
rg32601:233:796 [3] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
rg32601:233:796 [3] NCCL INFO ncclCommInitRank comm 0x556992648a70 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId db000 commId 0x5b6041024b89da51 - Init COMPLETE
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
`trust_remote_code` is not supported anymore.
Please check that the Hugging Face dataset 'parquet' isn't based on a loading script and remove `trust_remote_code`.
If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.
Running tokenizer on dataset (num_proc=16):   0%|          | 0/33047 [00:00<?, ? examples/s]